diff --git a/Documentation/devicetree/bindings/dma/xilinx/axi-dma.txt b/Documentation/devicetree/bindings/dma/xilinx/axi-dma.txt
new file mode 100644
index 000000000..f4f5b018d
--- /dev/null
+++ b/Documentation/devicetree/bindings/dma/xilinx/axi-dma.txt
@@ -0,0 +1,38 @@
+* Xilinx AXI DMA Test client
+
+Required properties:
+- compatible: Should be "xlnx,axi-dma-test-1.00.a"
+- dmas: a list of <[DMA device phandle] [Channel ID]> pairs,
+	where Channel ID is '0' for write/tx and '1' for read/rx
+	channel.
+- dma-names: a list of DMA channel names, one per "dmas" entry
+
+Example:
+++++++++
+
+dmatest_0: dmatest@0 {
+	compatible ="xlnx,axi-dma-test-1.00.a";
+	dmas = <&axi_dma_0 0
+		&axi_dma_0 1>;
+	dma-names = "axidma0", "axidma1";
+} ;
+
+
+Xilinx AXI DMA Device Node Example
+++++++++++++++++++++++++++++++++++++
+
+axi_dma_0: axidma@40400000 {
+	compatible = "xlnx,axi-dma-1.00.a";
+	#dma-cells = <1>;
+	reg = < 0x40400000 0x10000 >;
+	dma-channel@40400000 {
+		compatible = "xlnx,axi-dma-mm2s-channel";
+		interrupts = < 0 59 4 >;
+		xlnx,datawidth = <0x40>;
+	} ;
+	dma-channel@40400030 {
+		compatible = "xlnx,axi-dma-s2mm-channel";
+		interrupts = < 0 58 4 >;
+		xlnx,datawidth = <0x40>;
+	} ;
+} ;
diff --git a/Documentation/devicetree/bindings/dma/xilinx/ps-pcie-dma.txt b/Documentation/devicetree/bindings/dma/xilinx/ps-pcie-dma.txt
new file mode 100644
index 000000000..acdcc445f
--- /dev/null
+++ b/Documentation/devicetree/bindings/dma/xilinx/ps-pcie-dma.txt
@@ -0,0 +1,67 @@
+* Xilinx PS PCIe Root DMA
+
+Required properties:
+- compatible: Should be "xlnx,ps_pcie_dma-1.00.a"
+- reg: Register offset for Root DMA channels
+- reg-names: Name for the register. Should be "ps_pcie_regbase"
+- interrupts: Interrupt pin for Root DMA
+- interrupt-names: Name for the interrupt. Should be "ps_pcie_rootdma_intr"
+- interrupt-parent: Should be gic in case of zynqmp
+- rootdma: Indicates this platform device is root dma.
+	This is required as the same platform driver will be invoked by pcie end points too
+- dma_vendorid: 16 bit PCIe device vendor id.
+	This can be later used by dma client for matching while using dma_request_channel
+- dma_deviceid: 16 bit PCIe device id
+	This can be later used by dma client for matching while using dma_request_channel
+- numchannels: Indicates number of channels to be enabled for the device.
+	Valid values are from 1 to 4 for zynqmp
+- ps_pcie_channel : One for each channel to be enabled.
+	This array contains channel specific properties.
+	Index 0: Direction of channel
+		Direction of channel can be either PCIe Memory to AXI memory i.e., Host to Card or
+		AXI Memory to PCIe memory i.e., Card to Host
+		PCIe to AXI Channel Direction is represented as 0x1
+		AXI to PCIe Channel Direction is represented as 0x0
+	Index 1: Number of Buffer Descriptors
+		This number describes number of buffer descriptors to be allocated for a channel
+	Index 2: Number of Queues
+		Each Channel has four DMA Buffer Descriptor Queues.
+		By default All four Queues will be managed by Root DMA driver.
+		User may choose to have only two queues either Source and it's Status Queue or
+			Destination and it's Status Queue to be handled by Driver.
+		The other two queues need to be handled by user logic which will not be part of this driver.
+		All Queues on Host is represented by 0x4
+		Two Queues on Host is represented by 0x2
+	Index 3: Coalesce Count
+		This number indicates the number of transfers after which interrupt needs to
+		be raised for the particular channel. The allowed range is from 0 to 255
+	Index 4: Coalesce Count Timer frequency
+		This property is used to control the frequency of poll timer. Poll timer is
+		created for a channel whenever coalesce count value (>= 1) is programmed for the particular
+		channel. This timer is helpful in draining out completed transactions even though interrupt is
+		not generated.
+
+Client Usage:
+	DMA clients can request for these channels using dma_request_channel API
+
+
+Xilinx PS PCIe Root DMA node Example
+++++++++++++++++++++++++++++++++++++
+
+	pci_rootdma: rootdma@fd0f0000 {
+		compatible = "xlnx,ps_pcie_dma-1.00.a";
+		reg = <0x0 0xfd0f0000 0x0 0x1000>;
+		reg-names = "ps_pcie_regbase";
+		interrupts = <0 117 4>;
+		interrupt-names = "ps_pcie_rootdma_intr";
+		interrupt-parent = <&gic>;
+		rootdma;
+		dma_vendorid = /bits/ 16 <0x10EE>;
+		dma_deviceid = /bits/ 16 <0xD021>;
+		numchannels = <0x4>;
+		#size-cells = <0x5>;
+		ps_pcie_channel0 = <0x1 0x7CF 0x4 0x0 0x3E8>;
+		ps_pcie_channel1 = <0x0 0x7CF 0x4 0x0 0x3E8>;
+		ps_pcie_channel2 = <0x1 0x7CF 0x4 0x0 0x3E8>;
+		ps_pcie_channel3 = <0x0 0x7CF 0x4 0x0 0x3E8>;
+    };
diff --git a/Documentation/devicetree/bindings/dma/xilinx/vdmatest.txt b/Documentation/devicetree/bindings/dma/xilinx/vdmatest.txt
new file mode 100644
index 000000000..5821fdc3e
--- /dev/null
+++ b/Documentation/devicetree/bindings/dma/xilinx/vdmatest.txt
@@ -0,0 +1,39 @@
+* Xilinx Video DMA Test client
+
+Required properties:
+- compatible: Should be "xlnx,axi-vdma-test-1.00.a"
+- dmas: a list of <[Video DMA device phandle] [Channel ID]> pairs,
+	where Channel ID is '0' for write/tx and '1' for read/rx
+	channel.
+- dma-names: a list of DMA channel names, one per "dmas" entry
+- xlnx,num-fstores: Should be the number of framebuffers as configured in
+	VDMA device node.
+
+Example:
+++++++++
+
+vdmatest_0: vdmatest@0 {
+	compatible ="xlnx,axi-vdma-test-1.00.a";
+	dmas = <&axi_vdma_0 0
+		&axi_vdma_0 1>;
+	dma-names = "vdma0", "vdma1";
+	xlnx,num-fstores = <0x8>;
+} ;
+
+
+Xilinx Video DMA Device Node Example
+++++++++++++++++++++++++++++++++++++
+axi_vdma_0: axivdma@44A40000 {
+	compatible = "xlnx,axi-vdma-1.00.a";
+	...
+	dma-channel@44A40000 {
+		...
+		xlnx,num-fstores = <0x8>;
+		...
+	} ;
+	dma-channel@44A40030 {
+		...
+		xlnx,num-fstores = <0x8>;
+		...
+	} ;
+} ;
diff --git a/Documentation/devicetree/bindings/dma/xilinx/xilinx_dma.txt b/Documentation/devicetree/bindings/dma/xilinx/xilinx_dma.txt
index 325aca52c..d1700a5c3 100644
--- a/Documentation/devicetree/bindings/dma/xilinx/xilinx_dma.txt
+++ b/Documentation/devicetree/bindings/dma/xilinx/xilinx_dma.txt
@@ -110,7 +110,11 @@ axi_vdma_0: axivdma@40030000 {
 Required properties:
 - dmas: a list of <[Video DMA device phandle] [Channel ID]> pairs,
 	where Channel ID is '0' for write/tx and '1' for read/rx
-	channel.
+	channel. For MCMDA, MM2S channel(write/tx) ID start from
+	'0' and is in [0-15] range. S2MM channel(read/rx) ID start
+	from '16' and is in [16-31] range. These channels ID are
+	fixed irrespective of IP configuration.
+
 - dma-names: a list of DMA channel names, one per "dmas" entry
 
 Example:
diff --git a/Documentation/devicetree/bindings/dma/xilinx/xilinx_frmbuf.txt b/Documentation/devicetree/bindings/dma/xilinx/xilinx_frmbuf.txt
new file mode 100644
index 000000000..cd1f25fab
--- /dev/null
+++ b/Documentation/devicetree/bindings/dma/xilinx/xilinx_frmbuf.txt
@@ -0,0 +1,126 @@
+The Xilinx framebuffer DMA engine supports two soft IP blocks: one IP
+block is used for reading video frame data from memory (FB Read) to the device
+and the other IP block is used for writing video frame data from the device
+to memory (FB Write).  Both the FB Read/Write IP blocks are aware of the
+format of the data being written to or read from memory including RGB and
+YUV in packed, planar, and semi-planar formats.  Because the FB Read/Write
+is format aware, only one buffer pointer is needed by the IP blocks even
+when planar or semi-planar format are used.
+
+FB Read Required propertie(s):
+- compatible		: Should be "xlnx,axi-frmbuf-rd-v2.1" or
+			  "xlnx,axi-frmbuf-rd-v2.2". Older string
+			  "xlnx,axi-frmbuf-rd-v2" is now deprecated.
+
+Note: Compatible string "xlnx,axi-frmbuf-rd" and the hardware it
+represented is no longer supported.
+
+FB Write Required propertie(s):
+- compatible		: Should be "xlnx,axi-frmbuf-wr-v2.1" or
+			  "xlnx,axi-frmbuf-wr-v2.2". Older string
+			  "xlnx,axi-frmbuf-wr-v2" is now deprecated.
+
+Note: Compatible string "xlnx,axi-frmbuf-wr" and the hardware it
+represented is no longer supported.
+
+Required Properties Common to both FB Read and FB Write:
+- #dma-cells		: should be 1
+- interrupt-parent	: Interrupt controller the interrupt is routed through
+- interrupts		: Should contain DMA channel interrupt
+- reset-gpios		: Should contain GPIO reset phandle
+- reg			: Memory map for module access
+- xlnx,dma-addr-width	: Size of dma address pointer in IP (either 32 or 64)
+- xlnx,vid-formats	: A list of strings indicating what video memory
+			  formats the IP has been configured to support.
+			  See VIDEO FORMATS table below and examples.
+
+Required Properties Common to both FB Read and FB Write for v2.1:
+- xlnx,pixels-per-clock	: Pixels per clock set in IP (1, 2, 4 or 8)
+- clocks: Reference to the AXI Streaming clock feeding the AP_CLK
+- clock-names: Must have "ap_clk"
+- xlnx,max-height	: Maximum number of lines.
+			  Valid range from 64 to 8640.
+- xlnx,max-width	: Maximum number of pixels in a line.
+			  Valid range from 64 to 15360.
+
+Optional Properties Common to both FB Read and FB Write for v2.1:
+- xlnx,dma-align	: DMA alignment required in bytes.
+			  If absent then dma alignment is calculated as
+			  pixels per clock * 8.
+			  If present it should be power of 2 and at least
+			  pixels per clock * 8.
+			  Minimum is 8, 16, 32 when pixels-per-clock is
+			  1, 2 or 4.
+- xlnx,fid		: Field ID enabled for interlaced video support.
+			  Can be absent for progressive video.
+
+VIDEO FORMATS
+The following table describes the legal string values to be used for
+the xlnx,vid-formats property.  To the left is the string value and the
+two columns to the right describe how this is mapped to an equivalent V4L2
+and DRM fourcc code---respectively---by the driver.
+
+IP FORMAT	DTS String	V4L2 Fourcc		DRM Fourcc
+-------------|----------------|----------------------|---------------------
+RGB8		bgr888		V4L2_PIX_FMT_RGB24	DRM_FORMAT_BGR888
+BGR8		rgb888		V4L2_PIX_FMT_BGR24	DRM_FORMAT_RGB888
+RGBX8		xbgr8888	V4L2_PIX_FMT_BGRX32	DRM_FORMAT_XBGR8888
+RGBA8		abgr8888	<not supported>		DRM_FORMAT_ABGR8888
+BGRA8		argb8888	<not supported>		DRM_FORMAT_ARGB8888
+BGRX8		xrgb8888	V4L2_PIX_FMT_XBGR32	DRM_FORMAT_XRGB8888
+RGBX10		xbgr2101010	V4L2_PIX_FMT_XBGR30	DRM_FORMAT_XBGR2101010
+RGBX12		xbgr2121212	V4L2_PIX_FMT_XBGR40	<not supported>
+RGBX16		rgb16		V4L2_PIX_FMT_BGR40	<not supported>
+YUV8		vuy888		V4L2_PIX_FMT_VUY24	DRM_FORMAT_VUY888
+YUVX8		xvuy8888	V4L2_PIX_FMT_XVUY32	DRM_FORMAT_XVUY8888
+Y_U_V8		y_u_v8		V4L2_PIX_FMT_YUV444M	DRM_FORMAT_YUV444
+YUYV8		yuyv		V4L2_PIX_FMT_YUYV	DRM_FORMAT_YUYV
+UYVY8		uyvy		V4L2_PIX_FMT_UYVY	DRM_FORMAT_UYVY
+YUVA8		avuy8888	<not supported>		DRM_FORMAT_AVUY
+YUVX10		yuvx2101010	V4L2_PIX_FMT_XVUY10	DRM_FORMAT_XVUY2101010
+Y8		y8		V4L2_PIX_FMT_GREY	DRM_FORMAT_Y8
+Y10		y10		V4L2_PIX_FMT_XY10	DRM_FORMAT_Y10
+Y_UV8		nv16		V4L2_PIX_FMT_NV16	DRM_FORMAT_NV16
+Y_UV8		nv16		V4L2_PIX_FMT_NV16M	DRM_FORMAT_NV16
+Y_UV8_420	nv12		V4L2_PIX_FMT_NV12	DRM_FORMAT_NV12
+Y_UV8_420	nv12		V4L2_PIX_FMT_NV12M	DRM_FORMAT_NV12
+Y_UV10		xv20		V4L2_PIX_FMT_XV20M	DRM_FORMAT_XV20
+Y_UV10		xv20		V4L2_PIX_FMT_XV20	<not supported>
+Y_UV10_420	xv15		V4L2_PIX_FMT_XV15M	DRM_FORMAT_XV15
+Y_UV10_420	xv15		V4L2_PIX_FMT_XV20	<not supported>
+
+Examples:
+
+FB Read Example:
+++++++++
+v_frmbuf_rd_0: v_frmbuf_rd@80000000 {
+	#dma-cells = <1>;
+	compatible = "xlnx,axi-frmbuf-rd-v2.1";
+	interrupt-parent = <&gic>;
+	interrupts = <0 92 4>;
+	reset-gpios = <&gpio 80 1>;
+	reg = <0x0 0x80000000 0x0 0x10000>;
+	xlnx,dma-addr-width = <32>;
+	xlnx,vid-formats = "bgr888","xbgr8888";
+	xlnx,pixels-per-clock = <1>;
+	xlnx,dma-align = <8>;
+	clocks = <&vid_stream_clk>;
+	clock-names = "ap_clk"
+};
+
+FB Write Example:
+++++++++
+v_frmbuf_wr_0: v_frmbuf_wr@80000000 {
+	#dma-cells = <1>;
+	compatible = "xlnx,axi-frmbuf-wr-v2.1";
+	interrupt-parent = <&gic>;
+	interrupts = <0 92 4>;
+	reset-gpios = <&gpio 80 1>;
+	reg = <0x0 0x80000000 0x0 0x10000>;
+	xlnx,dma-addr-width = <64>;
+	xlnx,vid-formats = "bgr888","yuyv","nv16","nv12";
+	xlnx,pixels-per-clock = <2>;
+	xlnx,dma-align = <16>;
+	clocks = <&vid_stream_clk>;
+	clock-names = "ap_clk"
+};
diff --git a/Documentation/devicetree/bindings/xlnx,ctrl-fb.txt b/Documentation/devicetree/bindings/xlnx,ctrl-fb.txt
new file mode 100644
index 000000000..8abc053df
--- /dev/null
+++ b/Documentation/devicetree/bindings/xlnx,ctrl-fb.txt
@@ -0,0 +1,22 @@
+The Xilinx framebuffer DMA engine supports two soft IP blocks: one IP
+block is used for reading video frame data from memory (FB Read) to the device
+and the other IP block is used for writing video frame data from the device
+to memory (FB Write).  Both the FB Read/Write IP blocks are aware of the
+format of the data being written to or read from memory including RGB and
+YUV in packed, planar, and semi-planar formats.  Because the FB Read/Write
+is format aware, only one buffer pointer is needed by the IP blocks even
+when planar or semi-planar format are used.
+
+Required properties:
+ - compatible: Should be "xlnx,ctrl-fbwr-1.0" for framebuffer Write OR
+	       "xlnx,ctrl-fbrd-1.0" for framebuffer Read.
+ - reg: Base address and size of the IP core.
+ - reset-gpios: gpio to reset the framebuffer IP
+
+Example:
+
+        fbwr@0xa0000000 {
+                compatible = "xlnx,ctrl-fbwr-1.0";
+                reg = <0x0 0xa0000000 0x0 0x10000>;
+                reset-gpios = <&gpio 82 1>;
+        };
diff --git a/drivers/dma/Kconfig b/drivers/dma/Kconfig
index 08013345d..2453cec1e 100644
--- a/drivers/dma/Kconfig
+++ b/drivers/dma/Kconfig
@@ -702,6 +702,35 @@ config XILINX_ZYNQMP_DPDMA
 	  driver provides the dmaengine required by the DisplayPort subsystem
 	  display driver.
 
+config XILINX_FRMBUF
+	tristate "Xilinx Framebuffer"
+	select DMA_ENGINE
+	help
+	 Enable support for Xilinx Framebuffer DMA.
+
+config XILINX_PS_PCIE_DMA
+	tristate "Xilinx PS PCIe DMA support"
+	depends on (PCI && X86_64 || ARM64)
+	select DMA_ENGINE
+	help
+	  Enable support for the Xilinx PS PCIe DMA engine present
+	  in recent Xilinx ZynqMP chipsets.
+
+	  Say Y here if you have such a chipset.
+
+	  If unsure, say N.
+
+config XILINX_PS_PCIE_DMA_TEST
+	tristate "Xilinx PS PCIe DMA test client"
+	depends on XILINX_PS_PCIE_DMA
+	help
+	  Enable support for the test client of Xilinx PS PCIe DMA engine
+	  in recent Xilinx ZynqMP chipsets.
+
+	  Say Y here if you have such a chipset.
+
+	  If unsure, say N.
+
 config ZX_DMA
 	tristate "ZTE ZX DMA support"
 	depends on ARCH_ZX || COMPILE_TEST
@@ -758,4 +787,18 @@ config DMATEST
 config DMA_ENGINE_RAID
 	bool
 
+config XILINX_DMATEST
+	tristate "DMA Test client for AXI DMA and MCDMA"
+	depends on XILINX_DMA
+	help
+	  Simple DMA test client. Say N unless you're debugging a
+	  DMA Device driver.
+
+config XILINX_VDMATEST
+	tristate "DMA Test client for VDMA"
+	depends on XILINX_DMA
+	help
+	  Simple xilinx VDMA test client. Say N unless you're debugging a
+	  DMA Device driver.
+
 endif
diff --git a/drivers/dma/dmaengine.c b/drivers/dma/dmaengine.c
index af3ee288b..fe6a460c4 100644
--- a/drivers/dma/dmaengine.c
+++ b/drivers/dma/dmaengine.c
@@ -1086,7 +1086,6 @@ static int __dma_async_device_channel_register(struct dma_device *device,
 	kfree(chan->dev);
  err_free_local:
 	free_percpu(chan->local);
-	chan->local = NULL;
 	return rc;
 }
 
diff --git a/drivers/dma/pl330.c b/drivers/dma/pl330.c
index dfbf51418..08a0dc94d 100644
--- a/drivers/dma/pl330.c
+++ b/drivers/dma/pl330.c
@@ -1527,8 +1527,6 @@ static int pl330_submit_req(struct pl330_thread *thrd,
 
 	/* First dry run to check if req is acceptable */
 	ret = _setup_req(pl330, 1, thrd, idx, &xs);
-	if (ret < 0)
-		goto xfer_exit;
 
 	if (ret > pl330->mcbufsz / 2) {
 		dev_info(pl330->ddma.dev, "%s:%d Try increasing mcbufsz (%i/%i)\n",
@@ -2754,7 +2752,7 @@ static struct dma_async_tx_descriptor *pl330_prep_dma_cyclic(
 		return NULL;
 
 	pch->cyclic = true;
-	desc->txd.flags = flags;
+	desc->txd.flags = (enum dma_ctrl_flags) flags;
 
 	return &desc->txd;
 }
@@ -2806,7 +2804,7 @@ pl330_prep_dma_memcpy(struct dma_chan *chan, dma_addr_t dst,
 
 	desc->bytes_requested = len;
 
-	desc->txd.flags = flags;
+	desc->txd.flags = (enum dma_ctrl_flags) flags;
 
 	return &desc->txd;
 }
@@ -2891,7 +2889,7 @@ pl330_prep_slave_sg(struct dma_chan *chan, struct scatterlist *sgl,
 	}
 
 	/* Return the last desc in the chain */
-	desc->txd.flags = flg;
+	desc->txd.flags = (enum dma_ctrl_flags) flg;
 	return &desc->txd;
 }
 
diff --git a/drivers/dma/xilinx/Makefile b/drivers/dma/xilinx/Makefile
index 767bb45f6..47810fafb 100644
--- a/drivers/dma/xilinx/Makefile
+++ b/drivers/dma/xilinx/Makefile
@@ -1,4 +1,10 @@
 # SPDX-License-Identifier: GPL-2.0-only
+obj-$(CONFIG_XILINX_DMATEST) += axidmatest.o
+obj-$(CONFIG_XILINX_VDMATEST) += vdmatest.o
 obj-$(CONFIG_XILINX_DMA) += xilinx_dma.o
 obj-$(CONFIG_XILINX_ZYNQMP_DMA) += zynqmp_dma.o
 obj-$(CONFIG_XILINX_ZYNQMP_DPDMA) += xilinx_dpdma.o
+xilinx_ps_pcie_dma-objs := xilinx_ps_pcie_main.o xilinx_ps_pcie_platform.o
+obj-$(CONFIG_XILINX_PS_PCIE_DMA) += xilinx_ps_pcie_dma.o
+obj-$(CONFIG_XILINX_PS_PCIE_DMA_TEST) += xilinx_ps_pcie_dma_client.o
+obj-$(CONFIG_XILINX_FRMBUF) += xilinx_frmbuf.o
diff --git a/drivers/dma/xilinx/axidmatest.c b/drivers/dma/xilinx/axidmatest.c
new file mode 100644
index 000000000..4b30e6433
--- /dev/null
+++ b/drivers/dma/xilinx/axidmatest.c
@@ -0,0 +1,698 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * XILINX AXI DMA and MCDMA Engine test module
+ *
+ * Copyright (C) 2010 Xilinx, Inc. All rights reserved.
+ *
+ * Based on Atmel DMA Test Client
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/delay.h>
+#include <linux/dmaengine.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/of_dma.h>
+#include <linux/platform_device.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/sched/task.h>
+#include <linux/dma/xilinx_dma.h>
+
+static unsigned int test_buf_size = 16384;
+module_param(test_buf_size, uint, 0444);
+MODULE_PARM_DESC(test_buf_size, "Size of the memcpy test buffer");
+
+static unsigned int iterations = 5;
+module_param(iterations, uint, 0444);
+MODULE_PARM_DESC(iterations,
+		 "Iterations before stopping test (default: infinite)");
+
+/*
+ * Initialization patterns. All bytes in the source buffer has bit 7
+ * set, all bytes in the destination buffer has bit 7 cleared.
+ *
+ * Bit 6 is set for all bytes which are to be copied by the DMA
+ * engine. Bit 5 is set for all bytes which are to be overwritten by
+ * the DMA engine.
+ *
+ * The remaining bits are the inverse of a counter which increments by
+ * one for each byte address.
+ */
+#define PATTERN_SRC		0x80
+#define PATTERN_DST		0x00
+#define PATTERN_COPY		0x40
+#define PATTERN_OVERWRITE	0x20
+#define PATTERN_COUNT_MASK	0x1f
+
+#define XILINX_DMATEST_BD_CNT	11
+
+struct dmatest_slave_thread {
+	struct list_head node;
+	struct task_struct *task;
+	struct dma_chan *tx_chan;
+	struct dma_chan *rx_chan;
+	u8 **srcs;
+	u8 **dsts;
+	enum dma_transaction_type type;
+	bool done;
+};
+
+struct dmatest_chan {
+	struct list_head node;
+	struct dma_chan *chan;
+	struct list_head threads;
+};
+
+/*
+ * These are protected by dma_list_mutex since they're only used by
+ * the DMA filter function callback
+ */
+static DECLARE_WAIT_QUEUE_HEAD(thread_wait);
+static LIST_HEAD(dmatest_channels);
+static unsigned int nr_channels;
+
+static unsigned long long dmatest_persec(s64 runtime, unsigned int val)
+{
+	unsigned long long per_sec = 1000000;
+
+	if (runtime <= 0)
+		return 0;
+
+	/* drop precision until runtime is 32-bits */
+	while (runtime > UINT_MAX) {
+		runtime >>= 1;
+		per_sec <<= 1;
+	}
+
+	per_sec *= val;
+	do_div(per_sec, runtime);
+	return per_sec;
+}
+
+static unsigned long long dmatest_KBs(s64 runtime, unsigned long long len)
+{
+	return dmatest_persec(runtime, len >> 10);
+}
+
+static bool is_threaded_test_run(struct dmatest_chan *tx_dtc,
+				 struct dmatest_chan *rx_dtc)
+{
+	struct dmatest_slave_thread *thread;
+	int ret = false;
+
+	list_for_each_entry(thread, &tx_dtc->threads, node) {
+		if (!thread->done)
+			ret = true;
+	}
+
+	list_for_each_entry(thread, &rx_dtc->threads, node) {
+		if (!thread->done)
+			ret = true;
+	}
+	return ret;
+}
+
+static unsigned long dmatest_random(void)
+{
+	unsigned long buf;
+
+	get_random_bytes(&buf, sizeof(buf));
+	return buf;
+}
+
+static void dmatest_init_srcs(u8 **bufs, unsigned int start, unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+		for ( ; i < start + len; i++)
+			buf[i] = PATTERN_SRC | PATTERN_COPY
+				| (~i & PATTERN_COUNT_MASK);
+		for ( ; i < test_buf_size; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+	}
+}
+
+static void dmatest_init_dsts(u8 **bufs, unsigned int start, unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+		for ( ; i < start + len; i++)
+			buf[i] = PATTERN_DST | PATTERN_OVERWRITE
+				| (~i & PATTERN_COUNT_MASK);
+		for ( ; i < test_buf_size; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+	}
+}
+
+static void dmatest_mismatch(u8 actual, u8 pattern, unsigned int index,
+			     unsigned int counter, bool is_srcbuf)
+{
+	u8 diff = actual ^ pattern;
+	u8 expected = pattern | (~counter & PATTERN_COUNT_MASK);
+	const char *thread_name = current->comm;
+
+	if (is_srcbuf)
+		pr_warn("%s: srcbuf[0x%x] overwritten! Expected %02x, got %02x\n",
+			thread_name, index, expected, actual);
+	else if ((pattern & PATTERN_COPY) &&
+		 (diff & (PATTERN_COPY | PATTERN_OVERWRITE)))
+		pr_warn("%s: dstbuf[0x%x] not copied! Expected %02x, got %02x\n",
+			thread_name, index, expected, actual);
+	else if (diff & PATTERN_SRC)
+		pr_warn("%s: dstbuf[0x%x] was copied! Expected %02x, got %02x\n",
+			thread_name, index, expected, actual);
+	else
+		pr_warn("%s: dstbuf[0x%x] mismatch! Expected %02x, got %02x\n",
+			thread_name, index, expected, actual);
+}
+
+static unsigned int dmatest_verify(u8 **bufs, unsigned int start,
+				   unsigned int end, unsigned int counter,
+				   u8 pattern, bool is_srcbuf)
+{
+	unsigned int i;
+	unsigned int error_count = 0;
+	u8 actual;
+	u8 expected;
+	u8 *buf;
+	unsigned int counter_orig = counter;
+
+	for (; (buf = *bufs); bufs++) {
+		counter = counter_orig;
+		for (i = start; i < end; i++) {
+			actual = buf[i];
+			expected = pattern | (~counter & PATTERN_COUNT_MASK);
+			if (actual != expected) {
+				if (error_count < 32)
+					dmatest_mismatch(actual, pattern, i,
+							 counter, is_srcbuf);
+				error_count++;
+			}
+			counter++;
+		}
+	}
+
+	if (error_count > 32)
+		pr_warn("%s: %u errors suppressed\n",
+			current->comm, error_count - 32);
+
+	return error_count;
+}
+
+static void dmatest_slave_tx_callback(void *completion)
+{
+	complete(completion);
+}
+
+static void dmatest_slave_rx_callback(void *completion)
+{
+	complete(completion);
+}
+
+/* Function for slave transfers
+ * Each thread requires 2 channels, one for transmit, and one for receive
+ */
+static int dmatest_slave_func(void *data)
+{
+	struct dmatest_slave_thread	*thread = data;
+	struct dma_chan *tx_chan;
+	struct dma_chan *rx_chan;
+	const char *thread_name;
+	unsigned int src_off, dst_off, len;
+	unsigned int error_count;
+	unsigned int failed_tests = 0;
+	unsigned int total_tests = 0;
+	dma_cookie_t tx_cookie;
+	dma_cookie_t rx_cookie;
+	enum dma_status status;
+	enum dma_ctrl_flags flags;
+	int ret;
+	int src_cnt;
+	int dst_cnt;
+	int bd_cnt = XILINX_DMATEST_BD_CNT;
+	int i;
+
+	ktime_t	ktime, start, diff;
+	ktime_t	filltime = 0;
+	ktime_t	comparetime = 0;
+	s64 runtime = 0;
+	unsigned long long total_len = 0;
+	thread_name = current->comm;
+	ret = -ENOMEM;
+
+
+	/* Ensure that all previous reads are complete */
+	smp_rmb();
+	tx_chan = thread->tx_chan;
+	rx_chan = thread->rx_chan;
+	dst_cnt = bd_cnt;
+	src_cnt = bd_cnt;
+
+	thread->srcs = kcalloc(src_cnt + 1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->srcs)
+		goto err_srcs;
+	for (i = 0; i < src_cnt; i++) {
+		thread->srcs[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->srcs[i])
+			goto err_srcbuf;
+	}
+	thread->srcs[i] = NULL;
+
+	thread->dsts = kcalloc(dst_cnt + 1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->dsts)
+		goto err_dsts;
+	for (i = 0; i < dst_cnt; i++) {
+		thread->dsts[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->dsts[i])
+			goto err_dstbuf;
+	}
+	thread->dsts[i] = NULL;
+
+	set_user_nice(current, 10);
+
+	flags = DMA_CTRL_ACK | DMA_PREP_INTERRUPT;
+
+	ktime = ktime_get();
+	while (!kthread_should_stop() &&
+	       !(iterations && total_tests >= iterations)) {
+		struct dma_device *tx_dev = tx_chan->device;
+		struct dma_device *rx_dev = rx_chan->device;
+		struct dma_async_tx_descriptor *txd = NULL;
+		struct dma_async_tx_descriptor *rxd = NULL;
+		dma_addr_t dma_srcs[XILINX_DMATEST_BD_CNT];
+		dma_addr_t dma_dsts[XILINX_DMATEST_BD_CNT];
+		struct completion rx_cmp;
+		struct completion tx_cmp;
+		unsigned long rx_tmo =
+				msecs_to_jiffies(300000); /* RX takes longer */
+		unsigned long tx_tmo = msecs_to_jiffies(30000);
+		u8 align = 0;
+		struct scatterlist tx_sg[XILINX_DMATEST_BD_CNT];
+		struct scatterlist rx_sg[XILINX_DMATEST_BD_CNT];
+
+		total_tests++;
+
+		/* honor larger alignment restrictions */
+		align = tx_dev->copy_align;
+		if (rx_dev->copy_align > align)
+			align = rx_dev->copy_align;
+
+		if (1 << align > test_buf_size) {
+			pr_err("%u-byte buffer too small for %d-byte alignment\n",
+			       test_buf_size, 1 << align);
+			break;
+		}
+
+		len = dmatest_random() % test_buf_size + 1;
+		len = (len >> align) << align;
+		if (!len)
+			len = 1 << align;
+		src_off = dmatest_random() % (test_buf_size - len + 1);
+		dst_off = dmatest_random() % (test_buf_size - len + 1);
+
+		src_off = (src_off >> align) << align;
+		dst_off = (dst_off >> align) << align;
+
+		start = ktime_get();
+		dmatest_init_srcs(thread->srcs, src_off, len);
+		dmatest_init_dsts(thread->dsts, dst_off, len);
+		diff = ktime_sub(ktime_get(), start);
+		filltime = ktime_add(filltime, diff);
+
+		for (i = 0; i < src_cnt; i++) {
+			u8 *buf = thread->srcs[i] + src_off;
+
+			dma_srcs[i] = dma_map_single(tx_dev->dev, buf, len,
+						     DMA_MEM_TO_DEV);
+		}
+
+		for (i = 0; i < dst_cnt; i++) {
+			dma_dsts[i] = dma_map_single(rx_dev->dev,
+						     thread->dsts[i],
+						     test_buf_size,
+						     DMA_BIDIRECTIONAL);
+		}
+
+		sg_init_table(tx_sg, bd_cnt);
+		sg_init_table(rx_sg, bd_cnt);
+
+		for (i = 0; i < bd_cnt; i++) {
+			sg_dma_address(&tx_sg[i]) = dma_srcs[i];
+			sg_dma_address(&rx_sg[i]) = dma_dsts[i] + dst_off;
+
+			sg_dma_len(&tx_sg[i]) = len;
+			sg_dma_len(&rx_sg[i]) = len;
+			total_len += len;
+		}
+
+		rxd = rx_dev->device_prep_slave_sg(rx_chan, rx_sg, bd_cnt,
+				DMA_DEV_TO_MEM, flags, NULL);
+
+		txd = tx_dev->device_prep_slave_sg(tx_chan, tx_sg, bd_cnt,
+				DMA_MEM_TO_DEV, flags, NULL);
+
+		if (!rxd || !txd) {
+			for (i = 0; i < src_cnt; i++)
+				dma_unmap_single(tx_dev->dev, dma_srcs[i], len,
+						 DMA_MEM_TO_DEV);
+			for (i = 0; i < dst_cnt; i++)
+				dma_unmap_single(rx_dev->dev, dma_dsts[i],
+						 test_buf_size,
+						 DMA_BIDIRECTIONAL);
+			pr_warn("%s: #%u: prep error with src_off=0x%x ",
+				thread_name, total_tests - 1, src_off);
+			pr_warn("dst_off=0x%x len=0x%x\n",
+				dst_off, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+
+		init_completion(&rx_cmp);
+		rxd->callback = dmatest_slave_rx_callback;
+		rxd->callback_param = &rx_cmp;
+		rx_cookie = rxd->tx_submit(rxd);
+
+		init_completion(&tx_cmp);
+		txd->callback = dmatest_slave_tx_callback;
+		txd->callback_param = &tx_cmp;
+		tx_cookie = txd->tx_submit(txd);
+
+		if (dma_submit_error(rx_cookie) ||
+		    dma_submit_error(tx_cookie)) {
+			pr_warn("%s: #%u: submit error %d/%d with src_off=0x%x ",
+				thread_name, total_tests - 1,
+				rx_cookie, tx_cookie, src_off);
+			pr_warn("dst_off=0x%x len=0x%x\n",
+				dst_off, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+		dma_async_issue_pending(rx_chan);
+		dma_async_issue_pending(tx_chan);
+
+		tx_tmo = wait_for_completion_timeout(&tx_cmp, tx_tmo);
+
+		status = dma_async_is_tx_complete(tx_chan, tx_cookie,
+						  NULL, NULL);
+
+		if (tx_tmo == 0) {
+			pr_warn("%s: #%u: tx test timed out\n",
+				thread_name, total_tests - 1);
+			failed_tests++;
+			continue;
+		} else if (status != DMA_COMPLETE) {
+			pr_warn("%s: #%u: tx got completion callback, ",
+				thread_name, total_tests - 1);
+			pr_warn("but status is \'%s\'\n",
+				status == DMA_ERROR ? "error" :
+				"in progress");
+			failed_tests++;
+			continue;
+		}
+
+		rx_tmo = wait_for_completion_timeout(&rx_cmp, rx_tmo);
+		status = dma_async_is_tx_complete(rx_chan, rx_cookie,
+						  NULL, NULL);
+
+		if (rx_tmo == 0) {
+			pr_warn("%s: #%u: rx test timed out\n",
+				thread_name, total_tests - 1);
+			failed_tests++;
+			continue;
+		} else if (status != DMA_COMPLETE) {
+			pr_warn("%s: #%u: rx got completion callback, ",
+				thread_name, total_tests - 1);
+			pr_warn("but status is \'%s\'\n",
+				status == DMA_ERROR ? "error" :
+				"in progress");
+			failed_tests++;
+			continue;
+		}
+
+		/* Unmap by myself */
+		for (i = 0; i < dst_cnt; i++)
+			dma_unmap_single(rx_dev->dev, dma_dsts[i],
+					 test_buf_size, DMA_BIDIRECTIONAL);
+
+		error_count = 0;
+		start = ktime_get();
+		pr_debug("%s: verifying source buffer...\n", thread_name);
+		error_count += dmatest_verify(thread->srcs, 0, src_off,
+				0, PATTERN_SRC, true);
+		error_count += dmatest_verify(thread->srcs, src_off,
+				src_off + len, src_off,
+				PATTERN_SRC | PATTERN_COPY, true);
+		error_count += dmatest_verify(thread->srcs, src_off + len,
+				test_buf_size, src_off + len,
+				PATTERN_SRC, true);
+
+		pr_debug("%s: verifying dest buffer...\n",
+			 thread->task->comm);
+		error_count += dmatest_verify(thread->dsts, 0, dst_off,
+				0, PATTERN_DST, false);
+		error_count += dmatest_verify(thread->dsts, dst_off,
+				dst_off + len, src_off,
+				PATTERN_SRC | PATTERN_COPY, false);
+		error_count += dmatest_verify(thread->dsts, dst_off + len,
+				test_buf_size, dst_off + len,
+				PATTERN_DST, false);
+		diff = ktime_sub(ktime_get(), start);
+		comparetime = ktime_add(comparetime, diff);
+
+		if (error_count) {
+			pr_warn("%s: #%u: %u errors with ",
+				thread_name, total_tests - 1, error_count);
+			pr_warn("src_off=0x%x dst_off=0x%x len=0x%x\n",
+				src_off, dst_off, len);
+			failed_tests++;
+		} else {
+			pr_debug("%s: #%u: No errors with ",
+				 thread_name, total_tests - 1);
+			pr_debug("src_off=0x%x dst_off=0x%x len=0x%x\n",
+				 src_off, dst_off, len);
+		}
+	}
+
+	ktime = ktime_sub(ktime_get(), ktime);
+	ktime = ktime_sub(ktime, comparetime);
+	ktime = ktime_sub(ktime, filltime);
+	runtime = ktime_to_us(ktime);
+
+	ret = 0;
+	for (i = 0; thread->dsts[i]; i++)
+		kfree(thread->dsts[i]);
+err_dstbuf:
+	kfree(thread->dsts);
+err_dsts:
+	for (i = 0; thread->srcs[i]; i++)
+		kfree(thread->srcs[i]);
+err_srcbuf:
+	kfree(thread->srcs);
+err_srcs:
+	pr_notice("%s: terminating after %u tests, %u failures %llu iops %llu KB/s (status %d)\n",
+		  thread_name, total_tests, failed_tests,
+		  dmatest_persec(runtime, total_tests),
+		  dmatest_KBs(runtime, total_len), ret);
+
+	thread->done = true;
+	wake_up(&thread_wait);
+
+	return ret;
+}
+
+static void dmatest_cleanup_channel(struct dmatest_chan *dtc)
+{
+	struct dmatest_slave_thread *thread;
+	struct dmatest_slave_thread *_thread;
+	int ret;
+
+	list_for_each_entry_safe(thread, _thread, &dtc->threads, node) {
+		ret = kthread_stop(thread->task);
+		pr_debug("dmatest: thread %s exited with status %d\n",
+			 thread->task->comm, ret);
+		list_del(&thread->node);
+		put_task_struct(thread->task);
+		kfree(thread);
+	}
+	kfree(dtc);
+}
+
+static int dmatest_add_slave_threads(struct dmatest_chan *tx_dtc,
+				     struct dmatest_chan *rx_dtc)
+{
+	struct dmatest_slave_thread *thread;
+	struct dma_chan *tx_chan = tx_dtc->chan;
+	struct dma_chan *rx_chan = rx_dtc->chan;
+	int ret;
+
+	thread = kzalloc(sizeof(struct dmatest_slave_thread), GFP_KERNEL);
+	if (!thread) {
+		pr_warn("dmatest: No memory for slave thread %s-%s\n",
+			dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+	}
+
+	thread->tx_chan = tx_chan;
+	thread->rx_chan = rx_chan;
+	thread->type = (enum dma_transaction_type)DMA_SLAVE;
+
+	/* Ensure that all previous writes are complete */
+	smp_wmb();
+	thread->task = kthread_run(dmatest_slave_func, thread, "%s-%s",
+				   dma_chan_name(tx_chan),
+				   dma_chan_name(rx_chan));
+	ret = PTR_ERR(thread->task);
+	if (IS_ERR(thread->task)) {
+		pr_warn("dmatest: Failed to run thread %s-%s\n",
+			dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+		kfree(thread);
+		return ret;
+	}
+
+	/* srcbuf and dstbuf are allocated by the thread itself */
+	get_task_struct(thread->task);
+	list_add_tail(&thread->node, &tx_dtc->threads);
+
+	/* Added one thread with 2 channels */
+	return 1;
+}
+
+static int dmatest_add_slave_channels(struct dma_chan *tx_chan,
+				      struct dma_chan *rx_chan)
+{
+	struct dmatest_chan *tx_dtc;
+	struct dmatest_chan *rx_dtc;
+	unsigned int thread_count = 0;
+
+	tx_dtc = kmalloc(sizeof(struct dmatest_chan), GFP_KERNEL);
+	if (!tx_dtc) {
+		pr_warn("dmatest: No memory for tx %s\n",
+			dma_chan_name(tx_chan));
+		return -ENOMEM;
+	}
+
+	rx_dtc = kmalloc(sizeof(struct dmatest_chan), GFP_KERNEL);
+	if (!rx_dtc) {
+		pr_warn("dmatest: No memory for rx %s\n",
+			dma_chan_name(rx_chan));
+		return -ENOMEM;
+	}
+
+	tx_dtc->chan = tx_chan;
+	rx_dtc->chan = rx_chan;
+	INIT_LIST_HEAD(&tx_dtc->threads);
+	INIT_LIST_HEAD(&rx_dtc->threads);
+
+	dmatest_add_slave_threads(tx_dtc, rx_dtc);
+	thread_count += 1;
+
+	pr_info("dmatest: Started %u threads using %s %s\n",
+		thread_count, dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+
+	list_add_tail(&tx_dtc->node, &dmatest_channels);
+	list_add_tail(&rx_dtc->node, &dmatest_channels);
+	nr_channels += 2;
+
+	if (iterations)
+		wait_event(thread_wait, !is_threaded_test_run(tx_dtc, rx_dtc));
+
+	return 0;
+}
+
+static int xilinx_axidmatest_probe(struct platform_device *pdev)
+{
+	struct dma_chan *chan, *rx_chan;
+	int err;
+
+	chan = dma_request_chan(&pdev->dev, "axidma0");
+	if (IS_ERR(chan)) {
+		err = PTR_ERR(chan);
+		if (err != -EPROBE_DEFER)
+			pr_err("xilinx_dmatest: No Tx channel\n");
+		return err;
+	}
+
+	rx_chan = dma_request_chan(&pdev->dev, "axidma1");
+	if (IS_ERR(rx_chan)) {
+		err = PTR_ERR(rx_chan);
+		if (err != -EPROBE_DEFER)
+			pr_err("xilinx_dmatest: No Rx channel\n");
+		goto free_tx;
+	}
+
+	err = dmatest_add_slave_channels(chan, rx_chan);
+	if (err) {
+		pr_err("xilinx_dmatest: Unable to add channels\n");
+		goto free_rx;
+	}
+
+	return 0;
+
+free_rx:
+	dma_release_channel(rx_chan);
+free_tx:
+	dma_release_channel(chan);
+
+	return err;
+}
+
+static int xilinx_axidmatest_remove(struct platform_device *pdev)
+{
+	struct dmatest_chan *dtc, *_dtc;
+	struct dma_chan *chan;
+
+	list_for_each_entry_safe(dtc, _dtc, &dmatest_channels, node) {
+		list_del(&dtc->node);
+		chan = dtc->chan;
+		dmatest_cleanup_channel(dtc);
+		pr_info("xilinx_dmatest: dropped channel %s\n",
+			dma_chan_name(chan));
+		dmaengine_terminate_all(chan);
+		dma_release_channel(chan);
+	}
+	return 0;
+}
+
+static const struct of_device_id xilinx_axidmatest_of_ids[] = {
+	{ .compatible = "xlnx,axi-dma-test-1.00.a",},
+	{}
+};
+
+static struct platform_driver xilinx_axidmatest_driver = {
+	.driver = {
+		.name = "xilinx_axidmatest",
+		.of_match_table = xilinx_axidmatest_of_ids,
+	},
+	.probe = xilinx_axidmatest_probe,
+	.remove = xilinx_axidmatest_remove,
+};
+
+static int __init axidma_init(void)
+{
+	return platform_driver_register(&xilinx_axidmatest_driver);
+}
+late_initcall(axidma_init);
+
+static void __exit axidma_exit(void)
+{
+	platform_driver_unregister(&xilinx_axidmatest_driver);
+}
+module_exit(axidma_exit)
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION("Xilinx AXI DMA Test Client");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/vdmatest.c b/drivers/dma/xilinx/vdmatest.c
new file mode 100644
index 000000000..0ee97fb8f
--- /dev/null
+++ b/drivers/dma/xilinx/vdmatest.c
@@ -0,0 +1,666 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * XILINX VDMA Engine test client driver
+ *
+ * Copyright (C) 2010-2014 Xilinx, Inc. All rights reserved.
+ *
+ * Based on Atmel DMA Test Client
+ *
+ * Description:
+ * This is a simple Xilinx VDMA test client for AXI VDMA driver.
+ * This test assumes both the channels of VDMA are enabled in the
+ * hardware design and configured in back-to-back connection. Test
+ * starts by pumping the data onto one channel (MM2S) and then
+ * compares the data that is received on the other channel (S2MM).
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/dma/xilinx_dma.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/of_dma.h>
+#include <linux/platform_device.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+#include <linux/sched/task.h>
+#include <linux/wait.h>
+
+static unsigned int test_buf_size = 64;
+module_param(test_buf_size, uint, 0444);
+MODULE_PARM_DESC(test_buf_size, "Size of the memcpy test buffer");
+
+static unsigned int iterations = 1;
+module_param(iterations, uint, 0444);
+MODULE_PARM_DESC(iterations,
+		"Iterations before stopping test (default: infinite)");
+
+static unsigned int hsize = 64;
+module_param(hsize, uint, 0444);
+MODULE_PARM_DESC(hsize, "Horizontal size in bytes");
+
+static unsigned int vsize = 32;
+module_param(vsize, uint, 0444);
+MODULE_PARM_DESC(vsize, "Vertical size in bytes");
+
+/*
+ * Initialization patterns. All bytes in the source buffer has bit 7
+ * set, all bytes in the destination buffer has bit 7 cleared.
+ *
+ * Bit 6 is set for all bytes which are to be copied by the DMA
+ * engine. Bit 5 is set for all bytes which are to be overwritten by
+ * the DMA engine.
+ *
+ * The remaining bits are the inverse of a counter which increments by
+ * one for each byte address.
+ */
+#define PATTERN_SRC		0x80
+#define PATTERN_DST		0x00
+#define PATTERN_COPY		0x40
+#define PATTERN_OVERWRITE	0x20
+#define PATTERN_COUNT_MASK	0x1f
+
+/* Maximum number of frame buffers */
+#define MAX_NUM_FRAMES	32
+
+/**
+ * struct vdmatest_slave_thread - VDMA test thread
+ * @node: Thread node
+ * @task: Task structure pointer
+ * @tx_chan: Tx channel pointer
+ * @rx_chan: Rx Channel pointer
+ * @srcs: Source buffer
+ * @dsts: Destination buffer
+ * @type: DMA transaction type
+ */
+struct xilinx_vdmatest_slave_thread {
+	struct list_head node;
+	struct task_struct *task;
+	struct dma_chan *tx_chan;
+	struct dma_chan *rx_chan;
+	u8 **srcs;
+	u8 **dsts;
+	enum dma_transaction_type type;
+	bool done;
+};
+
+/**
+ * struct vdmatest_chan - VDMA Test channel
+ * @node: Channel node
+ * @chan: DMA channel pointer
+ * @threads: List of VDMA test threads
+ */
+struct xilinx_vdmatest_chan {
+	struct list_head node;
+	struct dma_chan *chan;
+	struct list_head threads;
+};
+
+/* Global variables */
+static DECLARE_WAIT_QUEUE_HEAD(thread_wait);
+static LIST_HEAD(xilinx_vdmatest_channels);
+static unsigned int nr_channels;
+static unsigned int frm_cnt;
+static dma_addr_t dma_srcs[MAX_NUM_FRAMES];
+static dma_addr_t dma_dsts[MAX_NUM_FRAMES];
+static struct dma_interleaved_template xt;
+
+static bool is_threaded_test_run(struct xilinx_vdmatest_chan *tx_dtc,
+					struct xilinx_vdmatest_chan *rx_dtc)
+{
+	struct xilinx_vdmatest_slave_thread *thread;
+	int ret = false;
+
+	list_for_each_entry(thread, &tx_dtc->threads, node) {
+		if (!thread->done)
+			ret = true;
+	}
+
+	list_for_each_entry(thread, &rx_dtc->threads, node) {
+		if (!thread->done)
+			ret = true;
+	}
+	return ret;
+}
+
+static void xilinx_vdmatest_init_srcs(u8 **bufs, unsigned int start,
+					unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+		for (; i < start + len; i++)
+			buf[i] = PATTERN_SRC | PATTERN_COPY
+				| (~i & PATTERN_COUNT_MASK);
+		for (; i < test_buf_size; i++)
+			buf[i] = PATTERN_SRC | (~i & PATTERN_COUNT_MASK);
+		buf++;
+	}
+}
+
+static void xilinx_vdmatest_init_dsts(u8 **bufs, unsigned int start,
+					unsigned int len)
+{
+	unsigned int i;
+	u8 *buf;
+
+	for (; (buf = *bufs); bufs++) {
+		for (i = 0; i < start; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+		for (; i < start + len; i++)
+			buf[i] = PATTERN_DST | PATTERN_OVERWRITE
+				| (~i & PATTERN_COUNT_MASK);
+		for (; i < test_buf_size; i++)
+			buf[i] = PATTERN_DST | (~i & PATTERN_COUNT_MASK);
+	}
+}
+
+static void xilinx_vdmatest_mismatch(u8 actual, u8 pattern, unsigned int index,
+		unsigned int counter, bool is_srcbuf)
+{
+	u8 diff = actual ^ pattern;
+	u8 expected = pattern | (~counter & PATTERN_COUNT_MASK);
+	const char *thread_name = current->comm;
+
+	if (is_srcbuf)
+		pr_warn(
+		"%s: srcbuf[0x%x] overwritten! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else if ((pattern & PATTERN_COPY)
+			&& (diff & (PATTERN_COPY | PATTERN_OVERWRITE)))
+		pr_warn(
+		"%s: dstbuf[0x%x] not copied! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else if (diff & PATTERN_SRC)
+		pr_warn(
+		"%s: dstbuf[0x%x] was copied! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+	else
+		pr_warn(
+		"%s: dstbuf[0x%x] mismatch! Expected %02x, got %02x\n",
+				thread_name, index, expected, actual);
+}
+
+static unsigned int xilinx_vdmatest_verify(u8 **bufs, unsigned int start,
+		unsigned int end, unsigned int counter, u8 pattern,
+		bool is_srcbuf)
+{
+	unsigned int i, error_count = 0;
+	u8 actual, expected, *buf;
+	unsigned int counter_orig = counter;
+
+	for (; (buf = *bufs); bufs++) {
+		counter = counter_orig;
+		for (i = start; i < end; i++) {
+			actual = buf[i];
+			expected = pattern | (~counter & PATTERN_COUNT_MASK);
+			if (actual != expected) {
+				if (error_count < 32)
+					xilinx_vdmatest_mismatch(actual,
+							pattern, i,
+							counter, is_srcbuf);
+				error_count++;
+			}
+			counter++;
+		}
+	}
+
+	if (error_count > 32)
+		pr_warn("%s: %u errors suppressed\n",
+			current->comm, error_count - 32);
+
+	return error_count;
+}
+
+static void xilinx_vdmatest_slave_tx_callback(void *completion)
+{
+	pr_debug("Got tx callback\n");
+	complete(completion);
+}
+
+static void xilinx_vdmatest_slave_rx_callback(void *completion)
+{
+	pr_debug("Got rx callback\n");
+	complete(completion);
+}
+
+/*
+ * Function for slave transfers
+ * Each thread requires 2 channels, one for transmit, and one for receive
+ */
+static int xilinx_vdmatest_slave_func(void *data)
+{
+	struct xilinx_vdmatest_slave_thread *thread = data;
+	struct dma_chan *tx_chan, *rx_chan;
+	const char *thread_name;
+	unsigned int len, error_count;
+	unsigned int failed_tests = 0, total_tests = 0;
+	dma_cookie_t tx_cookie = 0, rx_cookie = 0;
+	enum dma_status status;
+	enum dma_ctrl_flags flags;
+	int ret = -ENOMEM, i;
+	struct xilinx_vdma_config config;
+
+	thread_name = current->comm;
+
+	/* Limit testing scope here */
+	test_buf_size = hsize * vsize;
+
+	/* This barrier ensures 'thread' is initialized and
+	 * we get valid DMA channels
+	 */
+	smp_rmb();
+	tx_chan = thread->tx_chan;
+	rx_chan = thread->rx_chan;
+
+	thread->srcs = kcalloc(frm_cnt+1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->srcs)
+		goto err_srcs;
+	for (i = 0; i < frm_cnt; i++) {
+		thread->srcs[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->srcs[i])
+			goto err_srcbuf;
+	}
+
+	thread->dsts = kcalloc(frm_cnt+1, sizeof(u8 *), GFP_KERNEL);
+	if (!thread->dsts)
+		goto err_dsts;
+	for (i = 0; i < frm_cnt; i++) {
+		thread->dsts[i] = kmalloc(test_buf_size, GFP_KERNEL);
+		if (!thread->dsts[i])
+			goto err_dstbuf;
+	}
+
+	set_user_nice(current, 10);
+
+	flags = DMA_CTRL_ACK | DMA_PREP_INTERRUPT;
+
+	while (!kthread_should_stop()
+		&& !(iterations && total_tests >= iterations)) {
+		struct dma_device *tx_dev = tx_chan->device;
+		struct dma_device *rx_dev = rx_chan->device;
+		struct dma_async_tx_descriptor *txd = NULL;
+		struct dma_async_tx_descriptor *rxd = NULL;
+		struct completion rx_cmp, tx_cmp;
+		unsigned long rx_tmo =
+				msecs_to_jiffies(30000); /* RX takes longer */
+		unsigned long tx_tmo = msecs_to_jiffies(30000);
+		u8 align = 0;
+
+		total_tests++;
+
+		/* honor larger alignment restrictions */
+		align = tx_dev->copy_align;
+		if (rx_dev->copy_align > align)
+			align = rx_dev->copy_align;
+
+		if (1 << align > test_buf_size) {
+			pr_err("%u-byte buffer too small for %d-byte alignment\n",
+			       test_buf_size, 1 << align);
+			break;
+		}
+
+		len = test_buf_size;
+		xilinx_vdmatest_init_srcs(thread->srcs, 0, len);
+		xilinx_vdmatest_init_dsts(thread->dsts, 0, len);
+
+		/* Zero out configuration */
+		memset(&config, 0, sizeof(struct xilinx_vdma_config));
+
+		/* Set up hardware configuration information */
+		config.frm_cnt_en = 1;
+		config.coalesc = frm_cnt * 10;
+		config.park = 1;
+		xilinx_vdma_channel_set_config(tx_chan, &config);
+
+		xilinx_vdma_channel_set_config(rx_chan, &config);
+
+		for (i = 0; i < frm_cnt; i++) {
+			dma_dsts[i] = dma_map_single(rx_dev->dev,
+							thread->dsts[i],
+							test_buf_size,
+							DMA_DEV_TO_MEM);
+
+			if (dma_mapping_error(rx_dev->dev, dma_dsts[i])) {
+				failed_tests++;
+				continue;
+			}
+			xt.dst_start = dma_dsts[i];
+			xt.dir = DMA_DEV_TO_MEM;
+			xt.numf = vsize;
+			xt.sgl[0].size = hsize;
+			xt.sgl[0].icg = 0;
+			xt.frame_size = 1;
+			rxd = rx_dev->device_prep_interleaved_dma(rx_chan,
+								  &xt, flags);
+			rx_cookie = rxd->tx_submit(rxd);
+		}
+
+		for (i = 0; i < frm_cnt; i++) {
+			u8 *buf = thread->srcs[i];
+
+			dma_srcs[i] = dma_map_single(tx_dev->dev, buf, len,
+							DMA_MEM_TO_DEV);
+
+			if (dma_mapping_error(tx_dev->dev, dma_srcs[i])) {
+				failed_tests++;
+				continue;
+			}
+			xt.src_start = dma_srcs[i];
+			xt.dir = DMA_MEM_TO_DEV;
+			xt.numf = vsize;
+			xt.sgl[0].size = hsize;
+			xt.sgl[0].icg = 0;
+			xt.frame_size = 1;
+			txd = tx_dev->device_prep_interleaved_dma(tx_chan,
+								  &xt, flags);
+			tx_cookie = txd->tx_submit(txd);
+		}
+
+		if (!rxd || !txd) {
+			for (i = 0; i < frm_cnt; i++)
+				dma_unmap_single(tx_dev->dev, dma_srcs[i], len,
+						DMA_MEM_TO_DEV);
+			for (i = 0; i < frm_cnt; i++)
+				dma_unmap_single(rx_dev->dev, dma_dsts[i],
+						test_buf_size,
+						DMA_DEV_TO_MEM);
+			pr_warn("%s: #%u: prep error with len=0x%x ",
+					thread_name, total_tests - 1, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+
+		init_completion(&rx_cmp);
+		rxd->callback = xilinx_vdmatest_slave_rx_callback;
+		rxd->callback_param = &rx_cmp;
+
+		init_completion(&tx_cmp);
+		txd->callback = xilinx_vdmatest_slave_tx_callback;
+		txd->callback_param = &tx_cmp;
+
+		if (dma_submit_error(rx_cookie) ||
+				dma_submit_error(tx_cookie)) {
+			pr_warn("%s: #%u: submit error %d/%d with len=0x%x ",
+					thread_name, total_tests - 1,
+					rx_cookie, tx_cookie, len);
+			msleep(100);
+			failed_tests++;
+			continue;
+		}
+		dma_async_issue_pending(tx_chan);
+		dma_async_issue_pending(rx_chan);
+
+		tx_tmo = wait_for_completion_timeout(&tx_cmp, tx_tmo);
+
+		status = dma_async_is_tx_complete(tx_chan, tx_cookie,
+							NULL, NULL);
+
+		if (tx_tmo == 0) {
+			pr_warn("%s: #%u: tx test timed out\n",
+					thread_name, total_tests - 1);
+			failed_tests++;
+			continue;
+		} else if (status != DMA_COMPLETE) {
+			pr_warn(
+			"%s: #%u: tx got completion callback, ",
+				   thread_name, total_tests - 1);
+			pr_warn("but status is \'%s\'\n",
+				   status == DMA_ERROR ? "error" :
+							"in progress");
+			failed_tests++;
+			continue;
+		}
+
+		rx_tmo = wait_for_completion_timeout(&rx_cmp, rx_tmo);
+		status = dma_async_is_tx_complete(rx_chan, rx_cookie,
+							NULL, NULL);
+
+		if (rx_tmo == 0) {
+			pr_warn("%s: #%u: rx test timed out\n",
+					thread_name, total_tests - 1);
+			failed_tests++;
+			continue;
+		} else if (status != DMA_COMPLETE) {
+			pr_warn(
+			"%s: #%u: rx got completion callback, ",
+					thread_name, total_tests - 1);
+			pr_warn("but status is \'%s\'\n",
+					status == DMA_ERROR ? "error" :
+							"in progress");
+			failed_tests++;
+			continue;
+		}
+
+		/* Unmap by myself */
+		for (i = 0; i < frm_cnt; i++)
+			dma_unmap_single(rx_dev->dev, dma_dsts[i],
+					 test_buf_size, DMA_DEV_TO_MEM);
+
+		error_count = 0;
+
+		pr_debug("%s: verifying source buffer...\n", thread_name);
+		error_count += xilinx_vdmatest_verify(thread->srcs, 0, 0,
+				0, PATTERN_SRC, true);
+		error_count += xilinx_vdmatest_verify(thread->srcs, 0,
+				len, 0, PATTERN_SRC | PATTERN_COPY, true);
+		error_count += xilinx_vdmatest_verify(thread->srcs, len,
+				test_buf_size, len, PATTERN_SRC, true);
+
+		pr_debug("%s: verifying dest buffer...\n",
+				thread->task->comm);
+		error_count += xilinx_vdmatest_verify(thread->dsts, 0, 0,
+				0, PATTERN_DST, false);
+		error_count += xilinx_vdmatest_verify(thread->dsts, 0,
+				len, 0, PATTERN_SRC | PATTERN_COPY, false);
+		error_count += xilinx_vdmatest_verify(thread->dsts, len,
+				test_buf_size, len, PATTERN_DST, false);
+
+		if (error_count) {
+			pr_warn("%s: #%u: %u errors with len=0x%x\n",
+				thread_name, total_tests - 1, error_count, len);
+			failed_tests++;
+		} else {
+			pr_debug("%s: #%u: No errors with len=0x%x\n",
+				thread_name, total_tests - 1, len);
+		}
+	}
+
+	ret = 0;
+	for (i = 0; thread->dsts[i]; i++)
+		kfree(thread->dsts[i]);
+err_dstbuf:
+	kfree(thread->dsts);
+err_dsts:
+	for (i = 0; thread->srcs[i]; i++)
+		kfree(thread->srcs[i]);
+err_srcbuf:
+	kfree(thread->srcs);
+err_srcs:
+	pr_notice("%s: terminating after %u tests, %u failures (status %d)\n",
+			thread_name, total_tests, failed_tests, ret);
+
+	thread->done = true;
+	wake_up(&thread_wait);
+
+	return ret;
+}
+
+static void xilinx_vdmatest_cleanup_channel(struct xilinx_vdmatest_chan *dtc)
+{
+	struct xilinx_vdmatest_slave_thread *thread, *_thread;
+	int ret;
+
+	list_for_each_entry_safe(thread, _thread,
+				&dtc->threads, node) {
+		ret = kthread_stop(thread->task);
+		pr_info("xilinx_vdmatest: thread %s exited with status %d\n",
+				thread->task->comm, ret);
+		list_del(&thread->node);
+		put_task_struct(thread->task);
+		kfree(thread);
+	}
+	kfree(dtc);
+}
+
+static int
+xilinx_vdmatest_add_slave_threads(struct xilinx_vdmatest_chan *tx_dtc,
+					struct xilinx_vdmatest_chan *rx_dtc)
+{
+	struct xilinx_vdmatest_slave_thread *thread;
+	struct dma_chan *tx_chan = tx_dtc->chan;
+	struct dma_chan *rx_chan = rx_dtc->chan;
+
+	thread = kzalloc(sizeof(struct xilinx_vdmatest_slave_thread),
+			GFP_KERNEL);
+	if (!thread)
+		pr_warn("xilinx_vdmatest: No memory for slave thread %s-%s\n",
+			   dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+
+	thread->tx_chan = tx_chan;
+	thread->rx_chan = rx_chan;
+	thread->type = (enum dma_transaction_type)DMA_SLAVE;
+
+	/* This barrier ensures the DMA channels in the 'thread'
+	 * are initialized
+	 */
+	smp_wmb();
+	thread->task = kthread_run(xilinx_vdmatest_slave_func, thread, "%s-%s",
+		dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+	if (IS_ERR(thread->task)) {
+		pr_warn("xilinx_vdmatest: Failed to run thread %s-%s\n",
+				dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+		kfree(thread);
+		return PTR_ERR(thread->task);
+	}
+
+	get_task_struct(thread->task);
+	list_add_tail(&thread->node, &tx_dtc->threads);
+
+	/* Added one thread with 2 channels */
+	return 1;
+}
+
+static int xilinx_vdmatest_add_slave_channels(struct dma_chan *tx_chan,
+					struct dma_chan *rx_chan)
+{
+	struct xilinx_vdmatest_chan *tx_dtc, *rx_dtc;
+	unsigned int thread_count = 0;
+
+	tx_dtc = kmalloc(sizeof(struct xilinx_vdmatest_chan), GFP_KERNEL);
+	if (!tx_dtc)
+		return -ENOMEM;
+
+	rx_dtc = kmalloc(sizeof(struct xilinx_vdmatest_chan), GFP_KERNEL);
+	if (!rx_dtc)
+		return -ENOMEM;
+
+	tx_dtc->chan = tx_chan;
+	rx_dtc->chan = rx_chan;
+	INIT_LIST_HEAD(&tx_dtc->threads);
+	INIT_LIST_HEAD(&rx_dtc->threads);
+
+	xilinx_vdmatest_add_slave_threads(tx_dtc, rx_dtc);
+	thread_count += 1;
+
+	pr_info("xilinx_vdmatest: Started %u threads using %s %s\n",
+		thread_count, dma_chan_name(tx_chan), dma_chan_name(rx_chan));
+
+	list_add_tail(&tx_dtc->node, &xilinx_vdmatest_channels);
+	list_add_tail(&rx_dtc->node, &xilinx_vdmatest_channels);
+	nr_channels += 2;
+
+	if (iterations)
+		wait_event(thread_wait, !is_threaded_test_run(tx_dtc, rx_dtc));
+
+	return 0;
+}
+
+static int xilinx_vdmatest_probe(struct platform_device *pdev)
+{
+	struct dma_chan *chan, *rx_chan;
+	int err;
+
+	err = of_property_read_u32(pdev->dev.of_node,
+					"xlnx,num-fstores", &frm_cnt);
+	if (err < 0) {
+		pr_err("xilinx_vdmatest: missing xlnx,num-fstores property\n");
+		return err;
+	}
+
+	chan = dma_request_chan(&pdev->dev, "vdma0");
+	if (IS_ERR(chan)) {
+		err = PTR_ERR(chan);
+		if (err != -EPROBE_DEFER)
+			pr_err("xilinx_vdmatest: No Tx channel\n");
+		return err;
+	}
+
+	rx_chan = dma_request_chan(&pdev->dev, "vdma1");
+	if (IS_ERR(rx_chan)) {
+		err = PTR_ERR(rx_chan);
+		if (err != -EPROBE_DEFER)
+			pr_err("xilinx_vdmatest: No Rx channel\n");
+		goto free_tx;
+	}
+
+	err = xilinx_vdmatest_add_slave_channels(chan, rx_chan);
+	if (err) {
+		pr_err("xilinx_vdmatest: Unable to add channels\n");
+		goto free_rx;
+	}
+	return 0;
+
+free_rx:
+	dma_release_channel(rx_chan);
+free_tx:
+	dma_release_channel(chan);
+
+	return err;
+}
+
+static int xilinx_vdmatest_remove(struct platform_device *pdev)
+{
+	struct xilinx_vdmatest_chan *dtc, *_dtc;
+	struct dma_chan *chan;
+
+	list_for_each_entry_safe(dtc, _dtc, &xilinx_vdmatest_channels, node) {
+		list_del(&dtc->node);
+		chan = dtc->chan;
+		xilinx_vdmatest_cleanup_channel(dtc);
+		pr_info("xilinx_vdmatest: dropped channel %s\n",
+			dma_chan_name(chan));
+		dmaengine_terminate_async(chan);
+		dma_release_channel(chan);
+	}
+	return 0;
+}
+
+static const struct of_device_id xilinx_vdmatest_of_ids[] = {
+	{ .compatible = "xlnx,axi-vdma-test-1.00.a",},
+	{}
+};
+
+static struct platform_driver xilinx_vdmatest_driver = {
+	.driver = {
+		.name = "xilinx_vdmatest",
+		.owner = THIS_MODULE,
+		.of_match_table = xilinx_vdmatest_of_ids,
+	},
+	.probe = xilinx_vdmatest_probe,
+	.remove = xilinx_vdmatest_remove,
+};
+
+module_platform_driver(xilinx_vdmatest_driver);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION("Xilinx AXI VDMA Test Client");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/xilinx_dma.c b/drivers/dma/xilinx/xilinx_dma.c
index cab4719e4..d3d8fd154 100644
--- a/drivers/dma/xilinx/xilinx_dma.c
+++ b/drivers/dma/xilinx/xilinx_dma.c
@@ -792,7 +792,7 @@ static void xilinx_vdma_free_tx_segment(struct xilinx_dma_chan *chan,
 }
 
 /**
- * xilinx_dma_tx_descriptor - Allocate transaction descriptor
+ * xilinx_dma_alloc_tx_descriptor - Allocate transaction descriptor
  * @chan: Driver specific DMA channel
  *
  * Return: The allocated descriptor on success and NULL on failure.
@@ -2466,7 +2466,7 @@ static int xilinx_dma_terminate_all(struct dma_chan *dchan)
 }
 
 /**
- * xilinx_dma_channel_set_config - Configure VDMA channel
+ * xilinx_vdma_channel_set_config - Configure VDMA channel
  * Run-time configuration for Axi VDMA, supports:
  * . halt the channel
  * . configure interrupt coalescing and inter-packet delay threshold
@@ -2907,7 +2907,7 @@ static int xilinx_dma_chan_probe(struct xilinx_dma_device *xdev,
  * @xdev: Driver specific device structure
  * @node: Device node
  *
- * Return: 0 always.
+ * Return: '0' on success and failure value on error.
  */
 static int xilinx_dma_child_probe(struct xilinx_dma_device *xdev,
 				    struct device_node *node)
@@ -2919,8 +2919,11 @@ static int xilinx_dma_child_probe(struct xilinx_dma_device *xdev,
 	if (xdev->dma_config->dmatype == XDMA_TYPE_AXIMCDMA && ret < 0)
 		dev_warn(xdev->dev, "missing dma-channels property\n");
 
-	for (i = 0; i < nr_channels; i++)
-		xilinx_dma_chan_probe(xdev, node);
+	for (i = 0; i < nr_channels; i++) {
+		ret = xilinx_dma_chan_probe(xdev, node);
+		if (ret)
+			return ret;
+	}
 
 	return 0;
 }
diff --git a/drivers/dma/xilinx/xilinx_dpdma.c b/drivers/dma/xilinx/xilinx_dpdma.c
index 6c7098032..75e5ef31a 100644
--- a/drivers/dma/xilinx/xilinx_dpdma.c
+++ b/drivers/dma/xilinx/xilinx_dpdma.c
@@ -531,7 +531,7 @@ static void xilinx_dpdma_sw_desc_set_dma_addrs(struct xilinx_dpdma_device *xdev,
 	for (i = 1; i < num_src_addr; i++) {
 		u32 *addr = &hw_desc->src_addr2;
 
-		addr[i-1] = lower_32_bits(dma_addr[i]);
+		addr[i - 1] = lower_32_bits(dma_addr[i]);
 
 		if (xdev->ext_addr) {
 			u32 *addr_ext = &hw_desc->addr_ext_23;
@@ -681,6 +681,84 @@ static void xilinx_dpdma_chan_free_tx_desc(struct virt_dma_desc *vdesc)
 	kfree(desc);
 }
 
+/**
+ * xilinx_dpdma_chan_prep_cyclic - Prepare a cyclic dma descriptor
+ * @chan: DPDMA channel
+ * @buf_addr: buffer address
+ * @buf_len: buffer length
+ * @period_len: number of periods
+ * @flags: tx flags argument passed in to prepare function
+ *
+ * Prepare a tx descriptor incudling internal software/hardware descriptors
+ * for the given cyclic transaction.
+ *
+ * Return: A dma async tx descriptor on success, or NULL.
+ */
+static struct dma_async_tx_descriptor *
+xilinx_dpdma_chan_prep_cyclic(struct xilinx_dpdma_chan *chan,
+			      dma_addr_t buf_addr, size_t buf_len,
+			      size_t period_len, unsigned long flags)
+{
+	struct xilinx_dpdma_tx_desc *tx_desc;
+	struct xilinx_dpdma_sw_desc *sw_desc, *last = NULL;
+	unsigned int periods = buf_len / period_len;
+	unsigned int i;
+
+	tx_desc = xilinx_dpdma_chan_alloc_tx_desc(chan);
+	if (!tx_desc)
+		return (void *)tx_desc;
+
+	for (i = 0; i < periods; i++) {
+		struct xilinx_dpdma_hw_desc *hw_desc;
+
+		if (!IS_ALIGNED(buf_addr, XILINX_DPDMA_ALIGN_BYTES)) {
+			dev_err(chan->xdev->dev,
+				"buffer should be aligned at %d B\n",
+				XILINX_DPDMA_ALIGN_BYTES);
+			goto error;
+		}
+
+		sw_desc = xilinx_dpdma_chan_alloc_sw_desc(chan);
+		if (!sw_desc)
+			goto error;
+
+		xilinx_dpdma_sw_desc_set_dma_addrs(chan->xdev, sw_desc, last,
+						   &buf_addr, 1);
+		hw_desc = &sw_desc->hw;
+		hw_desc->xfer_size = period_len;
+		hw_desc->hsize_stride =
+			FIELD_PREP(XILINX_DPDMA_DESC_HSIZE_STRIDE_HSIZE_MASK,
+				   period_len) |
+			FIELD_PREP(XILINX_DPDMA_DESC_HSIZE_STRIDE_STRIDE_MASK,
+				   period_len);
+		hw_desc->control |= XILINX_DPDMA_DESC_CONTROL_PREEMBLE;
+		hw_desc->control |= XILINX_DPDMA_DESC_CONTROL_IGNORE_DONE;
+		hw_desc->control |= XILINX_DPDMA_DESC_CONTROL_COMPLETE_INTR;
+
+		list_add_tail(&sw_desc->node, &tx_desc->descriptors);
+
+		buf_addr += period_len;
+		last = sw_desc;
+	}
+
+	sw_desc = list_first_entry(&tx_desc->descriptors,
+				   struct xilinx_dpdma_sw_desc, node);
+	last->hw.next_desc = lower_32_bits(sw_desc->dma_addr);
+	if (chan->xdev->ext_addr)
+		last->hw.addr_ext |=
+			FIELD_PREP(XILINX_DPDMA_DESC_ADDR_EXT_NEXT_ADDR_MASK,
+				   upper_32_bits(sw_desc->dma_addr));
+
+	last->hw.control |= XILINX_DPDMA_DESC_CONTROL_LAST_OF_FRAME;
+
+	return vchan_tx_prep(&chan->vchan, &tx_desc->vdesc, flags);
+
+error:
+	xilinx_dpdma_chan_free_tx_desc(&tx_desc->vdesc);
+
+	return NULL;
+}
+
 /**
  * xilinx_dpdma_chan_prep_interleaved_dma - Prepare an interleaved dma
  *					    descriptor
@@ -703,8 +781,9 @@ xilinx_dpdma_chan_prep_interleaved_dma(struct xilinx_dpdma_chan *chan,
 	size_t stride = hsize + xt->sgl[0].icg;
 
 	if (!IS_ALIGNED(xt->src_start, XILINX_DPDMA_ALIGN_BYTES)) {
-		dev_err(chan->xdev->dev, "buffer should be aligned at %d B\n",
-			XILINX_DPDMA_ALIGN_BYTES);
+		dev_err(chan->xdev->dev,
+			"chan%u: buffer should be aligned at %d B\n",
+			chan->id, XILINX_DPDMA_ALIGN_BYTES);
 		return NULL;
 	}
 
@@ -917,7 +996,7 @@ static u32 xilinx_dpdma_chan_ostand(struct xilinx_dpdma_chan *chan)
 }
 
 /**
- * xilinx_dpdma_chan_no_ostand - Notify no outstanding transaction event
+ * xilinx_dpdma_chan_notify_no_ostand - Notify no outstanding transaction event
  * @chan: DPDMA channel
  *
  * Notify waiters for no outstanding event, so waiters can stop the channel
@@ -936,7 +1015,9 @@ static int xilinx_dpdma_chan_notify_no_ostand(struct xilinx_dpdma_chan *chan)
 
 	cnt = xilinx_dpdma_chan_ostand(chan);
 	if (cnt) {
-		dev_dbg(chan->xdev->dev, "%d outstanding transactions\n", cnt);
+		dev_dbg(chan->xdev->dev,
+			"chan%u: %d outstanding transactions\n",
+			chan->id, cnt);
 		return -EWOULDBLOCK;
 	}
 
@@ -972,8 +1053,8 @@ static int xilinx_dpdma_chan_wait_no_ostand(struct xilinx_dpdma_chan *chan)
 		return 0;
 	}
 
-	dev_err(chan->xdev->dev, "not ready to stop: %d trans\n",
-		xilinx_dpdma_chan_ostand(chan));
+	dev_err(chan->xdev->dev, "chan%u: not ready to stop: %d trans\n",
+		chan->id, xilinx_dpdma_chan_ostand(chan));
 
 	if (ret == 0)
 		return -ETIMEDOUT;
@@ -1007,8 +1088,8 @@ static int xilinx_dpdma_chan_poll_no_ostand(struct xilinx_dpdma_chan *chan)
 		return 0;
 	}
 
-	dev_err(chan->xdev->dev, "not ready to stop: %d trans\n",
-		xilinx_dpdma_chan_ostand(chan));
+	dev_err(chan->xdev->dev, "chan%u: not ready to stop: %d trans\n",
+		chan->id, xilinx_dpdma_chan_ostand(chan));
 
 	return -ETIMEDOUT;
 }
@@ -1062,7 +1143,8 @@ static void xilinx_dpdma_chan_done_irq(struct xilinx_dpdma_chan *chan)
 		vchan_cyclic_callback(&active->vdesc);
 	else
 		dev_warn(chan->xdev->dev,
-			 "DONE IRQ with no active descriptor!\n");
+			 "chan%u: DONE IRQ with no active descriptor!\n",
+			 chan->id);
 
 	spin_unlock_irqrestore(&chan->lock, flags);
 }
@@ -1094,8 +1176,12 @@ static void xilinx_dpdma_chan_vsync_irq(struct  xilinx_dpdma_chan *chan)
 	/* If the retrigger raced with vsync, retry at the next frame. */
 	sw_desc = list_first_entry(&pending->descriptors,
 				   struct xilinx_dpdma_sw_desc, node);
-	if (sw_desc->hw.desc_id != desc_id)
+	if (sw_desc->hw.desc_id != desc_id) {
+		dev_dbg(chan->xdev->dev,
+			"chan%u: vsync race lost (%u != %u), retrying\n",
+			chan->id, sw_desc->hw.desc_id, desc_id);
 		goto out;
+	}
 
 	/*
 	 * Complete the active descriptor, if any, promote the pending
@@ -1151,10 +1237,12 @@ static void xilinx_dpdma_chan_handle_err(struct xilinx_dpdma_chan *chan)
 
 	spin_lock_irqsave(&chan->lock, flags);
 
-	dev_dbg(xdev->dev, "cur desc addr = 0x%04x%08x\n",
+	dev_dbg(xdev->dev, "chan%u: cur desc addr = 0x%04x%08x\n",
+		chan->id,
 		dpdma_read(chan->reg, XILINX_DPDMA_CH_DESC_START_ADDRE),
 		dpdma_read(chan->reg, XILINX_DPDMA_CH_DESC_START_ADDR));
-	dev_dbg(xdev->dev, "cur payload addr = 0x%04x%08x\n",
+	dev_dbg(xdev->dev, "chan%u: cur payload addr = 0x%04x%08x\n",
+		chan->id,
 		dpdma_read(chan->reg, XILINX_DPDMA_CH_PYLD_CUR_ADDRE),
 		dpdma_read(chan->reg, XILINX_DPDMA_CH_PYLD_CUR_ADDR));
 
@@ -1170,7 +1258,8 @@ static void xilinx_dpdma_chan_handle_err(struct xilinx_dpdma_chan *chan)
 	xilinx_dpdma_chan_dump_tx_desc(chan, active);
 
 	if (active->error)
-		dev_dbg(xdev->dev, "repeated error on desc\n");
+		dev_dbg(xdev->dev, "chan%u: repeated error on desc\n",
+			chan->id);
 
 	/* Reschedule if there's no new descriptor */
 	if (!chan->desc.pending &&
@@ -1189,6 +1278,23 @@ static void xilinx_dpdma_chan_handle_err(struct xilinx_dpdma_chan *chan)
 /* -----------------------------------------------------------------------------
  * DMA Engine Operations
  */
+static struct dma_async_tx_descriptor *
+xilinx_dpdma_prep_dma_cyclic(struct dma_chan *dchan, dma_addr_t buf_addr,
+			     size_t buf_len, size_t period_len,
+			     enum dma_transfer_direction direction,
+			     unsigned long flags)
+{
+	struct xilinx_dpdma_chan *chan = to_xilinx_chan(dchan);
+
+	if (direction != DMA_MEM_TO_DEV)
+		return NULL;
+
+	if (buf_len % period_len)
+		return NULL;
+
+	return xilinx_dpdma_chan_prep_cyclic(chan, buf_addr, buf_len,
+						 period_len, flags);
+}
 
 static struct dma_async_tx_descriptor *
 xilinx_dpdma_prep_interleaved_dma(struct dma_chan *dchan,
@@ -1235,7 +1341,8 @@ static int xilinx_dpdma_alloc_chan_resources(struct dma_chan *dchan)
 					  align, 0);
 	if (!chan->desc_pool) {
 		dev_err(chan->xdev->dev,
-			"failed to allocate a descriptor pool\n");
+			"chan%u: failed to allocate a descriptor pool\n",
+			chan->id);
 		return -ENOMEM;
 	}
 
@@ -1588,7 +1695,7 @@ static struct dma_chan *of_dma_xilinx_xlate(struct of_phandle_args *dma_spec,
 					    struct of_dma *ofdma)
 {
 	struct xilinx_dpdma_device *xdev = ofdma->of_dma_data;
-	uint32_t chan_id = dma_spec->args[0];
+	u32 chan_id = dma_spec->args[0];
 
 	if (chan_id >= ARRAY_SIZE(xdev->chan))
 		return NULL;
@@ -1665,6 +1772,7 @@ static int xilinx_dpdma_probe(struct platform_device *pdev)
 
 	dma_cap_set(DMA_SLAVE, ddev->cap_mask);
 	dma_cap_set(DMA_PRIVATE, ddev->cap_mask);
+	dma_cap_set(DMA_CYCLIC, ddev->cap_mask);
 	dma_cap_set(DMA_INTERLEAVE, ddev->cap_mask);
 	dma_cap_set(DMA_REPEAT, ddev->cap_mask);
 	dma_cap_set(DMA_LOAD_EOT, ddev->cap_mask);
@@ -1672,6 +1780,7 @@ static int xilinx_dpdma_probe(struct platform_device *pdev)
 
 	ddev->device_alloc_chan_resources = xilinx_dpdma_alloc_chan_resources;
 	ddev->device_free_chan_resources = xilinx_dpdma_free_chan_resources;
+	ddev->device_prep_dma_cyclic = xilinx_dpdma_prep_dma_cyclic;
 	ddev->device_prep_interleaved_dma = xilinx_dpdma_prep_interleaved_dma;
 	/* TODO: Can we achieve better granularity ? */
 	ddev->device_tx_status = dma_cookie_status;
diff --git a/drivers/dma/xilinx/xilinx_frmbuf.c b/drivers/dma/xilinx/xilinx_frmbuf.c
new file mode 100644
index 000000000..186ff3021
--- /dev/null
+++ b/drivers/dma/xilinx/xilinx_frmbuf.c
@@ -0,0 +1,1880 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+/*
+ * DMAEngine driver for Xilinx Framebuffer IP
+ *
+ * Copyright (C) 2016 - 2021 Xilinx, Inc.
+ *
+ * Authors: Radhey Shyam Pandey <radheys@xilinx.com>
+ *          John Nichols <jnichol@xilinx.com>
+ *          Jeffrey Mouroux <jmouroux@xilinx.com>
+ *
+ * Based on the Freescale DMA driver.
+ *
+ * Description:
+ * The AXI Framebuffer core is a soft Xilinx IP core that
+ * provides high-bandwidth direct memory access between memory
+ * and AXI4-Stream.
+ */
+
+#include <linux/bitops.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/dma/xilinx_frmbuf.h>
+#include <linux/dmapool.h>
+#include <linux/gpio/consumer.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/iopoll.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/of_dma.h>
+#include <linux/of_irq.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/videodev2.h>
+
+#include <drm/drm_fourcc.h>
+
+#include "../dmaengine.h"
+
+/* Register/Descriptor Offsets */
+#define XILINX_FRMBUF_CTRL_OFFSET		0x00
+#define XILINX_FRMBUF_GIE_OFFSET		0x04
+#define XILINX_FRMBUF_IE_OFFSET			0x08
+#define XILINX_FRMBUF_ISR_OFFSET		0x0c
+#define XILINX_FRMBUF_WIDTH_OFFSET		0x10
+#define XILINX_FRMBUF_HEIGHT_OFFSET		0x18
+#define XILINX_FRMBUF_STRIDE_OFFSET		0x20
+#define XILINX_FRMBUF_FMT_OFFSET		0x28
+#define XILINX_FRMBUF_ADDR_OFFSET		0x30
+#define XILINX_FRMBUF_ADDR2_OFFSET		0x3c
+#define XILINX_FRMBUF_FID_OFFSET		0x48
+#define XILINX_FRMBUF_FID_MODE_OFFSET	0x50
+#define XILINX_FRMBUF_ADDR3_OFFSET		0x54
+#define XILINX_FRMBUF_FID_ERR_OFFSET	0x58
+#define XILINX_FRMBUF_FID_OUT_OFFSET	0x60
+#define XILINX_FRMBUF_RD_ADDR3_OFFSET		0x74
+
+/* Control Registers */
+#define XILINX_FRMBUF_CTRL_AP_START		BIT(0)
+#define XILINX_FRMBUF_CTRL_AP_DONE		BIT(1)
+#define XILINX_FRMBUF_CTRL_AP_IDLE		BIT(2)
+#define XILINX_FRMBUF_CTRL_AP_READY		BIT(3)
+#define XILINX_FRMBUF_CTRL_FLUSH		BIT(5)
+#define XILINX_FRMBUF_CTRL_FLUSH_DONE		BIT(6)
+#define XILINX_FRMBUF_CTRL_AUTO_RESTART		BIT(7)
+#define XILINX_FRMBUF_GIE_EN			BIT(0)
+
+/* Interrupt Status and Control */
+#define XILINX_FRMBUF_IE_AP_DONE		BIT(0)
+#define XILINX_FRMBUF_IE_AP_READY		BIT(1)
+
+#define XILINX_FRMBUF_ISR_AP_DONE_IRQ		BIT(0)
+#define XILINX_FRMBUF_ISR_AP_READY_IRQ		BIT(1)
+
+#define XILINX_FRMBUF_ISR_ALL_IRQ_MASK	\
+		(XILINX_FRMBUF_ISR_AP_DONE_IRQ | \
+		XILINX_FRMBUF_ISR_AP_READY_IRQ)
+
+/* Video Format Register Settings */
+#define XILINX_FRMBUF_FMT_RGBX8			10
+#define XILINX_FRMBUF_FMT_YUVX8			11
+#define XILINX_FRMBUF_FMT_YUYV8			12
+#define XILINX_FRMBUF_FMT_RGBA8			13
+#define XILINX_FRMBUF_FMT_YUVA8			14
+#define XILINX_FRMBUF_FMT_RGBX10		15
+#define XILINX_FRMBUF_FMT_YUVX10		16
+#define XILINX_FRMBUF_FMT_Y_UV8			18
+#define XILINX_FRMBUF_FMT_Y_UV8_420		19
+#define XILINX_FRMBUF_FMT_RGB8			20
+#define XILINX_FRMBUF_FMT_YUV8			21
+#define XILINX_FRMBUF_FMT_Y_UV10		22
+#define XILINX_FRMBUF_FMT_Y_UV10_420		23
+#define XILINX_FRMBUF_FMT_Y8			24
+#define XILINX_FRMBUF_FMT_Y10			25
+#define XILINX_FRMBUF_FMT_BGRA8			26
+#define XILINX_FRMBUF_FMT_BGRX8			27
+#define XILINX_FRMBUF_FMT_UYVY8			28
+#define XILINX_FRMBUF_FMT_BGR8			29
+#define XILINX_FRMBUF_FMT_RGBX12		30
+#define XILINX_FRMBUF_FMT_RGB16			35
+#define XILINX_FRMBUF_FMT_Y_U_V8		42
+
+/* FID Register */
+#define XILINX_FRMBUF_FID_MASK			BIT(0)
+
+/* FID ERR Register */
+#define XILINX_FRMBUF_FID_ERR_MASK		BIT(0)
+#define XILINX_FRMBUF_FID_OUT_MASK		BIT(0)
+
+#define XILINX_FRMBUF_ALIGN_MUL			8
+
+#define WAIT_FOR_FLUSH_DONE			25
+
+/* Pixels per clock property flag */
+#define XILINX_PPC_PROP				BIT(0)
+#define XILINX_FLUSH_PROP			BIT(1)
+#define XILINX_FID_PROP				BIT(2)
+#define XILINX_CLK_PROP				BIT(3)
+#define XILINX_THREE_PLANES_PROP		BIT(4)
+#define XILINX_FID_ERR_DETECT_PROP		BIT(5)
+
+#define XILINX_FRMBUF_MIN_HEIGHT		(64)
+#define XILINX_FRMBUF_MIN_WIDTH			(64)
+
+/**
+ * struct xilinx_frmbuf_desc_hw - Hardware Descriptor
+ * @luma_plane_addr: Luma or packed plane buffer address
+ * @chroma_plane_addr: Chroma plane buffer address
+ * @vsize: Vertical Size
+ * @hsize: Horizontal Size
+ * @stride: Number of bytes between the first
+ *	    pixels of each horizontal line
+ */
+struct xilinx_frmbuf_desc_hw {
+	dma_addr_t luma_plane_addr;
+	dma_addr_t chroma_plane_addr[2];
+	u32 vsize;
+	u32 hsize;
+	u32 stride;
+};
+
+/**
+ * struct xilinx_frmbuf_tx_descriptor - Per Transaction structure
+ * @async_tx: Async transaction descriptor
+ * @hw: Hardware descriptor
+ * @node: Node in the channel descriptors list
+ * @fid: Field ID of buffer
+ * @earlycb: Whether the callback should be called when in staged state
+ */
+struct xilinx_frmbuf_tx_descriptor {
+	struct dma_async_tx_descriptor async_tx;
+	struct xilinx_frmbuf_desc_hw hw;
+	struct list_head node;
+	u32 fid;
+	u32 earlycb;
+};
+
+/**
+ * struct xilinx_frmbuf_chan - Driver specific dma channel structure
+ * @xdev: Driver specific device structure
+ * @lock: Descriptor operation lock
+ * @chan_node: Member of a list of framebuffer channel instances
+ * @pending_list: Descriptors waiting
+ * @done_list: Complete descriptors
+ * @staged_desc: Next buffer to be programmed
+ * @active_desc: Currently active buffer being read/written to
+ * @common: DMA common channel
+ * @dev: The dma device
+ * @write_addr: callback that will write dma addresses to IP (32 or 64 bit)
+ * @irq: Channel IRQ
+ * @direction: Transfer direction
+ * @idle: Channel idle state
+ * @tasklet: Cleanup work after irq
+ * @vid_fmt: Reference to currently assigned video format description
+ * @hw_fid: FID enabled in hardware flag
+ * @mode: Select operation mode
+ * @fid_err_flag: Field id error detection flag
+ * @fid_out_val: Field id out val
+ * @fid_mode: Select fid mode
+ */
+struct xilinx_frmbuf_chan {
+	struct xilinx_frmbuf_device *xdev;
+	/* Descriptor operation lock */
+	spinlock_t lock;
+	struct list_head chan_node;
+	struct list_head pending_list;
+	struct list_head done_list;
+	struct xilinx_frmbuf_tx_descriptor *staged_desc;
+	struct xilinx_frmbuf_tx_descriptor *active_desc;
+	struct dma_chan common;
+	struct device *dev;
+	void (*write_addr)(struct xilinx_frmbuf_chan *chan, u32 reg,
+			   dma_addr_t value);
+	int irq;
+	enum dma_transfer_direction direction;
+	bool idle;
+	struct tasklet_struct tasklet;
+	const struct xilinx_frmbuf_format_desc *vid_fmt;
+	bool hw_fid;
+	enum operation_mode mode;
+	u8 fid_err_flag;
+	u8 fid_out_val;
+	enum fid_modes fid_mode;
+};
+
+/**
+ * struct xilinx_frmbuf_format_desc - lookup table to match fourcc to format
+ * @dts_name: Device tree name for this entry.
+ * @id: Format ID
+ * @bpw: Bits of pixel data + padding in a 32-bit word (luma plane for semi-pl)
+ * @ppw: Number of pixels represented in a 32-bit word (luma plane for semi-pl)
+ * @num_planes: Expected number of plane buffers in framebuffer for this format
+ * @drm_fmt: DRM video framework equivalent fourcc code
+ * @v4l2_fmt: Video 4 Linux framework equivalent fourcc code
+ * @fmt_bitmask: Flag identifying this format in device-specific "enabled"
+ *	bitmap
+ */
+struct xilinx_frmbuf_format_desc {
+	const char *dts_name;
+	u32 id;
+	u32 bpw;
+	u32 ppw;
+	u32 num_planes;
+	u32 drm_fmt;
+	u32 v4l2_fmt;
+	u32 fmt_bitmask;
+};
+
+static LIST_HEAD(frmbuf_chan_list);
+static DEFINE_MUTEX(frmbuf_chan_list_lock);
+
+static const struct xilinx_frmbuf_format_desc xilinx_frmbuf_formats[] = {
+	{
+		.dts_name = "xbgr8888",
+		.id = XILINX_FRMBUF_FMT_RGBX8,
+		.bpw = 32,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_XBGR8888,
+		.v4l2_fmt = V4L2_PIX_FMT_BGRX32,
+		.fmt_bitmask = BIT(0),
+	},
+	{
+		.dts_name = "xbgr2101010",
+		.id = XILINX_FRMBUF_FMT_RGBX10,
+		.bpw = 32,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_XBGR2101010,
+		.v4l2_fmt = V4L2_PIX_FMT_XBGR30,
+		.fmt_bitmask = BIT(1),
+	},
+	{
+		.dts_name = "xrgb8888",
+		.id = XILINX_FRMBUF_FMT_BGRX8,
+		.bpw = 32,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_XRGB8888,
+		.v4l2_fmt = V4L2_PIX_FMT_XBGR32,
+		.fmt_bitmask = BIT(2),
+	},
+	{
+		.dts_name = "xvuy8888",
+		.id = XILINX_FRMBUF_FMT_YUVX8,
+		.bpw = 32,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_XVUY8888,
+		.v4l2_fmt = V4L2_PIX_FMT_XVUY32,
+		.fmt_bitmask = BIT(5),
+	},
+	{
+		.dts_name = "vuy888",
+		.id = XILINX_FRMBUF_FMT_YUV8,
+		.bpw = 24,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_VUY888,
+		.v4l2_fmt = V4L2_PIX_FMT_VUY24,
+		.fmt_bitmask = BIT(6),
+	},
+	{
+		.dts_name = "yuvx2101010",
+		.id = XILINX_FRMBUF_FMT_YUVX10,
+		.bpw = 32,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_XVUY2101010,
+		.v4l2_fmt = V4L2_PIX_FMT_XVUY10,
+		.fmt_bitmask = BIT(7),
+	},
+	{
+		.dts_name = "yuyv",
+		.id = XILINX_FRMBUF_FMT_YUYV8,
+		.bpw = 32,
+		.ppw = 2,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_YUYV,
+		.v4l2_fmt = V4L2_PIX_FMT_YUYV,
+		.fmt_bitmask = BIT(8),
+	},
+	{
+		.dts_name = "uyvy",
+		.id = XILINX_FRMBUF_FMT_UYVY8,
+		.bpw = 32,
+		.ppw = 2,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_UYVY,
+		.v4l2_fmt = V4L2_PIX_FMT_UYVY,
+		.fmt_bitmask = BIT(9),
+	},
+	{
+		.dts_name = "nv16",
+		.id = XILINX_FRMBUF_FMT_Y_UV8,
+		.bpw = 32,
+		.ppw = 4,
+		.num_planes = 2,
+		.drm_fmt = DRM_FORMAT_NV16,
+		.v4l2_fmt = V4L2_PIX_FMT_NV16M,
+		.fmt_bitmask = BIT(11),
+	},
+	{
+		.dts_name = "nv16",
+		.id = XILINX_FRMBUF_FMT_Y_UV8,
+		.bpw = 32,
+		.ppw = 4,
+		.num_planes = 2,
+		.drm_fmt = 0,
+		.v4l2_fmt = V4L2_PIX_FMT_NV16,
+		.fmt_bitmask = BIT(11),
+	},
+	{
+		.dts_name = "nv12",
+		.id = XILINX_FRMBUF_FMT_Y_UV8_420,
+		.bpw = 32,
+		.ppw = 4,
+		.num_planes = 2,
+		.drm_fmt = DRM_FORMAT_NV12,
+		.v4l2_fmt = V4L2_PIX_FMT_NV12M,
+		.fmt_bitmask = BIT(12),
+	},
+	{
+		.dts_name = "nv12",
+		.id = XILINX_FRMBUF_FMT_Y_UV8_420,
+		.bpw = 32,
+		.ppw = 4,
+		.num_planes = 2,
+		.drm_fmt = 0,
+		.v4l2_fmt = V4L2_PIX_FMT_NV12,
+		.fmt_bitmask = BIT(12),
+	},
+	{
+		.dts_name = "xv15",
+		.id = XILINX_FRMBUF_FMT_Y_UV10_420,
+		.bpw = 32,
+		.ppw = 3,
+		.num_planes = 2,
+		.drm_fmt = DRM_FORMAT_XV15,
+		.v4l2_fmt = V4L2_PIX_FMT_XV15M,
+		.fmt_bitmask = BIT(13),
+	},
+	{
+		.dts_name = "xv15",
+		.id = XILINX_FRMBUF_FMT_Y_UV10_420,
+		.bpw = 32,
+		.ppw = 3,
+		.num_planes = 2,
+		.drm_fmt = 0,
+		.v4l2_fmt = V4L2_PIX_FMT_XV15,
+		.fmt_bitmask = BIT(13),
+	},
+	{
+		.dts_name = "xv20",
+		.id = XILINX_FRMBUF_FMT_Y_UV10,
+		.bpw = 32,
+		.ppw = 3,
+		.num_planes = 2,
+		.drm_fmt = DRM_FORMAT_XV20,
+		.v4l2_fmt = V4L2_PIX_FMT_XV20M,
+		.fmt_bitmask = BIT(14),
+	},
+	{
+		.dts_name = "xv20",
+		.id = XILINX_FRMBUF_FMT_Y_UV10,
+		.bpw = 32,
+		.ppw = 3,
+		.num_planes = 2,
+		.drm_fmt = 0,
+		.v4l2_fmt = V4L2_PIX_FMT_XV20,
+		.fmt_bitmask = BIT(14),
+	},
+	{
+		.dts_name = "bgr888",
+		.id = XILINX_FRMBUF_FMT_RGB8,
+		.bpw = 24,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_BGR888,
+		.v4l2_fmt = V4L2_PIX_FMT_RGB24,
+		.fmt_bitmask = BIT(15),
+	},
+	{
+		.dts_name = "y8",
+		.id = XILINX_FRMBUF_FMT_Y8,
+		.bpw = 32,
+		.ppw = 4,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_Y8,
+		.v4l2_fmt = V4L2_PIX_FMT_GREY,
+		.fmt_bitmask = BIT(16),
+	},
+	{
+		.dts_name = "y10",
+		.id = XILINX_FRMBUF_FMT_Y10,
+		.bpw = 32,
+		.ppw = 3,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_Y10,
+		.v4l2_fmt = V4L2_PIX_FMT_XY10,
+		.fmt_bitmask = BIT(17),
+	},
+	{
+		.dts_name = "rgb888",
+		.id = XILINX_FRMBUF_FMT_BGR8,
+		.bpw = 24,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_RGB888,
+		.v4l2_fmt = V4L2_PIX_FMT_BGR24,
+		.fmt_bitmask = BIT(18),
+	},
+	{
+		.dts_name = "abgr8888",
+		.id = XILINX_FRMBUF_FMT_RGBA8,
+		.bpw = 32,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_ABGR8888,
+		.v4l2_fmt = 0,
+		.fmt_bitmask = BIT(19),
+	},
+	{
+		.dts_name = "argb8888",
+		.id = XILINX_FRMBUF_FMT_BGRA8,
+		.bpw = 32,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_ARGB8888,
+		.v4l2_fmt = 0,
+		.fmt_bitmask = BIT(20),
+	},
+	{
+		.dts_name = "avuy8888",
+		.id = XILINX_FRMBUF_FMT_YUVA8,
+		.bpw = 32,
+		.ppw = 1,
+		.num_planes = 1,
+		.drm_fmt = DRM_FORMAT_AVUY,
+		.v4l2_fmt = 0,
+		.fmt_bitmask = BIT(21),
+	},
+	{
+		.dts_name = "xbgr4121212",
+		.id = XILINX_FRMBUF_FMT_RGBX12,
+		.bpw = 40,
+		.ppw = 1,
+		.num_planes = 1,
+		.v4l2_fmt = V4L2_PIX_FMT_XBGR40,
+		.fmt_bitmask = BIT(22),
+	},
+	{
+		.dts_name = "rgb16",
+		.id = XILINX_FRMBUF_FMT_RGB16,
+		.bpw = 48,
+		.ppw = 1,
+		.num_planes = 1,
+		.v4l2_fmt = V4L2_PIX_FMT_BGR48,
+		.fmt_bitmask = BIT(23),
+	},
+	{
+		.dts_name = "y_u_v8",
+		.id = XILINX_FRMBUF_FMT_Y_U_V8,
+		.bpw = 32,
+		.ppw = 4,
+		.num_planes = 3,
+		.v4l2_fmt = V4L2_PIX_FMT_YUV444M,
+		.drm_fmt = DRM_FORMAT_YUV444,
+		.fmt_bitmask = BIT(24),
+	},
+};
+
+/**
+ * struct xilinx_frmbuf_feature - dt or IP property structure
+ * @direction: dma transfer mode and direction
+ * @flags: Bitmask of properties enabled in IP or dt
+ */
+struct xilinx_frmbuf_feature {
+	enum dma_transfer_direction direction;
+	u32 flags;
+};
+
+/**
+ * struct xilinx_frmbuf_device - dma device structure
+ * @regs: I/O mapped base address
+ * @dev: Device Structure
+ * @common: DMA device structure
+ * @chan: Driver specific dma channel
+ * @rst_gpio: GPIO reset
+ * @enabled_vid_fmts: Bitmask of video formats enabled in hardware
+ * @drm_memory_fmts: Array of supported DRM fourcc codes
+ * @drm_fmt_cnt: Count of supported DRM fourcc codes
+ * @v4l2_memory_fmts: Array of supported V4L2 fourcc codes
+ * @v4l2_fmt_cnt: Count of supported V4L2 fourcc codes
+ * @cfg: Pointer to Framebuffer Feature config struct
+ * @max_width: Maximum pixel width supported in IP.
+ * @max_height: Maximum number of lines supported in IP.
+ * @ppc: Pixels per clock supported in IP.
+ * @ap_clk: Video core clock
+ */
+struct xilinx_frmbuf_device {
+	void __iomem *regs;
+	struct device *dev;
+	struct dma_device common;
+	struct xilinx_frmbuf_chan chan;
+	struct gpio_desc *rst_gpio;
+	u32 enabled_vid_fmts;
+	u32 drm_memory_fmts[ARRAY_SIZE(xilinx_frmbuf_formats)];
+	u32 drm_fmt_cnt;
+	u32 v4l2_memory_fmts[ARRAY_SIZE(xilinx_frmbuf_formats)];
+	u32 v4l2_fmt_cnt;
+	const struct xilinx_frmbuf_feature *cfg;
+	u32 max_width;
+	u32 max_height;
+	u32 ppc;
+	struct clk *ap_clk;
+};
+
+static const struct xilinx_frmbuf_feature xlnx_fbwr_cfg_v20 = {
+	.direction = DMA_DEV_TO_MEM,
+};
+
+static const struct xilinx_frmbuf_feature xlnx_fbwr_cfg_v21 = {
+	.direction = DMA_DEV_TO_MEM,
+	.flags = XILINX_PPC_PROP | XILINX_FLUSH_PROP
+		| XILINX_FID_PROP | XILINX_CLK_PROP,
+};
+
+static const struct xilinx_frmbuf_feature xlnx_fbwr_cfg_v22 = {
+	.direction = DMA_DEV_TO_MEM,
+	.flags = XILINX_PPC_PROP | XILINX_FLUSH_PROP
+		| XILINX_FID_PROP | XILINX_CLK_PROP
+		| XILINX_THREE_PLANES_PROP,
+};
+
+static const struct xilinx_frmbuf_feature xlnx_fbrd_cfg_v20 = {
+	.direction = DMA_MEM_TO_DEV,
+};
+
+static const struct xilinx_frmbuf_feature xlnx_fbrd_cfg_v21 = {
+	.direction = DMA_MEM_TO_DEV,
+	.flags = XILINX_PPC_PROP | XILINX_FLUSH_PROP
+		| XILINX_FID_PROP | XILINX_CLK_PROP,
+};
+
+static const struct xilinx_frmbuf_feature xlnx_fbrd_cfg_v22 = {
+	.direction = DMA_MEM_TO_DEV,
+	.flags = XILINX_PPC_PROP | XILINX_FLUSH_PROP
+		| XILINX_FID_PROP | XILINX_CLK_PROP
+		| XILINX_THREE_PLANES_PROP
+		| XILINX_FID_ERR_DETECT_PROP,
+};
+
+static const struct of_device_id xilinx_frmbuf_of_ids[] = {
+	{ .compatible = "xlnx,axi-frmbuf-wr-v2",
+		.data = (void *)&xlnx_fbwr_cfg_v20},
+	{ .compatible = "xlnx,axi-frmbuf-wr-v2.1",
+		.data = (void *)&xlnx_fbwr_cfg_v21},
+	{ .compatible = "xlnx,axi-frmbuf-wr-v2.2",
+		.data = (void *)&xlnx_fbwr_cfg_v22},
+	{ .compatible = "xlnx,axi-frmbuf-rd-v2",
+		.data = (void *)&xlnx_fbrd_cfg_v20},
+	{ .compatible = "xlnx,axi-frmbuf-rd-v2.1",
+		.data = (void *)&xlnx_fbrd_cfg_v21},
+	{ .compatible = "xlnx,axi-frmbuf-rd-v2.2",
+		.data = (void *)&xlnx_fbrd_cfg_v22},
+	{/* end of list */}
+};
+
+/******************************PROTOTYPES*************************************/
+#define to_xilinx_chan(chan) \
+	container_of(chan, struct xilinx_frmbuf_chan, common)
+#define to_dma_tx_descriptor(tx) \
+	container_of(tx, struct xilinx_frmbuf_tx_descriptor, async_tx)
+
+static inline u32 frmbuf_read(struct xilinx_frmbuf_chan *chan, u32 reg)
+{
+	return ioread32(chan->xdev->regs + reg);
+}
+
+static inline void frmbuf_write(struct xilinx_frmbuf_chan *chan, u32 reg,
+				u32 value)
+{
+	iowrite32(value, chan->xdev->regs + reg);
+}
+
+static inline void frmbuf_writeq(struct xilinx_frmbuf_chan *chan, u32 reg,
+				 u64 value)
+{
+	iowrite32(lower_32_bits(value), chan->xdev->regs + reg);
+	iowrite32(upper_32_bits(value), chan->xdev->regs + reg + 4);
+}
+
+static void writeq_addr(struct xilinx_frmbuf_chan *chan, u32 reg,
+			dma_addr_t addr)
+{
+	frmbuf_writeq(chan, reg, (u64)addr);
+}
+
+static void write_addr(struct xilinx_frmbuf_chan *chan, u32 reg,
+		       dma_addr_t addr)
+{
+	frmbuf_write(chan, reg, addr);
+}
+
+static inline void frmbuf_clr(struct xilinx_frmbuf_chan *chan, u32 reg,
+			      u32 clr)
+{
+	frmbuf_write(chan, reg, frmbuf_read(chan, reg) & ~clr);
+}
+
+static inline void frmbuf_set(struct xilinx_frmbuf_chan *chan, u32 reg,
+			      u32 set)
+{
+	frmbuf_write(chan, reg, frmbuf_read(chan, reg) | set);
+}
+
+static void frmbuf_init_format_array(struct xilinx_frmbuf_device *xdev)
+{
+	u32 i, cnt;
+
+	for (i = 0; i < ARRAY_SIZE(xilinx_frmbuf_formats); i++) {
+		if (!(xdev->enabled_vid_fmts &
+		      xilinx_frmbuf_formats[i].fmt_bitmask))
+			continue;
+
+		if (xilinx_frmbuf_formats[i].drm_fmt) {
+			cnt = xdev->drm_fmt_cnt++;
+			xdev->drm_memory_fmts[cnt] =
+				xilinx_frmbuf_formats[i].drm_fmt;
+		}
+
+		if (xilinx_frmbuf_formats[i].v4l2_fmt) {
+			cnt = xdev->v4l2_fmt_cnt++;
+			xdev->v4l2_memory_fmts[cnt] =
+				xilinx_frmbuf_formats[i].v4l2_fmt;
+		}
+	}
+}
+
+static struct xilinx_frmbuf_chan *frmbuf_find_chan(struct dma_chan *chan)
+{
+	struct xilinx_frmbuf_chan *xil_chan;
+	bool found_xchan = false;
+
+	mutex_lock(&frmbuf_chan_list_lock);
+	list_for_each_entry(xil_chan, &frmbuf_chan_list, chan_node) {
+		if (chan == &xil_chan->common) {
+			found_xchan = true;
+			break;
+		}
+	}
+	mutex_unlock(&frmbuf_chan_list_lock);
+
+	if (!found_xchan) {
+		dev_dbg(chan->device->dev,
+			"dma chan not a Video Framebuffer channel instance\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	return xil_chan;
+}
+
+static struct xilinx_frmbuf_device *frmbuf_find_dev(struct dma_chan *chan)
+{
+	struct xilinx_frmbuf_chan *xchan, *temp;
+	struct xilinx_frmbuf_device *xdev;
+	bool is_frmbuf_chan = false;
+
+	list_for_each_entry_safe(xchan, temp, &frmbuf_chan_list, chan_node) {
+		if (chan == &xchan->common)
+			is_frmbuf_chan = true;
+	}
+
+	if (!is_frmbuf_chan)
+		return ERR_PTR(-ENODEV);
+
+	xchan = to_xilinx_chan(chan);
+	xdev = container_of(xchan, struct xilinx_frmbuf_device, chan);
+
+	return xdev;
+}
+
+static int frmbuf_verify_format(struct dma_chan *chan, u32 fourcc, u32 type)
+{
+	struct xilinx_frmbuf_chan *xil_chan = to_xilinx_chan(chan);
+	u32 i, sz = ARRAY_SIZE(xilinx_frmbuf_formats);
+
+	for (i = 0; i < sz; i++) {
+		if ((type == XDMA_DRM &&
+		     fourcc != xilinx_frmbuf_formats[i].drm_fmt) ||
+		   (type == XDMA_V4L2 &&
+		    fourcc != xilinx_frmbuf_formats[i].v4l2_fmt))
+			continue;
+
+		if (!(xilinx_frmbuf_formats[i].fmt_bitmask &
+		      xil_chan->xdev->enabled_vid_fmts))
+			return -EINVAL;
+
+		/*
+		 * The Alpha color formats are supported in Framebuffer Read
+		 * IP only as corresponding DRM formats.
+		 */
+		if (type == XDMA_DRM &&
+		    (xilinx_frmbuf_formats[i].drm_fmt == DRM_FORMAT_ABGR8888 ||
+		     xilinx_frmbuf_formats[i].drm_fmt == DRM_FORMAT_ARGB8888 ||
+		     xilinx_frmbuf_formats[i].drm_fmt == DRM_FORMAT_AVUY) &&
+		    xil_chan->direction != DMA_MEM_TO_DEV)
+			return -EINVAL;
+
+		xil_chan->vid_fmt = &xilinx_frmbuf_formats[i];
+		return 0;
+	}
+	return -EINVAL;
+}
+
+static void xilinx_xdma_set_config(struct dma_chan *chan, u32 fourcc, u32 type)
+{
+	struct xilinx_frmbuf_chan *xil_chan;
+	struct xilinx_frmbuf_device *xdev;
+	const struct xilinx_frmbuf_format_desc *old_vid_fmt;
+	int ret;
+
+	xil_chan = frmbuf_find_chan(chan);
+	if (IS_ERR(xil_chan))
+		return;
+
+	xdev = frmbuf_find_dev(chan);
+	if (IS_ERR(xdev))
+		return;
+
+	/* Save old video format */
+	old_vid_fmt = xil_chan->vid_fmt;
+
+	ret = frmbuf_verify_format(chan, fourcc, type);
+	if (ret == -EINVAL) {
+		dev_err(chan->device->dev,
+			"Framebuffer not configured for fourcc 0x%x\n",
+			fourcc);
+		return;
+	}
+
+	if ((!(xdev->cfg->flags & XILINX_THREE_PLANES_PROP)) &&
+	    xil_chan->vid_fmt->id == XILINX_FRMBUF_FMT_Y_U_V8) {
+		dev_err(chan->device->dev, "doesn't support %s format\n",
+			xil_chan->vid_fmt->dts_name);
+		/* Restore to old video format */
+		xil_chan->vid_fmt = old_vid_fmt;
+		return;
+	}
+}
+
+void xilinx_xdma_set_mode(struct dma_chan *chan, enum operation_mode
+			  mode)
+{
+	struct xilinx_frmbuf_chan *xil_chan;
+
+	xil_chan = frmbuf_find_chan(chan);
+	if (IS_ERR(xil_chan))
+		return;
+
+	xil_chan->mode = mode;
+
+	return;
+
+} EXPORT_SYMBOL_GPL(xilinx_xdma_set_mode);
+
+void xilinx_xdma_drm_config(struct dma_chan *chan, u32 drm_fourcc)
+{
+	xilinx_xdma_set_config(chan, drm_fourcc, XDMA_DRM);
+
+} EXPORT_SYMBOL_GPL(xilinx_xdma_drm_config);
+
+void xilinx_xdma_v4l2_config(struct dma_chan *chan, u32 v4l2_fourcc)
+{
+	xilinx_xdma_set_config(chan, v4l2_fourcc, XDMA_V4L2);
+
+} EXPORT_SYMBOL_GPL(xilinx_xdma_v4l2_config);
+
+int xilinx_xdma_get_drm_vid_fmts(struct dma_chan *chan, u32 *fmt_cnt,
+				 u32 **fmts)
+{
+	struct xilinx_frmbuf_device *xdev;
+
+	xdev = frmbuf_find_dev(chan);
+
+	if (IS_ERR(xdev))
+		return PTR_ERR(xdev);
+
+	*fmt_cnt = xdev->drm_fmt_cnt;
+	*fmts = xdev->drm_memory_fmts;
+
+	return 0;
+}
+EXPORT_SYMBOL(xilinx_xdma_get_drm_vid_fmts);
+
+int xilinx_xdma_get_v4l2_vid_fmts(struct dma_chan *chan, u32 *fmt_cnt,
+				  u32 **fmts)
+{
+	struct xilinx_frmbuf_device *xdev;
+
+	xdev = frmbuf_find_dev(chan);
+
+	if (IS_ERR(xdev))
+		return PTR_ERR(xdev);
+
+	*fmt_cnt = xdev->v4l2_fmt_cnt;
+	*fmts = xdev->v4l2_memory_fmts;
+
+	return 0;
+}
+EXPORT_SYMBOL(xilinx_xdma_get_v4l2_vid_fmts);
+
+int xilinx_xdma_get_fid(struct dma_chan *chan,
+			struct dma_async_tx_descriptor *async_tx, u32 *fid)
+{
+	struct xilinx_frmbuf_device *xdev;
+	struct xilinx_frmbuf_tx_descriptor *desc;
+
+	xdev = frmbuf_find_dev(chan);
+	if (IS_ERR(xdev))
+		return PTR_ERR(xdev);
+
+	if (!async_tx || !fid)
+		return -EINVAL;
+
+	if (xdev->chan.direction != DMA_DEV_TO_MEM)
+		return -EINVAL;
+
+	desc = to_dma_tx_descriptor(async_tx);
+	if (!desc)
+		return -EINVAL;
+
+	*fid = desc->fid;
+	return 0;
+}
+EXPORT_SYMBOL(xilinx_xdma_get_fid);
+
+int xilinx_xdma_set_fid(struct dma_chan *chan,
+			struct dma_async_tx_descriptor *async_tx, u32 fid)
+{
+	struct xilinx_frmbuf_device *xdev;
+	struct xilinx_frmbuf_tx_descriptor *desc;
+
+	if (fid > 1 || !async_tx)
+		return -EINVAL;
+
+	xdev = frmbuf_find_dev(chan);
+	if (IS_ERR(xdev))
+		return PTR_ERR(xdev);
+
+	if (xdev->chan.direction != DMA_MEM_TO_DEV)
+		return -EINVAL;
+
+	desc = to_dma_tx_descriptor(async_tx);
+	if (!desc)
+		return -EINVAL;
+
+	desc->fid = fid;
+	return 0;
+}
+EXPORT_SYMBOL(xilinx_xdma_set_fid);
+
+int xilinx_xdma_get_fid_err_flag(struct dma_chan *chan,
+				 u32 *fid_err_flag)
+{
+	struct xilinx_frmbuf_device *xdev;
+
+	xdev = frmbuf_find_dev(chan);
+	if (IS_ERR(xdev))
+		return PTR_ERR(xdev);
+
+	if (xdev->chan.direction != DMA_DEV_TO_MEM || !xdev->chan.idle)
+		return -EINVAL;
+
+	*fid_err_flag = xdev->chan.fid_err_flag;
+
+	return 0;
+}
+EXPORT_SYMBOL(xilinx_xdma_get_fid_err_flag);
+
+int xilinx_xdma_get_fid_out(struct dma_chan *chan,
+			    u32 *fid_out_val)
+{
+	struct xilinx_frmbuf_device *xdev;
+
+	xdev = frmbuf_find_dev(chan);
+	if (IS_ERR(xdev))
+		return PTR_ERR(xdev);
+
+	if (xdev->chan.direction != DMA_DEV_TO_MEM || !xdev->chan.idle)
+		return -EINVAL;
+
+	*fid_out_val = xdev->chan.fid_out_val;
+
+	return 0;
+}
+EXPORT_SYMBOL(xilinx_xdma_get_fid_out);
+
+int xilinx_xdma_get_width_align(struct dma_chan *chan, u32 *width_align)
+{
+	struct xilinx_frmbuf_device *xdev;
+
+	xdev = frmbuf_find_dev(chan);
+	if (IS_ERR(xdev))
+		return PTR_ERR(xdev);
+	*width_align = xdev->ppc;
+
+	return 0;
+}
+EXPORT_SYMBOL(xilinx_xdma_get_width_align);
+
+int xilinx_xdma_get_earlycb(struct dma_chan *chan,
+			    struct dma_async_tx_descriptor *async_tx,
+			    u32 *earlycb)
+{
+	struct xilinx_frmbuf_device *xdev;
+	struct xilinx_frmbuf_tx_descriptor *desc;
+
+	xdev = frmbuf_find_dev(chan);
+	if (IS_ERR(xdev))
+		return PTR_ERR(xdev);
+
+	if (!async_tx || !earlycb)
+		return -EINVAL;
+
+	desc = to_dma_tx_descriptor(async_tx);
+	if (!desc)
+		return -EINVAL;
+
+	*earlycb = desc->earlycb;
+	return 0;
+}
+EXPORT_SYMBOL(xilinx_xdma_get_earlycb);
+
+int xilinx_xdma_set_earlycb(struct dma_chan *chan,
+			    struct dma_async_tx_descriptor *async_tx,
+			    u32 earlycb)
+{
+	struct xilinx_frmbuf_device *xdev;
+	struct xilinx_frmbuf_tx_descriptor *desc;
+
+	if (!async_tx)
+		return -EINVAL;
+
+	xdev = frmbuf_find_dev(chan);
+	if (IS_ERR(xdev))
+		return PTR_ERR(xdev);
+
+	desc = to_dma_tx_descriptor(async_tx);
+	if (!desc)
+		return -EINVAL;
+
+	desc->earlycb = earlycb;
+	return 0;
+}
+EXPORT_SYMBOL(xilinx_xdma_set_earlycb);
+
+/**
+ * of_dma_xilinx_xlate - Translation function
+ * @dma_spec: Pointer to DMA specifier as found in the device tree
+ * @ofdma: Pointer to DMA controller data
+ *
+ * Return: DMA channel pointer on success or error code on error
+ */
+static struct dma_chan *of_dma_xilinx_xlate(struct of_phandle_args *dma_spec,
+					    struct of_dma *ofdma)
+{
+	struct xilinx_frmbuf_device *xdev = ofdma->of_dma_data;
+
+	return dma_get_slave_channel(&xdev->chan.common);
+}
+
+/* -----------------------------------------------------------------------------
+ * Descriptors alloc and free
+ */
+
+/**
+ * xilinx_frmbuf_alloc_tx_descriptor - Allocate transaction descriptor
+ * @chan: Driver specific dma channel
+ *
+ * Return: The allocated descriptor on success and NULL on failure.
+ */
+static struct xilinx_frmbuf_tx_descriptor *
+xilinx_frmbuf_alloc_tx_descriptor(struct xilinx_frmbuf_chan *chan)
+{
+	struct xilinx_frmbuf_tx_descriptor *desc;
+
+	desc = kzalloc(sizeof(*desc), GFP_KERNEL);
+	if (!desc)
+		return NULL;
+
+	return desc;
+}
+
+/**
+ * xilinx_frmbuf_free_desc_list - Free descriptors list
+ * @chan: Driver specific dma channel
+ * @list: List to parse and delete the descriptor
+ */
+static void xilinx_frmbuf_free_desc_list(struct xilinx_frmbuf_chan *chan,
+					 struct list_head *list)
+{
+	struct xilinx_frmbuf_tx_descriptor *desc, *next;
+
+	list_for_each_entry_safe(desc, next, list, node) {
+		list_del(&desc->node);
+		kfree(desc);
+	}
+}
+
+/**
+ * xilinx_frmbuf_free_descriptors - Free channel descriptors
+ * @chan: Driver specific dma channel
+ */
+static void xilinx_frmbuf_free_descriptors(struct xilinx_frmbuf_chan *chan)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	xilinx_frmbuf_free_desc_list(chan, &chan->pending_list);
+	xilinx_frmbuf_free_desc_list(chan, &chan->done_list);
+	kfree(chan->active_desc);
+	kfree(chan->staged_desc);
+
+	chan->staged_desc = NULL;
+	chan->active_desc = NULL;
+	INIT_LIST_HEAD(&chan->pending_list);
+	INIT_LIST_HEAD(&chan->done_list);
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+/**
+ * xilinx_frmbuf_free_chan_resources - Free channel resources
+ * @dchan: DMA channel
+ */
+static void xilinx_frmbuf_free_chan_resources(struct dma_chan *dchan)
+{
+	struct xilinx_frmbuf_chan *chan = to_xilinx_chan(dchan);
+
+	xilinx_frmbuf_free_descriptors(chan);
+}
+
+/**
+ * xilinx_frmbuf_chan_desc_cleanup - Clean channel descriptors
+ * @chan: Driver specific dma channel
+ */
+static void xilinx_frmbuf_chan_desc_cleanup(struct xilinx_frmbuf_chan *chan)
+{
+	struct xilinx_frmbuf_tx_descriptor *desc, *next;
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	list_for_each_entry_safe(desc, next, &chan->done_list, node) {
+		dma_async_tx_callback callback;
+		void *callback_param;
+
+		list_del(&desc->node);
+
+		/* Run the link descriptor callback function */
+		callback = desc->async_tx.callback;
+		callback_param = desc->async_tx.callback_param;
+		if (callback) {
+			spin_unlock_irqrestore(&chan->lock, flags);
+			callback(callback_param);
+			spin_lock_irqsave(&chan->lock, flags);
+		}
+
+		/* Run any dependencies, then free the descriptor */
+		dma_run_dependencies(&desc->async_tx);
+		kfree(desc);
+	}
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+/**
+ * xilinx_frmbuf_do_tasklet - Schedule completion tasklet
+ * @data: Pointer to the Xilinx frmbuf channel structure
+ */
+static void xilinx_frmbuf_do_tasklet(unsigned long data)
+{
+	struct xilinx_frmbuf_chan *chan = (struct xilinx_frmbuf_chan *)data;
+
+	xilinx_frmbuf_chan_desc_cleanup(chan);
+}
+
+/**
+ * xilinx_frmbuf_alloc_chan_resources - Allocate channel resources
+ * @dchan: DMA channel
+ *
+ * Return: '0' on success and failure value on error
+ */
+static int xilinx_frmbuf_alloc_chan_resources(struct dma_chan *dchan)
+{
+	dma_cookie_init(dchan);
+
+	return 0;
+}
+
+/**
+ * xilinx_frmbuf_tx_status - Get frmbuf transaction status
+ * @dchan: DMA channel
+ * @cookie: Transaction identifier
+ * @txstate: Transaction state
+ *
+ * Return: fmrbuf transaction status
+ */
+static enum dma_status xilinx_frmbuf_tx_status(struct dma_chan *dchan,
+					       dma_cookie_t cookie,
+					       struct dma_tx_state *txstate)
+{
+	return dma_cookie_status(dchan, cookie, txstate);
+}
+
+/**
+ * xilinx_frmbuf_halt - Halt frmbuf channel
+ * @chan: Driver specific dma channel
+ */
+static void xilinx_frmbuf_halt(struct xilinx_frmbuf_chan *chan)
+{
+	frmbuf_clr(chan, XILINX_FRMBUF_CTRL_OFFSET,
+		   XILINX_FRMBUF_CTRL_AP_START | chan->mode);
+	chan->idle = true;
+}
+
+/**
+ * xilinx_frmbuf_start - Start dma channel
+ * @chan: Driver specific dma channel
+ */
+static void xilinx_frmbuf_start(struct xilinx_frmbuf_chan *chan)
+{
+	frmbuf_set(chan, XILINX_FRMBUF_CTRL_OFFSET,
+		   XILINX_FRMBUF_CTRL_AP_START | chan->mode);
+	chan->idle = false;
+}
+
+/**
+ * xilinx_frmbuf_complete_descriptor - Mark the active descriptor as complete
+ * This function is invoked with spinlock held
+ * @chan : xilinx frmbuf channel
+ *
+ * CONTEXT: hardirq
+ */
+static void xilinx_frmbuf_complete_descriptor(struct xilinx_frmbuf_chan *chan)
+{
+	struct xilinx_frmbuf_tx_descriptor *desc = chan->active_desc;
+
+	/*
+	 * In case of frame buffer write, read the fid register
+	 * and associate it with descriptor
+	 */
+	if (chan->direction == DMA_DEV_TO_MEM && chan->hw_fid)
+		desc->fid = frmbuf_read(chan, XILINX_FRMBUF_FID_OFFSET) &
+			    XILINX_FRMBUF_FID_MASK;
+
+	dma_cookie_complete(&desc->async_tx);
+	list_add_tail(&desc->node, &chan->done_list);
+}
+
+/**
+ * xilinx_frmbuf_start_transfer - Starts frmbuf transfer
+ * @chan: Driver specific channel struct pointer
+ */
+static void xilinx_frmbuf_start_transfer(struct xilinx_frmbuf_chan *chan)
+{
+	struct xilinx_frmbuf_tx_descriptor *desc;
+	struct xilinx_frmbuf_device *xdev;
+
+	xdev = container_of(chan, struct xilinx_frmbuf_device, chan);
+
+	if (!chan->idle)
+		return;
+
+	if (chan->staged_desc) {
+		chan->active_desc = chan->staged_desc;
+		chan->staged_desc = NULL;
+	}
+
+	if (list_empty(&chan->pending_list))
+		return;
+
+	desc = list_first_entry(&chan->pending_list,
+				struct xilinx_frmbuf_tx_descriptor,
+				node);
+
+	if (desc->earlycb == EARLY_CALLBACK_START_DESC) {
+		dma_async_tx_callback callback;
+		void *callback_param;
+
+		callback = desc->async_tx.callback;
+		callback_param = desc->async_tx.callback_param;
+		if (callback) {
+			callback(callback_param);
+			desc->async_tx.callback = NULL;
+			chan->active_desc = desc;
+		}
+	}
+
+	/* Start the transfer */
+	chan->write_addr(chan, XILINX_FRMBUF_ADDR_OFFSET,
+			 desc->hw.luma_plane_addr);
+	chan->write_addr(chan, XILINX_FRMBUF_ADDR2_OFFSET,
+			 desc->hw.chroma_plane_addr[0]);
+	if (xdev->cfg->flags & XILINX_THREE_PLANES_PROP) {
+		if (chan->direction == DMA_MEM_TO_DEV)
+			chan->write_addr(chan, XILINX_FRMBUF_RD_ADDR3_OFFSET,
+					 desc->hw.chroma_plane_addr[1]);
+		else
+			chan->write_addr(chan, XILINX_FRMBUF_ADDR3_OFFSET,
+					 desc->hw.chroma_plane_addr[1]);
+	}
+
+	/* HW expects these parameters to be same for one transaction */
+	frmbuf_write(chan, XILINX_FRMBUF_WIDTH_OFFSET, desc->hw.hsize);
+	frmbuf_write(chan, XILINX_FRMBUF_STRIDE_OFFSET, desc->hw.stride);
+	frmbuf_write(chan, XILINX_FRMBUF_HEIGHT_OFFSET, desc->hw.vsize);
+	frmbuf_write(chan, XILINX_FRMBUF_FMT_OFFSET, chan->vid_fmt->id);
+
+	/* If it is framebuffer read IP set the FID */
+	if (chan->direction == DMA_MEM_TO_DEV && chan->hw_fid)
+		frmbuf_write(chan, XILINX_FRMBUF_FID_OFFSET, desc->fid);
+
+	/* Start the hardware */
+	xilinx_frmbuf_start(chan);
+	list_del(&desc->node);
+
+	/* No staging descriptor required when auto restart is disabled */
+	if (chan->mode == AUTO_RESTART)
+		chan->staged_desc = desc;
+	else
+		chan->active_desc = desc;
+}
+
+/**
+ * xilinx_frmbuf_issue_pending - Issue pending transactions
+ * @dchan: DMA channel
+ */
+static void xilinx_frmbuf_issue_pending(struct dma_chan *dchan)
+{
+	struct xilinx_frmbuf_chan *chan = to_xilinx_chan(dchan);
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+	xilinx_frmbuf_start_transfer(chan);
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+/**
+ * xilinx_frmbuf_reset - Reset frmbuf channel
+ * @chan: Driver specific dma channel
+ */
+static void xilinx_frmbuf_reset(struct xilinx_frmbuf_chan *chan)
+{
+	/* reset ip */
+	gpiod_set_value(chan->xdev->rst_gpio, 1);
+	udelay(1);
+	gpiod_set_value(chan->xdev->rst_gpio, 0);
+}
+
+/**
+ * xilinx_frmbuf_chan_reset - Reset frmbuf channel and enable interrupts
+ * @chan: Driver specific frmbuf channel
+ */
+static void xilinx_frmbuf_chan_reset(struct xilinx_frmbuf_chan *chan)
+{
+	xilinx_frmbuf_reset(chan);
+	frmbuf_write(chan, XILINX_FRMBUF_IE_OFFSET, XILINX_FRMBUF_IE_AP_DONE);
+	frmbuf_write(chan, XILINX_FRMBUF_GIE_OFFSET, XILINX_FRMBUF_GIE_EN);
+	chan->fid_err_flag = 0;
+	chan->fid_out_val = 0;
+}
+
+/**
+ * xilinx_frmbuf_irq_handler - frmbuf Interrupt handler
+ * @irq: IRQ number
+ * @data: Pointer to the Xilinx frmbuf channel structure
+ *
+ * Return: IRQ_HANDLED/IRQ_NONE
+ */
+static irqreturn_t xilinx_frmbuf_irq_handler(int irq, void *data)
+{
+	struct xilinx_frmbuf_chan *chan = data;
+	u32 status;
+	dma_async_tx_callback callback = NULL;
+	void *callback_param;
+	struct xilinx_frmbuf_tx_descriptor *desc;
+
+	status = frmbuf_read(chan, XILINX_FRMBUF_ISR_OFFSET);
+	if (!(status & XILINX_FRMBUF_ISR_ALL_IRQ_MASK))
+		return IRQ_NONE;
+
+	frmbuf_write(chan, XILINX_FRMBUF_ISR_OFFSET,
+		     status & XILINX_FRMBUF_ISR_ALL_IRQ_MASK);
+
+	/* Check if callback function needs to be called early */
+	desc = chan->staged_desc;
+	if (desc && desc->earlycb == EARLY_CALLBACK) {
+		callback = desc->async_tx.callback;
+		callback_param = desc->async_tx.callback_param;
+		if (callback) {
+			callback(callback_param);
+			desc->async_tx.callback = NULL;
+		}
+	}
+
+	if (status & XILINX_FRMBUF_ISR_AP_DONE_IRQ) {
+		spin_lock(&chan->lock);
+		chan->idle = true;
+		if (chan->active_desc) {
+			xilinx_frmbuf_complete_descriptor(chan);
+			chan->active_desc = NULL;
+		}
+
+		/* Update fid err detect flag and out value */
+		if (chan->direction == DMA_MEM_TO_DEV &&
+		    chan->hw_fid && chan->idle &&
+		    chan->xdev->cfg->flags & XILINX_FID_ERR_DETECT_PROP) {
+			if (chan->mode == AUTO_RESTART)
+				chan->fid_mode = FID_MODE_2;
+			else
+				chan->fid_mode = FID_MODE_1;
+
+			frmbuf_write(chan, XILINX_FRMBUF_FID_MODE_OFFSET,
+				     chan->fid_mode);
+			dev_dbg(chan->xdev->dev, "fid mode = %d\n",
+				frmbuf_read(chan, XILINX_FRMBUF_FID_MODE_OFFSET));
+
+			chan->fid_err_flag = frmbuf_read(chan,
+							 XILINX_FRMBUF_FID_ERR_OFFSET) &
+							XILINX_FRMBUF_FID_ERR_MASK;
+			chan->fid_out_val = frmbuf_read(chan,
+							XILINX_FRMBUF_FID_OUT_OFFSET) &
+							XILINX_FRMBUF_FID_OUT_MASK;
+			dev_dbg(chan->xdev->dev, "fid err cnt = 0x%x\n",
+				frmbuf_read(chan, XILINX_FRMBUF_FID_ERR_OFFSET));
+		}
+
+		xilinx_frmbuf_start_transfer(chan);
+		spin_unlock(&chan->lock);
+	}
+
+	tasklet_schedule(&chan->tasklet);
+	return IRQ_HANDLED;
+}
+
+/**
+ * xilinx_frmbuf_tx_submit - Submit DMA transaction
+ * @tx: Async transaction descriptor
+ *
+ * Return: cookie value on success and failure value on error
+ */
+static dma_cookie_t xilinx_frmbuf_tx_submit(struct dma_async_tx_descriptor *tx)
+{
+	struct xilinx_frmbuf_tx_descriptor *desc = to_dma_tx_descriptor(tx);
+	struct xilinx_frmbuf_chan *chan = to_xilinx_chan(tx->chan);
+	dma_cookie_t cookie;
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+	cookie = dma_cookie_assign(tx);
+	list_add_tail(&desc->node, &chan->pending_list);
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return cookie;
+}
+
+/**
+ * xilinx_frmbuf_dma_prep_interleaved - prepare a descriptor for a
+ *	DMA_SLAVE transaction
+ * @dchan: DMA channel
+ * @xt: Interleaved template pointer
+ * @flags: transfer ack flags
+ *
+ * Return: Async transaction descriptor on success and NULL on failure
+ */
+static struct dma_async_tx_descriptor *
+xilinx_frmbuf_dma_prep_interleaved(struct dma_chan *dchan,
+				   struct dma_interleaved_template *xt,
+				   unsigned long flags)
+{
+	struct xilinx_frmbuf_chan *chan = to_xilinx_chan(dchan);
+	struct xilinx_frmbuf_tx_descriptor *desc;
+	struct xilinx_frmbuf_desc_hw *hw;
+	u32 vsize, hsize;
+
+	if (chan->direction != xt->dir || !chan->vid_fmt)
+		goto error;
+
+	if (!xt->numf || !xt->sgl[0].size)
+		goto error;
+
+	if (xt->frame_size != chan->vid_fmt->num_planes)
+		goto error;
+
+	vsize = xt->numf;
+	hsize = (xt->sgl[0].size * chan->vid_fmt->ppw * 8) /
+		 chan->vid_fmt->bpw;
+	/* hsize calc should not have resulted in an odd number */
+	if (hsize & 1)
+		hsize++;
+
+	if (vsize > chan->xdev->max_height || hsize > chan->xdev->max_width) {
+		dev_dbg(chan->xdev->dev,
+			"vsize %d max vsize %d hsize %d max hsize %d\n",
+			vsize, chan->xdev->max_height, hsize,
+			chan->xdev->max_width);
+		dev_err(chan->xdev->dev, "Requested size not supported!\n");
+		goto error;
+	}
+
+	desc = xilinx_frmbuf_alloc_tx_descriptor(chan);
+	if (!desc)
+		return NULL;
+
+	dma_async_tx_descriptor_init(&desc->async_tx, &chan->common);
+	desc->async_tx.tx_submit = xilinx_frmbuf_tx_submit;
+	async_tx_ack(&desc->async_tx);
+
+	hw = &desc->hw;
+	hw->vsize = xt->numf;
+	hw->stride = xt->sgl[0].icg + xt->sgl[0].size;
+	hw->hsize = (xt->sgl[0].size * chan->vid_fmt->ppw * 8) /
+		     chan->vid_fmt->bpw;
+
+	/* hsize calc should not have resulted in an odd number */
+	if (hw->hsize & 1)
+		hw->hsize++;
+
+	if (chan->direction == DMA_MEM_TO_DEV) {
+		hw->luma_plane_addr = xt->src_start;
+		if (xt->frame_size == 2 || xt->frame_size == 3)
+			hw->chroma_plane_addr[0] =
+				xt->src_start +
+				xt->numf * hw->stride +
+				xt->sgl[0].src_icg;
+		if (xt->frame_size == 3)
+			hw->chroma_plane_addr[1] =
+				hw->chroma_plane_addr[0] +
+				xt->numf * hw->stride +
+				xt->sgl[0].src_icg;
+	} else {
+		hw->luma_plane_addr = xt->dst_start;
+		if (xt->frame_size == 2 || xt->frame_size == 3)
+			hw->chroma_plane_addr[0] =
+				xt->dst_start +
+				xt->numf * hw->stride +
+				xt->sgl[0].dst_icg;
+		if (xt->frame_size == 3)
+			hw->chroma_plane_addr[1] =
+				hw->chroma_plane_addr[0] +
+				xt->numf * hw->stride +
+				xt->sgl[0].dst_icg;
+	}
+
+	return &desc->async_tx;
+
+error:
+	dev_err(chan->xdev->dev,
+		"Invalid dma template or missing dma video fmt config\n");
+	return NULL;
+}
+
+/**
+ * xilinx_frmbuf_terminate_all - Halt the channel and free descriptors
+ * @dchan: Driver specific dma channel pointer
+ *
+ * Return: 0
+ */
+static int xilinx_frmbuf_terminate_all(struct dma_chan *dchan)
+{
+	struct xilinx_frmbuf_chan *chan = to_xilinx_chan(dchan);
+
+	xilinx_frmbuf_halt(chan);
+	xilinx_frmbuf_free_descriptors(chan);
+	/* worst case frame-to-frame boundary; ensure frame output complete */
+	msleep(50);
+
+	if (chan->xdev->cfg->flags & XILINX_FLUSH_PROP) {
+		u8 count;
+
+		/*
+		 * Flush the framebuffer FIFO and
+		 * wait for max 50ms for flush done
+		 */
+		frmbuf_set(chan, XILINX_FRMBUF_CTRL_OFFSET,
+			   XILINX_FRMBUF_CTRL_FLUSH);
+		for (count = WAIT_FOR_FLUSH_DONE; count > 0; count--) {
+			if (frmbuf_read(chan, XILINX_FRMBUF_CTRL_OFFSET) &
+					XILINX_FRMBUF_CTRL_FLUSH_DONE)
+				break;
+			usleep_range(2000, 2100);
+		}
+
+		if (!count)
+			dev_err(chan->xdev->dev, "Framebuffer Flush not done!\n");
+	}
+
+	xilinx_frmbuf_chan_reset(chan);
+
+	return 0;
+}
+
+/**
+ * xilinx_frmbuf_synchronize - kill tasklet to stop further descr processing
+ * @dchan: Driver specific dma channel pointer
+ */
+static void xilinx_frmbuf_synchronize(struct dma_chan *dchan)
+{
+	struct xilinx_frmbuf_chan *chan = to_xilinx_chan(dchan);
+
+	tasklet_kill(&chan->tasklet);
+}
+
+/* -----------------------------------------------------------------------------
+ * Probe and remove
+ */
+
+/**
+ * xilinx_frmbuf_chan_remove - Per Channel remove function
+ * @chan: Driver specific dma channel
+ */
+static void xilinx_frmbuf_chan_remove(struct xilinx_frmbuf_chan *chan)
+{
+	/* Disable all interrupts */
+	frmbuf_clr(chan, XILINX_FRMBUF_IE_OFFSET,
+		   XILINX_FRMBUF_ISR_ALL_IRQ_MASK);
+
+	tasklet_kill(&chan->tasklet);
+	list_del(&chan->common.device_node);
+
+	mutex_lock(&frmbuf_chan_list_lock);
+	list_del(&chan->chan_node);
+	mutex_unlock(&frmbuf_chan_list_lock);
+}
+
+/**
+ * xilinx_frmbuf_chan_probe - Per Channel Probing
+ * It get channel features from the device tree entry and
+ * initialize special channel handling routines
+ *
+ * @xdev: Driver specific device structure
+ * @node: Device node
+ *
+ * Return: '0' on success and failure value on error
+ */
+static int xilinx_frmbuf_chan_probe(struct xilinx_frmbuf_device *xdev,
+				    struct device_node *node)
+{
+	struct xilinx_frmbuf_chan *chan;
+	int err;
+	u32 dma_addr_size = 0;
+
+	chan = &xdev->chan;
+
+	chan->dev = xdev->dev;
+	chan->xdev = xdev;
+	chan->idle = true;
+	chan->fid_err_flag = 0;
+	chan->fid_out_val = 0;
+	chan->mode = AUTO_RESTART;
+
+	err = of_property_read_u32(node, "xlnx,dma-addr-width",
+				   &dma_addr_size);
+	if (err || (dma_addr_size != 32 && dma_addr_size != 64)) {
+		dev_err(xdev->dev, "missing or invalid addr width dts prop\n");
+		return err;
+	}
+
+	if (dma_addr_size == 64 && sizeof(dma_addr_t) == sizeof(u64))
+		chan->write_addr = writeq_addr;
+	else
+		chan->write_addr = write_addr;
+
+	if (xdev->cfg->flags & XILINX_FID_PROP)
+		chan->hw_fid = of_property_read_bool(node, "xlnx,fid");
+
+	spin_lock_init(&chan->lock);
+	INIT_LIST_HEAD(&chan->pending_list);
+	INIT_LIST_HEAD(&chan->done_list);
+
+	chan->irq = irq_of_parse_and_map(node, 0);
+	err = devm_request_irq(xdev->dev, chan->irq, xilinx_frmbuf_irq_handler,
+			       IRQF_SHARED, "xilinx_framebuffer", chan);
+
+	if (err) {
+		dev_err(xdev->dev, "unable to request IRQ %d\n", chan->irq);
+		return err;
+	}
+
+	tasklet_init(&chan->tasklet, xilinx_frmbuf_do_tasklet,
+		     (unsigned long)chan);
+
+	/*
+	 * Initialize the DMA channel and add it to the DMA engine channels
+	 * list.
+	 */
+	chan->common.device = &xdev->common;
+
+	list_add_tail(&chan->common.device_node, &xdev->common.channels);
+
+	mutex_lock(&frmbuf_chan_list_lock);
+	list_add_tail(&chan->chan_node, &frmbuf_chan_list);
+	mutex_unlock(&frmbuf_chan_list_lock);
+
+	xilinx_frmbuf_chan_reset(chan);
+
+	return 0;
+}
+
+/**
+ * xilinx_frmbuf_probe - Driver probe function
+ * @pdev: Pointer to the platform_device structure
+ *
+ * Return: '0' on success and failure value on error
+ */
+static int xilinx_frmbuf_probe(struct platform_device *pdev)
+{
+	struct device_node *node = pdev->dev.of_node;
+	struct xilinx_frmbuf_device *xdev;
+	struct resource *io;
+	enum dma_transfer_direction dma_dir;
+	const struct of_device_id *match;
+	int err;
+	u32 i, j, align, max_width, max_height;
+	int hw_vid_fmt_cnt;
+	const char *vid_fmts[ARRAY_SIZE(xilinx_frmbuf_formats)];
+
+	xdev = devm_kzalloc(&pdev->dev, sizeof(*xdev), GFP_KERNEL);
+	if (!xdev)
+		return -ENOMEM;
+
+	xdev->dev = &pdev->dev;
+
+	match = of_match_node(xilinx_frmbuf_of_ids, node);
+	if (!match)
+		return -ENODEV;
+
+	xdev->cfg = match->data;
+
+	dma_dir = (enum dma_transfer_direction)xdev->cfg->direction;
+
+	if (xdev->cfg->flags & XILINX_CLK_PROP) {
+		xdev->ap_clk = devm_clk_get(xdev->dev, "ap_clk");
+		if (IS_ERR(xdev->ap_clk)) {
+			err = PTR_ERR(xdev->ap_clk);
+			dev_err(xdev->dev, "failed to get ap_clk (%d)\n", err);
+			return err;
+		}
+	} else {
+		dev_info(xdev->dev, "assuming clock is enabled!\n");
+	}
+
+	xdev->rst_gpio = devm_gpiod_get(&pdev->dev, "reset",
+					GPIOD_OUT_HIGH);
+	if (IS_ERR(xdev->rst_gpio)) {
+		err = PTR_ERR(xdev->rst_gpio);
+		if (err == -EPROBE_DEFER)
+			dev_info(&pdev->dev,
+				 "Probe deferred due to GPIO reset defer\n");
+		else
+			dev_err(&pdev->dev,
+				"Unable to locate reset property in dt\n");
+		return err;
+	}
+
+	gpiod_set_value_cansleep(xdev->rst_gpio, 0x0);
+
+	io = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	xdev->regs = devm_ioremap_resource(&pdev->dev, io);
+	if (IS_ERR(xdev->regs))
+		return PTR_ERR(xdev->regs);
+
+	if (xdev->cfg->flags & XILINX_THREE_PLANES_PROP)
+		max_height = 8640;
+	else
+		max_height = 4320;
+
+	err = of_property_read_u32(node, "xlnx,max-height", &xdev->max_height);
+	if (err < 0) {
+		dev_err(xdev->dev, "xlnx,max-height is missing!");
+		return -EINVAL;
+	} else if (xdev->max_height > max_height ||
+		   xdev->max_height < XILINX_FRMBUF_MIN_HEIGHT) {
+		dev_err(&pdev->dev, "Invalid height in dt");
+		return -EINVAL;
+	}
+
+	if (xdev->cfg->flags & XILINX_THREE_PLANES_PROP)
+		max_width = 15360;
+	else
+		max_width = 8192;
+
+	err = of_property_read_u32(node, "xlnx,max-width", &xdev->max_width);
+	if (err < 0) {
+		dev_err(xdev->dev, "xlnx,max-width is missing!");
+		return -EINVAL;
+	} else if (xdev->max_width > max_width ||
+		   xdev->max_width < XILINX_FRMBUF_MIN_WIDTH) {
+		dev_err(&pdev->dev, "Invalid width in dt");
+		return -EINVAL;
+	}
+
+	/* Initialize the DMA engine */
+	if (xdev->cfg->flags & XILINX_PPC_PROP) {
+		err = of_property_read_u32(node, "xlnx,pixels-per-clock", &xdev->ppc);
+		if (err || (xdev->ppc != 1 && xdev->ppc != 2 &&
+			    xdev->ppc != 4 && xdev->ppc != 8)) {
+			dev_err(&pdev->dev, "missing or invalid pixels per clock dts prop\n");
+			return err;
+		}
+		err = of_property_read_u32(node, "xlnx,dma-align", &align);
+		if (err)
+			align = xdev->ppc * XILINX_FRMBUF_ALIGN_MUL;
+
+		if (align < (xdev->ppc * XILINX_FRMBUF_ALIGN_MUL) ||
+		    ffs(align) != fls(align)) {
+			dev_err(&pdev->dev, "invalid dma align dts prop\n");
+			return -EINVAL;
+		}
+	} else {
+		align = 16;
+	}
+
+	xdev->common.copy_align = (enum dmaengine_alignment)(fls(align) - 1);
+	xdev->common.dev = &pdev->dev;
+
+	if (xdev->cfg->flags & XILINX_CLK_PROP) {
+		err = clk_prepare_enable(xdev->ap_clk);
+		if (err) {
+			dev_err(&pdev->dev, " failed to enable ap_clk (%d)\n",
+				err);
+			return err;
+		}
+	}
+
+	INIT_LIST_HEAD(&xdev->common.channels);
+	dma_cap_set(DMA_SLAVE, xdev->common.cap_mask);
+	dma_cap_set(DMA_PRIVATE, xdev->common.cap_mask);
+
+	/* Initialize the channels */
+	err = xilinx_frmbuf_chan_probe(xdev, node);
+	if (err < 0)
+		goto disable_clk;
+
+	xdev->chan.direction = dma_dir;
+
+	if (xdev->chan.direction == DMA_DEV_TO_MEM) {
+		xdev->common.directions = BIT(DMA_DEV_TO_MEM);
+		dev_info(&pdev->dev, "Xilinx AXI frmbuf DMA_DEV_TO_MEM\n");
+	} else if (xdev->chan.direction == DMA_MEM_TO_DEV) {
+		xdev->common.directions = BIT(DMA_MEM_TO_DEV);
+		dev_info(&pdev->dev, "Xilinx AXI frmbuf DMA_MEM_TO_DEV\n");
+	} else {
+		err = -EINVAL;
+		goto remove_chan;
+	}
+
+	/* read supported video formats and update internal table */
+	hw_vid_fmt_cnt = of_property_count_strings(node, "xlnx,vid-formats");
+
+	err = of_property_read_string_array(node, "xlnx,vid-formats",
+					    vid_fmts, hw_vid_fmt_cnt);
+	if (err < 0) {
+		dev_err(&pdev->dev,
+			"Missing or invalid xlnx,vid-formats dts prop\n");
+		goto remove_chan;
+	}
+
+	for (i = 0; i < hw_vid_fmt_cnt; i++) {
+		const char *vid_fmt_name = vid_fmts[i];
+
+		for (j = 0; j < ARRAY_SIZE(xilinx_frmbuf_formats); j++) {
+			const char *dts_name =
+				xilinx_frmbuf_formats[j].dts_name;
+
+			if (strcmp(vid_fmt_name, dts_name))
+				continue;
+
+			xdev->enabled_vid_fmts |=
+				xilinx_frmbuf_formats[j].fmt_bitmask;
+		}
+	}
+
+	/* Determine supported vid framework formats */
+	frmbuf_init_format_array(xdev);
+
+	xdev->common.device_alloc_chan_resources =
+				xilinx_frmbuf_alloc_chan_resources;
+	xdev->common.device_free_chan_resources =
+				xilinx_frmbuf_free_chan_resources;
+	xdev->common.device_prep_interleaved_dma =
+				xilinx_frmbuf_dma_prep_interleaved;
+	xdev->common.device_terminate_all = xilinx_frmbuf_terminate_all;
+	xdev->common.device_synchronize = xilinx_frmbuf_synchronize;
+	xdev->common.device_tx_status = xilinx_frmbuf_tx_status;
+	xdev->common.device_issue_pending = xilinx_frmbuf_issue_pending;
+
+	platform_set_drvdata(pdev, xdev);
+
+	/* Register the DMA engine with the core */
+	dma_async_device_register(&xdev->common);
+
+	err = of_dma_controller_register(node, of_dma_xilinx_xlate, xdev);
+	if (err < 0) {
+		dev_err(&pdev->dev, "Unable to register DMA to DT\n");
+		goto error;
+	}
+
+	dev_info(&pdev->dev, "Xilinx AXI FrameBuffer Engine Driver Probed!!\n");
+
+	return 0;
+error:
+	dma_async_device_unregister(&xdev->common);
+remove_chan:
+	xilinx_frmbuf_chan_remove(&xdev->chan);
+disable_clk:
+	clk_disable_unprepare(xdev->ap_clk);
+	return err;
+}
+
+/**
+ * xilinx_frmbuf_remove - Driver remove function
+ * @pdev: Pointer to the platform_device structure
+ *
+ * Return: Always '0'
+ */
+static int xilinx_frmbuf_remove(struct platform_device *pdev)
+{
+	struct xilinx_frmbuf_device *xdev = platform_get_drvdata(pdev);
+
+	of_dma_controller_free(pdev->dev.of_node);
+
+	dma_async_device_unregister(&xdev->common);
+	xilinx_frmbuf_chan_remove(&xdev->chan);
+	clk_disable_unprepare(xdev->ap_clk);
+
+	return 0;
+}
+
+MODULE_DEVICE_TABLE(of, xilinx_frmbuf_of_ids);
+
+static struct platform_driver xilinx_frmbuf_driver = {
+	.driver = {
+		.name = "xilinx-frmbuf",
+		.of_match_table = xilinx_frmbuf_of_ids,
+	},
+	.probe = xilinx_frmbuf_probe,
+	.remove = xilinx_frmbuf_remove,
+};
+
+module_platform_driver(xilinx_frmbuf_driver);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_DESCRIPTION("Xilinx Framebuffer driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/xilinx_ps_pcie.h b/drivers/dma/xilinx/xilinx_ps_pcie.h
new file mode 100644
index 000000000..81d634d15
--- /dev/null
+++ b/drivers/dma/xilinx/xilinx_ps_pcie.h
@@ -0,0 +1,44 @@
+/*
+ * Xilinx PS PCIe DMA Engine platform header file
+ *
+ * Copyright (C) 2010-2017 Xilinx, Inc. All rights reserved.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation
+ */
+
+#ifndef __XILINX_PS_PCIE_H
+#define __XILINX_PS_PCIE_H
+
+#include <linux/delay.h>
+#include <linux/dma-direction.h>
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/ioport.h>
+#include <linux/irqreturn.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mempool.h>
+#include <linux/of.h>
+#include <linux/pci.h>
+#include <linux/property.h>
+#include <linux/platform_device.h>
+#include <linux/timer.h>
+#include <linux/dma/xilinx_ps_pcie_dma.h>
+
+/**
+ * dma_platform_driver_register - This will be invoked by module init
+ *
+ * Return: returns status of platform_driver_register
+ */
+int dma_platform_driver_register(void);
+/**
+ * dma_platform_driver_unregister - This will be invoked by module exit
+ *
+ * Return: returns void after unregustering platform driver
+ */
+void dma_platform_driver_unregister(void);
+
+#endif
diff --git a/drivers/dma/xilinx/xilinx_ps_pcie_dma_client.c b/drivers/dma/xilinx/xilinx_ps_pcie_dma_client.c
new file mode 100644
index 000000000..299613383
--- /dev/null
+++ b/drivers/dma/xilinx/xilinx_ps_pcie_dma_client.c
@@ -0,0 +1,1402 @@
+/*
+ * XILINX PS PCIe DMA Engine test module
+ *
+ * Copyright (C) 2017 Xilinx, Inc. All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/cdev.h>
+#include <linux/dma-direction.h>
+#include <linux/dmaengine.h>
+#include <linux/dma-mapping.h>
+#include <linux/kdev_t.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pci_ids.h>
+#include <linux/pagemap.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/version.h>
+#include <linux/dma/xilinx_ps_pcie_dma.h>
+
+#include "../dmaengine.h"
+
+#define DRV_MODULE_NAME		"ps_pcie_dma_client"
+
+#define DMA_SCRATCH0_REG_OFFSET   (0x50)
+#define DMA_SCRATCH1_REG_OFFSET   (0x54)
+#define DMA_AXI_INTR_ASSRT_REG_OFFSET   (0x74)
+
+#define DMA_SW_INTR_ASSRT_BIT      BIT(3)
+
+#define DMA_BAR_NUMBER 0
+
+#define CHAR_DRIVER_NAME               "ps_pcie_dmachan"
+
+#define PIO_CHAR_DRIVER_NAME           "ps_pcie_pio"
+#define EP_TRANSLATION_CHECK            0xCCCCCCCC
+
+#define PIO_MEMORY_BAR_NUMBER            2
+
+#define XPIO_CLIENT_MAGIC 'P'
+#define IOCTL_EP_CHECK_TRANSLATION     _IO(XPIO_CLIENT_MAGIC, 0x01)
+
+#define XPS_PCIE_DMA_CLIENT_MAGIC 'S'
+
+#define IGET_ASYNC_TRANSFERINFO   _IO(XPS_PCIE_DMA_CLIENT_MAGIC, 0x01)
+#define ISET_ASYNC_TRANSFERINFO   _IO(XPS_PCIE_DMA_CLIENT_MAGIC, 0x02)
+
+#define DMA_TRANSACTION_SUCCESSFUL 1
+#define DMA_TRANSACTION_FAILURE    0
+
+#define MAX_LIST 1024
+
+struct dma_transfer_info {
+	char __user *buff_address;
+	unsigned int buff_size;
+	loff_t    offset;
+	enum dma_data_direction direction;
+};
+
+struct buff_info {
+	bool status;
+	unsigned int buff_size;
+	char __user *buff_address;
+};
+
+struct usrbuff_info {
+	struct buff_info buff_list[MAX_LIST];
+	unsigned int expected;
+};
+
+enum pio_status {
+	PIO_SUPPORTED = 0,
+	PIO_NOT_SUPPORTED
+};
+
+enum dma_transfer_mode {
+	MEMORY_MAPPED = 0,
+	STREAMING
+};
+
+struct dma_deviceproperties {
+	u16     pci_vendorid;
+	u16     pci_deviceid;
+	u16     board_number;
+	enum pio_status pio_transfers;
+	enum dma_transfer_mode mode;
+	enum dma_data_direction direction[MAX_ALLOWED_CHANNELS_IN_HW];
+};
+
+struct xlnx_completed_info {
+	struct list_head clist;
+	struct buff_info buffer;
+};
+
+struct xlnx_ps_pcie_dma_client_channel {
+	struct device *dev;
+	struct dma_chan *chan;
+	struct ps_pcie_dma_channel_match match;
+	enum dma_data_direction direction;
+	enum dma_transfer_mode mode;
+	struct xlnx_completed_info completed;
+	spinlock_t channel_lock; /* Lock to serialize transfers on channel */
+};
+
+struct xlnx_ps_pcie_dma_client_device {
+	struct dma_deviceproperties *properties;
+
+	struct xlnx_ps_pcie_dma_client_channel
+		pcie_dma_chan[MAX_ALLOWED_CHANNELS_IN_HW];
+
+	dev_t char_device;
+	struct cdev xps_pcie_chardev;
+	struct device *chardev[MAX_ALLOWED_CHANNELS_IN_HW];
+
+	dev_t pio_char_device;
+	struct cdev xpio_char_dev;
+	struct device *xpio_char_device;
+	struct mutex  pio_chardev_mutex; /* Exclusive access to ioctl */
+	struct completion trans_cmpltn;
+	u32 pio_translation_size;
+
+	struct list_head dev_node;
+};
+
+struct xlnx_ps_pcie_dma_asynchronous_transaction {
+	dma_cookie_t cookie;
+	struct page **cache_pages;
+	unsigned int num_pages;
+	struct sg_table *sg;
+	struct xlnx_ps_pcie_dma_client_channel *chan;
+	struct xlnx_completed_info *buffer_info;
+	struct dma_async_tx_descriptor **txd;
+};
+
+static struct class *g_ps_pcie_dma_client_class; /* global device class */
+static struct list_head g_ps_pcie_dma_client_list;
+
+/*
+ * Keep adding to this list to interact with multiple DMA devices
+ */
+static struct dma_deviceproperties g_dma_deviceproperties_list[] = {
+		{
+			.pci_vendorid = PCI_VENDOR_ID_XILINX,
+			.pci_deviceid = ZYNQMP_DMA_DEVID,
+			.board_number = 0,
+			.pio_transfers = PIO_SUPPORTED,
+			.mode          = MEMORY_MAPPED,
+			/* Make sure the channel direction is same
+			 * as what is configured in DMA device
+			 */
+			.direction = {DMA_TO_DEVICE, DMA_FROM_DEVICE,
+					DMA_TO_DEVICE, DMA_FROM_DEVICE}
+		}
+};
+
+/**
+ * ps_pcie_dma_sync_transfer_cbk - Callback handler for Synchronous transfers.
+ * Handles both S2C and C2S transfer call backs.
+ * Indicates to blocked applications that DMA transfers are complete
+ *
+ * @data: Callback parameter
+ *
+ * Return: void
+ */
+static void ps_pcie_dma_sync_transfer_cbk(void *data)
+{
+	struct completion *compl = (struct completion *)data;
+
+	if (compl)
+		complete(compl);
+}
+
+/**
+ * initiate_sync_transfer - Programs both Source Q
+ * and Destination Q of channel after setting up sg lists and transaction
+ * specific data. This functions waits until transaction completion is notified
+ *
+ * @channel: Pointer to the PS PCIe DMA channel structure
+ * @buffer: User land virtual address containing data to be sent or received
+ * @length: Length of user land buffer
+ * @f_offset: AXI domain address to which data pointed by user buffer has to
+ *	      be sent/received from
+ * @direction: Transfer of data direction
+ *
+ * Return: 0 on success and non zero value for failure
+ */
+static ssize_t initiate_sync_transfer(
+			struct xlnx_ps_pcie_dma_client_channel *channel,
+			const char __user *buffer, size_t length,
+			loff_t *f_offset, enum dma_data_direction direction)
+{
+	int offset;
+	unsigned int alloc_pages;
+	unsigned long first, last, nents = 1;
+	struct page **cache_pages;
+	struct dma_chan *chan = NULL;
+	struct dma_device *device;
+	struct dma_async_tx_descriptor **txd = NULL;
+	dma_cookie_t cookie = 0;
+	enum dma_ctrl_flags flags = 0;
+	int err;
+	struct sg_table *sg;
+	enum dma_transfer_direction d_direction;
+	int i;
+	struct completion *cmpl_ptr;
+	enum dma_status status;
+	struct scatterlist *selem;
+	size_t elem_len = 0;
+
+	chan = channel->chan;
+	device = chan->device;
+
+	offset = offset_in_page(buffer);
+	first = ((unsigned long)buffer & PAGE_MASK) >> PAGE_SHIFT;
+	last = (((unsigned long)buffer + length - 1) & PAGE_MASK) >>
+		PAGE_SHIFT;
+	alloc_pages = (last - first) + 1;
+
+	cache_pages = devm_kzalloc(channel->dev,
+				   (alloc_pages * (sizeof(struct page *))),
+				   GFP_ATOMIC);
+	if (!cache_pages) {
+		dev_err(channel->dev,
+			"Unable to allocate memory for page table holder\n");
+		err = PTR_ERR(cache_pages);
+		goto err_out_cachepages_alloc;
+	}
+
+	err = get_user_pages_fast((unsigned long)buffer, alloc_pages,
+				  !(direction), cache_pages);
+	if (err <= 0) {
+		dev_err(channel->dev, "Unable to pin user pages\n");
+		err = PTR_ERR(cache_pages);
+		goto err_out_pin_pages;
+	} else if (err < alloc_pages) {
+		dev_err(channel->dev, "Only pinned few user pages %d\n", err);
+		err = PTR_ERR(cache_pages);
+		for (i = 0; i < err; i++)
+			put_page(cache_pages[i]);
+		goto err_out_pin_pages;
+	}
+
+	sg = devm_kzalloc(channel->dev, sizeof(struct sg_table), GFP_ATOMIC);
+	if (!sg) {
+		err = PTR_ERR(sg);
+		goto err_out_alloc_sg_table;
+	}
+
+	err = sg_alloc_table_from_pages(sg, cache_pages, alloc_pages, offset,
+					length, GFP_ATOMIC);
+	if (err < 0) {
+		dev_err(channel->dev, "Unable to create sg table\n");
+		goto err_out_sg_to_sgl;
+	}
+
+	err = dma_map_sg(channel->dev, sg->sgl, sg->nents, direction);
+	if (err == 0) {
+		dev_err(channel->dev, "Unable to map buffer to sg table\n");
+		err = PTR_ERR(sg);
+		goto err_out_dma_map_sg;
+	}
+
+	cmpl_ptr = devm_kzalloc(channel->dev, sizeof(struct completion),
+				GFP_ATOMIC);
+	if (!cmpl_ptr) {
+		err = PTR_ERR(cmpl_ptr);
+		goto err_out_cmpl_ptr;
+	}
+
+	init_completion(cmpl_ptr);
+
+	if (channel->mode == MEMORY_MAPPED)
+		nents = sg->nents;
+
+	txd = devm_kzalloc(channel->dev, sizeof(*txd)
+					* nents, GFP_ATOMIC);
+	if (!txd) {
+		err = PTR_ERR(txd);
+		goto err_out_cmpl_ptr;
+	}
+
+	if (channel->mode == MEMORY_MAPPED) {
+		for (i = 0, selem = (sg->sgl); i < sg->nents; i++,
+		     selem = sg_next(selem)) {
+			if ((i + 1) == sg->nents)
+				flags = DMA_PREP_INTERRUPT | DMA_CTRL_ACK;
+
+			if (direction == DMA_TO_DEVICE) {
+				txd[i] = device->device_prep_dma_memcpy(chan,
+					(dma_addr_t)(*f_offset) + elem_len,
+					selem->dma_address, selem->length,
+					flags);
+			} else {
+				txd[i] = device->device_prep_dma_memcpy(chan,
+					selem->dma_address,
+					(dma_addr_t)(*f_offset) + elem_len,
+					selem->length, flags);
+			}
+
+			elem_len += selem->length;
+
+			if (!txd[i]) {
+				err = PTR_ERR(txd[i]);
+				goto err_out_no_prep_sg_async_desc;
+			}
+		}
+	} else {
+		if (direction == DMA_TO_DEVICE)
+			d_direction = DMA_MEM_TO_DEV;
+		else
+			d_direction = DMA_DEV_TO_MEM;
+
+		flags = DMA_CTRL_ACK | DMA_PREP_INTERRUPT;
+
+		txd[0] = device->device_prep_slave_sg(chan, sg->sgl, sg->nents,
+						   d_direction, flags, NULL);
+		if (!txd[0]) {
+			err = PTR_ERR(txd[0]);
+			goto err_out_no_slave_sg_async_descriptor;
+		}
+	}
+
+	if (channel->mode == MEMORY_MAPPED) {
+		for (i = 0; i < sg->nents; i++) {
+			if ((i + 1) == sg->nents) {
+				txd[i]->callback =
+					ps_pcie_dma_sync_transfer_cbk;
+				txd[i]->callback_param = cmpl_ptr;
+			}
+
+			cookie = txd[i]->tx_submit(txd[i]);
+			if (dma_submit_error(cookie)) {
+				err = (int)cookie;
+				dev_err(channel->dev,
+					"Unable to submit transaction\n");
+				goto free_transaction;
+			}
+		}
+	} else {
+		txd[0]->callback = ps_pcie_dma_sync_transfer_cbk;
+		txd[0]->callback_param = cmpl_ptr;
+
+		cookie = txd[0]->tx_submit(txd[0]);
+		if (dma_submit_error(cookie)) {
+			err = (int)cookie;
+			dev_err(channel->dev,
+				"Unable to submit transaction\n");
+			goto free_transaction;
+		}
+	}
+
+	dma_async_issue_pending(chan);
+
+	wait_for_completion_killable(cmpl_ptr);
+
+	status = dmaengine_tx_status(chan, cookie, NULL);
+	if (status == DMA_COMPLETE)
+		err = length;
+	else
+		err = -1;
+
+	dma_unmap_sg(channel->dev, sg->sgl, sg->nents, direction);
+	devm_kfree(channel->dev, cmpl_ptr);
+	devm_kfree(channel->dev, txd);
+	sg_free_table(sg);
+	devm_kfree(channel->dev, sg);
+	for (i = 0; i < alloc_pages; i++)
+		put_page(cache_pages[i]);
+	devm_kfree(channel->dev, cache_pages);
+
+	return (ssize_t)err;
+
+free_transaction:
+err_out_no_prep_sg_async_desc:
+err_out_no_slave_sg_async_descriptor:
+	devm_kfree(channel->dev, cmpl_ptr);
+	devm_kfree(channel->dev, txd);
+err_out_cmpl_ptr:
+	dma_unmap_sg(channel->dev, sg->sgl, sg->nents, direction);
+err_out_dma_map_sg:
+	sg_free_table(sg);
+err_out_sg_to_sgl:
+	devm_kfree(channel->dev, sg);
+err_out_alloc_sg_table:
+	for (i = 0; i < alloc_pages; i++)
+		put_page(cache_pages[i]);
+err_out_pin_pages:
+	devm_kfree(channel->dev, cache_pages);
+err_out_cachepages_alloc:
+
+	return (ssize_t)err;
+}
+
+static ssize_t
+ps_pcie_dma_read(struct file *file,
+		 char __user *buffer,
+		 size_t length,
+		 loff_t *f_offset)
+{
+	struct xlnx_ps_pcie_dma_client_channel *chan;
+	ssize_t ret;
+
+	chan = file->private_data;
+
+	if (chan->direction != DMA_FROM_DEVICE) {
+		dev_err(chan->dev, "Invalid data direction for channel\n");
+		ret = -EINVAL;
+		goto c2s_err_direction;
+	}
+
+	ret = initiate_sync_transfer(chan, buffer, length, f_offset,
+				     DMA_FROM_DEVICE);
+
+	if (ret != length)
+		dev_dbg(chan->dev, "Read synchronous transfer unsuccessful\n");
+
+c2s_err_direction:
+	return ret;
+}
+
+static ssize_t
+ps_pcie_dma_write(struct file *file,
+		  const char __user *buffer,
+		  size_t length,
+		  loff_t *f_offset)
+{
+	struct xlnx_ps_pcie_dma_client_channel *chan;
+	ssize_t ret;
+
+	chan =   file->private_data;
+
+	if (chan->direction != DMA_TO_DEVICE) {
+		dev_err(chan->dev,
+			"Invalid data direction for channel\n");
+		ret = -EINVAL;
+		goto s2c_err_direction;
+	}
+
+	ret = initiate_sync_transfer(chan, buffer, length, f_offset,
+				     DMA_TO_DEVICE);
+
+	if (ret != length)
+		dev_dbg(chan->dev, "Write synchronous transfer unsuccessful\n");
+
+s2c_err_direction:
+	return ret;
+}
+
+static int ps_pcie_dma_open(struct inode *in, struct file *file)
+{
+	struct xlnx_ps_pcie_dma_client_device *xdev;
+	int minor_num = iminor(in);
+
+	xdev = container_of(in->i_cdev,
+			    struct xlnx_ps_pcie_dma_client_device,
+			    xps_pcie_chardev);
+
+	file->private_data = &xdev->pcie_dma_chan[minor_num];
+
+	return 0;
+}
+
+static int ps_pcie_dma_release(struct inode *in, struct file *filp)
+{
+	return 0;
+}
+
+static int update_completed_info(struct xlnx_ps_pcie_dma_client_channel *chan,
+				 struct usrbuff_info *usr_buff)
+{
+	int retval = 0;
+	unsigned int expected, count = 0;
+	struct xlnx_completed_info *entry;
+	struct xlnx_completed_info *next;
+
+	if (list_empty(&chan->completed.clist))
+		goto update_expected;
+
+	if (copy_from_user((void *)&expected,
+			   (void __user *)&usr_buff->expected,
+			   sizeof(unsigned int)) != 0) {
+		pr_err("Expected count copy failure\n");
+		retval = -ENXIO;
+		return retval;
+	}
+
+	if (expected > MAX_LIST) {
+		retval = -ENXIO;
+		return retval;
+	}
+
+	list_for_each_entry_safe(entry, next, &chan->completed.clist, clist) {
+		if (copy_to_user((void __user *)(usr_buff->buff_list + count),
+				 (void *)&entry->buffer,
+				 sizeof(struct buff_info)) != 0) {
+			pr_err("update user completed count copy failed\n");
+			retval = -ENXIO;
+			break;
+		}
+		count++;
+		spin_lock(&chan->channel_lock);
+		list_del(&entry->clist);
+		spin_unlock(&chan->channel_lock);
+		devm_kfree(chan->dev, entry);
+		if (count == expected)
+			break;
+	}
+
+update_expected:
+	if (copy_to_user((void __user *)&usr_buff->expected, (void *)&count,
+			 (sizeof(unsigned int))) != 0) {
+		pr_err("update user expected count copy failure\n");
+		retval = -ENXIO;
+	}
+
+	return retval;
+}
+
+/**
+ * ps_pcie_dma_async_transfer_cbk - Callback handler for Asynchronous transfers.
+ * Handles both S2C and C2S transfer call backs. Stores transaction information
+ * in a list for a user application to poll for this information
+ *
+ * @data: Callback parameter
+ *
+ * Return: void
+ */
+static void ps_pcie_dma_async_transfer_cbk(void *data)
+{
+	struct xlnx_ps_pcie_dma_asynchronous_transaction *trans =
+		(struct xlnx_ps_pcie_dma_asynchronous_transaction *)data;
+	enum dma_status status;
+	struct dma_tx_state state;
+	unsigned int i;
+
+	dma_unmap_sg(trans->chan->dev, trans->sg->sgl, trans->sg->nents,
+		     trans->chan->direction);
+	sg_free_table(trans->sg);
+	devm_kfree(trans->chan->dev, trans->sg);
+	devm_kfree(trans->chan->dev, trans->txd);
+	for (i = 0; i < trans->num_pages; i++)
+		put_page(trans->cache_pages[i]);
+	devm_kfree(trans->chan->dev, trans->cache_pages);
+
+	status = dmaengine_tx_status(trans->chan->chan, trans->cookie, &state);
+
+	if (status == DMA_COMPLETE)
+		trans->buffer_info->buffer.status = DMA_TRANSACTION_SUCCESSFUL;
+	else
+		trans->buffer_info->buffer.status = DMA_TRANSACTION_SUCCESSFUL;
+
+	spin_lock(&trans->chan->channel_lock);
+	list_add_tail(&trans->buffer_info->clist,
+		      &trans->chan->completed.clist);
+	spin_unlock(&trans->chan->channel_lock);
+	devm_kfree(trans->chan->dev, trans);
+}
+
+/**
+ * initiate_async_transfer - Programs both Source Q
+ * and Destination Q of channel after setting up sg lists and transaction
+ * specific data. This functions returns after setting up transfer
+ *
+ * @channel: Pointer to the PS PCIe DMA channel structure
+ * @buffer: User land virtual address containing data to be sent or received
+ * @length: Length of user land buffer
+ * @f_offset: AXI domain address to which data pointed by user buffer has to
+ *	      be sent/received from
+ * @direction: Transfer of data direction
+ *
+ * Return: 0 on success and non zero value for failure
+ */
+static int initiate_async_transfer(
+		struct xlnx_ps_pcie_dma_client_channel *channel,
+		char __user *buffer, size_t length, loff_t *f_offset,
+		enum dma_data_direction direction)
+{
+	int offset;
+	unsigned int alloc_pages;
+	unsigned long first, last, nents = 1;
+	struct page **cache_pages;
+	struct dma_chan *chan = NULL;
+	struct dma_device *device;
+	struct dma_async_tx_descriptor **txd = NULL;
+	dma_cookie_t cookie;
+	enum dma_ctrl_flags flags = 0;
+	struct xlnx_ps_pcie_dma_asynchronous_transaction *trans;
+	int err;
+	struct sg_table *sg;
+	enum dma_transfer_direction d_direction;
+	int i;
+	struct scatterlist *selem;
+	size_t elem_len = 0;
+
+	chan = channel->chan;
+	device = chan->device;
+
+	offset = offset_in_page(buffer);
+	first = ((unsigned long)buffer & PAGE_MASK) >> PAGE_SHIFT;
+	last = (((unsigned long)buffer + length - 1) & PAGE_MASK) >>
+		PAGE_SHIFT;
+	alloc_pages = (last - first) + 1;
+
+	cache_pages = devm_kzalloc(channel->dev,
+				   (alloc_pages * (sizeof(struct page *))),
+				   GFP_ATOMIC);
+	if (!cache_pages) {
+		err = PTR_ERR(cache_pages);
+		goto err_out_cachepages_alloc;
+	}
+
+	err = get_user_pages_fast((unsigned long)buffer, alloc_pages,
+				  !(direction), cache_pages);
+	if (err <= 0) {
+		dev_err(channel->dev, "Unable to pin user pages\n");
+		err = PTR_ERR(cache_pages);
+		goto err_out_pin_pages;
+	} else if (err < alloc_pages) {
+		dev_err(channel->dev, "Only pinned few user pages %d\n", err);
+		err = PTR_ERR(cache_pages);
+		for (i = 0; i < err; i++)
+			put_page(cache_pages[i]);
+		goto err_out_pin_pages;
+	}
+
+	sg = devm_kzalloc(channel->dev, sizeof(struct sg_table), GFP_ATOMIC);
+	if (!sg) {
+		err = PTR_ERR(sg);
+		goto err_out_alloc_sg_table;
+	}
+
+	err = sg_alloc_table_from_pages(sg, cache_pages, alloc_pages, offset,
+					length, GFP_ATOMIC);
+	if (err < 0) {
+		dev_err(channel->dev, "Unable to create sg table\n");
+		goto err_out_sg_to_sgl;
+	}
+
+	err = dma_map_sg(channel->dev, sg->sgl, sg->nents, direction);
+	if (err == 0) {
+		dev_err(channel->dev,
+			"Unable to map user buffer to sg table\n");
+		err = PTR_ERR(sg);
+		goto err_out_dma_map_sg;
+	}
+
+	trans = devm_kzalloc(channel->dev, sizeof(*trans), GFP_ATOMIC);
+	if (!trans) {
+		err = PTR_ERR(trans);
+		goto err_out_trans_ptr;
+	}
+
+	trans->buffer_info = devm_kzalloc(channel->dev,
+					  sizeof(struct xlnx_completed_info),
+					  GFP_ATOMIC);
+
+	if (!trans->buffer_info) {
+		err = PTR_ERR(trans->buffer_info);
+		goto err_out_no_completion_info;
+	}
+
+	if (channel->mode == MEMORY_MAPPED)
+		nents = sg->nents;
+
+	txd = devm_kzalloc(channel->dev,
+			   sizeof(*txd) * nents, GFP_ATOMIC);
+	if (!txd) {
+		err = PTR_ERR(txd);
+		goto err_out_no_completion_info;
+	}
+
+	trans->txd = txd;
+
+	if (channel->mode == MEMORY_MAPPED) {
+		for (i = 0, selem = (sg->sgl); i < sg->nents; i++,
+		     selem = sg_next(selem)) {
+			if ((i + 1) == sg->nents)
+				flags = DMA_PREP_INTERRUPT | DMA_CTRL_ACK;
+
+			if (direction == DMA_TO_DEVICE) {
+				txd[i] = device->device_prep_dma_memcpy(chan,
+					(dma_addr_t)(*f_offset) + elem_len,
+					selem->dma_address, selem->length,
+					flags);
+			} else {
+				txd[i] = device->device_prep_dma_memcpy(chan,
+					selem->dma_address,
+					(dma_addr_t)(*f_offset) + elem_len,
+					selem->length, flags);
+			}
+
+			elem_len += selem->length;
+
+			if (!txd[i]) {
+				err = PTR_ERR(txd[i]);
+				goto err_out_no_prep_sg_async_desc;
+			}
+		}
+	} else {
+		if (direction == DMA_TO_DEVICE)
+			d_direction = DMA_MEM_TO_DEV;
+		else
+			d_direction = DMA_DEV_TO_MEM;
+
+		flags = DMA_CTRL_ACK | DMA_PREP_INTERRUPT;
+
+		txd[0] = device->device_prep_slave_sg(chan, sg->sgl, sg->nents,
+						   d_direction, flags, NULL);
+		if (!txd[0]) {
+			err = PTR_ERR(txd[0]);
+			goto err_out_no_slave_sg_async_descriptor;
+		}
+	}
+
+	trans->buffer_info->buffer.buff_address = buffer;
+	trans->buffer_info->buffer.buff_size = length;
+	trans->cache_pages = cache_pages;
+	trans->num_pages   = alloc_pages;
+	trans->chan = channel;
+	trans->sg = sg;
+
+	if (channel->mode == MEMORY_MAPPED) {
+		for (i = 0; i < sg->nents; i++) {
+			cookie = txd[i]->tx_submit(txd[i]);
+			if (dma_submit_error(cookie)) {
+				err = (int)cookie;
+				dev_err(channel->dev,
+					"Unable to submit transaction\n");
+				goto free_transaction;
+			}
+
+			if ((i + 1) == sg->nents) {
+				txd[i]->callback =
+					ps_pcie_dma_async_transfer_cbk;
+				txd[i]->callback_param = trans;
+				trans->cookie = cookie;
+			}
+		}
+
+	} else {
+		txd[0]->callback = ps_pcie_dma_async_transfer_cbk;
+		txd[0]->callback_param = trans;
+
+		cookie = txd[0]->tx_submit(txd[0]);
+		if (dma_submit_error(cookie)) {
+			err = (int)cookie;
+			dev_err(channel->dev,
+				"Unable to submit transaction\n");
+			goto free_transaction;
+		}
+
+		trans->cookie = cookie;
+	}
+
+	dma_async_issue_pending(chan);
+
+	return length;
+
+free_transaction:
+err_out_no_prep_sg_async_desc:
+err_out_no_slave_sg_async_descriptor:
+	devm_kfree(channel->dev, trans->buffer_info);
+	devm_kfree(channel->dev, txd);
+err_out_no_completion_info:
+	devm_kfree(channel->dev, trans);
+err_out_trans_ptr:
+	dma_unmap_sg(channel->dev, sg->sgl, sg->nents, direction);
+err_out_dma_map_sg:
+	sg_free_table(sg);
+err_out_sg_to_sgl:
+	devm_kfree(channel->dev, sg);
+err_out_alloc_sg_table:
+	for (i = 0; i < alloc_pages; i++)
+		put_page(cache_pages[i]);
+err_out_pin_pages:
+	devm_kfree(channel->dev, cache_pages);
+err_out_cachepages_alloc:
+
+	return err;
+}
+
+static long ps_pcie_dma_ioctl(struct file *filp, unsigned int cmd,
+			      unsigned long arg)
+{
+	int retval = 0;
+	struct xlnx_ps_pcie_dma_client_channel *chan;
+	struct dma_transfer_info transfer_info;
+
+	if (_IOC_TYPE(cmd) != XPS_PCIE_DMA_CLIENT_MAGIC)
+		return -ENOTTY;
+
+	chan = filp->private_data;
+
+	switch (cmd) {
+	case ISET_ASYNC_TRANSFERINFO:
+		if (copy_from_user((void *)&transfer_info,
+				   (void __user *)arg,
+				   sizeof(struct dma_transfer_info)) != 0) {
+			pr_err("Copy from user asynchronous params\n");
+			retval = -ENXIO;
+			return retval;
+		}
+		if (transfer_info.direction != chan->direction) {
+			retval = -EINVAL;
+			return retval;
+		}
+		retval = initiate_async_transfer(chan,
+						 transfer_info.buff_address,
+						 transfer_info.buff_size,
+						 &transfer_info.offset,
+						 transfer_info.direction);
+		break;
+	case IGET_ASYNC_TRANSFERINFO:
+		retval = update_completed_info(chan,
+					       (struct usrbuff_info *)arg);
+		break;
+	default:
+		pr_err("Unsupported ioctl command received\n");
+		retval = -1;
+	}
+
+	return (long)retval;
+}
+
+static const struct file_operations ps_pcie_dma_comm_fops = {
+	.owner		= THIS_MODULE,
+	.read		= ps_pcie_dma_read,
+	.write		= ps_pcie_dma_write,
+	.unlocked_ioctl = ps_pcie_dma_ioctl,
+	.open		= ps_pcie_dma_open,
+	.release	= ps_pcie_dma_release,
+};
+
+static void pio_sw_intr_cbk(void *data)
+{
+	struct completion *compl = (struct completion *)data;
+
+	if (compl)
+		complete(compl);
+}
+
+static long pio_ioctl(struct file *filp, unsigned int cmd,
+		      unsigned long arg)
+{
+	char *bar_memory = NULL;
+	u32 translation_size = 0;
+	long err = 0;
+	struct dma_async_tx_descriptor *intr_txd = NULL;
+	dma_cookie_t cookie;
+	struct dma_chan *chan = NULL;
+	struct dma_device *device;
+	enum dma_ctrl_flags flags;
+	struct xlnx_ps_pcie_dma_client_device *xdev;
+	struct ps_pcie_dma_channel_match *xlnx_match;
+	struct BAR_PARAMS *barinfo;
+
+	xdev = filp->private_data;
+	chan = xdev->pcie_dma_chan[0].chan;
+	device = chan->device;
+	flags = DMA_CTRL_ACK | DMA_PREP_INTERRUPT;
+
+	xlnx_match =
+		(struct ps_pcie_dma_channel_match *)chan->private;
+
+	barinfo = ((struct BAR_PARAMS *)(xlnx_match->bar_params) +
+			DMA_BAR_NUMBER);
+	bar_memory = (__force char *)barinfo->BAR_VIRT_ADDR;
+
+	xdev = filp->private_data;
+
+	switch (cmd) {
+	case IOCTL_EP_CHECK_TRANSLATION:
+
+		mutex_lock(&xdev->pio_chardev_mutex);
+		reinit_completion(&xdev->trans_cmpltn);
+
+		intr_txd = device->device_prep_dma_interrupt(chan, flags);
+		if (!intr_txd) {
+			err = -EAGAIN;
+			mutex_unlock(&xdev->pio_chardev_mutex);
+			return err;
+		}
+
+		intr_txd->callback       = pio_sw_intr_cbk;
+		intr_txd->callback_param = &xdev->trans_cmpltn;
+
+		cookie = intr_txd->tx_submit(intr_txd);
+		if (dma_submit_error(cookie)) {
+			err =  cookie;
+			pr_err("Unable to submit interrupt transaction\n");
+			mutex_unlock(&xdev->pio_chardev_mutex);
+			return err;
+		}
+
+		dma_async_issue_pending(chan);
+
+		iowrite32(EP_TRANSLATION_CHECK, (void __iomem *)(bar_memory +
+						DMA_SCRATCH0_REG_OFFSET));
+		iowrite32(DMA_SW_INTR_ASSRT_BIT, (void __iomem *)(bar_memory +
+						DMA_AXI_INTR_ASSRT_REG_OFFSET));
+
+		wait_for_completion_interruptible(&xdev->trans_cmpltn);
+		translation_size = ioread32((void __iomem *)bar_memory +
+					    DMA_SCRATCH1_REG_OFFSET);
+		if (translation_size > 0)
+			xdev->pio_translation_size = translation_size;
+		else
+			err = -EAGAIN;
+		iowrite32(0, (void __iomem *)(bar_memory +
+					    DMA_SCRATCH1_REG_OFFSET));
+		mutex_unlock(&xdev->pio_chardev_mutex);
+		break;
+
+	default:
+		err = -EINVAL;
+	}
+	return err;
+}
+
+static ssize_t
+pio_read(struct file *file, char __user *buffer, size_t length,
+	 loff_t *f_offset)
+{
+	char *bar_memory = NULL;
+	struct xlnx_ps_pcie_dma_client_device *xdev;
+	struct ps_pcie_dma_channel_match *xlnx_match;
+	ssize_t num_bytes = 0;
+	struct BAR_PARAMS *barinfo;
+
+	xdev = file->private_data;
+	xlnx_match = (struct ps_pcie_dma_channel_match *)
+				xdev->pcie_dma_chan[0].chan->private;
+
+	barinfo = ((struct BAR_PARAMS *)(xlnx_match->bar_params) +
+			PIO_MEMORY_BAR_NUMBER);
+	bar_memory = (__force char *)barinfo->BAR_VIRT_ADDR;
+
+	if (length > xdev->pio_translation_size) {
+		pr_err("Error! Invalid buffer length supplied at PIO read\n");
+		num_bytes = -1;
+		return num_bytes;
+	}
+
+	if ((length + *f_offset)
+			> xdev->pio_translation_size) {
+		pr_err("Error! Invalid buffer offset supplied at PIO read\n");
+		num_bytes = -1;
+		return num_bytes;
+	}
+
+	bar_memory += *f_offset;
+
+	num_bytes = copy_to_user(buffer, bar_memory, length);
+	if (num_bytes != 0) {
+		pr_err("Error! copy_to_user failed at PIO read\n");
+		num_bytes = length - num_bytes;
+	} else {
+		num_bytes = length;
+	}
+
+	return num_bytes;
+}
+
+static ssize_t
+pio_write(struct file *file, const char __user *buffer,
+	  size_t length, loff_t *f_offset)
+{
+	char *bar_memory = NULL;
+	struct xlnx_ps_pcie_dma_client_device *xdev;
+	struct ps_pcie_dma_channel_match *xlnx_match;
+	ssize_t num_bytes = 0;
+	struct BAR_PARAMS *barinfo;
+
+	xdev = file->private_data;
+	xlnx_match = (struct ps_pcie_dma_channel_match *)
+			xdev->pcie_dma_chan[0].chan->private;
+
+	barinfo = ((struct BAR_PARAMS *)(xlnx_match->bar_params) +
+			PIO_MEMORY_BAR_NUMBER);
+	bar_memory = (__force char *)barinfo->BAR_VIRT_ADDR;
+
+	if (length > xdev->pio_translation_size) {
+		pr_err("Error! Invalid buffer length supplied at PIO write\n");
+		num_bytes = -1;
+		return num_bytes;
+	}
+
+	if ((length + *f_offset)
+			> xdev->pio_translation_size) {
+		pr_err("Error! Invalid buffer offset supplied at PIO write\n");
+		num_bytes = -1;
+		return num_bytes;
+	}
+
+	bar_memory += *f_offset;
+
+	num_bytes = copy_from_user(bar_memory, buffer, length);
+
+	if (num_bytes != 0) {
+		pr_err("Error! copy_from_user failed at PIO write\n");
+		num_bytes = length - num_bytes;
+	} else {
+		num_bytes = length;
+	}
+
+	return num_bytes;
+}
+
+static int pio_open(struct inode *in, struct file *file)
+{
+	struct xlnx_ps_pcie_dma_client_device *xdev;
+
+	xdev = container_of(in->i_cdev,
+			    struct xlnx_ps_pcie_dma_client_device,
+			    xpio_char_dev);
+
+	file->private_data = xdev;
+
+	return 0;
+}
+
+static int pio_release(struct inode *in, struct file *filp)
+{
+	return 0;
+}
+
+static const struct file_operations ps_pcie_pio_fops = {
+	.owner		= THIS_MODULE,
+	.read		= pio_read,
+	.write          = pio_write,
+	.unlocked_ioctl = pio_ioctl,
+	.open		= pio_open,
+	.release	= pio_release,
+};
+
+static void destroy_char_iface_for_pio(
+		struct xlnx_ps_pcie_dma_client_device *xdev)
+{
+	device_destroy(g_ps_pcie_dma_client_class,
+		       MKDEV(MAJOR(xdev->pio_char_device), 0));
+	cdev_del(&xdev->xpio_char_dev);
+	unregister_chrdev_region(xdev->pio_char_device, 1);
+}
+
+static void destroy_char_iface_for_dma(
+		struct xlnx_ps_pcie_dma_client_device *xdev)
+{
+	int i;
+	struct xlnx_completed_info *entry, *next;
+
+	for (i = 0; i < MAX_ALLOWED_CHANNELS_IN_HW; i++) {
+		list_for_each_entry_safe(entry, next,
+					 &xdev->pcie_dma_chan[i].completed.clist,
+					 clist) {
+			spin_lock(&xdev->pcie_dma_chan[i].channel_lock);
+			list_del(&entry->clist);
+			spin_unlock(&xdev->pcie_dma_chan[i].channel_lock);
+			kfree(entry);
+		}
+		device_destroy(g_ps_pcie_dma_client_class,
+			       MKDEV(MAJOR(xdev->char_device), i));
+	}
+	cdev_del(&xdev->xps_pcie_chardev);
+	unregister_chrdev_region(xdev->char_device, MAX_ALLOWED_CHANNELS_IN_HW);
+}
+
+static void delete_char_dev_interfaces(
+	struct xlnx_ps_pcie_dma_client_device *xdev)
+{
+	destroy_char_iface_for_dma(xdev);
+	if (xdev->properties->pio_transfers == PIO_SUPPORTED)
+		destroy_char_iface_for_pio(xdev);
+}
+
+static void release_dma_channels(struct xlnx_ps_pcie_dma_client_device *xdev)
+{
+	int i;
+
+	for (i = 0; i < MAX_ALLOWED_CHANNELS_IN_HW; i++)
+		dma_release_channel(xdev->pcie_dma_chan[i].chan);
+}
+
+static void delete_char_devices(void)
+{
+	struct xlnx_ps_pcie_dma_client_device *entry, *next;
+
+	list_for_each_entry_safe(entry, next, &g_ps_pcie_dma_client_list,
+				 dev_node) {
+		list_del(&entry->dev_node);
+		delete_char_dev_interfaces(entry);
+		release_dma_channels(entry);
+		kfree(entry);
+	}
+}
+
+static bool ps_pcie_dma_filter(struct dma_chan *chan, void *param)
+{
+	struct ps_pcie_dma_channel_match *client_match =
+		(struct ps_pcie_dma_channel_match *)param;
+
+	struct ps_pcie_dma_channel_match *dma_channel_match =
+		(struct ps_pcie_dma_channel_match *)chan->private;
+
+	if (client_match && dma_channel_match) {
+		if (client_match->pci_vendorid != 0 &&
+		    dma_channel_match->pci_vendorid != 0) {
+			if (client_match->pci_vendorid == dma_channel_match->pci_vendorid) {
+				if (client_match->pci_deviceid == dma_channel_match->pci_deviceid &&
+				    client_match->channel_number == dma_channel_match->channel_number &&
+				    client_match->direction == dma_channel_match->direction) {
+					return true;
+				}
+			}
+		}
+	}
+	return false;
+}
+
+static int acquire_dma_channels(struct xlnx_ps_pcie_dma_client_device *xdev)
+{
+	int err;
+	int i;
+	dma_cap_mask_t mask;
+	struct ps_pcie_dma_channel_match *match;
+
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_SLAVE | DMA_PRIVATE, mask);
+
+	for (i = 0; i < MAX_ALLOWED_CHANNELS_IN_HW; i++) {
+		match = &xdev->pcie_dma_chan[i].match;
+		match->board_number = xdev->properties->board_number;
+		match->pci_deviceid = xdev->properties->pci_deviceid;
+		match->pci_vendorid = xdev->properties->pci_vendorid;
+		match->channel_number = i;
+		match->direction = xdev->properties->direction[i];
+
+		xdev->pcie_dma_chan[i].chan =
+			dma_request_channel(mask, ps_pcie_dma_filter, match);
+
+		if (!xdev->pcie_dma_chan[i].chan) {
+			pr_err("Error channel handle %d board %d channel\n",
+			       match->board_number,
+			       match->channel_number);
+			err = -EINVAL;
+			goto err_out_no_channels;
+		}
+		xdev->pcie_dma_chan[i].dev =
+				xdev->pcie_dma_chan[i].chan->device->dev;
+		xdev->pcie_dma_chan[i].direction =
+				xdev->properties->direction[i];
+		xdev->pcie_dma_chan[i].mode =
+				xdev->properties->mode;
+		INIT_LIST_HEAD(&xdev->pcie_dma_chan[i].completed.clist);
+		spin_lock_init(&xdev->pcie_dma_chan[i].channel_lock);
+	}
+
+	return 0;
+
+err_out_no_channels:
+	while (i > 0) {
+		i--;
+		dma_release_channel(xdev->pcie_dma_chan[i].chan);
+	}
+	return err;
+}
+
+static int create_char_dev_iface_for_dma_device(
+		struct xlnx_ps_pcie_dma_client_device *xdev)
+{
+	int err = 0;
+	int i;
+
+	WARN_ON(!xdev);
+
+	err = alloc_chrdev_region(&xdev->char_device, 0,
+				  MAX_ALLOWED_CHANNELS_IN_HW,
+				  CHAR_DRIVER_NAME);
+	if (err < 0) {
+		pr_err("Unable to allocate char device region\n");
+		return err;
+	}
+
+	xdev->xps_pcie_chardev.owner = THIS_MODULE;
+	cdev_init(&xdev->xps_pcie_chardev, &ps_pcie_dma_comm_fops);
+	xdev->xps_pcie_chardev.dev = xdev->char_device;
+
+	err = cdev_add(&xdev->xps_pcie_chardev, xdev->char_device,
+		       MAX_ALLOWED_CHANNELS_IN_HW);
+	if (err < 0) {
+		pr_err("PS PCIe DMA unable to add cdev\n");
+		goto err_out_cdev_add;
+	}
+
+	for (i = 0; i < MAX_ALLOWED_CHANNELS_IN_HW; i++) {
+		xdev->chardev[i] =
+			device_create(g_ps_pcie_dma_client_class,
+				      xdev->pcie_dma_chan[i].dev,
+				      MKDEV(MAJOR(xdev->char_device), i),
+				      xdev,
+				      "%s%d_%d", CHAR_DRIVER_NAME,
+				      i, xdev->properties->board_number);
+
+		if (!xdev->chardev[i]) {
+			err = PTR_ERR(xdev->chardev[i]);
+			pr_err(
+			"PS PCIe DMA Unable to create device %d\n", i);
+			goto err_out_dev_create;
+		}
+	}
+
+	return 0;
+
+err_out_dev_create:
+	while (--i >= 0) {
+		device_destroy(g_ps_pcie_dma_client_class,
+			       MKDEV(MAJOR(xdev->char_device), i));
+	}
+	cdev_del(&xdev->xps_pcie_chardev);
+err_out_cdev_add:
+	unregister_chrdev_region(xdev->char_device, MAX_ALLOWED_CHANNELS_IN_HW);
+	return err;
+}
+
+static int create_char_dev_iface_for_pio(
+		struct xlnx_ps_pcie_dma_client_device *xdev)
+{
+	int err;
+
+	err = alloc_chrdev_region(&xdev->pio_char_device, 0, 1,
+				  PIO_CHAR_DRIVER_NAME);
+	if (err < 0) {
+		pr_err("Unable to allocate pio character device region\n");
+		return err;
+	}
+
+	xdev->xpio_char_dev.owner = THIS_MODULE;
+	cdev_init(&xdev->xpio_char_dev, &ps_pcie_pio_fops);
+	xdev->xpio_char_dev.dev = xdev->pio_char_device;
+
+	err = cdev_add(&xdev->xpio_char_dev, xdev->pio_char_device, 1);
+	if (err < 0) {
+		pr_err("PS PCIe DMA unable to add cdev for pio\n");
+		goto err_out_pio_cdev_add;
+	}
+
+	xdev->xpio_char_device =
+		device_create(g_ps_pcie_dma_client_class,
+			      xdev->pcie_dma_chan[0].dev,
+			      MKDEV(MAJOR(xdev->pio_char_device), 0),
+			      xdev, "%s_%d", PIO_CHAR_DRIVER_NAME,
+			      xdev->properties->board_number);
+
+	if (!xdev->xpio_char_device) {
+		err = PTR_ERR(xdev->xpio_char_device);
+		pr_err("PS PCIe DMA Unable to create pio device\n");
+		goto err_out_pio_dev_create;
+	}
+
+	mutex_init(&xdev->pio_chardev_mutex);
+	xdev->pio_translation_size = 0;
+	init_completion(&xdev->trans_cmpltn);
+
+	return 0;
+
+err_out_pio_dev_create:
+	cdev_del(&xdev->xpio_char_dev);
+err_out_pio_cdev_add:
+	unregister_chrdev_region(xdev->pio_char_device, 1);
+	return err;
+}
+
+static int create_char_dev_interfaces(
+	struct xlnx_ps_pcie_dma_client_device *xdev)
+{
+	int err;
+
+	err = create_char_dev_iface_for_dma_device(xdev);
+
+	if (err != 0) {
+		pr_err("Unable to create char dev dma iface %d\n",
+		       xdev->properties->pci_deviceid);
+		goto no_char_iface_for_dma;
+	}
+
+	if (xdev->properties->pio_transfers == PIO_SUPPORTED) {
+		err = create_char_dev_iface_for_pio(xdev);
+		if (err != 0) {
+			pr_err("Unable to create char dev pio iface %d\n",
+			       xdev->properties->pci_deviceid);
+			goto no_char_iface_for_pio;
+		}
+	}
+
+	return 0;
+
+no_char_iface_for_pio:
+	destroy_char_iface_for_dma(xdev);
+no_char_iface_for_dma:
+	return err;
+}
+
+static int setup_char_devices(u16 dev_prop_index)
+{
+	struct xlnx_ps_pcie_dma_client_device *xdev;
+	int err;
+	int i;
+
+	xdev = kzalloc(sizeof(*xdev), GFP_KERNEL);
+	if (!xdev) {
+		err = -ENOMEM;
+		return err;
+	}
+
+	xdev->properties = &g_dma_deviceproperties_list[dev_prop_index];
+
+	err = acquire_dma_channels(xdev);
+	if (err != 0) {
+		pr_err("Unable to acquire dma channels %d\n",
+		       dev_prop_index);
+		goto err_no_dma_channels;
+	}
+
+	err = create_char_dev_interfaces(xdev);
+	if (err != 0) {
+		pr_err("Unable to create char dev interfaces %d\n",
+		       dev_prop_index);
+		goto err_no_char_dev_ifaces;
+	}
+
+	list_add_tail(&xdev->dev_node, &g_ps_pcie_dma_client_list);
+
+	return 0;
+
+err_no_char_dev_ifaces:
+	for (i = 0; i < MAX_ALLOWED_CHANNELS_IN_HW; i++)
+		dma_release_channel(xdev->pcie_dma_chan[i].chan);
+err_no_dma_channels:
+	kfree(xdev);
+	return err;
+}
+
+/**
+ * ps_pcie_dma_client_init - Driver init function
+ *
+ * Return: 0 on success. Non zero on failure
+ */
+static int __init ps_pcie_dma_client_init(void)
+{
+	int err;
+	int i;
+	size_t num_dma_dev_properties;
+
+	INIT_LIST_HEAD(&g_ps_pcie_dma_client_list);
+
+	g_ps_pcie_dma_client_class = class_create(THIS_MODULE, DRV_MODULE_NAME);
+	if (IS_ERR(g_ps_pcie_dma_client_class)) {
+		pr_err("%s failed to create class\n", DRV_MODULE_NAME);
+		return PTR_ERR(g_ps_pcie_dma_client_class);
+	}
+
+	num_dma_dev_properties = ARRAY_SIZE(g_dma_deviceproperties_list);
+	for (i = 0; i < num_dma_dev_properties; i++) {
+		err = setup_char_devices(i);
+		if (err) {
+			pr_err("Error creating char devices for %d\n", i);
+			goto err_no_char_devices;
+		}
+	}
+
+	pr_info("PS PCIe DMA Client Driver Init successful\n");
+	return 0;
+
+err_no_char_devices:
+	delete_char_devices();
+
+	if (g_ps_pcie_dma_client_class)
+		class_destroy(g_ps_pcie_dma_client_class);
+	return err;
+}
+late_initcall(ps_pcie_dma_client_init);
+
+/**
+ * ps_pcie_dma_client_exit - Driver exit function
+ *
+ */
+static void __exit ps_pcie_dma_client_exit(void)
+{
+	delete_char_devices();
+
+	if (g_ps_pcie_dma_client_class)
+		class_destroy(g_ps_pcie_dma_client_class);
+}
+
+module_exit(ps_pcie_dma_client_exit);
+
+MODULE_AUTHOR("Xilinx Inc");
+MODULE_DESCRIPTION("Xilinx PS PCIe DMA client Driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/xilinx_ps_pcie_main.c b/drivers/dma/xilinx/xilinx_ps_pcie_main.c
new file mode 100644
index 000000000..cb3151219
--- /dev/null
+++ b/drivers/dma/xilinx/xilinx_ps_pcie_main.c
@@ -0,0 +1,200 @@
+/*
+ * XILINX PS PCIe driver
+ *
+ * Copyright (C) 2017 Xilinx, Inc. All rights reserved.
+ *
+ * Description
+ * PS PCIe DMA is memory mapped DMA used to execute PS to PL transfers
+ * on ZynqMP UltraScale+ Devices.
+ * This PCIe driver creates a platform device with specific platform
+ * info enabling creation of DMA device corresponding to the channel
+ * information provided in the properties
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation
+ */
+
+#include "xilinx_ps_pcie.h"
+#include "../dmaengine.h"
+
+#define DRV_MODULE_NAME		  "ps_pcie_dma"
+
+static int ps_pcie_dma_probe(struct pci_dev *pdev,
+			     const struct pci_device_id *ent);
+static void ps_pcie_dma_remove(struct pci_dev *pdev);
+
+static u32 channel_properties_pcie_axi[] = {
+	(u32)(PCIE_AXI_DIRECTION), (u32)(NUMBER_OF_BUFFER_DESCRIPTORS),
+	(u32)(DEFAULT_DMA_QUEUES), (u32)(CHANNEL_COAELSE_COUNT),
+	(u32)(CHANNEL_POLL_TIMER_FREQUENCY) };
+
+static u32 channel_properties_axi_pcie[] = {
+	(u32)(AXI_PCIE_DIRECTION), (u32)(NUMBER_OF_BUFFER_DESCRIPTORS),
+	(u32)(DEFAULT_DMA_QUEUES), (u32)(CHANNEL_COAELSE_COUNT),
+	(u32)(CHANNEL_POLL_TIMER_FREQUENCY) };
+
+static struct property_entry generic_pcie_ep_property[] = {
+	PROPERTY_ENTRY_U32("numchannels", (u32)MAX_NUMBER_OF_CHANNELS),
+	PROPERTY_ENTRY_U32_ARRAY("ps_pcie_channel0",
+				 channel_properties_pcie_axi),
+	PROPERTY_ENTRY_U32_ARRAY("ps_pcie_channel1",
+				 channel_properties_axi_pcie),
+	PROPERTY_ENTRY_U32_ARRAY("ps_pcie_channel2",
+				 channel_properties_pcie_axi),
+	PROPERTY_ENTRY_U32_ARRAY("ps_pcie_channel3",
+				 channel_properties_axi_pcie),
+	{ },
+};
+
+static const struct platform_device_info xlnx_std_platform_dev_info = {
+	.name           = XLNX_PLATFORM_DRIVER_NAME,
+	.properties     = generic_pcie_ep_property,
+};
+
+/**
+ * ps_pcie_dma_probe - Driver probe function
+ * @pdev: Pointer to the pci_dev structure
+ * @ent: pci device id
+ *
+ * Return: '0' on success and failure value on error
+ */
+static int ps_pcie_dma_probe(struct pci_dev *pdev,
+			     const struct pci_device_id *ent)
+{
+	int err;
+	struct platform_device *platform_dev;
+	struct platform_device_info platform_dev_info;
+
+	dev_info(&pdev->dev, "PS PCIe DMA Driver probe\n");
+
+	err = pcim_enable_device(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "Cannot enable PCI device, aborting\n");
+		return err;
+	}
+
+	err = pci_set_dma_mask(pdev, DMA_BIT_MASK(64));
+	if (err) {
+		dev_info(&pdev->dev, "Cannot set 64 bit DMA mask\n");
+		err = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));
+		if (err) {
+			dev_err(&pdev->dev, "DMA mask set error\n");
+			return err;
+		}
+	}
+
+	err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));
+	if (err) {
+		dev_info(&pdev->dev, "Cannot set 64 bit consistent DMA mask\n");
+		err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));
+		if (err) {
+			dev_err(&pdev->dev, "Cannot set consistent DMA mask\n");
+			return err;
+		}
+	}
+
+	pci_set_master(pdev);
+
+	/* For Root DMA platform device will be created through device tree */
+	if (pdev->vendor == PCI_VENDOR_ID_XILINX &&
+	    pdev->device == ZYNQMP_RC_DMA_DEVID)
+		return 0;
+
+	memcpy(&platform_dev_info, &xlnx_std_platform_dev_info,
+	       sizeof(xlnx_std_platform_dev_info));
+
+	/* Do device specific channel configuration changes to
+	 * platform_dev_info.properties if required
+	 * More information on channel properties can be found
+	 * at Documentation/devicetree/bindings/dma/xilinx/ps-pcie-dma.txt
+	 */
+
+	platform_dev_info.parent = &pdev->dev;
+	platform_dev_info.data = &pdev;
+	platform_dev_info.size_data = sizeof(struct pci_dev **);
+
+	platform_dev = platform_device_register_full(&platform_dev_info);
+	if (IS_ERR(platform_dev)) {
+		dev_err(&pdev->dev,
+			"Cannot create platform device, aborting\n");
+		return PTR_ERR(platform_dev);
+	}
+
+	pci_set_drvdata(pdev, platform_dev);
+
+	dev_info(&pdev->dev, "PS PCIe DMA driver successfully probed\n");
+
+	return 0;
+}
+
+static struct pci_device_id ps_pcie_dma_tbl[] = {
+	{ PCI_DEVICE(PCI_VENDOR_ID_XILINX, ZYNQMP_DMA_DEVID) },
+	{ PCI_DEVICE(PCI_VENDOR_ID_XILINX, ZYNQMP_RC_DMA_DEVID) },
+	{ }
+};
+
+static struct pci_driver ps_pcie_dma_driver = {
+	.name     = DRV_MODULE_NAME,
+	.id_table = ps_pcie_dma_tbl,
+	.probe    = ps_pcie_dma_probe,
+	.remove   = ps_pcie_dma_remove,
+};
+
+/**
+ * ps_pcie_init - Driver init function
+ *
+ * Return: 0 on success. Non zero on failure
+ */
+static int __init ps_pcie_init(void)
+{
+	int ret;
+
+	pr_info("%s init()\n", DRV_MODULE_NAME);
+
+	ret = pci_register_driver(&ps_pcie_dma_driver);
+	if (ret)
+		return ret;
+
+	ret = dma_platform_driver_register();
+	if (ret)
+		pci_unregister_driver(&ps_pcie_dma_driver);
+
+	return ret;
+}
+
+/**
+ * ps_pcie_dma_remove - Driver remove function
+ * @pdev: Pointer to the pci_dev structure
+ *
+ * Return: void
+ */
+static void ps_pcie_dma_remove(struct pci_dev *pdev)
+{
+	struct platform_device *platform_dev;
+
+	platform_dev = (struct platform_device *)pci_get_drvdata(pdev);
+
+	if (platform_dev)
+		platform_device_unregister(platform_dev);
+}
+
+/**
+ * ps_pcie_exit - Driver exit function
+ *
+ * Return: void
+ */
+static void __exit ps_pcie_exit(void)
+{
+	pr_info("%s exit()\n", DRV_MODULE_NAME);
+
+	dma_platform_driver_unregister();
+	pci_unregister_driver(&ps_pcie_dma_driver);
+}
+
+module_init(ps_pcie_init);
+module_exit(ps_pcie_exit);
+
+MODULE_AUTHOR("Xilinx Inc");
+MODULE_DESCRIPTION("Xilinx PS PCIe DMA Driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/dma/xilinx/xilinx_ps_pcie_platform.c b/drivers/dma/xilinx/xilinx_ps_pcie_platform.c
new file mode 100644
index 000000000..17ad2cbfd
--- /dev/null
+++ b/drivers/dma/xilinx/xilinx_ps_pcie_platform.c
@@ -0,0 +1,3170 @@
+/*
+ * XILINX PS PCIe DMA driver
+ *
+ * Copyright (C) 2017 Xilinx, Inc. All rights reserved.
+ *
+ * Description
+ * PS PCIe DMA is memory mapped DMA used to execute PS to PL transfers
+ * on ZynqMP UltraScale+ Devices
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation
+ */
+
+#include "xilinx_ps_pcie.h"
+#include "../dmaengine.h"
+
+#define PLATFORM_DRIVER_NAME		  "ps_pcie_pform_dma"
+#define MAX_BARS 6
+
+#define DMA_BAR_NUMBER 0
+
+#define MIN_SW_INTR_TRANSACTIONS       2
+
+#define CHANNEL_PROPERTY_LENGTH 50
+#define WORKQ_NAME_SIZE		100
+#define INTR_HANDLR_NAME_SIZE   100
+
+#define PS_PCIE_DMA_IRQ_NOSHARE    0
+
+#define MAX_COALESCE_COUNT     255
+
+#define DMA_CHANNEL_REGS_SIZE 0x80
+
+#define DMA_SRCQPTRLO_REG_OFFSET  (0x00) /* Source Q pointer Lo */
+#define DMA_SRCQPTRHI_REG_OFFSET  (0x04) /* Source Q pointer Hi */
+#define DMA_SRCQSZ_REG_OFFSET     (0x08) /* Source Q size */
+#define DMA_SRCQLMT_REG_OFFSET    (0x0C) /* Source Q limit */
+#define DMA_DSTQPTRLO_REG_OFFSET  (0x10) /* Destination Q pointer Lo */
+#define DMA_DSTQPTRHI_REG_OFFSET  (0x14) /* Destination Q pointer Hi */
+#define DMA_DSTQSZ_REG_OFFSET     (0x18) /* Destination Q size */
+#define DMA_DSTQLMT_REG_OFFSET    (0x1C) /* Destination Q limit */
+#define DMA_SSTAQPTRLO_REG_OFFSET (0x20) /* Source Status Q pointer Lo */
+#define DMA_SSTAQPTRHI_REG_OFFSET (0x24) /* Source Status Q pointer Hi */
+#define DMA_SSTAQSZ_REG_OFFSET    (0x28) /* Source Status Q size */
+#define DMA_SSTAQLMT_REG_OFFSET   (0x2C) /* Source Status Q limit */
+#define DMA_DSTAQPTRLO_REG_OFFSET (0x30) /* Destination Status Q pointer Lo */
+#define DMA_DSTAQPTRHI_REG_OFFSET (0x34) /* Destination Status Q pointer Hi */
+#define DMA_DSTAQSZ_REG_OFFSET    (0x38) /* Destination Status Q size */
+#define DMA_DSTAQLMT_REG_OFFSET   (0x3C) /* Destination Status Q limit */
+#define DMA_SRCQNXT_REG_OFFSET    (0x40) /* Source Q next */
+#define DMA_DSTQNXT_REG_OFFSET    (0x44) /* Destination Q next */
+#define DMA_SSTAQNXT_REG_OFFSET   (0x48) /* Source Status Q next */
+#define DMA_DSTAQNXT_REG_OFFSET   (0x4C) /* Destination Status Q next */
+#define DMA_SCRATCH0_REG_OFFSET   (0x50) /* Scratch pad register 0 */
+
+#define DMA_PCIE_INTR_CNTRL_REG_OFFSET  (0x60) /* DMA PCIe intr control reg */
+#define DMA_PCIE_INTR_STATUS_REG_OFFSET (0x64) /* DMA PCIe intr status reg */
+#define DMA_AXI_INTR_CNTRL_REG_OFFSET   (0x68) /* DMA AXI intr control reg */
+#define DMA_AXI_INTR_STATUS_REG_OFFSET  (0x6C) /* DMA AXI intr status reg */
+#define DMA_PCIE_INTR_ASSRT_REG_OFFSET  (0x70) /* PCIe intr assert reg */
+#define DMA_AXI_INTR_ASSRT_REG_OFFSET   (0x74) /* AXI intr assert register */
+#define DMA_CNTRL_REG_OFFSET            (0x78) /* DMA control register */
+#define DMA_STATUS_REG_OFFSET           (0x7C) /* DMA status register */
+
+#define DMA_CNTRL_RST_BIT               BIT(1)
+#define DMA_CNTRL_64BIT_STAQ_ELEMSZ_BIT BIT(2)
+#define DMA_CNTRL_ENABL_BIT             BIT(0)
+#define DMA_STATUS_DMA_PRES_BIT         BIT(15)
+#define DMA_STATUS_DMA_RUNNING_BIT      BIT(0)
+#define DMA_QPTRLO_QLOCAXI_BIT          BIT(0)
+#define DMA_QPTRLO_Q_ENABLE_BIT         BIT(1)
+#define DMA_INTSTATUS_DMAERR_BIT        BIT(1)
+#define DMA_INTSTATUS_SGLINTR_BIT       BIT(2)
+#define DMA_INTSTATUS_SWINTR_BIT        BIT(3)
+#define DMA_INTCNTRL_ENABLINTR_BIT      BIT(0)
+#define DMA_INTCNTRL_DMAERRINTR_BIT     BIT(1)
+#define DMA_INTCNTRL_DMASGINTR_BIT      BIT(2)
+#define DMA_SW_INTR_ASSRT_BIT           BIT(3)
+
+#define SOURCE_CONTROL_BD_BYTE_COUNT_MASK       GENMASK(23, 0)
+#define SOURCE_CONTROL_BD_LOC_AXI		BIT(24)
+#define SOURCE_CONTROL_BD_EOP_BIT               BIT(25)
+#define SOURCE_CONTROL_BD_INTR_BIT              BIT(26)
+#define SOURCE_CONTROL_BACK_TO_BACK_PACK_BIT    BIT(25)
+#define SOURCE_CONTROL_ATTRIBUTES_MASK          GENMASK(31, 28)
+#define SRC_CTL_ATTRIB_BIT_SHIFT                (29)
+
+#define STA_BD_COMPLETED_BIT            BIT(0)
+#define STA_BD_SOURCE_ERROR_BIT         BIT(1)
+#define STA_BD_DESTINATION_ERROR_BIT    BIT(2)
+#define STA_BD_INTERNAL_ERROR_BIT       BIT(3)
+#define STA_BD_UPPER_STATUS_NONZERO_BIT BIT(31)
+#define STA_BD_BYTE_COUNT_MASK          GENMASK(30, 4)
+
+#define STA_BD_BYTE_COUNT_SHIFT         4
+
+#define DMA_INTCNTRL_SGCOLSCCNT_BIT_SHIFT (16)
+
+#define DMA_SRC_Q_LOW_BIT_SHIFT   GENMASK(5, 0)
+
+#define MAX_TRANSFER_LENGTH       0x1000000
+
+#define AXI_ATTRIBUTE       0x3
+#define PCI_ATTRIBUTE       0x2
+
+#define ROOTDMA_Q_READ_ATTRIBUTE 0x8
+
+/*
+ * User Id programmed into Source Q will be copied into Status Q of Destination
+ */
+#define DEFAULT_UID 1
+
+/*
+ * DMA channel registers
+ */
+struct DMA_ENGINE_REGISTERS {
+	u32 src_q_low;          /* 0x00 */
+	u32 src_q_high;         /* 0x04 */
+	u32 src_q_size;         /* 0x08 */
+	u32 src_q_limit;        /* 0x0C */
+	u32 dst_q_low;          /* 0x10 */
+	u32 dst_q_high;         /* 0x14 */
+	u32 dst_q_size;         /* 0x18 */
+	u32 dst_q_limit;        /* 0x1c */
+	u32 stas_q_low;         /* 0x20 */
+	u32 stas_q_high;        /* 0x24 */
+	u32 stas_q_size;        /* 0x28 */
+	u32 stas_q_limit;       /* 0x2C */
+	u32 stad_q_low;         /* 0x30 */
+	u32 stad_q_high;        /* 0x34 */
+	u32 stad_q_size;        /* 0x38 */
+	u32 stad_q_limit;       /* 0x3C */
+	u32 src_q_next;         /* 0x40 */
+	u32 dst_q_next;         /* 0x44 */
+	u32 stas_q_next;        /* 0x48 */
+	u32 stad_q_next;        /* 0x4C */
+	u32 scrathc0;           /* 0x50 */
+	u32 scrathc1;           /* 0x54 */
+	u32 scrathc2;           /* 0x58 */
+	u32 scrathc3;           /* 0x5C */
+	u32 pcie_intr_cntrl;    /* 0x60 */
+	u32 pcie_intr_status;   /* 0x64 */
+	u32 axi_intr_cntrl;     /* 0x68 */
+	u32 axi_intr_status;    /* 0x6C */
+	u32 pcie_intr_assert;   /* 0x70 */
+	u32 axi_intr_assert;    /* 0x74 */
+	u32 dma_channel_ctrl;   /* 0x78 */
+	u32 dma_channel_status; /* 0x7C */
+} __attribute__((__packed__));
+
+/**
+ * struct SOURCE_DMA_DESCRIPTOR - Source Hardware Descriptor
+ * @system_address: 64 bit buffer physical address
+ * @control_byte_count: Byte count/buffer length and control flags
+ * @user_handle: User handle gets copied to status q on completion
+ * @user_id: User id gets copied to status q of destination
+ */
+struct SOURCE_DMA_DESCRIPTOR {
+	u64 system_address;
+	u32 control_byte_count;
+	u16 user_handle;
+	u16 user_id;
+} __attribute__((__packed__));
+
+/**
+ * struct DEST_DMA_DESCRIPTOR - Destination Hardware Descriptor
+ * @system_address: 64 bit buffer physical address
+ * @control_byte_count: Byte count/buffer length and control flags
+ * @user_handle: User handle gets copied to status q on completion
+ * @reserved: Reserved field
+ */
+struct DEST_DMA_DESCRIPTOR {
+	u64 system_address;
+	u32 control_byte_count;
+	u16 user_handle;
+	u16 reserved;
+} __attribute__((__packed__));
+
+/**
+ * struct STATUS_DMA_DESCRIPTOR - Status Hardware Descriptor
+ * @status_flag_byte_count: Byte count/buffer length and status flags
+ * @user_handle: User handle gets copied from src/dstq on completion
+ * @user_id: User id gets copied from srcq
+ */
+struct STATUS_DMA_DESCRIPTOR {
+	u32 status_flag_byte_count;
+	u16 user_handle;
+	u16 user_id;
+} __attribute__((__packed__));
+
+enum PACKET_CONTEXT_AVAILABILITY {
+	FREE = 0,    /*Packet transfer Parameter context is free.*/
+	IN_USE       /*Packet transfer Parameter context is in use.*/
+};
+
+struct ps_pcie_transfer_elements {
+	struct list_head node;
+	dma_addr_t src_pa;
+	dma_addr_t dst_pa;
+	u32 transfer_bytes;
+};
+
+struct  ps_pcie_tx_segment {
+	struct list_head node;
+	struct dma_async_tx_descriptor async_tx;
+	struct list_head transfer_nodes;
+	u32 src_elements;
+	u32 dst_elements;
+	u32 total_transfer_bytes;
+};
+
+struct ps_pcie_intr_segment {
+	struct list_head node;
+	struct dma_async_tx_descriptor async_intr_tx;
+};
+
+/*
+ * The context structure stored for each DMA transaction
+ * This structure is maintained separately for Src Q and Destination Q
+ * @availability_status: Indicates whether packet context is available
+ * @idx_sop: Indicates starting index of buffer descriptor for a transfer
+ * @idx_eop: Indicates ending index of buffer descriptor for a transfer
+ * @sgl: Indicates either src or dst sglist for the transaction
+ */
+struct PACKET_TRANSFER_PARAMS {
+	enum PACKET_CONTEXT_AVAILABILITY availability_status;
+	u16 idx_sop;
+	u16 idx_eop;
+	struct ps_pcie_tx_segment *seg;
+};
+
+enum CHANNEL_STATE {
+	CHANNEL_RESOURCE_UNALLOCATED = 0, /*  Channel resources not allocated */
+	CHANNEL_UNAVIALBLE,               /*  Channel inactive */
+	CHANNEL_AVAILABLE,                /*  Channel available for transfers */
+	CHANNEL_ERROR                     /*  Channel encountered errors */
+};
+
+enum BUFFER_LOCATION {
+	BUFFER_LOC_PCI = 0,
+	BUFFER_LOC_AXI,
+	BUFFER_LOC_INVALID
+};
+
+enum dev_channel_properties {
+	DMA_CHANNEL_DIRECTION = 0,
+	NUM_DESCRIPTORS,
+	NUM_QUEUES,
+	COALESE_COUNT,
+	POLL_TIMER_FREQUENCY
+};
+
+/*
+ * struct ps_pcie_dma_chan - Driver specific DMA channel structure
+ * @xdev: Driver specific device structure
+ * @dev: The dma device
+ * @common:  DMA common channel
+ * @chan_base: Pointer to Channel registers
+ * @channel_number: DMA channel number in the device
+ * @num_queues: Number of queues per channel.
+ *		It should be four for memory mapped case and
+ *		two for Streaming case
+ * @direction: Transfer direction
+ * @state: Indicates channel state
+ * @channel_lock: Spin lock to be used before changing channel state
+ * @cookie_lock: Spin lock to be used before assigning cookie for a transaction
+ * @coalesce_count: Indicates number of packet transfers before interrupts
+ * @poll_timer_freq:Indicates frequency of polling for completed transactions
+ * @poll_timer: Timer to poll dma buffer descriptors if coalesce count is > 0
+ * @src_avail_descriptors: Available sgl source descriptors
+ * @src_desc_lock: Lock for synchronizing src_avail_descriptors
+ * @dst_avail_descriptors: Available sgl destination descriptors
+ * @dst_desc_lock: Lock for synchronizing
+ *		dst_avail_descriptors
+ * @src_sgl_bd_pa: Physical address of Source SGL buffer Descriptors
+ * @psrc_sgl_bd: Virtual address of Source SGL buffer Descriptors
+ * @src_sgl_freeidx: Holds index of Source SGL buffer descriptor to be filled
+ * @sglDestinationQLock:Lock to serialize Destination Q updates
+ * @dst_sgl_bd_pa: Physical address of Dst SGL buffer Descriptors
+ * @pdst_sgl_bd: Virtual address of Dst SGL buffer Descriptors
+ * @dst_sgl_freeidx: Holds index of Destination SGL
+ * @src_sta_bd_pa: Physical address of StatusQ buffer Descriptors
+ * @psrc_sta_bd: Virtual address of Src StatusQ buffer Descriptors
+ * @src_staprobe_idx: Holds index of Status Q to be examined for SrcQ updates
+ * @src_sta_hw_probe_idx: Holds index of maximum limit of Status Q for hardware
+ * @dst_sta_bd_pa: Physical address of Dst StatusQ buffer Descriptor
+ * @pdst_sta_bd: Virtual address of Dst Status Q buffer Descriptors
+ * @dst_staprobe_idx: Holds index of Status Q to be examined for updates
+ * @dst_sta_hw_probe_idx: Holds index of max limit of Dst Status Q for hardware
+ * @@read_attribute: Describes the attributes of buffer in srcq
+ * @@write_attribute: Describes the attributes of buffer in dstq
+ * @@intr_status_offset: Register offset to be cheked on receiving interrupt
+ * @@intr_status_offset: Register offset to be used to control interrupts
+ * @ppkt_ctx_srcq: Virtual address of packet context to Src Q updates
+ * @idx_ctx_srcq_head: Holds index of packet context to be filled for Source Q
+ * @idx_ctx_srcq_tail: Holds index of packet context to be examined for Source Q
+ * @ppkt_ctx_dstq: Virtual address of packet context to Dst Q updates
+ * @idx_ctx_dstq_head: Holds index of packet context to be filled for Dst Q
+ * @idx_ctx_dstq_tail: Holds index of packet context to be examined for Dst Q
+ * @pending_list_lock: Lock to be taken before updating pending transfers list
+ * @pending_list: List of transactions submitted to channel
+ * @active_list_lock: Lock to be taken before transferring transactions from
+ *			pending list to active list which will be subsequently
+ *				submitted to hardware
+ * @active_list: List of transactions that will be submitted to hardware
+ * @pending_interrupts_lock: Lock to be taken before updating pending Intr list
+ * @pending_interrupts_list: List of interrupt transactions submitted to channel
+ * @active_interrupts_lock: Lock to be taken before transferring transactions
+ *			from pending interrupt list to active interrupt list
+ * @active_interrupts_list: List of interrupt transactions that are active
+ * @transactions_pool: Mem pool to allocate dma transactions quickly
+ * @intr_transactions_pool: Mem pool to allocate interrupt transactions quickly
+ * @sw_intrs_wrkq: Work Q which performs handling of software intrs
+ * @handle_sw_intrs:Work function handling software interrupts
+ * @maintenance_workq: Work Q to perform maintenance tasks during stop or error
+ * @handle_chan_reset: Work that invokes channel reset function
+ * @handle_chan_shutdown: Work that invokes channel shutdown function
+ * @handle_chan_terminate: Work that invokes channel transactions termination
+ * @chan_shutdown_complt: Completion variable which says shutdown is done
+ * @chan_terminate_complete: Completion variable which says terminate is done
+ * @primary_desc_cleanup: Work Q which performs work related to sgl handling
+ * @handle_primary_desc_cleanup: Work that invokes src Q, dst Q cleanup
+ *				and programming
+ * @chan_programming: Work Q which performs work related to channel programming
+ * @handle_chan_programming: Work that invokes channel programming function
+ * @srcq_desc_cleanup: Work Q which performs src Q descriptor cleanup
+ * @handle_srcq_desc_cleanup: Work function handling Src Q completions
+ * @dstq_desc_cleanup: Work Q which performs dst Q descriptor cleanup
+ * @handle_dstq_desc_cleanup: Work function handling Dst Q completions
+ * @srcq_work_complete: Src Q Work completion variable for primary work
+ * @dstq_work_complete: Dst Q Work completion variable for primary work
+ */
+struct ps_pcie_dma_chan {
+	struct xlnx_pcie_dma_device *xdev;
+	struct device *dev;
+
+	struct dma_chan common;
+
+	struct DMA_ENGINE_REGISTERS *chan_base;
+	u16 channel_number;
+
+	u32 num_queues;
+	enum dma_data_direction direction;
+	enum BUFFER_LOCATION srcq_buffer_location;
+	enum BUFFER_LOCATION dstq_buffer_location;
+
+	u32 total_descriptors;
+
+	enum CHANNEL_STATE state;
+	spinlock_t channel_lock; /* For changing channel state */
+
+	spinlock_t cookie_lock;  /* For acquiring cookie from dma framework*/
+
+	u32 coalesce_count;
+	u32 poll_timer_freq;
+
+	struct timer_list poll_timer;
+
+	u32 src_avail_descriptors;
+	spinlock_t src_desc_lock; /* For handling srcq available descriptors */
+
+	u32 dst_avail_descriptors;
+	spinlock_t dst_desc_lock; /* For handling dstq available descriptors */
+
+	dma_addr_t src_sgl_bd_pa;
+	struct SOURCE_DMA_DESCRIPTOR *psrc_sgl_bd;
+	u32 src_sgl_freeidx;
+
+	dma_addr_t dst_sgl_bd_pa;
+	struct DEST_DMA_DESCRIPTOR *pdst_sgl_bd;
+	u32 dst_sgl_freeidx;
+
+	dma_addr_t src_sta_bd_pa;
+	struct STATUS_DMA_DESCRIPTOR *psrc_sta_bd;
+	u32 src_staprobe_idx;
+	u32 src_sta_hw_probe_idx;
+
+	dma_addr_t dst_sta_bd_pa;
+	struct STATUS_DMA_DESCRIPTOR *pdst_sta_bd;
+	u32 dst_staprobe_idx;
+	u32 dst_sta_hw_probe_idx;
+
+	u32 read_attribute;
+	u32 write_attribute;
+
+	u32 intr_status_offset;
+	u32 intr_control_offset;
+
+	struct PACKET_TRANSFER_PARAMS *ppkt_ctx_srcq;
+	u16 idx_ctx_srcq_head;
+	u16 idx_ctx_srcq_tail;
+
+	struct PACKET_TRANSFER_PARAMS *ppkt_ctx_dstq;
+	u16 idx_ctx_dstq_head;
+	u16 idx_ctx_dstq_tail;
+
+	spinlock_t  pending_list_lock; /* For handling dma pending_list */
+	struct list_head pending_list;
+	spinlock_t  active_list_lock; /* For handling dma active_list */
+	struct list_head active_list;
+
+	spinlock_t pending_interrupts_lock; /* For dma pending interrupts list*/
+	struct list_head pending_interrupts_list;
+	spinlock_t active_interrupts_lock;  /* For dma active interrupts list*/
+	struct list_head active_interrupts_list;
+
+	mempool_t *transactions_pool;
+	mempool_t *tx_elements_pool;
+	mempool_t *intr_transactions_pool;
+
+	struct workqueue_struct *sw_intrs_wrkq;
+	struct work_struct handle_sw_intrs;
+
+	struct workqueue_struct *maintenance_workq;
+	struct work_struct handle_chan_reset;
+	struct work_struct handle_chan_shutdown;
+	struct work_struct handle_chan_terminate;
+
+	struct completion chan_shutdown_complt;
+	struct completion chan_terminate_complete;
+
+	struct workqueue_struct *primary_desc_cleanup;
+	struct work_struct handle_primary_desc_cleanup;
+
+	struct workqueue_struct *chan_programming;
+	struct work_struct handle_chan_programming;
+
+	struct workqueue_struct *srcq_desc_cleanup;
+	struct work_struct handle_srcq_desc_cleanup;
+	struct completion srcq_work_complete;
+
+	struct workqueue_struct *dstq_desc_cleanup;
+	struct work_struct handle_dstq_desc_cleanup;
+	struct completion dstq_work_complete;
+};
+
+/*
+ * struct xlnx_pcie_dma_device - Driver specific platform device structure
+ * @is_rootdma: Indicates whether the dma instance is root port dma
+ * @dma_buf_ext_addr: Indicates whether target system is 32 bit or 64 bit
+ * @bar_mask: Indicates available pcie bars
+ * @board_number: Count value of platform device
+ * @dev: Device structure pointer for pcie device
+ * @channels: Pointer to device DMA channels structure
+ * @common: DMA device structure
+ * @num_channels: Number of channels active for the device
+ * @reg_base: Base address of first DMA channel of the device
+ * @irq_vecs: Number of irq vectors allocated to pci device
+ * @pci_dev: Parent pci device which created this platform device
+ * @bar_info: PCIe bar related information
+ * @platform_irq_vec: Platform irq vector number for root dma
+ * @rootdma_vendor: PCI Vendor id for root dma
+ * @rootdma_device: PCI Device id for root dma
+ */
+struct xlnx_pcie_dma_device {
+	bool is_rootdma;
+	bool dma_buf_ext_addr;
+	u32 bar_mask;
+	u16 board_number;
+	struct device *dev;
+	struct ps_pcie_dma_chan *channels;
+	struct dma_device common;
+	int num_channels;
+	int irq_vecs;
+	void __iomem *reg_base;
+	struct pci_dev *pci_dev;
+	struct BAR_PARAMS bar_info[MAX_BARS];
+	int platform_irq_vec;
+	u16 rootdma_vendor;
+	u16 rootdma_device;
+};
+
+#define to_xilinx_chan(chan) \
+	container_of(chan, struct ps_pcie_dma_chan, common)
+#define to_ps_pcie_dma_tx_descriptor(tx) \
+	container_of(tx, struct ps_pcie_tx_segment, async_tx)
+#define to_ps_pcie_dma_tx_intr_descriptor(tx) \
+	container_of(tx, struct ps_pcie_intr_segment, async_intr_tx)
+
+/* Function Protypes */
+static u32 ps_pcie_dma_read(struct ps_pcie_dma_chan *chan, u32 reg);
+static void ps_pcie_dma_write(struct ps_pcie_dma_chan *chan, u32 reg,
+			      u32 value);
+static void ps_pcie_dma_clr_mask(struct ps_pcie_dma_chan *chan, u32 reg,
+				 u32 mask);
+static void ps_pcie_dma_set_mask(struct ps_pcie_dma_chan *chan, u32 reg,
+				 u32 mask);
+static int irq_setup(struct xlnx_pcie_dma_device *xdev);
+static int platform_irq_setup(struct xlnx_pcie_dma_device *xdev);
+static int chan_intr_setup(struct xlnx_pcie_dma_device *xdev);
+static int device_intr_setup(struct xlnx_pcie_dma_device *xdev);
+static int irq_probe(struct xlnx_pcie_dma_device *xdev);
+static int ps_pcie_check_intr_status(struct ps_pcie_dma_chan *chan);
+static irqreturn_t ps_pcie_dma_dev_intr_handler(int irq, void *data);
+static irqreturn_t ps_pcie_dma_chan_intr_handler(int irq, void *data);
+static int init_hw_components(struct ps_pcie_dma_chan *chan);
+static int init_sw_components(struct ps_pcie_dma_chan *chan);
+static void update_channel_read_attribute(struct ps_pcie_dma_chan *chan);
+static void update_channel_write_attribute(struct ps_pcie_dma_chan *chan);
+static void ps_pcie_chan_reset(struct ps_pcie_dma_chan *chan);
+static void poll_completed_transactions(struct timer_list *t);
+static bool check_descriptors_for_two_queues(struct ps_pcie_dma_chan *chan,
+					     struct ps_pcie_tx_segment *seg);
+static bool check_descriptors_for_all_queues(struct ps_pcie_dma_chan *chan,
+					     struct ps_pcie_tx_segment *seg);
+static bool check_descriptor_availability(struct ps_pcie_dma_chan *chan,
+					  struct ps_pcie_tx_segment *seg);
+static void handle_error(struct ps_pcie_dma_chan *chan);
+static void xlnx_ps_pcie_update_srcq(struct ps_pcie_dma_chan *chan,
+				     struct ps_pcie_tx_segment *seg);
+static void xlnx_ps_pcie_update_dstq(struct ps_pcie_dma_chan *chan,
+				     struct ps_pcie_tx_segment *seg);
+static void ps_pcie_chan_program_work(struct work_struct *work);
+static void dst_cleanup_work(struct work_struct *work);
+static void src_cleanup_work(struct work_struct *work);
+static void ps_pcie_chan_primary_work(struct work_struct *work);
+static int probe_channel_properties(struct platform_device *platform_dev,
+				    struct xlnx_pcie_dma_device *xdev,
+				    u16 channel_number);
+static void xlnx_ps_pcie_destroy_mempool(struct ps_pcie_dma_chan *chan);
+static void xlnx_ps_pcie_free_worker_queues(struct ps_pcie_dma_chan *chan);
+static void xlnx_ps_pcie_free_pkt_ctxts(struct ps_pcie_dma_chan *chan);
+static void xlnx_ps_pcie_free_descriptors(struct ps_pcie_dma_chan *chan);
+static int xlnx_ps_pcie_channel_activate(struct ps_pcie_dma_chan *chan);
+static void xlnx_ps_pcie_channel_quiesce(struct ps_pcie_dma_chan *chan);
+static void ivk_cbk_for_pending(struct ps_pcie_dma_chan *chan);
+static void xlnx_ps_pcie_reset_channel(struct ps_pcie_dma_chan *chan);
+static void xlnx_ps_pcie_free_poll_timer(struct ps_pcie_dma_chan *chan);
+static int xlnx_ps_pcie_alloc_poll_timer(struct ps_pcie_dma_chan *chan);
+static void terminate_transactions_work(struct work_struct *work);
+static void chan_shutdown_work(struct work_struct *work);
+static void chan_reset_work(struct work_struct *work);
+static int xlnx_ps_pcie_alloc_worker_threads(struct ps_pcie_dma_chan *chan);
+static int xlnx_ps_pcie_alloc_mempool(struct ps_pcie_dma_chan *chan);
+static int xlnx_ps_pcie_alloc_pkt_contexts(struct ps_pcie_dma_chan *chan);
+static int dma_alloc_descriptors_two_queues(struct ps_pcie_dma_chan *chan);
+static int dma_alloc_decriptors_all_queues(struct ps_pcie_dma_chan *chan);
+static void xlnx_ps_pcie_dma_free_chan_resources(struct dma_chan *dchan);
+static int xlnx_ps_pcie_dma_alloc_chan_resources(struct dma_chan *dchan);
+static dma_cookie_t xilinx_dma_tx_submit(struct dma_async_tx_descriptor *tx);
+static dma_cookie_t xilinx_intr_tx_submit(struct dma_async_tx_descriptor *tx);
+static struct dma_async_tx_descriptor *
+xlnx_ps_pcie_dma_prep_memcpy(struct dma_chan *channel, dma_addr_t dma_dst,
+			     dma_addr_t dma_src, size_t len,
+			     unsigned long flags);
+static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_slave_sg(
+		struct dma_chan *channel, struct scatterlist *sgl,
+		unsigned int sg_len, enum dma_transfer_direction direction,
+		unsigned long flags, void *context);
+static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_interrupt(
+		struct dma_chan *channel, unsigned long flags);
+static void xlnx_ps_pcie_dma_issue_pending(struct dma_chan *channel);
+static int xlnx_ps_pcie_dma_terminate_all(struct dma_chan *channel);
+static int read_rootdma_config(struct platform_device *platform_dev,
+			       struct xlnx_pcie_dma_device *xdev);
+static int read_epdma_config(struct platform_device *platform_dev,
+			     struct xlnx_pcie_dma_device *xdev);
+static int xlnx_pcie_dma_driver_probe(struct platform_device *platform_dev);
+static int xlnx_pcie_dma_driver_remove(struct platform_device *platform_dev);
+
+/* IO accessors */
+static inline u32 ps_pcie_dma_read(struct ps_pcie_dma_chan *chan, u32 reg)
+{
+	return ioread32((void __iomem *)((char *)(chan->chan_base) + reg));
+}
+
+static inline void ps_pcie_dma_write(struct ps_pcie_dma_chan *chan, u32 reg,
+				     u32 value)
+{
+	iowrite32(value, (void __iomem *)((char *)(chan->chan_base) + reg));
+}
+
+static inline void ps_pcie_dma_clr_mask(struct ps_pcie_dma_chan *chan, u32 reg,
+					u32 mask)
+{
+	ps_pcie_dma_write(chan, reg, ps_pcie_dma_read(chan, reg) & ~mask);
+}
+
+static inline void ps_pcie_dma_set_mask(struct ps_pcie_dma_chan *chan, u32 reg,
+					u32 mask)
+{
+	ps_pcie_dma_write(chan, reg, ps_pcie_dma_read(chan, reg) | mask);
+}
+
+/**
+ * ps_pcie_dma_dev_intr_handler - This will be invoked for MSI/Legacy interrupts
+ *
+ * @irq: IRQ number
+ * @data: Pointer to the PS PCIe DMA channel structure
+ *
+ * Return: IRQ_HANDLED/IRQ_NONE
+ */
+static irqreturn_t ps_pcie_dma_dev_intr_handler(int irq, void *data)
+{
+	struct xlnx_pcie_dma_device *xdev =
+		(struct xlnx_pcie_dma_device *)data;
+	struct ps_pcie_dma_chan *chan = NULL;
+	int i;
+	int err = -1;
+	int ret = -1;
+
+	for (i = 0; i < xdev->num_channels; i++) {
+		chan = &xdev->channels[i];
+		err = ps_pcie_check_intr_status(chan);
+		if (err == 0)
+			ret = 0;
+	}
+
+	return (ret == 0) ? IRQ_HANDLED : IRQ_NONE;
+}
+
+/**
+ * ps_pcie_dma_chan_intr_handler - This will be invoked for MSI-X interrupts
+ *
+ * @irq: IRQ number
+ * @data: Pointer to the PS PCIe DMA channel structure
+ *
+ * Return: IRQ_HANDLED
+ */
+static irqreturn_t ps_pcie_dma_chan_intr_handler(int irq, void *data)
+{
+	struct ps_pcie_dma_chan *chan = (struct ps_pcie_dma_chan *)data;
+
+	ps_pcie_check_intr_status(chan);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * chan_intr_setup - Requests Interrupt handler for individual channels
+ *
+ * @xdev: Driver specific data for device
+ *
+ * Return: 0 on success and non zero value on failure.
+ */
+static int chan_intr_setup(struct xlnx_pcie_dma_device *xdev)
+{
+	struct ps_pcie_dma_chan *chan;
+	int i;
+	int err = 0;
+
+	for (i = 0; i < xdev->num_channels; i++) {
+		chan = &xdev->channels[i];
+		err = devm_request_irq(xdev->dev,
+				       pci_irq_vector(xdev->pci_dev, i),
+				       ps_pcie_dma_chan_intr_handler,
+				       PS_PCIE_DMA_IRQ_NOSHARE,
+				       "PS PCIe DMA Chan Intr handler", chan);
+		if (err) {
+			dev_err(xdev->dev,
+				"Irq %d for chan %d error %d\n",
+				pci_irq_vector(xdev->pci_dev, i),
+				chan->channel_number, err);
+			break;
+		}
+	}
+
+	if (err) {
+		while (--i >= 0) {
+			chan = &xdev->channels[i];
+			devm_free_irq(xdev->dev,
+				      pci_irq_vector(xdev->pci_dev, i), chan);
+		}
+	}
+
+	return err;
+}
+
+/**
+ * device_intr_setup - Requests interrupt handler for DMA device
+ *
+ * @xdev: Driver specific data for device
+ *
+ * Return: 0 on success and non zero value on failure.
+ */
+static int device_intr_setup(struct xlnx_pcie_dma_device *xdev)
+{
+	int err;
+	unsigned long intr_flags = IRQF_SHARED;
+
+	if (xdev->pci_dev->msix_enabled || xdev->pci_dev->msi_enabled)
+		intr_flags = PS_PCIE_DMA_IRQ_NOSHARE;
+
+	err = devm_request_irq(xdev->dev,
+			       pci_irq_vector(xdev->pci_dev, 0),
+			       ps_pcie_dma_dev_intr_handler,
+			       intr_flags,
+			       "PS PCIe DMA Intr Handler", xdev);
+	if (err)
+		dev_err(xdev->dev, "Couldn't request irq %d\n",
+			pci_irq_vector(xdev->pci_dev, 0));
+
+	return err;
+}
+
+/**
+ * irq_setup - Requests interrupts based on the interrupt type detected
+ *
+ * @xdev: Driver specific data for device
+ *
+ * Return: 0 on success and non zero value on failure.
+ */
+static int irq_setup(struct xlnx_pcie_dma_device *xdev)
+{
+	int err;
+
+	if (xdev->irq_vecs == xdev->num_channels)
+		err = chan_intr_setup(xdev);
+	else
+		err = device_intr_setup(xdev);
+
+	return err;
+}
+
+static int platform_irq_setup(struct xlnx_pcie_dma_device *xdev)
+{
+	int err;
+
+	err = devm_request_irq(xdev->dev,
+			       xdev->platform_irq_vec,
+			       ps_pcie_dma_dev_intr_handler,
+			       IRQF_SHARED,
+			       "PS PCIe Root DMA Handler", xdev);
+	if (err)
+		dev_err(xdev->dev, "Couldn't request irq %d\n",
+			xdev->platform_irq_vec);
+
+	return err;
+}
+
+/**
+ * irq_probe - Checks which interrupt types can be serviced by hardware
+ *
+ * @xdev: Driver specific data for device
+ *
+ * Return: Number of interrupt vectors when successful or -ENOSPC on failure
+ */
+static int irq_probe(struct xlnx_pcie_dma_device *xdev)
+{
+	struct pci_dev *pdev;
+
+	pdev = xdev->pci_dev;
+
+	xdev->irq_vecs = pci_alloc_irq_vectors(pdev, 1, xdev->num_channels,
+					       PCI_IRQ_ALL_TYPES);
+	return xdev->irq_vecs;
+}
+
+/**
+ * ps_pcie_check_intr_status - Checks channel interrupt status
+ *
+ * @chan: Pointer to the PS PCIe DMA channel structure
+ *
+ * Return: 0 if interrupt is pending on channel
+ *		   -1 if no interrupt is pending on channel
+ */
+static int ps_pcie_check_intr_status(struct ps_pcie_dma_chan *chan)
+{
+	int err = -1;
+	u32 status;
+
+	if (chan->state != CHANNEL_AVAILABLE)
+		return err;
+
+	status = ps_pcie_dma_read(chan, chan->intr_status_offset);
+
+	if (status & DMA_INTSTATUS_SGLINTR_BIT) {
+		if (chan->primary_desc_cleanup) {
+			queue_work(chan->primary_desc_cleanup,
+				   &chan->handle_primary_desc_cleanup);
+		}
+		/* Clearing Persistent bit */
+		ps_pcie_dma_set_mask(chan, chan->intr_status_offset,
+				     DMA_INTSTATUS_SGLINTR_BIT);
+		err = 0;
+	}
+
+	if (status & DMA_INTSTATUS_SWINTR_BIT) {
+		if (chan->sw_intrs_wrkq)
+			queue_work(chan->sw_intrs_wrkq, &chan->handle_sw_intrs);
+		/* Clearing Persistent bit */
+		ps_pcie_dma_set_mask(chan, chan->intr_status_offset,
+				     DMA_INTSTATUS_SWINTR_BIT);
+		err = 0;
+	}
+
+	if (status & DMA_INTSTATUS_DMAERR_BIT) {
+		dev_err(chan->dev,
+			"DMA Channel %d ControlStatus Reg: 0x%x",
+			chan->channel_number, status);
+		dev_err(chan->dev,
+			"Chn %d SrcQLmt = %d SrcQSz = %d SrcQNxt = %d",
+			chan->channel_number,
+			chan->chan_base->src_q_limit,
+			chan->chan_base->src_q_size,
+			chan->chan_base->src_q_next);
+		dev_err(chan->dev,
+			"Chn %d SrcStaLmt = %d SrcStaSz = %d SrcStaNxt = %d",
+			chan->channel_number,
+			chan->chan_base->stas_q_limit,
+			chan->chan_base->stas_q_size,
+			chan->chan_base->stas_q_next);
+		dev_err(chan->dev,
+			"Chn %d DstQLmt = %d DstQSz = %d DstQNxt = %d",
+			chan->channel_number,
+			chan->chan_base->dst_q_limit,
+			chan->chan_base->dst_q_size,
+			chan->chan_base->dst_q_next);
+		dev_err(chan->dev,
+			"Chan %d DstStaLmt = %d DstStaSz = %d DstStaNxt = %d",
+			chan->channel_number,
+			chan->chan_base->stad_q_limit,
+			chan->chan_base->stad_q_size,
+			chan->chan_base->stad_q_next);
+		/* Clearing Persistent bit */
+		ps_pcie_dma_set_mask(chan, chan->intr_status_offset,
+				     DMA_INTSTATUS_DMAERR_BIT);
+
+		handle_error(chan);
+
+		err = 0;
+	}
+
+	return err;
+}
+
+static int init_hw_components(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->psrc_sgl_bd && chan->psrc_sta_bd) {
+		/*  Programming SourceQ and StatusQ bd addresses */
+		chan->chan_base->src_q_next = 0;
+		chan->chan_base->src_q_high =
+			upper_32_bits(chan->src_sgl_bd_pa);
+		chan->chan_base->src_q_size = chan->total_descriptors;
+		chan->chan_base->src_q_limit = 0;
+		if (chan->xdev->is_rootdma) {
+			chan->chan_base->src_q_low = ROOTDMA_Q_READ_ATTRIBUTE
+						     | DMA_QPTRLO_QLOCAXI_BIT;
+		} else {
+			chan->chan_base->src_q_low = 0;
+		}
+		chan->chan_base->src_q_low |=
+			(lower_32_bits((chan->src_sgl_bd_pa))
+			 & ~(DMA_SRC_Q_LOW_BIT_SHIFT))
+			| DMA_QPTRLO_Q_ENABLE_BIT;
+
+		chan->chan_base->stas_q_next = 0;
+		chan->chan_base->stas_q_high =
+			upper_32_bits(chan->src_sta_bd_pa);
+		chan->chan_base->stas_q_size = chan->total_descriptors;
+		chan->chan_base->stas_q_limit = chan->total_descriptors - 1;
+		if (chan->xdev->is_rootdma) {
+			chan->chan_base->stas_q_low = ROOTDMA_Q_READ_ATTRIBUTE
+						      | DMA_QPTRLO_QLOCAXI_BIT;
+		} else {
+			chan->chan_base->stas_q_low = 0;
+		}
+		chan->chan_base->stas_q_low |=
+			(lower_32_bits(chan->src_sta_bd_pa)
+			 & ~(DMA_SRC_Q_LOW_BIT_SHIFT))
+			| DMA_QPTRLO_Q_ENABLE_BIT;
+	}
+
+	if (chan->pdst_sgl_bd && chan->pdst_sta_bd) {
+		/*  Programming DestinationQ and StatusQ buffer descriptors */
+		chan->chan_base->dst_q_next = 0;
+		chan->chan_base->dst_q_high =
+			upper_32_bits(chan->dst_sgl_bd_pa);
+		chan->chan_base->dst_q_size = chan->total_descriptors;
+		chan->chan_base->dst_q_limit = 0;
+		if (chan->xdev->is_rootdma) {
+			chan->chan_base->dst_q_low = ROOTDMA_Q_READ_ATTRIBUTE
+						     | DMA_QPTRLO_QLOCAXI_BIT;
+		} else {
+			chan->chan_base->dst_q_low = 0;
+		}
+		chan->chan_base->dst_q_low |=
+			(lower_32_bits(chan->dst_sgl_bd_pa)
+			 & ~(DMA_SRC_Q_LOW_BIT_SHIFT))
+			| DMA_QPTRLO_Q_ENABLE_BIT;
+
+		chan->chan_base->stad_q_next = 0;
+		chan->chan_base->stad_q_high =
+			upper_32_bits(chan->dst_sta_bd_pa);
+		chan->chan_base->stad_q_size = chan->total_descriptors;
+		chan->chan_base->stad_q_limit = chan->total_descriptors - 1;
+		if (chan->xdev->is_rootdma) {
+			chan->chan_base->stad_q_low = ROOTDMA_Q_READ_ATTRIBUTE
+						      | DMA_QPTRLO_QLOCAXI_BIT;
+		} else {
+			chan->chan_base->stad_q_low = 0;
+		}
+		chan->chan_base->stad_q_low |=
+			(lower_32_bits(chan->dst_sta_bd_pa)
+			 & ~(DMA_SRC_Q_LOW_BIT_SHIFT))
+			| DMA_QPTRLO_Q_ENABLE_BIT;
+	}
+
+	return 0;
+}
+
+static void update_channel_read_attribute(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->xdev->is_rootdma) {
+		/* For Root DMA, Host Memory and Buffer Descriptors
+		 * will be on AXI side
+		 */
+		if (chan->srcq_buffer_location == BUFFER_LOC_PCI) {
+			chan->read_attribute = (AXI_ATTRIBUTE <<
+						SRC_CTL_ATTRIB_BIT_SHIFT) |
+						SOURCE_CONTROL_BD_LOC_AXI;
+		} else if (chan->srcq_buffer_location == BUFFER_LOC_AXI) {
+			chan->read_attribute = AXI_ATTRIBUTE <<
+					       SRC_CTL_ATTRIB_BIT_SHIFT;
+		}
+	} else {
+		if (chan->srcq_buffer_location == BUFFER_LOC_PCI) {
+			chan->read_attribute = PCI_ATTRIBUTE <<
+					       SRC_CTL_ATTRIB_BIT_SHIFT;
+		} else if (chan->srcq_buffer_location == BUFFER_LOC_AXI) {
+			chan->read_attribute = (AXI_ATTRIBUTE <<
+						SRC_CTL_ATTRIB_BIT_SHIFT) |
+						SOURCE_CONTROL_BD_LOC_AXI;
+		}
+	}
+}
+
+static void update_channel_write_attribute(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->xdev->is_rootdma) {
+		/* For Root DMA, Host Memory and Buffer Descriptors
+		 * will be on AXI side
+		 */
+		if (chan->dstq_buffer_location == BUFFER_LOC_PCI) {
+			chan->write_attribute = (AXI_ATTRIBUTE <<
+						 SRC_CTL_ATTRIB_BIT_SHIFT) |
+						SOURCE_CONTROL_BD_LOC_AXI;
+		} else if (chan->srcq_buffer_location == BUFFER_LOC_AXI) {
+			chan->write_attribute = AXI_ATTRIBUTE <<
+						SRC_CTL_ATTRIB_BIT_SHIFT;
+		}
+	} else {
+		if (chan->dstq_buffer_location == BUFFER_LOC_PCI) {
+			chan->write_attribute = PCI_ATTRIBUTE <<
+						SRC_CTL_ATTRIB_BIT_SHIFT;
+		} else if (chan->dstq_buffer_location == BUFFER_LOC_AXI) {
+			chan->write_attribute = (AXI_ATTRIBUTE <<
+						 SRC_CTL_ATTRIB_BIT_SHIFT) |
+						SOURCE_CONTROL_BD_LOC_AXI;
+		}
+	}
+	chan->write_attribute |= SOURCE_CONTROL_BACK_TO_BACK_PACK_BIT;
+}
+
+static int init_sw_components(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->ppkt_ctx_srcq && chan->psrc_sgl_bd &&
+	    chan->psrc_sta_bd) {
+		memset(chan->ppkt_ctx_srcq, 0,
+		       sizeof(struct PACKET_TRANSFER_PARAMS)
+		       * chan->total_descriptors);
+
+		memset(chan->psrc_sgl_bd, 0,
+		       sizeof(struct SOURCE_DMA_DESCRIPTOR)
+		       * chan->total_descriptors);
+
+		memset(chan->psrc_sta_bd, 0,
+		       sizeof(struct STATUS_DMA_DESCRIPTOR)
+		       * chan->total_descriptors);
+
+		chan->src_avail_descriptors = chan->total_descriptors;
+
+		chan->src_sgl_freeidx = 0;
+		chan->src_staprobe_idx = 0;
+		chan->src_sta_hw_probe_idx = chan->total_descriptors - 1;
+		chan->idx_ctx_srcq_head = 0;
+		chan->idx_ctx_srcq_tail = 0;
+	}
+
+	if (chan->ppkt_ctx_dstq && chan->pdst_sgl_bd &&
+	    chan->pdst_sta_bd) {
+		memset(chan->ppkt_ctx_dstq, 0,
+		       sizeof(struct PACKET_TRANSFER_PARAMS)
+		       * chan->total_descriptors);
+
+		memset(chan->pdst_sgl_bd, 0,
+		       sizeof(struct DEST_DMA_DESCRIPTOR)
+		       * chan->total_descriptors);
+
+		memset(chan->pdst_sta_bd, 0,
+		       sizeof(struct STATUS_DMA_DESCRIPTOR)
+		       * chan->total_descriptors);
+
+		chan->dst_avail_descriptors = chan->total_descriptors;
+
+		chan->dst_sgl_freeidx = 0;
+		chan->dst_staprobe_idx = 0;
+		chan->dst_sta_hw_probe_idx = chan->total_descriptors - 1;
+		chan->idx_ctx_dstq_head = 0;
+		chan->idx_ctx_dstq_tail = 0;
+	}
+
+	return 0;
+}
+
+/**
+ * ps_pcie_chan_reset - Resets channel, by programming relevant registers
+ *
+ * @chan: PS PCIe DMA channel information holder
+ * Return: void
+ */
+static void ps_pcie_chan_reset(struct ps_pcie_dma_chan *chan)
+{
+	/* Enable channel reset */
+	ps_pcie_dma_set_mask(chan, DMA_CNTRL_REG_OFFSET, DMA_CNTRL_RST_BIT);
+
+	mdelay(10);
+
+	/* Disable channel reset */
+	ps_pcie_dma_clr_mask(chan, DMA_CNTRL_REG_OFFSET, DMA_CNTRL_RST_BIT);
+}
+
+/**
+ * poll_completed_transactions - Function invoked by poll timer
+ *
+ * @t: Pointer to timer triggering this callback
+ * Return: void
+ */
+static void poll_completed_transactions(struct timer_list *t)
+{
+	struct ps_pcie_dma_chan *chan = from_timer(chan, t, poll_timer);
+
+	if (chan->state == CHANNEL_AVAILABLE) {
+		queue_work(chan->primary_desc_cleanup,
+			   &chan->handle_primary_desc_cleanup);
+	}
+
+	mod_timer(&chan->poll_timer, jiffies + chan->poll_timer_freq);
+}
+
+static bool check_descriptors_for_two_queues(struct ps_pcie_dma_chan *chan,
+					     struct ps_pcie_tx_segment *seg)
+{
+	if (seg->src_elements) {
+		if (chan->src_avail_descriptors >=
+		    seg->src_elements) {
+			return true;
+		}
+	} else if (seg->dst_elements) {
+		if (chan->dst_avail_descriptors >=
+		    seg->dst_elements) {
+			return true;
+		}
+	}
+
+	return false;
+}
+
+static bool check_descriptors_for_all_queues(struct ps_pcie_dma_chan *chan,
+					     struct ps_pcie_tx_segment *seg)
+{
+	if (chan->src_avail_descriptors >=
+		seg->src_elements &&
+	    chan->dst_avail_descriptors >=
+		seg->dst_elements) {
+		return true;
+	}
+
+	return false;
+}
+
+static bool check_descriptor_availability(struct ps_pcie_dma_chan *chan,
+					  struct ps_pcie_tx_segment *seg)
+{
+	if (chan->num_queues == DEFAULT_DMA_QUEUES)
+		return check_descriptors_for_all_queues(chan, seg);
+	else
+		return check_descriptors_for_two_queues(chan, seg);
+}
+
+static void handle_error(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->state != CHANNEL_AVAILABLE)
+		return;
+
+	spin_lock(&chan->channel_lock);
+	chan->state = CHANNEL_ERROR;
+	spin_unlock(&chan->channel_lock);
+
+	if (chan->maintenance_workq)
+		queue_work(chan->maintenance_workq, &chan->handle_chan_reset);
+}
+
+static void xlnx_ps_pcie_update_srcq(struct ps_pcie_dma_chan *chan,
+				     struct ps_pcie_tx_segment *seg)
+{
+	struct SOURCE_DMA_DESCRIPTOR *pdesc;
+	struct PACKET_TRANSFER_PARAMS *pkt_ctx = NULL;
+	struct ps_pcie_transfer_elements *ele = NULL;
+	u32 i = 0;
+
+	pkt_ctx = chan->ppkt_ctx_srcq + chan->idx_ctx_srcq_head;
+	if (pkt_ctx->availability_status == IN_USE) {
+		dev_err(chan->dev,
+			"src pkt context not avail for channel %d\n",
+			chan->channel_number);
+		handle_error(chan);
+		return;
+	}
+
+	pkt_ctx->availability_status = IN_USE;
+
+	if (chan->srcq_buffer_location == BUFFER_LOC_PCI)
+		pkt_ctx->seg = seg;
+
+	/*  Get the address of the next available DMA Descriptor */
+	pdesc = chan->psrc_sgl_bd + chan->src_sgl_freeidx;
+	pkt_ctx->idx_sop = chan->src_sgl_freeidx;
+
+	/* Build transactions using information in the scatter gather list */
+	list_for_each_entry(ele, &seg->transfer_nodes, node) {
+		if (chan->xdev->dma_buf_ext_addr) {
+			pdesc->system_address =
+				(u64)ele->src_pa;
+		} else {
+			pdesc->system_address =
+				(u32)ele->src_pa;
+		}
+
+		pdesc->control_byte_count = (ele->transfer_bytes &
+					    SOURCE_CONTROL_BD_BYTE_COUNT_MASK) |
+					    chan->read_attribute;
+
+		pdesc->user_handle = chan->idx_ctx_srcq_head;
+		pdesc->user_id = DEFAULT_UID;
+		/* Check if this is last descriptor */
+		if (i == (seg->src_elements - 1)) {
+			pkt_ctx->idx_eop = chan->src_sgl_freeidx;
+			pdesc->control_byte_count |= SOURCE_CONTROL_BD_EOP_BIT;
+			if ((seg->async_tx.flags & DMA_PREP_INTERRUPT) ==
+						   DMA_PREP_INTERRUPT) {
+				pdesc->control_byte_count |=
+					SOURCE_CONTROL_BD_INTR_BIT;
+			}
+		}
+		chan->src_sgl_freeidx++;
+		if (chan->src_sgl_freeidx == chan->total_descriptors)
+			chan->src_sgl_freeidx = 0;
+		pdesc = chan->psrc_sgl_bd + chan->src_sgl_freeidx;
+		spin_lock(&chan->src_desc_lock);
+		chan->src_avail_descriptors--;
+		spin_unlock(&chan->src_desc_lock);
+		i++;
+	}
+
+	chan->chan_base->src_q_limit = chan->src_sgl_freeidx;
+	chan->idx_ctx_srcq_head++;
+	if (chan->idx_ctx_srcq_head == chan->total_descriptors)
+		chan->idx_ctx_srcq_head = 0;
+}
+
+static void xlnx_ps_pcie_update_dstq(struct ps_pcie_dma_chan *chan,
+				     struct ps_pcie_tx_segment *seg)
+{
+	struct DEST_DMA_DESCRIPTOR *pdesc;
+	struct PACKET_TRANSFER_PARAMS *pkt_ctx = NULL;
+	struct ps_pcie_transfer_elements *ele = NULL;
+	u32 i = 0;
+
+	pkt_ctx = chan->ppkt_ctx_dstq + chan->idx_ctx_dstq_head;
+	if (pkt_ctx->availability_status == IN_USE) {
+		dev_err(chan->dev,
+			"dst pkt context not avail for channel %d\n",
+			chan->channel_number);
+		handle_error(chan);
+
+		return;
+	}
+
+	pkt_ctx->availability_status = IN_USE;
+
+	if (chan->dstq_buffer_location == BUFFER_LOC_PCI)
+		pkt_ctx->seg = seg;
+
+	pdesc = chan->pdst_sgl_bd + chan->dst_sgl_freeidx;
+	pkt_ctx->idx_sop = chan->dst_sgl_freeidx;
+
+	/* Build transactions using information in the scatter gather list */
+	list_for_each_entry(ele, &seg->transfer_nodes, node) {
+		if (chan->xdev->dma_buf_ext_addr) {
+			pdesc->system_address =
+				(u64)ele->dst_pa;
+		} else {
+			pdesc->system_address =
+				(u32)ele->dst_pa;
+		}
+		pdesc->control_byte_count = (ele->transfer_bytes &
+					SOURCE_CONTROL_BD_BYTE_COUNT_MASK) |
+						chan->write_attribute;
+
+		pdesc->user_handle = chan->idx_ctx_dstq_head;
+		/* Check if this is last descriptor */
+		if (i == (seg->dst_elements - 1))
+			pkt_ctx->idx_eop = chan->dst_sgl_freeidx;
+		chan->dst_sgl_freeidx++;
+		if (chan->dst_sgl_freeidx == chan->total_descriptors)
+			chan->dst_sgl_freeidx = 0;
+		pdesc = chan->pdst_sgl_bd + chan->dst_sgl_freeidx;
+		spin_lock(&chan->dst_desc_lock);
+		chan->dst_avail_descriptors--;
+		spin_unlock(&chan->dst_desc_lock);
+		i++;
+	}
+
+	chan->chan_base->dst_q_limit = chan->dst_sgl_freeidx;
+	chan->idx_ctx_dstq_head++;
+	if (chan->idx_ctx_dstq_head == chan->total_descriptors)
+		chan->idx_ctx_dstq_head = 0;
+}
+
+static void ps_pcie_chan_program_work(struct work_struct *work)
+{
+	struct ps_pcie_dma_chan *chan =
+		(struct ps_pcie_dma_chan *)container_of(work,
+				struct ps_pcie_dma_chan,
+				handle_chan_programming);
+	struct ps_pcie_tx_segment *seg = NULL;
+
+	while (chan->state == CHANNEL_AVAILABLE) {
+		spin_lock(&chan->active_list_lock);
+		seg = list_first_entry_or_null(&chan->active_list,
+					       struct ps_pcie_tx_segment, node);
+		spin_unlock(&chan->active_list_lock);
+
+		if (!seg)
+			break;
+
+		if (check_descriptor_availability(chan, seg) == false)
+			break;
+
+		spin_lock(&chan->active_list_lock);
+		list_del(&seg->node);
+		spin_unlock(&chan->active_list_lock);
+
+		if (seg->src_elements)
+			xlnx_ps_pcie_update_srcq(chan, seg);
+
+		if (seg->dst_elements)
+			xlnx_ps_pcie_update_dstq(chan, seg);
+	}
+}
+
+/**
+ * dst_cleanup_work - Goes through all completed elements in status Q
+ * and invokes callbacks for the concerned DMA transaction.
+ *
+ * @work: Work associated with the task
+ *
+ * Return: void
+ */
+static void dst_cleanup_work(struct work_struct *work)
+{
+	struct ps_pcie_dma_chan *chan =
+		(struct ps_pcie_dma_chan *)container_of(work,
+			struct ps_pcie_dma_chan, handle_dstq_desc_cleanup);
+
+	struct STATUS_DMA_DESCRIPTOR *psta_bd;
+	struct DEST_DMA_DESCRIPTOR *pdst_bd;
+	struct PACKET_TRANSFER_PARAMS *ppkt_ctx;
+	struct dmaengine_result rslt;
+	u32 completed_bytes;
+	u32 dstq_desc_idx;
+	struct ps_pcie_transfer_elements *ele, *ele_nxt;
+
+	psta_bd = chan->pdst_sta_bd + chan->dst_staprobe_idx;
+
+	while (psta_bd->status_flag_byte_count & STA_BD_COMPLETED_BIT) {
+		if (psta_bd->status_flag_byte_count &
+				STA_BD_DESTINATION_ERROR_BIT) {
+			dev_err(chan->dev,
+				"Dst Sts Elmnt %d chan %d has Destination Err",
+				chan->dst_staprobe_idx + 1,
+				chan->channel_number);
+			handle_error(chan);
+			break;
+		}
+		if (psta_bd->status_flag_byte_count & STA_BD_SOURCE_ERROR_BIT) {
+			dev_err(chan->dev,
+				"Dst Sts Elmnt %d chan %d has Source Error",
+				chan->dst_staprobe_idx + 1,
+				chan->channel_number);
+			handle_error(chan);
+			break;
+		}
+		if (psta_bd->status_flag_byte_count &
+				STA_BD_INTERNAL_ERROR_BIT) {
+			dev_err(chan->dev,
+				"Dst Sts Elmnt %d chan %d has Internal Error",
+				chan->dst_staprobe_idx + 1,
+				chan->channel_number);
+			handle_error(chan);
+			break;
+		}
+		/* we are using 64 bit USER field. */
+		if ((psta_bd->status_flag_byte_count &
+					STA_BD_UPPER_STATUS_NONZERO_BIT) == 0) {
+			dev_err(chan->dev,
+				"Dst Sts Elmnt %d for chan %d has NON ZERO",
+				chan->dst_staprobe_idx + 1,
+				chan->channel_number);
+			handle_error(chan);
+			break;
+		}
+
+		chan->idx_ctx_dstq_tail = psta_bd->user_handle;
+		ppkt_ctx = chan->ppkt_ctx_dstq + chan->idx_ctx_dstq_tail;
+		completed_bytes = (psta_bd->status_flag_byte_count &
+					STA_BD_BYTE_COUNT_MASK) >>
+						STA_BD_BYTE_COUNT_SHIFT;
+
+		memset(psta_bd, 0, sizeof(struct STATUS_DMA_DESCRIPTOR));
+
+		chan->dst_staprobe_idx++;
+
+		if (chan->dst_staprobe_idx == chan->total_descriptors)
+			chan->dst_staprobe_idx = 0;
+
+		chan->dst_sta_hw_probe_idx++;
+
+		if (chan->dst_sta_hw_probe_idx == chan->total_descriptors)
+			chan->dst_sta_hw_probe_idx = 0;
+
+		chan->chan_base->stad_q_limit = chan->dst_sta_hw_probe_idx;
+
+		psta_bd = chan->pdst_sta_bd + chan->dst_staprobe_idx;
+
+		dstq_desc_idx = ppkt_ctx->idx_sop;
+
+		do {
+			pdst_bd = chan->pdst_sgl_bd + dstq_desc_idx;
+			memset(pdst_bd, 0,
+			       sizeof(struct DEST_DMA_DESCRIPTOR));
+
+			spin_lock(&chan->dst_desc_lock);
+			chan->dst_avail_descriptors++;
+			spin_unlock(&chan->dst_desc_lock);
+
+			if (dstq_desc_idx == ppkt_ctx->idx_eop)
+				break;
+
+			dstq_desc_idx++;
+
+			if (dstq_desc_idx == chan->total_descriptors)
+				dstq_desc_idx = 0;
+
+		} while (1);
+
+		/* Invoking callback */
+		if (ppkt_ctx->seg) {
+			spin_lock(&chan->cookie_lock);
+			dma_cookie_complete(&ppkt_ctx->seg->async_tx);
+			spin_unlock(&chan->cookie_lock);
+			rslt.result = DMA_TRANS_NOERROR;
+			rslt.residue = ppkt_ctx->seg->total_transfer_bytes -
+					completed_bytes;
+			dmaengine_desc_get_callback_invoke(&ppkt_ctx->seg->async_tx,
+							   &rslt);
+			list_for_each_entry_safe(ele, ele_nxt,
+						 &ppkt_ctx->seg->transfer_nodes,
+						 node) {
+				list_del(&ele->node);
+				mempool_free(ele, chan->tx_elements_pool);
+			}
+			mempool_free(ppkt_ctx->seg, chan->transactions_pool);
+		}
+		memset(ppkt_ctx, 0, sizeof(struct PACKET_TRANSFER_PARAMS));
+	}
+
+	complete(&chan->dstq_work_complete);
+}
+
+/**
+ * src_cleanup_work - Goes through all completed elements in status Q and
+ * invokes callbacks for the concerned DMA transaction.
+ *
+ * @work: Work associated with the task
+ *
+ * Return: void
+ */
+static void src_cleanup_work(struct work_struct *work)
+{
+	struct ps_pcie_dma_chan *chan =
+		(struct ps_pcie_dma_chan *)container_of(
+		work, struct ps_pcie_dma_chan, handle_srcq_desc_cleanup);
+
+	struct STATUS_DMA_DESCRIPTOR *psta_bd;
+	struct SOURCE_DMA_DESCRIPTOR *psrc_bd;
+	struct PACKET_TRANSFER_PARAMS *ppkt_ctx;
+	struct dmaengine_result rslt;
+	u32 completed_bytes;
+	u32 srcq_desc_idx;
+	struct ps_pcie_transfer_elements *ele, *ele_nxt;
+
+	psta_bd = chan->psrc_sta_bd + chan->src_staprobe_idx;
+
+	while (psta_bd->status_flag_byte_count & STA_BD_COMPLETED_BIT) {
+		if (psta_bd->status_flag_byte_count &
+				STA_BD_DESTINATION_ERROR_BIT) {
+			dev_err(chan->dev,
+				"Src Sts Elmnt %d chan %d has Dst Error",
+				chan->src_staprobe_idx + 1,
+				chan->channel_number);
+			handle_error(chan);
+			break;
+		}
+		if (psta_bd->status_flag_byte_count & STA_BD_SOURCE_ERROR_BIT) {
+			dev_err(chan->dev,
+				"Src Sts Elmnt %d chan %d has Source Error",
+				chan->src_staprobe_idx + 1,
+				chan->channel_number);
+			handle_error(chan);
+			break;
+		}
+		if (psta_bd->status_flag_byte_count &
+				STA_BD_INTERNAL_ERROR_BIT) {
+			dev_err(chan->dev,
+				"Src Sts Elmnt %d chan %d has Internal Error",
+				chan->src_staprobe_idx + 1,
+				chan->channel_number);
+			handle_error(chan);
+			break;
+		}
+		if ((psta_bd->status_flag_byte_count
+				& STA_BD_UPPER_STATUS_NONZERO_BIT) == 0) {
+			dev_err(chan->dev,
+				"Src Sts Elmnt %d chan %d has NonZero",
+				chan->src_staprobe_idx + 1,
+				chan->channel_number);
+			handle_error(chan);
+			break;
+		}
+		chan->idx_ctx_srcq_tail = psta_bd->user_handle;
+		ppkt_ctx = chan->ppkt_ctx_srcq + chan->idx_ctx_srcq_tail;
+		completed_bytes = (psta_bd->status_flag_byte_count
+					& STA_BD_BYTE_COUNT_MASK) >>
+						STA_BD_BYTE_COUNT_SHIFT;
+
+		memset(psta_bd, 0, sizeof(struct STATUS_DMA_DESCRIPTOR));
+
+		chan->src_staprobe_idx++;
+
+		if (chan->src_staprobe_idx == chan->total_descriptors)
+			chan->src_staprobe_idx = 0;
+
+		chan->src_sta_hw_probe_idx++;
+
+		if (chan->src_sta_hw_probe_idx == chan->total_descriptors)
+			chan->src_sta_hw_probe_idx = 0;
+
+		chan->chan_base->stas_q_limit = chan->src_sta_hw_probe_idx;
+
+		psta_bd = chan->psrc_sta_bd + chan->src_staprobe_idx;
+
+		srcq_desc_idx = ppkt_ctx->idx_sop;
+
+		do {
+			psrc_bd = chan->psrc_sgl_bd + srcq_desc_idx;
+			memset(psrc_bd, 0,
+			       sizeof(struct SOURCE_DMA_DESCRIPTOR));
+
+			spin_lock(&chan->src_desc_lock);
+			chan->src_avail_descriptors++;
+			spin_unlock(&chan->src_desc_lock);
+
+			if (srcq_desc_idx == ppkt_ctx->idx_eop)
+				break;
+			srcq_desc_idx++;
+
+			if (srcq_desc_idx == chan->total_descriptors)
+				srcq_desc_idx = 0;
+
+		} while (1);
+
+		/* Invoking callback */
+		if (ppkt_ctx->seg) {
+			spin_lock(&chan->cookie_lock);
+			dma_cookie_complete(&ppkt_ctx->seg->async_tx);
+			spin_unlock(&chan->cookie_lock);
+			rslt.result = DMA_TRANS_NOERROR;
+			rslt.residue = ppkt_ctx->seg->total_transfer_bytes -
+					completed_bytes;
+			dmaengine_desc_get_callback_invoke(&ppkt_ctx->seg->async_tx,
+							   &rslt);
+			list_for_each_entry_safe(ele, ele_nxt,
+						 &ppkt_ctx->seg->transfer_nodes,
+						 node) {
+				list_del(&ele->node);
+				mempool_free(ele, chan->tx_elements_pool);
+			}
+			mempool_free(ppkt_ctx->seg, chan->transactions_pool);
+		}
+		memset(ppkt_ctx, 0, sizeof(struct PACKET_TRANSFER_PARAMS));
+	}
+
+	complete(&chan->srcq_work_complete);
+}
+
+/**
+ * ps_pcie_chan_primary_work - Masks out interrupts, invokes source Q and
+ * destination Q processing. Waits for source Q and destination Q processing
+ * and re enables interrupts. Same work is invoked by timer if coalesce count
+ * is greater than zero and interrupts are not invoked before the timeout period
+ *
+ * @work: Work associated with the task
+ *
+ * Return: void
+ */
+static void ps_pcie_chan_primary_work(struct work_struct *work)
+{
+	struct ps_pcie_dma_chan *chan =
+		(struct ps_pcie_dma_chan *)container_of(
+				work, struct ps_pcie_dma_chan,
+				handle_primary_desc_cleanup);
+
+	/* Disable interrupts for Channel */
+	ps_pcie_dma_clr_mask(chan, chan->intr_control_offset,
+			     DMA_INTCNTRL_ENABLINTR_BIT);
+
+	if (chan->psrc_sgl_bd) {
+		reinit_completion(&chan->srcq_work_complete);
+		if (chan->srcq_desc_cleanup)
+			queue_work(chan->srcq_desc_cleanup,
+				   &chan->handle_srcq_desc_cleanup);
+	}
+	if (chan->pdst_sgl_bd) {
+		reinit_completion(&chan->dstq_work_complete);
+		if (chan->dstq_desc_cleanup)
+			queue_work(chan->dstq_desc_cleanup,
+				   &chan->handle_dstq_desc_cleanup);
+	}
+
+	if (chan->psrc_sgl_bd)
+		wait_for_completion_interruptible(&chan->srcq_work_complete);
+	if (chan->pdst_sgl_bd)
+		wait_for_completion_interruptible(&chan->dstq_work_complete);
+
+	/* Enable interrupts for channel */
+	ps_pcie_dma_set_mask(chan, chan->intr_control_offset,
+			     DMA_INTCNTRL_ENABLINTR_BIT);
+
+	if (chan->chan_programming) {
+		queue_work(chan->chan_programming,
+			   &chan->handle_chan_programming);
+	}
+
+	if (chan->coalesce_count > 0 && chan->poll_timer.function)
+		mod_timer(&chan->poll_timer, jiffies + chan->poll_timer_freq);
+}
+
+static int read_rootdma_config(struct platform_device *platform_dev,
+			       struct xlnx_pcie_dma_device *xdev)
+{
+	int err;
+	struct resource *r;
+
+	err = dma_set_mask(&platform_dev->dev, DMA_BIT_MASK(64));
+	if (err) {
+		dev_info(&platform_dev->dev, "Cannot set 64 bit DMA mask\n");
+		err = dma_set_mask(&platform_dev->dev, DMA_BIT_MASK(32));
+		if (err) {
+			dev_err(&platform_dev->dev, "DMA mask set error\n");
+			return err;
+		}
+	}
+
+	err = dma_set_coherent_mask(&platform_dev->dev, DMA_BIT_MASK(64));
+	if (err) {
+		dev_info(&platform_dev->dev, "Cannot set 64 bit consistent DMA mask\n");
+		err = dma_set_coherent_mask(&platform_dev->dev,
+					    DMA_BIT_MASK(32));
+		if (err) {
+			dev_err(&platform_dev->dev, "Cannot set consistent DMA mask\n");
+			return err;
+		}
+	}
+
+	r = platform_get_resource_byname(platform_dev, IORESOURCE_MEM,
+					 "ps_pcie_regbase");
+	if (!r) {
+		dev_err(&platform_dev->dev,
+			"Unable to find memory resource for root dma\n");
+		return PTR_ERR(r);
+	}
+
+	xdev->reg_base = devm_ioremap_resource(&platform_dev->dev, r);
+	if (IS_ERR(xdev->reg_base)) {
+		dev_err(&platform_dev->dev, "ioresource error for root dma\n");
+		return PTR_ERR(xdev->reg_base);
+	}
+
+	xdev->platform_irq_vec =
+		platform_get_irq_byname(platform_dev,
+					"ps_pcie_rootdma_intr");
+	if (xdev->platform_irq_vec < 0) {
+		dev_err(&platform_dev->dev,
+			"Unable to get interrupt number for root dma\n");
+		return xdev->platform_irq_vec;
+	}
+
+	err = device_property_read_u16(&platform_dev->dev, "dma_vendorid",
+				       &xdev->rootdma_vendor);
+	if (err) {
+		dev_err(&platform_dev->dev,
+			"Unable to find RootDMA PCI Vendor Id\n");
+		return err;
+	}
+
+	err = device_property_read_u16(&platform_dev->dev, "dma_deviceid",
+				       &xdev->rootdma_device);
+	if (err) {
+		dev_err(&platform_dev->dev,
+			"Unable to find RootDMA PCI Device Id\n");
+		return err;
+	}
+
+	xdev->common.dev = xdev->dev;
+
+	return 0;
+}
+
+static int read_epdma_config(struct platform_device *platform_dev,
+			     struct xlnx_pcie_dma_device *xdev)
+{
+	int err;
+	struct pci_dev *pdev;
+	u16 i;
+	void __iomem * const *pci_iomap;
+	unsigned long pci_bar_length;
+
+	pdev = *((struct pci_dev **)(platform_dev->dev.platform_data));
+	xdev->pci_dev = pdev;
+
+	for (i = 0; i < MAX_BARS; i++) {
+		if (pci_resource_len(pdev, i) == 0)
+			continue;
+		xdev->bar_mask = xdev->bar_mask | (1 << (i));
+	}
+
+	err = pcim_iomap_regions(pdev, xdev->bar_mask, PLATFORM_DRIVER_NAME);
+	if (err) {
+		dev_err(&pdev->dev, "Cannot request PCI regions, aborting\n");
+		return err;
+	}
+
+	pci_iomap = pcim_iomap_table(pdev);
+	if (!pci_iomap) {
+		err = -ENOMEM;
+		return err;
+	}
+
+	for (i = 0; i < MAX_BARS; i++) {
+		pci_bar_length = pci_resource_len(pdev, i);
+		if (pci_bar_length == 0) {
+			xdev->bar_info[i].BAR_LENGTH = 0;
+			xdev->bar_info[i].BAR_PHYS_ADDR = 0;
+			xdev->bar_info[i].BAR_VIRT_ADDR = NULL;
+		} else {
+			xdev->bar_info[i].BAR_LENGTH =
+				pci_bar_length;
+			xdev->bar_info[i].BAR_PHYS_ADDR =
+				pci_resource_start(pdev, i);
+			xdev->bar_info[i].BAR_VIRT_ADDR =
+				(void *)pci_iomap[i];
+		}
+	}
+
+	xdev->reg_base = pci_iomap[DMA_BAR_NUMBER];
+
+	err = irq_probe(xdev);
+	if (err < 0) {
+		dev_err(&pdev->dev, "Cannot probe irq lines for device %d\n",
+			platform_dev->id);
+		return err;
+	}
+
+	xdev->common.dev = &pdev->dev;
+
+	return 0;
+}
+
+static int probe_channel_properties(struct platform_device *platform_dev,
+				    struct xlnx_pcie_dma_device *xdev,
+				    u16 channel_number)
+{
+	int i;
+	char propertyname[CHANNEL_PROPERTY_LENGTH];
+	int numvals, ret;
+	u32 *val;
+	struct ps_pcie_dma_chan *channel;
+	struct ps_pcie_dma_channel_match *xlnx_match;
+
+	snprintf(propertyname, CHANNEL_PROPERTY_LENGTH,
+		 "ps_pcie_channel%d", channel_number);
+
+	channel = &xdev->channels[channel_number];
+
+	spin_lock_init(&channel->channel_lock);
+	spin_lock_init(&channel->cookie_lock);
+
+	INIT_LIST_HEAD(&channel->pending_list);
+	spin_lock_init(&channel->pending_list_lock);
+
+	INIT_LIST_HEAD(&channel->active_list);
+	spin_lock_init(&channel->active_list_lock);
+
+	spin_lock_init(&channel->src_desc_lock);
+	spin_lock_init(&channel->dst_desc_lock);
+
+	INIT_LIST_HEAD(&channel->pending_interrupts_list);
+	spin_lock_init(&channel->pending_interrupts_lock);
+
+	INIT_LIST_HEAD(&channel->active_interrupts_list);
+	spin_lock_init(&channel->active_interrupts_lock);
+
+	init_completion(&channel->srcq_work_complete);
+	init_completion(&channel->dstq_work_complete);
+	init_completion(&channel->chan_shutdown_complt);
+	init_completion(&channel->chan_terminate_complete);
+
+	if (device_property_present(&platform_dev->dev, propertyname)) {
+		numvals = device_property_read_u32_array(&platform_dev->dev,
+							 propertyname, NULL, 0);
+
+		if (numvals < 0)
+			return numvals;
+
+		val = devm_kzalloc(&platform_dev->dev, sizeof(u32) * numvals,
+				   GFP_KERNEL);
+
+		if (!val)
+			return -ENOMEM;
+
+		ret = device_property_read_u32_array(&platform_dev->dev,
+						     propertyname, val,
+						     numvals);
+		if (ret < 0) {
+			dev_err(&platform_dev->dev,
+				"Unable to read property %s\n", propertyname);
+			return ret;
+		}
+
+		for (i = 0; i < numvals; i++) {
+			switch (i) {
+			case DMA_CHANNEL_DIRECTION:
+				channel->direction =
+					(val[DMA_CHANNEL_DIRECTION] ==
+						PCIE_AXI_DIRECTION) ?
+						DMA_TO_DEVICE : DMA_FROM_DEVICE;
+				break;
+			case NUM_DESCRIPTORS:
+				channel->total_descriptors =
+						val[NUM_DESCRIPTORS];
+				if (channel->total_descriptors >
+					MAX_DESCRIPTORS) {
+					dev_info(&platform_dev->dev,
+						 "Descriptors > alowd max\n");
+					channel->total_descriptors =
+							MAX_DESCRIPTORS;
+				}
+				break;
+			case NUM_QUEUES:
+				channel->num_queues = val[NUM_QUEUES];
+				switch (channel->num_queues) {
+				case DEFAULT_DMA_QUEUES:
+						break;
+				case TWO_DMA_QUEUES:
+						break;
+				default:
+				dev_info(&platform_dev->dev,
+					 "Incorrect Q number for dma chan\n");
+				channel->num_queues = DEFAULT_DMA_QUEUES;
+				}
+				break;
+			case COALESE_COUNT:
+				channel->coalesce_count = val[COALESE_COUNT];
+
+				if (channel->coalesce_count >
+					MAX_COALESCE_COUNT) {
+					dev_info(&platform_dev->dev,
+						 "Invalid coalesce Count\n");
+					channel->coalesce_count =
+						MAX_COALESCE_COUNT;
+				}
+				break;
+			case POLL_TIMER_FREQUENCY:
+				channel->poll_timer_freq =
+					val[POLL_TIMER_FREQUENCY];
+				break;
+			default:
+				dev_err(&platform_dev->dev,
+					"Check order of channel properties!\n");
+			}
+		}
+	} else {
+		dev_err(&platform_dev->dev,
+			"Property %s not present. Invalid configuration!\n",
+				propertyname);
+		return -ENOTSUPP;
+	}
+
+	if (channel->direction == DMA_TO_DEVICE) {
+		if (channel->num_queues == DEFAULT_DMA_QUEUES) {
+			channel->srcq_buffer_location = BUFFER_LOC_PCI;
+			channel->dstq_buffer_location = BUFFER_LOC_AXI;
+		} else {
+			channel->srcq_buffer_location = BUFFER_LOC_PCI;
+			channel->dstq_buffer_location = BUFFER_LOC_INVALID;
+		}
+	} else {
+		if (channel->num_queues == DEFAULT_DMA_QUEUES) {
+			channel->srcq_buffer_location = BUFFER_LOC_AXI;
+			channel->dstq_buffer_location = BUFFER_LOC_PCI;
+		} else {
+			channel->srcq_buffer_location = BUFFER_LOC_INVALID;
+			channel->dstq_buffer_location = BUFFER_LOC_PCI;
+		}
+	}
+
+	channel->xdev = xdev;
+	channel->channel_number = channel_number;
+
+	if (xdev->is_rootdma) {
+		channel->dev = xdev->dev;
+		channel->intr_status_offset = DMA_AXI_INTR_STATUS_REG_OFFSET;
+		channel->intr_control_offset = DMA_AXI_INTR_CNTRL_REG_OFFSET;
+	} else {
+		channel->dev = &xdev->pci_dev->dev;
+		channel->intr_status_offset = DMA_PCIE_INTR_STATUS_REG_OFFSET;
+		channel->intr_control_offset = DMA_PCIE_INTR_CNTRL_REG_OFFSET;
+	}
+
+	channel->chan_base =
+	(struct DMA_ENGINE_REGISTERS *)((__force char *)(xdev->reg_base) +
+				 (channel_number * DMA_CHANNEL_REGS_SIZE));
+
+	if ((channel->chan_base->dma_channel_status &
+				DMA_STATUS_DMA_PRES_BIT) == 0) {
+		dev_err(&platform_dev->dev,
+			"Hardware reports channel not present\n");
+		return -ENOTSUPP;
+	}
+
+	update_channel_read_attribute(channel);
+	update_channel_write_attribute(channel);
+
+	xlnx_match = devm_kzalloc(&platform_dev->dev,
+				  sizeof(struct ps_pcie_dma_channel_match),
+				  GFP_KERNEL);
+
+	if (!xlnx_match)
+		return -ENOMEM;
+
+	if (xdev->is_rootdma) {
+		xlnx_match->pci_vendorid = xdev->rootdma_vendor;
+		xlnx_match->pci_deviceid = xdev->rootdma_device;
+	} else {
+		xlnx_match->pci_vendorid = xdev->pci_dev->vendor;
+		xlnx_match->pci_deviceid = xdev->pci_dev->device;
+		xlnx_match->bar_params = xdev->bar_info;
+	}
+
+	xlnx_match->board_number = xdev->board_number;
+	xlnx_match->channel_number = channel_number;
+	xlnx_match->direction = xdev->channels[channel_number].direction;
+
+	channel->common.private = (void *)xlnx_match;
+
+	channel->common.device = &xdev->common;
+	list_add_tail(&channel->common.device_node, &xdev->common.channels);
+
+	return 0;
+}
+
+static void xlnx_ps_pcie_destroy_mempool(struct ps_pcie_dma_chan *chan)
+{
+	mempool_destroy(chan->transactions_pool);
+
+	mempool_destroy(chan->tx_elements_pool);
+
+	mempool_destroy(chan->intr_transactions_pool);
+}
+
+static void xlnx_ps_pcie_free_worker_queues(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->maintenance_workq)
+		destroy_workqueue(chan->maintenance_workq);
+
+	if (chan->sw_intrs_wrkq)
+		destroy_workqueue(chan->sw_intrs_wrkq);
+
+	if (chan->srcq_desc_cleanup)
+		destroy_workqueue(chan->srcq_desc_cleanup);
+
+	if (chan->dstq_desc_cleanup)
+		destroy_workqueue(chan->dstq_desc_cleanup);
+
+	if (chan->chan_programming)
+		destroy_workqueue(chan->chan_programming);
+
+	if (chan->primary_desc_cleanup)
+		destroy_workqueue(chan->primary_desc_cleanup);
+}
+
+static void xlnx_ps_pcie_free_pkt_ctxts(struct ps_pcie_dma_chan *chan)
+{
+	kfree(chan->ppkt_ctx_srcq);
+
+	kfree(chan->ppkt_ctx_dstq);
+}
+
+static void xlnx_ps_pcie_free_descriptors(struct ps_pcie_dma_chan *chan)
+{
+	ssize_t size;
+
+	if (chan->psrc_sgl_bd) {
+		size = chan->total_descriptors *
+			sizeof(struct SOURCE_DMA_DESCRIPTOR);
+		dma_free_coherent(chan->dev, size, chan->psrc_sgl_bd,
+				  chan->src_sgl_bd_pa);
+	}
+
+	if (chan->pdst_sgl_bd) {
+		size = chan->total_descriptors *
+			sizeof(struct DEST_DMA_DESCRIPTOR);
+		dma_free_coherent(chan->dev, size, chan->pdst_sgl_bd,
+				  chan->dst_sgl_bd_pa);
+	}
+
+	if (chan->psrc_sta_bd) {
+		size = chan->total_descriptors *
+			sizeof(struct STATUS_DMA_DESCRIPTOR);
+		dma_free_coherent(chan->dev, size, chan->psrc_sta_bd,
+				  chan->src_sta_bd_pa);
+	}
+
+	if (chan->pdst_sta_bd) {
+		size = chan->total_descriptors *
+			sizeof(struct STATUS_DMA_DESCRIPTOR);
+		dma_free_coherent(chan->dev, size, chan->pdst_sta_bd,
+				  chan->dst_sta_bd_pa);
+	}
+}
+
+static int xlnx_ps_pcie_channel_activate(struct ps_pcie_dma_chan *chan)
+{
+	u32 reg = chan->coalesce_count;
+
+	reg = reg << DMA_INTCNTRL_SGCOLSCCNT_BIT_SHIFT;
+
+	/* Enable Interrupts for channel */
+	ps_pcie_dma_set_mask(chan, chan->intr_control_offset,
+			     reg | DMA_INTCNTRL_ENABLINTR_BIT |
+			     DMA_INTCNTRL_DMAERRINTR_BIT |
+			     DMA_INTCNTRL_DMASGINTR_BIT);
+
+	/* Enable DMA */
+	ps_pcie_dma_set_mask(chan, DMA_CNTRL_REG_OFFSET,
+			     DMA_CNTRL_ENABL_BIT |
+			     DMA_CNTRL_64BIT_STAQ_ELEMSZ_BIT);
+
+	spin_lock(&chan->channel_lock);
+	chan->state = CHANNEL_AVAILABLE;
+	spin_unlock(&chan->channel_lock);
+
+	/* Activate timer if required */
+	if (chan->coalesce_count > 0 && !chan->poll_timer.function)
+		xlnx_ps_pcie_alloc_poll_timer(chan);
+
+	return 0;
+}
+
+static void xlnx_ps_pcie_channel_quiesce(struct ps_pcie_dma_chan *chan)
+{
+	/* Disable interrupts for Channel */
+	ps_pcie_dma_clr_mask(chan, chan->intr_control_offset,
+			     DMA_INTCNTRL_ENABLINTR_BIT);
+
+	/* Delete timer if it is created */
+	if (chan->coalesce_count > 0 && !chan->poll_timer.function)
+		xlnx_ps_pcie_free_poll_timer(chan);
+
+	/* Flush descriptor cleaning work queues */
+	if (chan->primary_desc_cleanup)
+		flush_workqueue(chan->primary_desc_cleanup);
+
+	/* Flush channel programming work queue */
+	if (chan->chan_programming)
+		flush_workqueue(chan->chan_programming);
+
+	/*  Clear the persistent bits */
+	ps_pcie_dma_set_mask(chan, chan->intr_status_offset,
+			     DMA_INTSTATUS_DMAERR_BIT |
+			     DMA_INTSTATUS_SGLINTR_BIT |
+			     DMA_INTSTATUS_SWINTR_BIT);
+
+	/* Disable DMA channel */
+	ps_pcie_dma_clr_mask(chan, DMA_CNTRL_REG_OFFSET, DMA_CNTRL_ENABL_BIT);
+
+	spin_lock(&chan->channel_lock);
+	chan->state = CHANNEL_UNAVIALBLE;
+	spin_unlock(&chan->channel_lock);
+}
+
+static void ivk_cbk_intr_seg(struct ps_pcie_intr_segment *intr_seg,
+			     struct ps_pcie_dma_chan *chan,
+			     enum dmaengine_tx_result result)
+{
+	struct dmaengine_result rslt;
+
+	rslt.result = result;
+	rslt.residue = 0;
+
+	spin_lock(&chan->cookie_lock);
+	dma_cookie_complete(&intr_seg->async_intr_tx);
+	spin_unlock(&chan->cookie_lock);
+
+	dmaengine_desc_get_callback_invoke(&intr_seg->async_intr_tx, &rslt);
+}
+
+static void ivk_cbk_seg(struct ps_pcie_tx_segment *seg,
+			struct ps_pcie_dma_chan *chan,
+			enum dmaengine_tx_result result)
+{
+	struct dmaengine_result rslt, *prslt;
+
+	spin_lock(&chan->cookie_lock);
+	dma_cookie_complete(&seg->async_tx);
+	spin_unlock(&chan->cookie_lock);
+
+	rslt.result = result;
+	if (seg->src_elements &&
+	    chan->srcq_buffer_location == BUFFER_LOC_PCI) {
+		rslt.residue = seg->total_transfer_bytes;
+		prslt = &rslt;
+	} else if (seg->dst_elements &&
+		   chan->dstq_buffer_location == BUFFER_LOC_PCI) {
+		rslt.residue = seg->total_transfer_bytes;
+		prslt = &rslt;
+	} else {
+		prslt = NULL;
+	}
+
+	dmaengine_desc_get_callback_invoke(&seg->async_tx, prslt);
+}
+
+static void ivk_cbk_ctx(struct PACKET_TRANSFER_PARAMS *ppkt_ctxt,
+			struct ps_pcie_dma_chan *chan,
+			enum dmaengine_tx_result result)
+{
+	if (ppkt_ctxt->availability_status == IN_USE) {
+		if (ppkt_ctxt->seg) {
+			ivk_cbk_seg(ppkt_ctxt->seg, chan, result);
+			mempool_free(ppkt_ctxt->seg,
+				     chan->transactions_pool);
+		}
+	}
+}
+
+static void ivk_cbk_for_pending(struct ps_pcie_dma_chan *chan)
+{
+	int i;
+	struct PACKET_TRANSFER_PARAMS *ppkt_ctxt;
+	struct ps_pcie_tx_segment *seg, *seg_nxt;
+	struct ps_pcie_intr_segment *intr_seg, *intr_seg_next;
+	struct ps_pcie_transfer_elements *ele, *ele_nxt;
+
+	if (chan->ppkt_ctx_srcq) {
+		if (chan->idx_ctx_srcq_tail != chan->idx_ctx_srcq_head) {
+			i = chan->idx_ctx_srcq_tail;
+			while (i != chan->idx_ctx_srcq_head) {
+				ppkt_ctxt = chan->ppkt_ctx_srcq + i;
+				ivk_cbk_ctx(ppkt_ctxt, chan,
+					    DMA_TRANS_READ_FAILED);
+				memset(ppkt_ctxt, 0,
+				       sizeof(struct PACKET_TRANSFER_PARAMS));
+				i++;
+				if (i == chan->total_descriptors)
+					i = 0;
+			}
+		}
+	}
+
+	if (chan->ppkt_ctx_dstq) {
+		if (chan->idx_ctx_dstq_tail != chan->idx_ctx_dstq_head) {
+			i = chan->idx_ctx_dstq_tail;
+			while (i != chan->idx_ctx_dstq_head) {
+				ppkt_ctxt = chan->ppkt_ctx_dstq + i;
+				ivk_cbk_ctx(ppkt_ctxt, chan,
+					    DMA_TRANS_WRITE_FAILED);
+				memset(ppkt_ctxt, 0,
+				       sizeof(struct PACKET_TRANSFER_PARAMS));
+				i++;
+				if (i == chan->total_descriptors)
+					i = 0;
+			}
+		}
+	}
+
+	list_for_each_entry_safe(seg, seg_nxt, &chan->active_list, node) {
+		ivk_cbk_seg(seg, chan, DMA_TRANS_ABORTED);
+		spin_lock(&chan->active_list_lock);
+		list_del(&seg->node);
+		spin_unlock(&chan->active_list_lock);
+		list_for_each_entry_safe(ele, ele_nxt,
+					 &seg->transfer_nodes, node) {
+			list_del(&ele->node);
+			mempool_free(ele, chan->tx_elements_pool);
+		}
+		mempool_free(seg, chan->transactions_pool);
+	}
+
+	list_for_each_entry_safe(seg, seg_nxt, &chan->pending_list, node) {
+		ivk_cbk_seg(seg, chan, DMA_TRANS_ABORTED);
+		spin_lock(&chan->pending_list_lock);
+		list_del(&seg->node);
+		spin_unlock(&chan->pending_list_lock);
+		list_for_each_entry_safe(ele, ele_nxt,
+					 &seg->transfer_nodes, node) {
+			list_del(&ele->node);
+			mempool_free(ele, chan->tx_elements_pool);
+		}
+		mempool_free(seg, chan->transactions_pool);
+	}
+
+	list_for_each_entry_safe(intr_seg, intr_seg_next,
+				 &chan->active_interrupts_list, node) {
+		ivk_cbk_intr_seg(intr_seg, chan, DMA_TRANS_ABORTED);
+		spin_lock(&chan->active_interrupts_lock);
+		list_del(&intr_seg->node);
+		spin_unlock(&chan->active_interrupts_lock);
+		mempool_free(intr_seg, chan->intr_transactions_pool);
+	}
+
+	list_for_each_entry_safe(intr_seg, intr_seg_next,
+				 &chan->pending_interrupts_list, node) {
+		ivk_cbk_intr_seg(intr_seg, chan, DMA_TRANS_ABORTED);
+		spin_lock(&chan->pending_interrupts_lock);
+		list_del(&intr_seg->node);
+		spin_unlock(&chan->pending_interrupts_lock);
+		mempool_free(intr_seg, chan->intr_transactions_pool);
+	}
+}
+
+static void xlnx_ps_pcie_reset_channel(struct ps_pcie_dma_chan *chan)
+{
+	xlnx_ps_pcie_channel_quiesce(chan);
+
+	ivk_cbk_for_pending(chan);
+
+	ps_pcie_chan_reset(chan);
+
+	init_sw_components(chan);
+	init_hw_components(chan);
+
+	xlnx_ps_pcie_channel_activate(chan);
+}
+
+static void xlnx_ps_pcie_free_poll_timer(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->poll_timer.function) {
+		del_timer_sync(&chan->poll_timer);
+		chan->poll_timer.function = NULL;
+	}
+}
+
+static int xlnx_ps_pcie_alloc_poll_timer(struct ps_pcie_dma_chan *chan)
+{
+	timer_setup(&chan->poll_timer, poll_completed_transactions, 0);
+	chan->poll_timer.expires = jiffies + chan->poll_timer_freq;
+
+	add_timer(&chan->poll_timer);
+
+	return 0;
+}
+
+static void terminate_transactions_work(struct work_struct *work)
+{
+	struct ps_pcie_dma_chan *chan =
+		(struct ps_pcie_dma_chan *)container_of(work,
+			struct ps_pcie_dma_chan, handle_chan_terminate);
+
+	xlnx_ps_pcie_channel_quiesce(chan);
+	ivk_cbk_for_pending(chan);
+	xlnx_ps_pcie_channel_activate(chan);
+
+	complete(&chan->chan_terminate_complete);
+}
+
+static void chan_shutdown_work(struct work_struct *work)
+{
+	struct ps_pcie_dma_chan *chan =
+		(struct ps_pcie_dma_chan *)container_of(work,
+				struct ps_pcie_dma_chan, handle_chan_shutdown);
+
+	xlnx_ps_pcie_channel_quiesce(chan);
+
+	complete(&chan->chan_shutdown_complt);
+}
+
+static void chan_reset_work(struct work_struct *work)
+{
+	struct ps_pcie_dma_chan *chan =
+		(struct ps_pcie_dma_chan *)container_of(work,
+				struct ps_pcie_dma_chan, handle_chan_reset);
+
+	xlnx_ps_pcie_reset_channel(chan);
+}
+
+static void sw_intr_work(struct work_struct *work)
+{
+	struct ps_pcie_dma_chan *chan =
+		(struct ps_pcie_dma_chan *)container_of(work,
+				struct ps_pcie_dma_chan, handle_sw_intrs);
+	struct ps_pcie_intr_segment *intr_seg, *intr_seg_next;
+
+	list_for_each_entry_safe(intr_seg, intr_seg_next,
+				 &chan->active_interrupts_list, node) {
+		spin_lock(&chan->cookie_lock);
+		dma_cookie_complete(&intr_seg->async_intr_tx);
+		spin_unlock(&chan->cookie_lock);
+		dmaengine_desc_get_callback_invoke(&intr_seg->async_intr_tx,
+						   NULL);
+		spin_lock(&chan->active_interrupts_lock);
+		list_del(&intr_seg->node);
+		spin_unlock(&chan->active_interrupts_lock);
+	}
+}
+
+static int xlnx_ps_pcie_alloc_worker_threads(struct ps_pcie_dma_chan *chan)
+{
+	char wq_name[WORKQ_NAME_SIZE];
+
+	snprintf(wq_name, WORKQ_NAME_SIZE,
+		 "PS PCIe channel %d descriptor programming wq",
+		 chan->channel_number);
+	chan->chan_programming =
+		create_singlethread_workqueue((const char *)wq_name);
+	if (!chan->chan_programming) {
+		dev_err(chan->dev,
+			"Unable to create programming wq for chan %d",
+			chan->channel_number);
+		goto err_no_desc_program_wq;
+	} else {
+		INIT_WORK(&chan->handle_chan_programming,
+			  ps_pcie_chan_program_work);
+	}
+	memset(wq_name, 0, WORKQ_NAME_SIZE);
+
+	snprintf(wq_name, WORKQ_NAME_SIZE,
+		 "PS PCIe channel %d primary cleanup wq", chan->channel_number);
+	chan->primary_desc_cleanup =
+		create_singlethread_workqueue((const char *)wq_name);
+	if (!chan->primary_desc_cleanup) {
+		dev_err(chan->dev,
+			"Unable to create primary cleanup wq for channel %d",
+			chan->channel_number);
+		goto err_no_primary_clean_wq;
+	} else {
+		INIT_WORK(&chan->handle_primary_desc_cleanup,
+			  ps_pcie_chan_primary_work);
+	}
+	memset(wq_name, 0, WORKQ_NAME_SIZE);
+
+	snprintf(wq_name, WORKQ_NAME_SIZE,
+		 "PS PCIe channel %d maintenance works wq",
+		 chan->channel_number);
+	chan->maintenance_workq =
+		create_singlethread_workqueue((const char *)wq_name);
+	if (!chan->maintenance_workq) {
+		dev_err(chan->dev,
+			"Unable to create maintenance wq for channel %d",
+			chan->channel_number);
+		goto err_no_maintenance_wq;
+	} else {
+		INIT_WORK(&chan->handle_chan_reset, chan_reset_work);
+		INIT_WORK(&chan->handle_chan_shutdown, chan_shutdown_work);
+		INIT_WORK(&chan->handle_chan_terminate,
+			  terminate_transactions_work);
+	}
+	memset(wq_name, 0, WORKQ_NAME_SIZE);
+
+	snprintf(wq_name, WORKQ_NAME_SIZE,
+		 "PS PCIe channel %d software Interrupts wq",
+		 chan->channel_number);
+	chan->sw_intrs_wrkq =
+		create_singlethread_workqueue((const char *)wq_name);
+	if (!chan->sw_intrs_wrkq) {
+		dev_err(chan->dev,
+			"Unable to create sw interrupts wq for channel %d",
+			chan->channel_number);
+		goto err_no_sw_intrs_wq;
+	} else {
+		INIT_WORK(&chan->handle_sw_intrs, sw_intr_work);
+	}
+	memset(wq_name, 0, WORKQ_NAME_SIZE);
+
+	if (chan->psrc_sgl_bd) {
+		snprintf(wq_name, WORKQ_NAME_SIZE,
+			 "PS PCIe channel %d srcq handling wq",
+			 chan->channel_number);
+		chan->srcq_desc_cleanup =
+			create_singlethread_workqueue((const char *)wq_name);
+		if (!chan->srcq_desc_cleanup) {
+			dev_err(chan->dev,
+				"Unable to create src q completion wq chan %d",
+				chan->channel_number);
+			goto err_no_src_q_completion_wq;
+		} else {
+			INIT_WORK(&chan->handle_srcq_desc_cleanup,
+				  src_cleanup_work);
+		}
+		memset(wq_name, 0, WORKQ_NAME_SIZE);
+	}
+
+	if (chan->pdst_sgl_bd) {
+		snprintf(wq_name, WORKQ_NAME_SIZE,
+			 "PS PCIe channel %d dstq handling wq",
+			 chan->channel_number);
+		chan->dstq_desc_cleanup =
+			create_singlethread_workqueue((const char *)wq_name);
+		if (!chan->dstq_desc_cleanup) {
+			dev_err(chan->dev,
+				"Unable to create dst q completion wq chan %d",
+				chan->channel_number);
+			goto err_no_dst_q_completion_wq;
+		} else {
+			INIT_WORK(&chan->handle_dstq_desc_cleanup,
+				  dst_cleanup_work);
+		}
+		memset(wq_name, 0, WORKQ_NAME_SIZE);
+	}
+
+	return 0;
+err_no_dst_q_completion_wq:
+	if (chan->srcq_desc_cleanup)
+		destroy_workqueue(chan->srcq_desc_cleanup);
+err_no_src_q_completion_wq:
+	if (chan->sw_intrs_wrkq)
+		destroy_workqueue(chan->sw_intrs_wrkq);
+err_no_sw_intrs_wq:
+	if (chan->maintenance_workq)
+		destroy_workqueue(chan->maintenance_workq);
+err_no_maintenance_wq:
+	if (chan->primary_desc_cleanup)
+		destroy_workqueue(chan->primary_desc_cleanup);
+err_no_primary_clean_wq:
+	if (chan->chan_programming)
+		destroy_workqueue(chan->chan_programming);
+err_no_desc_program_wq:
+	return -ENOMEM;
+}
+
+static int xlnx_ps_pcie_alloc_mempool(struct ps_pcie_dma_chan *chan)
+{
+	chan->transactions_pool =
+		mempool_create_kmalloc_pool(chan->total_descriptors,
+					    sizeof(struct ps_pcie_tx_segment));
+
+	if (!chan->transactions_pool)
+		goto no_transactions_pool;
+
+	chan->tx_elements_pool =
+		mempool_create_kmalloc_pool(chan->total_descriptors,
+					    sizeof(struct ps_pcie_transfer_elements));
+
+	if (!chan->tx_elements_pool)
+		goto no_tx_elements_pool;
+
+	chan->intr_transactions_pool =
+	mempool_create_kmalloc_pool(MIN_SW_INTR_TRANSACTIONS,
+				    sizeof(struct ps_pcie_intr_segment));
+
+	if (!chan->intr_transactions_pool)
+		goto no_intr_transactions_pool;
+
+	return 0;
+
+no_intr_transactions_pool:
+	mempool_destroy(chan->tx_elements_pool);
+no_tx_elements_pool:
+	mempool_destroy(chan->transactions_pool);
+no_transactions_pool:
+	return -ENOMEM;
+}
+
+static int xlnx_ps_pcie_alloc_pkt_contexts(struct ps_pcie_dma_chan *chan)
+{
+	if (chan->psrc_sgl_bd) {
+		chan->ppkt_ctx_srcq =
+			kcalloc(chan->total_descriptors,
+				sizeof(struct PACKET_TRANSFER_PARAMS),
+				GFP_KERNEL);
+		if (!chan->ppkt_ctx_srcq) {
+			dev_err(chan->dev,
+				"Src pkt cxt allocation for chan %d failed\n",
+				chan->channel_number);
+			goto err_no_src_pkt_ctx;
+		}
+	}
+
+	if (chan->pdst_sgl_bd) {
+		chan->ppkt_ctx_dstq =
+			kcalloc(chan->total_descriptors,
+				sizeof(struct PACKET_TRANSFER_PARAMS),
+				GFP_KERNEL);
+		if (!chan->ppkt_ctx_dstq) {
+			dev_err(chan->dev,
+				"Dst pkt cxt for chan %d failed\n",
+				chan->channel_number);
+			goto err_no_dst_pkt_ctx;
+		}
+	}
+
+	return 0;
+
+err_no_dst_pkt_ctx:
+	kfree(chan->ppkt_ctx_srcq);
+
+err_no_src_pkt_ctx:
+	return -ENOMEM;
+}
+
+static int dma_alloc_descriptors_two_queues(struct ps_pcie_dma_chan *chan)
+{
+	size_t size;
+
+	void *sgl_base;
+	void *sta_base;
+	dma_addr_t phy_addr_sglbase;
+	dma_addr_t phy_addr_stabase;
+
+	size = chan->total_descriptors *
+		sizeof(struct SOURCE_DMA_DESCRIPTOR);
+
+	sgl_base = dma_alloc_coherent(chan->dev, size, &phy_addr_sglbase,
+				      GFP_KERNEL);
+
+	if (!sgl_base) {
+		dev_err(chan->dev,
+			"Sgl bds in two channel mode for chan %d failed\n",
+			chan->channel_number);
+		goto err_no_sgl_bds;
+	}
+
+	size = chan->total_descriptors * sizeof(struct STATUS_DMA_DESCRIPTOR);
+	sta_base = dma_alloc_coherent(chan->dev, size, &phy_addr_stabase,
+				      GFP_KERNEL);
+
+	if (!sta_base) {
+		dev_err(chan->dev,
+			"Sta bds in two channel mode for chan %d failed\n",
+			chan->channel_number);
+		goto err_no_sta_bds;
+	}
+
+	if (chan->direction == DMA_TO_DEVICE) {
+		chan->psrc_sgl_bd = sgl_base;
+		chan->src_sgl_bd_pa = phy_addr_sglbase;
+
+		chan->psrc_sta_bd = sta_base;
+		chan->src_sta_bd_pa = phy_addr_stabase;
+
+		chan->pdst_sgl_bd = NULL;
+		chan->dst_sgl_bd_pa = 0;
+
+		chan->pdst_sta_bd = NULL;
+		chan->dst_sta_bd_pa = 0;
+
+	} else if (chan->direction == DMA_FROM_DEVICE) {
+		chan->psrc_sgl_bd = NULL;
+		chan->src_sgl_bd_pa = 0;
+
+		chan->psrc_sta_bd = NULL;
+		chan->src_sta_bd_pa = 0;
+
+		chan->pdst_sgl_bd = sgl_base;
+		chan->dst_sgl_bd_pa = phy_addr_sglbase;
+
+		chan->pdst_sta_bd = sta_base;
+		chan->dst_sta_bd_pa = phy_addr_stabase;
+
+	} else {
+		dev_err(chan->dev,
+			"%d %s() Unsupported channel direction\n",
+			__LINE__, __func__);
+		goto unsupported_channel_direction;
+	}
+
+	return 0;
+
+unsupported_channel_direction:
+	size = chan->total_descriptors *
+		sizeof(struct STATUS_DMA_DESCRIPTOR);
+	dma_free_coherent(chan->dev, size, sta_base, phy_addr_stabase);
+err_no_sta_bds:
+	size = chan->total_descriptors *
+		sizeof(struct SOURCE_DMA_DESCRIPTOR);
+	dma_free_coherent(chan->dev, size, sgl_base, phy_addr_sglbase);
+err_no_sgl_bds:
+
+	return -ENOMEM;
+}
+
+static int dma_alloc_decriptors_all_queues(struct ps_pcie_dma_chan *chan)
+{
+	size_t size;
+
+	size = chan->total_descriptors *
+		sizeof(struct SOURCE_DMA_DESCRIPTOR);
+	chan->psrc_sgl_bd =
+		dma_alloc_coherent(chan->dev, size, &chan->src_sgl_bd_pa,
+				   GFP_KERNEL);
+
+	if (!chan->psrc_sgl_bd) {
+		dev_err(chan->dev,
+			"Alloc fail src q buffer descriptors for chan %d\n",
+			chan->channel_number);
+		goto err_no_src_sgl_descriptors;
+	}
+
+	size = chan->total_descriptors * sizeof(struct DEST_DMA_DESCRIPTOR);
+	chan->pdst_sgl_bd =
+		dma_alloc_coherent(chan->dev, size, &chan->dst_sgl_bd_pa,
+				   GFP_KERNEL);
+
+	if (!chan->pdst_sgl_bd) {
+		dev_err(chan->dev,
+			"Alloc fail dst q buffer descriptors for chan %d\n",
+			chan->channel_number);
+		goto err_no_dst_sgl_descriptors;
+	}
+
+	size = chan->total_descriptors * sizeof(struct STATUS_DMA_DESCRIPTOR);
+	chan->psrc_sta_bd =
+		dma_alloc_coherent(chan->dev, size, &chan->src_sta_bd_pa,
+				   GFP_KERNEL);
+
+	if (!chan->psrc_sta_bd) {
+		dev_err(chan->dev,
+			"Unable to allocate src q status bds for chan %d\n",
+			chan->channel_number);
+		goto err_no_src_sta_descriptors;
+	}
+
+	chan->pdst_sta_bd =
+		dma_alloc_coherent(chan->dev, size, &chan->dst_sta_bd_pa,
+				   GFP_KERNEL);
+
+	if (!chan->pdst_sta_bd) {
+		dev_err(chan->dev,
+			"Unable to allocate Dst q status bds for chan %d\n",
+			chan->channel_number);
+		goto err_no_dst_sta_descriptors;
+	}
+
+	return 0;
+
+err_no_dst_sta_descriptors:
+	size = chan->total_descriptors *
+		sizeof(struct STATUS_DMA_DESCRIPTOR);
+	dma_free_coherent(chan->dev, size, chan->psrc_sta_bd,
+			  chan->src_sta_bd_pa);
+err_no_src_sta_descriptors:
+	size = chan->total_descriptors *
+		sizeof(struct DEST_DMA_DESCRIPTOR);
+	dma_free_coherent(chan->dev, size, chan->pdst_sgl_bd,
+			  chan->dst_sgl_bd_pa);
+err_no_dst_sgl_descriptors:
+	size = chan->total_descriptors *
+		sizeof(struct SOURCE_DMA_DESCRIPTOR);
+	dma_free_coherent(chan->dev, size, chan->psrc_sgl_bd,
+			  chan->src_sgl_bd_pa);
+
+err_no_src_sgl_descriptors:
+	return -ENOMEM;
+}
+
+static void xlnx_ps_pcie_dma_free_chan_resources(struct dma_chan *dchan)
+{
+	struct ps_pcie_dma_chan *chan;
+
+	if (!dchan)
+		return;
+
+	chan = to_xilinx_chan(dchan);
+
+	if (chan->state == CHANNEL_RESOURCE_UNALLOCATED)
+		return;
+
+	if (chan->maintenance_workq) {
+		if (completion_done(&chan->chan_shutdown_complt))
+			reinit_completion(&chan->chan_shutdown_complt);
+		queue_work(chan->maintenance_workq,
+			   &chan->handle_chan_shutdown);
+		wait_for_completion_interruptible(&chan->chan_shutdown_complt);
+
+		xlnx_ps_pcie_free_worker_queues(chan);
+		xlnx_ps_pcie_free_pkt_ctxts(chan);
+		xlnx_ps_pcie_destroy_mempool(chan);
+		xlnx_ps_pcie_free_descriptors(chan);
+
+		spin_lock(&chan->channel_lock);
+		chan->state = CHANNEL_RESOURCE_UNALLOCATED;
+		spin_unlock(&chan->channel_lock);
+	}
+}
+
+static int xlnx_ps_pcie_dma_alloc_chan_resources(struct dma_chan *dchan)
+{
+	struct ps_pcie_dma_chan *chan;
+
+	if (!dchan)
+		return PTR_ERR(dchan);
+
+	chan = to_xilinx_chan(dchan);
+
+	if (chan->state != CHANNEL_RESOURCE_UNALLOCATED)
+		return 0;
+
+	if (chan->num_queues == DEFAULT_DMA_QUEUES) {
+		if (dma_alloc_decriptors_all_queues(chan) != 0) {
+			dev_err(chan->dev,
+				"Alloc fail bds for channel %d\n",
+				chan->channel_number);
+			goto err_no_descriptors;
+		}
+	} else if (chan->num_queues == TWO_DMA_QUEUES) {
+		if (dma_alloc_descriptors_two_queues(chan) != 0) {
+			dev_err(chan->dev,
+				"Alloc fail bds for two queues of channel %d\n",
+			chan->channel_number);
+			goto err_no_descriptors;
+		}
+	}
+
+	if (xlnx_ps_pcie_alloc_mempool(chan) != 0) {
+		dev_err(chan->dev,
+			"Unable to allocate memory pool for channel %d\n",
+			chan->channel_number);
+		goto err_no_mempools;
+	}
+
+	if (xlnx_ps_pcie_alloc_pkt_contexts(chan) != 0) {
+		dev_err(chan->dev,
+			"Unable to allocate packet contexts for channel %d\n",
+			chan->channel_number);
+		goto err_no_pkt_ctxts;
+	}
+
+	if (xlnx_ps_pcie_alloc_worker_threads(chan) != 0) {
+		dev_err(chan->dev,
+			"Unable to allocate worker queues for channel %d\n",
+			chan->channel_number);
+		goto err_no_worker_queues;
+	}
+
+	xlnx_ps_pcie_reset_channel(chan);
+
+	dma_cookie_init(dchan);
+
+	return 0;
+
+err_no_worker_queues:
+	xlnx_ps_pcie_free_pkt_ctxts(chan);
+err_no_pkt_ctxts:
+	xlnx_ps_pcie_destroy_mempool(chan);
+err_no_mempools:
+	xlnx_ps_pcie_free_descriptors(chan);
+err_no_descriptors:
+	return -ENOMEM;
+}
+
+static dma_cookie_t xilinx_intr_tx_submit(struct dma_async_tx_descriptor *tx)
+{
+	struct ps_pcie_intr_segment *intr_seg =
+		to_ps_pcie_dma_tx_intr_descriptor(tx);
+	struct ps_pcie_dma_chan *chan = to_xilinx_chan(tx->chan);
+	dma_cookie_t cookie;
+
+	if (chan->state != CHANNEL_AVAILABLE)
+		return -EINVAL;
+
+	spin_lock(&chan->cookie_lock);
+	cookie = dma_cookie_assign(tx);
+	spin_unlock(&chan->cookie_lock);
+
+	spin_lock(&chan->pending_interrupts_lock);
+	list_add_tail(&intr_seg->node, &chan->pending_interrupts_list);
+	spin_unlock(&chan->pending_interrupts_lock);
+
+	return cookie;
+}
+
+static dma_cookie_t xilinx_dma_tx_submit(struct dma_async_tx_descriptor *tx)
+{
+	struct ps_pcie_tx_segment *seg = to_ps_pcie_dma_tx_descriptor(tx);
+	struct ps_pcie_dma_chan *chan = to_xilinx_chan(tx->chan);
+	dma_cookie_t cookie;
+
+	if (chan->state != CHANNEL_AVAILABLE)
+		return -EINVAL;
+
+	spin_lock(&chan->cookie_lock);
+	cookie = dma_cookie_assign(tx);
+	spin_unlock(&chan->cookie_lock);
+
+	spin_lock(&chan->pending_list_lock);
+	list_add_tail(&seg->node, &chan->pending_list);
+	spin_unlock(&chan->pending_list_lock);
+
+	return cookie;
+}
+
+/**
+ * xlnx_ps_pcie_dma_prep_memcpy - prepare descriptors for a memcpy transaction
+ * @channel: DMA channel
+ * @dma_dst: destination address
+ * @dma_src: source address
+ * @len: transfer length
+ * @flags: transfer ack flags
+ *
+ * Return: Async transaction descriptor on success and NULL on failure
+ */
+static struct dma_async_tx_descriptor *
+xlnx_ps_pcie_dma_prep_memcpy(struct dma_chan *channel, dma_addr_t dma_dst,
+			     dma_addr_t dma_src, size_t len,
+			     unsigned long flags)
+{
+	struct ps_pcie_dma_chan *chan = to_xilinx_chan(channel);
+	struct ps_pcie_tx_segment *seg = NULL;
+	struct ps_pcie_transfer_elements *ele = NULL;
+	struct ps_pcie_transfer_elements *ele_nxt = NULL;
+	u32 i;
+
+	if (chan->state != CHANNEL_AVAILABLE)
+		return NULL;
+
+	if (chan->num_queues != DEFAULT_DMA_QUEUES) {
+		dev_err(chan->dev, "Only prep_slave_sg for channel %d\n",
+			chan->channel_number);
+		return NULL;
+	}
+
+	seg = mempool_alloc(chan->transactions_pool, GFP_ATOMIC);
+	if (!seg) {
+		dev_err(chan->dev, "Tx segment alloc for channel %d\n",
+			chan->channel_number);
+		return NULL;
+	}
+
+	memset(seg, 0, sizeof(*seg));
+	INIT_LIST_HEAD(&seg->transfer_nodes);
+
+	for (i = 0; i < len / MAX_TRANSFER_LENGTH; i++) {
+		ele = mempool_alloc(chan->tx_elements_pool, GFP_ATOMIC);
+		if (!ele) {
+			dev_err(chan->dev, "Tx element %d for channel %d\n",
+				i, chan->channel_number);
+			goto err_elements_prep_memcpy;
+		}
+		ele->src_pa = dma_src + (i * MAX_TRANSFER_LENGTH);
+		ele->dst_pa = dma_dst + (i * MAX_TRANSFER_LENGTH);
+		ele->transfer_bytes = MAX_TRANSFER_LENGTH;
+		list_add_tail(&ele->node, &seg->transfer_nodes);
+		seg->src_elements++;
+		seg->dst_elements++;
+		seg->total_transfer_bytes += ele->transfer_bytes;
+		ele = NULL;
+	}
+
+	if (len % MAX_TRANSFER_LENGTH) {
+		ele = mempool_alloc(chan->tx_elements_pool, GFP_ATOMIC);
+		if (!ele) {
+			dev_err(chan->dev, "Tx element %d for channel %d\n",
+				i, chan->channel_number);
+			goto err_elements_prep_memcpy;
+		}
+		ele->src_pa = dma_src + (i * MAX_TRANSFER_LENGTH);
+		ele->dst_pa = dma_dst + (i * MAX_TRANSFER_LENGTH);
+		ele->transfer_bytes = len % MAX_TRANSFER_LENGTH;
+		list_add_tail(&ele->node, &seg->transfer_nodes);
+		seg->src_elements++;
+		seg->dst_elements++;
+		seg->total_transfer_bytes += ele->transfer_bytes;
+	}
+
+	if (seg->src_elements > chan->total_descriptors) {
+		dev_err(chan->dev, "Insufficient descriptors in channel %d for dma transaction\n",
+			chan->channel_number);
+		goto err_elements_prep_memcpy;
+	}
+
+	dma_async_tx_descriptor_init(&seg->async_tx, &chan->common);
+	seg->async_tx.flags = flags;
+	async_tx_ack(&seg->async_tx);
+	seg->async_tx.tx_submit = xilinx_dma_tx_submit;
+
+	return &seg->async_tx;
+
+err_elements_prep_memcpy:
+	list_for_each_entry_safe(ele, ele_nxt, &seg->transfer_nodes, node) {
+		list_del(&ele->node);
+		mempool_free(ele, chan->tx_elements_pool);
+	}
+	mempool_free(seg, chan->transactions_pool);
+	return NULL;
+}
+
+static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_slave_sg(
+		struct dma_chan *channel, struct scatterlist *sgl,
+		unsigned int sg_len, enum dma_transfer_direction direction,
+		unsigned long flags, void *context)
+{
+	struct ps_pcie_dma_chan *chan = to_xilinx_chan(channel);
+	struct ps_pcie_tx_segment *seg = NULL;
+	struct scatterlist *sgl_ptr;
+	struct ps_pcie_transfer_elements *ele = NULL;
+	struct ps_pcie_transfer_elements *ele_nxt = NULL;
+	u32 i, j;
+
+	if (chan->state != CHANNEL_AVAILABLE)
+		return NULL;
+
+	if (!(is_slave_direction(direction)))
+		return NULL;
+
+	if (!sgl || sg_len == 0)
+		return NULL;
+
+	if (chan->num_queues != TWO_DMA_QUEUES) {
+		dev_err(chan->dev, "Only prep_dma_memcpy is supported channel %d\n",
+			chan->channel_number);
+		return NULL;
+	}
+
+	seg = mempool_alloc(chan->transactions_pool, GFP_ATOMIC);
+	if (!seg) {
+		dev_err(chan->dev, "Unable to allocate tx segment channel %d\n",
+			chan->channel_number);
+		return NULL;
+	}
+
+	memset(seg, 0, sizeof(*seg));
+
+	for_each_sg(sgl, sgl_ptr, sg_len, j) {
+		for (i = 0; i < sg_dma_len(sgl_ptr) / MAX_TRANSFER_LENGTH; i++) {
+			ele = mempool_alloc(chan->tx_elements_pool, GFP_ATOMIC);
+			if (!ele) {
+				dev_err(chan->dev, "Tx element %d for channel %d\n",
+					i, chan->channel_number);
+				goto err_elements_prep_slave_sg;
+			}
+			if (chan->direction == DMA_TO_DEVICE) {
+				ele->src_pa = sg_dma_address(sgl_ptr) +
+						(i * MAX_TRANSFER_LENGTH);
+				seg->src_elements++;
+			} else {
+				ele->dst_pa = sg_dma_address(sgl_ptr) +
+						(i * MAX_TRANSFER_LENGTH);
+				seg->dst_elements++;
+			}
+			ele->transfer_bytes = MAX_TRANSFER_LENGTH;
+			list_add_tail(&ele->node, &seg->transfer_nodes);
+			seg->total_transfer_bytes += ele->transfer_bytes;
+			ele = NULL;
+		}
+		if (sg_dma_len(sgl_ptr) % MAX_TRANSFER_LENGTH) {
+			ele = mempool_alloc(chan->tx_elements_pool, GFP_ATOMIC);
+			if (!ele) {
+				dev_err(chan->dev, "Tx element %d for channel %d\n",
+					i, chan->channel_number);
+				goto err_elements_prep_slave_sg;
+			}
+			if (chan->direction == DMA_TO_DEVICE) {
+				ele->src_pa = sg_dma_address(sgl_ptr) +
+						(i * MAX_TRANSFER_LENGTH);
+				seg->src_elements++;
+			} else {
+				ele->dst_pa = sg_dma_address(sgl_ptr) +
+						(i * MAX_TRANSFER_LENGTH);
+				seg->dst_elements++;
+			}
+			ele->transfer_bytes = sg_dma_len(sgl_ptr) %
+						MAX_TRANSFER_LENGTH;
+			list_add_tail(&ele->node, &seg->transfer_nodes);
+			seg->total_transfer_bytes += ele->transfer_bytes;
+		}
+	}
+
+	if (max(seg->src_elements, seg->dst_elements) >
+		chan->total_descriptors) {
+		dev_err(chan->dev, "Insufficient descriptors in channel %d for dma transaction\n",
+			chan->channel_number);
+		goto err_elements_prep_slave_sg;
+	}
+
+	dma_async_tx_descriptor_init(&seg->async_tx, &chan->common);
+	seg->async_tx.flags = flags;
+	async_tx_ack(&seg->async_tx);
+	seg->async_tx.tx_submit = xilinx_dma_tx_submit;
+
+	return &seg->async_tx;
+
+err_elements_prep_slave_sg:
+	list_for_each_entry_safe(ele, ele_nxt, &seg->transfer_nodes, node) {
+		list_del(&ele->node);
+		mempool_free(ele, chan->tx_elements_pool);
+	}
+	mempool_free(seg, chan->transactions_pool);
+	return NULL;
+}
+
+static void xlnx_ps_pcie_dma_issue_pending(struct dma_chan *channel)
+{
+	struct ps_pcie_dma_chan *chan;
+
+	if (!channel)
+		return;
+
+	chan = to_xilinx_chan(channel);
+
+	if (!list_empty(&chan->pending_list)) {
+		spin_lock(&chan->pending_list_lock);
+		spin_lock(&chan->active_list_lock);
+		list_splice_tail_init(&chan->pending_list,
+				      &chan->active_list);
+		spin_unlock(&chan->active_list_lock);
+		spin_unlock(&chan->pending_list_lock);
+	}
+
+	if (!list_empty(&chan->pending_interrupts_list)) {
+		spin_lock(&chan->pending_interrupts_lock);
+		spin_lock(&chan->active_interrupts_lock);
+		list_splice_tail_init(&chan->pending_interrupts_list,
+				      &chan->active_interrupts_list);
+		spin_unlock(&chan->active_interrupts_lock);
+		spin_unlock(&chan->pending_interrupts_lock);
+	}
+
+	if (chan->chan_programming)
+		queue_work(chan->chan_programming,
+			   &chan->handle_chan_programming);
+}
+
+static int xlnx_ps_pcie_dma_terminate_all(struct dma_chan *channel)
+{
+	struct ps_pcie_dma_chan *chan;
+
+	if (!channel)
+		return PTR_ERR(channel);
+
+	chan = to_xilinx_chan(channel);
+
+	if (chan->state != CHANNEL_AVAILABLE)
+		return 1;
+
+	if (chan->maintenance_workq) {
+		if (completion_done(&chan->chan_terminate_complete))
+			reinit_completion(&chan->chan_terminate_complete);
+		queue_work(chan->maintenance_workq,
+			   &chan->handle_chan_terminate);
+		wait_for_completion_interruptible(
+			   &chan->chan_terminate_complete);
+	}
+
+	return 0;
+}
+
+static struct dma_async_tx_descriptor *xlnx_ps_pcie_dma_prep_interrupt(
+		struct dma_chan *channel, unsigned long flags)
+{
+	struct ps_pcie_dma_chan *chan;
+	struct ps_pcie_intr_segment *intr_segment = NULL;
+
+	if (!channel)
+		return NULL;
+
+	chan = to_xilinx_chan(channel);
+
+	if (chan->state != CHANNEL_AVAILABLE)
+		return NULL;
+
+	intr_segment = mempool_alloc(chan->intr_transactions_pool, GFP_ATOMIC);
+
+	memset(intr_segment, 0, sizeof(*intr_segment));
+
+	dma_async_tx_descriptor_init(&intr_segment->async_intr_tx,
+				     &chan->common);
+	intr_segment->async_intr_tx.flags = flags;
+	async_tx_ack(&intr_segment->async_intr_tx);
+	intr_segment->async_intr_tx.tx_submit = xilinx_intr_tx_submit;
+
+	return &intr_segment->async_intr_tx;
+}
+
+static int xlnx_pcie_dma_driver_probe(struct platform_device *platform_dev)
+{
+	int err, i;
+	struct xlnx_pcie_dma_device *xdev;
+	static u16 board_number;
+
+	xdev = devm_kzalloc(&platform_dev->dev,
+			    sizeof(struct xlnx_pcie_dma_device), GFP_KERNEL);
+
+	if (!xdev)
+		return -ENOMEM;
+
+#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
+	xdev->dma_buf_ext_addr = true;
+#else
+	xdev->dma_buf_ext_addr = false;
+#endif
+
+	xdev->is_rootdma = device_property_read_bool(&platform_dev->dev,
+						     "rootdma");
+
+	xdev->dev = &platform_dev->dev;
+	xdev->board_number = board_number;
+
+	err = device_property_read_u32(&platform_dev->dev, "numchannels",
+				       &xdev->num_channels);
+	if (err) {
+		dev_err(&platform_dev->dev,
+			"Unable to find numchannels property\n");
+		goto platform_driver_probe_return;
+	}
+
+	if (xdev->num_channels == 0 || xdev->num_channels >
+		MAX_ALLOWED_CHANNELS_IN_HW) {
+		dev_warn(&platform_dev->dev,
+			 "Invalid xlnx-num_channels property value\n");
+		xdev->num_channels = MAX_ALLOWED_CHANNELS_IN_HW;
+	}
+
+	xdev->channels =
+	(struct ps_pcie_dma_chan *)devm_kzalloc(&platform_dev->dev,
+						sizeof(struct ps_pcie_dma_chan)
+							* xdev->num_channels,
+						GFP_KERNEL);
+	if (!xdev->channels) {
+		err = -ENOMEM;
+		goto platform_driver_probe_return;
+	}
+
+	if (xdev->is_rootdma)
+		err = read_rootdma_config(platform_dev, xdev);
+	else
+		err = read_epdma_config(platform_dev, xdev);
+
+	if (err) {
+		dev_err(&platform_dev->dev,
+			"Unable to initialize dma configuration\n");
+		goto platform_driver_probe_return;
+	}
+
+	/* Initialize the DMA engine */
+	INIT_LIST_HEAD(&xdev->common.channels);
+
+	dma_cap_set(DMA_SLAVE, xdev->common.cap_mask);
+	dma_cap_set(DMA_PRIVATE, xdev->common.cap_mask);
+	dma_cap_set(DMA_INTERRUPT, xdev->common.cap_mask);
+	dma_cap_set(DMA_MEMCPY, xdev->common.cap_mask);
+
+	xdev->common.src_addr_widths = DMA_SLAVE_BUSWIDTH_UNDEFINED;
+	xdev->common.dst_addr_widths = DMA_SLAVE_BUSWIDTH_UNDEFINED;
+	xdev->common.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);
+	xdev->common.device_alloc_chan_resources =
+		xlnx_ps_pcie_dma_alloc_chan_resources;
+	xdev->common.device_free_chan_resources =
+		xlnx_ps_pcie_dma_free_chan_resources;
+	xdev->common.device_terminate_all = xlnx_ps_pcie_dma_terminate_all;
+	xdev->common.device_tx_status =  dma_cookie_status;
+	xdev->common.device_issue_pending = xlnx_ps_pcie_dma_issue_pending;
+	xdev->common.device_prep_dma_interrupt =
+		xlnx_ps_pcie_dma_prep_interrupt;
+	xdev->common.device_prep_dma_memcpy = xlnx_ps_pcie_dma_prep_memcpy;
+	xdev->common.device_prep_slave_sg = xlnx_ps_pcie_dma_prep_slave_sg;
+	xdev->common.residue_granularity = DMA_RESIDUE_GRANULARITY_SEGMENT;
+
+	for (i = 0; i < xdev->num_channels; i++) {
+		err = probe_channel_properties(platform_dev, xdev, i);
+
+		if (err != 0) {
+			dev_err(xdev->dev,
+				"Unable to read channel properties\n");
+			goto platform_driver_probe_return;
+		}
+	}
+
+	if (xdev->is_rootdma)
+		err = platform_irq_setup(xdev);
+	else
+		err = irq_setup(xdev);
+	if (err) {
+		dev_err(xdev->dev, "Cannot request irq lines for device %d\n",
+			xdev->board_number);
+		goto platform_driver_probe_return;
+	}
+
+	err = dma_async_device_register(&xdev->common);
+	if (err) {
+		dev_err(xdev->dev,
+			"Unable to register board %d with dma framework\n",
+			xdev->board_number);
+		goto platform_driver_probe_return;
+	}
+
+	platform_set_drvdata(platform_dev, xdev);
+
+	board_number++;
+
+	dev_info(&platform_dev->dev, "PS PCIe Platform driver probed\n");
+	return 0;
+
+platform_driver_probe_return:
+	return err;
+}
+
+static int xlnx_pcie_dma_driver_remove(struct platform_device *platform_dev)
+{
+	struct xlnx_pcie_dma_device *xdev =
+		platform_get_drvdata(platform_dev);
+	int i;
+
+	for (i = 0; i < xdev->num_channels; i++)
+		xlnx_ps_pcie_dma_free_chan_resources(&xdev->channels[i].common);
+
+	dma_async_device_unregister(&xdev->common);
+
+	return 0;
+}
+
+#ifdef CONFIG_OF
+static const struct of_device_id xlnx_pcie_root_dma_of_ids[] = {
+	{ .compatible = "xlnx,ps_pcie_dma-1.00.a", },
+	{}
+};
+MODULE_DEVICE_TABLE(of, xlnx_pcie_root_dma_of_ids);
+#endif
+
+static struct platform_driver xlnx_pcie_dma_driver = {
+	.driver = {
+		.name = XLNX_PLATFORM_DRIVER_NAME,
+		.of_match_table = of_match_ptr(xlnx_pcie_root_dma_of_ids),
+		.owner = THIS_MODULE,
+	},
+	.probe =  xlnx_pcie_dma_driver_probe,
+	.remove = xlnx_pcie_dma_driver_remove,
+};
+
+int dma_platform_driver_register(void)
+{
+	return platform_driver_register(&xlnx_pcie_dma_driver);
+}
+
+void dma_platform_driver_unregister(void)
+{
+	platform_driver_unregister(&xlnx_pcie_dma_driver);
+}
diff --git a/drivers/dma/xilinx/zynqmp_dma.c b/drivers/dma/xilinx/zynqmp_dma.c
index 5fecf5aa6..f67495905 100644
--- a/drivers/dma/xilinx/zynqmp_dma.c
+++ b/drivers/dma/xilinx/zynqmp_dma.c
@@ -454,6 +454,7 @@ static void zynqmp_dma_free_desc_list(struct zynqmp_dma_chan *chan,
 
 	list_for_each_entry_safe(desc, next, list, node)
 		zynqmp_dma_free_descriptor(chan, desc);
+	INIT_LIST_HEAD(list);
 }
 
 /**
@@ -490,7 +491,8 @@ static int zynqmp_dma_alloc_chan_resources(struct dma_chan *dchan)
 	}
 
 	chan->desc_pool_v = dma_alloc_coherent(chan->dev,
-					       (2 * chan->desc_size * ZYNQMP_DMA_NUM_DESCS),
+					       ((size_t)(2 * chan->desc_size) *
+						ZYNQMP_DMA_NUM_DESCS),
 					       &chan->desc_pool_p, GFP_KERNEL);
 	if (!chan->desc_pool_v)
 		return -ENOMEM;
@@ -501,7 +503,8 @@ static int zynqmp_dma_alloc_chan_resources(struct dma_chan *dchan)
 					(i * ZYNQMP_DMA_DESC_SIZE(chan) * 2));
 		desc->dst_v = (struct zynqmp_dma_desc_ll *) (desc->src_v + 1);
 		desc->src_p = chan->desc_pool_p +
-				(i * ZYNQMP_DMA_DESC_SIZE(chan) * 2);
+				((dma_addr_t)i * ZYNQMP_DMA_DESC_SIZE(chan)
+				 * 2);
 		desc->dst_p = desc->src_p + ZYNQMP_DMA_DESC_SIZE(chan);
 	}
 
@@ -677,7 +680,8 @@ static void zynqmp_dma_free_chan_resources(struct dma_chan *dchan)
 	zynqmp_dma_free_descriptors(chan);
 	spin_unlock_irqrestore(&chan->lock, irqflags);
 	dma_free_coherent(chan->dev,
-		(2 * ZYNQMP_DMA_DESC_SIZE(chan) * ZYNQMP_DMA_NUM_DESCS),
+		((size_t)(2 * ZYNQMP_DMA_DESC_SIZE(chan)) *
+		 ZYNQMP_DMA_NUM_DESCS),
 		chan->desc_pool_v, chan->desc_pool_p);
 	kfree(chan->sw_desc_pool);
 	pm_runtime_mark_last_busy(chan->dev);
@@ -846,7 +850,7 @@ static struct dma_async_tx_descriptor *zynqmp_dma_prep_memcpy(
 
 	zynqmp_dma_desc_config_eod(chan, desc);
 	async_tx_ack(&first->async_tx);
-	first->async_tx.flags = flags;
+	first->async_tx.flags = (enum dma_ctrl_flags)flags;
 	return &first->async_tx;
 }
 
@@ -1077,7 +1081,11 @@ static int zynqmp_dma_probe(struct platform_device *pdev)
 	pm_runtime_set_autosuspend_delay(zdev->dev, ZDMA_PM_TIMEOUT);
 	pm_runtime_use_autosuspend(zdev->dev);
 	pm_runtime_enable(zdev->dev);
-	pm_runtime_get_sync(zdev->dev);
+	ret = pm_runtime_get_sync(zdev->dev);
+	if (ret < 0) {
+		dev_err(zdev->dev, "pm_runtime_get_sync() failed\n");
+		pm_runtime_disable(zdev->dev);
+	}
 	if (!pm_runtime_enabled(zdev->dev)) {
 		ret = zynqmp_dma_runtime_resume(zdev->dev);
 		if (ret)
@@ -1093,7 +1101,11 @@ static int zynqmp_dma_probe(struct platform_device *pdev)
 	p->dst_addr_widths = BIT(zdev->chan->bus_width / 8);
 	p->src_addr_widths = BIT(zdev->chan->bus_width / 8);
 
-	dma_async_device_register(&zdev->common);
+	ret = dma_async_device_register(&zdev->common);
+	if (ret) {
+		dev_err(zdev->dev, "failed to register the dma device\n");
+		goto free_chan_resources;
+	}
 
 	ret = of_dma_controller_register(pdev->dev.of_node,
 					 of_zynqmp_dma_xlate, zdev);
diff --git a/include/linux/dma/xilinx_frmbuf.h b/include/linux/dma/xilinx_frmbuf.h
new file mode 100644
index 000000000..aa048a3eb
--- /dev/null
+++ b/include/linux/dma/xilinx_frmbuf.h
@@ -0,0 +1,258 @@
+/*
+ * Xilinx Framebuffer DMA support header file
+ *
+ * Copyright (C) 2017 Xilinx, Inc. All rights reserved.
+ *
+ * This is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#ifndef __XILINX_FRMBUF_DMA_H
+#define __XILINX_FRMBUF_DMA_H
+
+#include <linux/dmaengine.h>
+
+/* Modes to enable early callback */
+/* To avoid first frame delay */
+#define EARLY_CALLBACK			BIT(1)
+/* Give callback at start of descriptor processing */
+#define EARLY_CALLBACK_START_DESC	BIT(2)
+/**
+ * enum vid_frmwork_type - Linux video framework type
+ * @XDMA_DRM: fourcc is of type DRM
+ * @XDMA_V4L2: fourcc is of type V4L2
+ */
+enum vid_frmwork_type {
+	XDMA_DRM = 0,
+	XDMA_V4L2,
+};
+
+/**
+ * enum operation_mode - FB IP control register field settings to select mode
+ * @DEFAULT : Use default mode, No explicit bit field settings required.
+ * @AUTO_RESTART : Use auto-restart mode by setting BIT(7) of control register.
+ */
+enum operation_mode {
+	DEFAULT = 0x0,
+	AUTO_RESTART = BIT(7),
+};
+
+/**
+ * enum fid_modes - FB IP fid mode register settings to select mode
+ * @FID_MODE_0: carries the fid value shared by application
+ * @FID_MODE_1: sets the fid after first frame
+ * @FID_MODE_2: sets the fid after second frame
+ */
+enum fid_modes {
+	FID_MODE_0 = 0,
+	FID_MODE_1 = 1,
+	FID_MODE_2 = 2,
+};
+
+#if IS_ENABLED(CONFIG_XILINX_FRMBUF)
+/**
+ * xilinx_xdma_set_mode - Set operation mode for framebuffer IP
+ * @chan: dma channel instance
+ * @mode: Famebuffer IP operation mode.
+ * This routine is used when utilizing "video format aware" Xilinx DMA IP
+ * (such as Video Framebuffer Read or Video Framebuffer Write).  This call
+ * must be made prior to dma_async_issue_pending(). This routine should be
+ * called by client driver to set the operation mode for framebuffer IP based
+ * upon the use-case, for e.g. for non-streaming usecases (like MEM2MEM) it's
+ * more appropriate to use default mode unlike streaming usecases where
+ * auto-restart mode is more suitable.
+ *
+ * auto-restart or free running mode.
+ */
+void xilinx_xdma_set_mode(struct dma_chan *chan, enum operation_mode mode);
+
+/**
+ * xilinx_xdma_drm_config - configure video format in video aware DMA
+ * @chan: dma channel instance
+ * @drm_fourcc: DRM fourcc code describing the memory layout of video data
+ *
+ * This routine is used when utilizing "video format aware" Xilinx DMA IP
+ * (such as Video Framebuffer Read or Video Framebuffer Write).  This call
+ * must be made prior to dma_async_issue_pending() to establish the video
+ * data memory format within the hardware DMA.
+ */
+void xilinx_xdma_drm_config(struct dma_chan *chan, u32 drm_fourcc);
+
+/**
+ * xilinx_xdma_v4l2_config - configure video format in video aware DMA
+ * @chan: dma channel instance
+ * @v4l2_fourcc: V4L2 fourcc code describing the memory layout of video data
+ *
+ * This routine is used when utilizing "video format aware" Xilinx DMA IP
+ * (such as Video Framebuffer Read or Video Framebuffer Write).  This call
+ * must be made prior to dma_async_issue_pending() to establish the video
+ * data memory format within the hardware DMA.
+ */
+void xilinx_xdma_v4l2_config(struct dma_chan *chan, u32 v4l2_fourcc);
+
+/**
+ * xilinx_xdma_get_drm_vid_fmts - obtain list of supported DRM mem formats
+ * @chan: dma channel instance
+ * @fmt_cnt: Output param - total count of supported DRM fourcc codes
+ * @fmts: Output param - pointer to array of DRM fourcc codes (not a copy)
+ *
+ * Return: a reference to an array of DRM fourcc codes supported by this
+ * instance of the Video Framebuffer Driver
+ */
+int xilinx_xdma_get_drm_vid_fmts(struct dma_chan *chan, u32 *fmt_cnt,
+				 u32 **fmts);
+
+/**
+ * xilinx_xdma_get_v4l2_vid_fmts - obtain list of supported V4L2 mem formats
+ * @chan: dma channel instance
+ * @fmt_cnt: Output param - total count of supported V4L2 fourcc codes
+ * @fmts: Output param - pointer to array of V4L2 fourcc codes (not a copy)
+ *
+ * Return: a reference to an array of V4L2 fourcc codes supported by this
+ * instance of the Video Framebuffer Driver
+ */
+int xilinx_xdma_get_v4l2_vid_fmts(struct dma_chan *chan, u32 *fmt_cnt,
+				  u32 **fmts);
+
+/**
+ * xilinx_xdma_get_fid - Get the Field ID of the buffer received.
+ * This function should be called from the callback function registered
+ * per descriptor in prep_interleaved.
+ *
+ * @chan: dma channel instance
+ * @async_tx: descriptor whose parent structure contains fid.
+ * @fid: Output param - Field ID of the buffer. 0 - even, 1 - odd.
+ *
+ * Return: 0 on success, -EINVAL in case of invalid chan
+ */
+int xilinx_xdma_get_fid(struct dma_chan *chan,
+			struct dma_async_tx_descriptor *async_tx, u32 *fid);
+
+/**
+ * xilinx_xdma_set_fid - Set the Field ID of the buffer to be transmitted
+ * @chan: dma channel instance
+ * @async_tx: dma async tx descriptor for the buffer
+ * @fid: Field ID of the buffer. 0 - even, 1 - odd.
+ *
+ * Return: 0 on success, -EINVAL in case of invalid chan
+ */
+int xilinx_xdma_set_fid(struct dma_chan *chan,
+			struct dma_async_tx_descriptor *async_tx, u32 fid);
+
+/**
+ * xilinx_xdma_get_fid_err_flag - Get the Field ID error flag.
+ *
+ * @chan: dma channel instance
+ * @fid_err_flag: Field id error detect flag. 0 - no error, 1 - error.
+ *
+ * Return: 0 on success, -EINVAL in case of invalid chan
+ */
+int xilinx_xdma_get_fid_err_flag(struct dma_chan *chan,
+				 u32 *fid_err_flag);
+
+/**
+ * xilinx_xdma_get_fid_out - Get the Field ID out signal value.
+ *
+ * @chan: dma channel instance
+ * @fid_out_val: Field id out signal value.
+ *
+ * Return: 0 on success, -EINVAL in case of invalid chan
+ */
+int xilinx_xdma_get_fid_out(struct dma_chan *chan,
+			    u32 *fid_out_val);
+
+/**:
+ * xilinx_xdma_get_earlycb - Get info if early callback has been enabled.
+ *
+ * @chan: dma channel instance
+ * @async_tx: descriptor whose parent structure contains fid.
+ * @earlycb: Output param - Early callback mode
+ *
+ * Return: 0 on success, -EINVAL in case of invalid chan
+ */
+int xilinx_xdma_get_earlycb(struct dma_chan *chan,
+			    struct dma_async_tx_descriptor *async_tx,
+			    u32 *earlycb);
+
+/**
+ * xilinx_xdma_set_earlycb - Enable/Disable early callback
+ * @chan: dma channel instance
+ * @async_tx: dma async tx descriptor for the buffer
+ * @earlycb: Enable early callback mode for descriptor
+ *
+ * Return: 0 on success, -EINVAL in case of invalid chan
+ */
+int xilinx_xdma_set_earlycb(struct dma_chan *chan,
+			    struct dma_async_tx_descriptor *async_tx,
+			    u32 earlycb);
+/**
+ * xilinx_xdma_get_width_align - Get width alignment value
+ *
+ * @chan: dma channel instance
+ * @width_align: width alignment value
+ *
+ * Return: 0 on success, -ENODEV in case no framebuffer device found
+ */
+int xilinx_xdma_get_width_align(struct dma_chan *chan, u32 *width_align);
+
+#else
+static inline void xilinx_xdma_set_mode(struct dma_chan *chan,
+					enum operation_mode mode)
+{ }
+
+static inline void xilinx_xdma_drm_config(struct dma_chan *chan, u32 drm_fourcc)
+{ }
+
+static inline void xilinx_xdma_v4l2_config(struct dma_chan *chan,
+					   u32 v4l2_fourcc)
+{ }
+
+static inline int xilinx_xdma_get_drm_vid_fmts(struct dma_chan *chan,
+					       u32 *fmt_cnt, u32 **fmts)
+{
+	return -ENODEV;
+}
+
+static inline int xilinx_xdma_get_v4l2_vid_fmts(struct dma_chan *chan,
+						u32 *fmt_cnt,u32 **fmts)
+{
+	return -ENODEV;
+}
+
+static inline int xilinx_xdma_get_fid(struct dma_chan *chan,
+				      struct dma_async_tx_descriptor *async_tx,
+				      u32 *fid)
+{
+	return -ENODEV;
+}
+
+static inline int xilinx_xdma_set_fid(struct dma_chan *chan,
+				      struct dma_async_tx_descriptor *async_tx,
+				      u32 fid)
+{
+	return -ENODEV;
+}
+
+static inline int xilinx_xdma_get_earlycb(struct dma_chan *chan,
+					  struct dma_async_tx_descriptor *atx,
+					  u32 *earlycb)
+{
+	return -ENODEV;
+}
+
+static inline int xilinx_xdma_set_earlycb(struct dma_chan *chan,
+					  struct dma_async_tx_descriptor *atx,
+					  u32 earlycb)
+{
+	return -ENODEV;
+}
+
+static inline int xilinx_xdma_get_width_align(struct dma_chan *chan, u32 *width_align)
+{
+	return -ENODEV;
+}
+#endif
+
+#endif /*__XILINX_FRMBUF_DMA_H*/
diff --git a/include/linux/dma/xilinx_ps_pcie_dma.h b/include/linux/dma/xilinx_ps_pcie_dma.h
new file mode 100644
index 000000000..7c9912bd4
--- /dev/null
+++ b/include/linux/dma/xilinx_ps_pcie_dma.h
@@ -0,0 +1,69 @@
+/*
+ * Xilinx PS PCIe DMA Engine support header file
+ *
+ * Copyright (C) 2010-2014 Xilinx, Inc. All rights reserved.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation
+ */
+
+#ifndef __DMA_XILINX_PS_PCIE_H
+#define __DMA_XILINX_PS_PCIE_H
+
+#include <linux/dma-mapping.h>
+#include <linux/dmaengine.h>
+
+#define XLNX_PLATFORM_DRIVER_NAME "xlnx-platform-dma-driver"
+
+#define ZYNQMP_DMA_DEVID	(0xD024)
+#define ZYNQMP_RC_DMA_DEVID	(0xD021)
+
+#define MAX_ALLOWED_CHANNELS_IN_HW	4
+
+#define MAX_NUMBER_OF_CHANNELS	MAX_ALLOWED_CHANNELS_IN_HW
+
+#define DEFAULT_DMA_QUEUES	4
+#define TWO_DMA_QUEUES		2
+
+#define NUMBER_OF_BUFFER_DESCRIPTORS	1999
+#define MAX_DESCRIPTORS			65536
+
+#define CHANNEL_COAELSE_COUNT		0
+
+#define CHANNEL_POLL_TIMER_FREQUENCY	1000 /* in milli seconds */
+
+#define PCIE_AXI_DIRECTION	DMA_TO_DEVICE
+#define AXI_PCIE_DIRECTION	DMA_FROM_DEVICE
+
+/**
+ * struct BAR_PARAMS - PCIe Bar Parameters
+ * @BAR_PHYS_ADDR: PCIe BAR Physical address
+ * @BAR_LENGTH: Length of PCIe BAR
+ * @BAR_VIRT_ADDR: Virtual Address to access PCIe BAR
+ */
+struct BAR_PARAMS {
+	dma_addr_t BAR_PHYS_ADDR; /**< Base physical address of BAR memory */
+	unsigned long BAR_LENGTH; /**< Length of BAR memory window */
+	void *BAR_VIRT_ADDR;      /**< Virtual Address of mapped BAR memory */
+};
+
+/**
+ * struct ps_pcie_dma_channel_match - Match structure for dma clients
+ * @pci_vendorid: PCIe Vendor id of PS PCIe DMA device
+ * @pci_deviceid: PCIe Device id of PS PCIe DMA device
+ * @board_number: Unique id to identify individual device in a system
+ * @channel_number: Unique channel number of the device
+ * @direction: DMA channel direction
+ * @bar_params: Pointer to BAR_PARAMS for accessing application specific data
+ */
+struct ps_pcie_dma_channel_match {
+	u16 pci_vendorid;
+	u16 pci_deviceid;
+	u16 board_number;
+	u16 channel_number;
+	enum dma_data_direction direction;
+	struct BAR_PARAMS *bar_params;
+};
+
+#endif
