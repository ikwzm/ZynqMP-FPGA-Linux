diff --git a/Documentation/devicetree/bindings/misc/jesd-phy.txt b/Documentation/devicetree/bindings/misc/jesd-phy.txt
new file mode 100644
index 000000000..84535cb1e
--- /dev/null
+++ b/Documentation/devicetree/bindings/misc/jesd-phy.txt
@@ -0,0 +1,24 @@
+* Xilinx JESD204B Phy
+
+Description:
+The LogiCORE™ IP JESD204 PHY core implements a JESD204B Physical interface supporting
+line rates between 1.0 and 12.5 Gb/s on 1 to 12 lanes using GTX, GTH, or GTP transceivers.
+
+Required properties:
+- compatible = "xlnx,jesd204-phy-2.0"
+- reg = Should contain JESD204B phy registers location and length
+- xlnx,pll-selection = The PLL selection 3 for QPLL and 1 For CPLL
+- xlnx,lanes = No of Lanes
+- xlnx,gt-refclk-freq = Reference frequency in Hz
+- clocks = The phandle to the clock tree
+
+Example:
+++++++++
+	jesd204_phycores:phy@41e10000 {
+		compatible = "xlnx,jesd204-phy-2.0";
+		reg = <0x41e10000 0x10000>;
+		xlnx,gt-refclk-freq = "156250000";
+		xlnx,lanes = <0x1>;
+		xlnx,pll-selection = <0x3>;
+		clocks = <&si570>;
+	};
diff --git a/Documentation/devicetree/bindings/misc/jesd204b.txt b/Documentation/devicetree/bindings/misc/jesd204b.txt
new file mode 100644
index 000000000..53f8192c8
--- /dev/null
+++ b/Documentation/devicetree/bindings/misc/jesd204b.txt
@@ -0,0 +1,28 @@
+* Xilinx JESD204B core
+
+Description:
+The LogiCORE™ IP JESD204 core implements a JESD204B core
+
+Required properties:
+- compatible = Should be one of
+		"xlnx,jesd204-5.1";
+		"xlnx,jesd204-5.2";
+		"xlnx,jesd204-6.1";
+- reg = Should contain JESD204B registers location and length
+- xlnx,frames-per-multiframe = No of frames per multiframe
+- xlnx,bytes-per-frame = No of bytes per frame
+- xlnx,lanes = No of Lanes
+- xlnx,subclass = subclass
+- xlnx,node-is-transmit = should be present only for transmit nodes
+
+Example:
+++++++++
+jesd_Tx_axi_0: jesd_Tx@44a20000 {
+	compatible = "xlnx,jesd204-5.1";
+	reg = <0x44a20000 0x10000>;
+	xlnx,frames-per-multiframe = <30>;
+	xlnx,bytes-per-frame = <2>;
+	xlnx,subclass = <1>;
+	xlnx,lanes = <0x2>;
+	xlnx,node-is-transmit;
+};
diff --git a/Documentation/devicetree/bindings/misc/xlnx,axi-traffic-gen.txt b/Documentation/devicetree/bindings/misc/xlnx,axi-traffic-gen.txt
new file mode 100644
index 000000000..6edb8f6a3
--- /dev/null
+++ b/Documentation/devicetree/bindings/misc/xlnx,axi-traffic-gen.txt
@@ -0,0 +1,25 @@
+* Xilinx AXI Traffic generator IP
+
+Required properties:
+- compatible: "xlnx,axi-traffic-gen"
+- interrupts: Should contain AXI Traffic Generator interrupts.
+- interrupt-parent: Must be core interrupt controller.
+- reg: Should contain AXI Traffic Generator registers location and length.
+- interrupt-names: Should contain both the intr names of device - error
+		   and completion.
+- xlnx,device-id: Device instance Id.
+
+Optional properties:
+- clocks: Input clock specifier. Refer to common clock bindings.
+
+Example:
+++++++++
+axi_traffic_gen_1: axi-traffic-gen@76000000 {
+	compatible = "xlnx,axi-traffic-gen-1.0", "xlnx,axi-traffic-gen";
+	clocks = <&clkc 15>;
+	interrupts = <0 2 2 2>;
+	interrupt-parent = <&axi_intc_1>;
+	interrupt-names = "err-out", "irq-out";
+	reg = <0x76000000 0x800000>;
+	xlnx,device-id = <0x0>;
+} ;
diff --git a/Documentation/devicetree/bindings/soc/xilinx/xlnx,ai-engine-npi.txt b/Documentation/devicetree/bindings/soc/xilinx/xlnx,ai-engine-npi.txt
new file mode 100644
index 000000000..b1c1466a3
--- /dev/null
+++ b/Documentation/devicetree/bindings/soc/xilinx/xlnx,ai-engine-npi.txt
@@ -0,0 +1,23 @@
+Xilinx AI Engine NPI
+--------------------
+
+The Xilinx AI Engine NPI space is where the privileged operations for AI Engine
+device are handled, such as reset and pll. The space is typically meant to be
+owned by platform management software, and this space is accessible only when
+the platform management software grants the access. Thus, this dt binding only
+works in such configuration, and in case the platform locks the access,
+the non-secure software fails to access the device.
+
+This is a temporary solution to allow direct access to NPI space.
+
+Required properties:
+
+- compatible: Must be "xlnx,ai-engine-npi"
+- reg: Physical base address and length of the registers set for the device.
+
+Example:
+
+	aie-npi@f70a0000 {
+		compatible = "xlnx,ai-engine-npi";
+		reg = <0x0 0xf70a0000 0x0 0x1000>;
+	};
diff --git a/Documentation/devicetree/bindings/soc/xilinx/xlnx,ai-engine.yaml b/Documentation/devicetree/bindings/soc/xilinx/xlnx,ai-engine.yaml
new file mode 100644
index 000000000..edc492f4e
--- /dev/null
+++ b/Documentation/devicetree/bindings/soc/xilinx/xlnx,ai-engine.yaml
@@ -0,0 +1,136 @@
+# SPDX-License-Identifier: GPL-2.0
+%YAML 1.2
+---
+$id: http://devicetree.org/schemas/soc/xilinx/xlnx,ai-engine.yaml#
+$schema: http://devicetree.org/meta-schemas/core.yaml#
+
+title: Xilinux AI Engine
+
+maintainers:
+  - Wendy Liang <wendy.liang@xilinx.com>
+
+description: |+
+  The Xilinx AI Engine is a tile processor with many cores (up to 400) that
+  can run in parallel. The data routing between cores is configured through
+  internal switches, and shim tiles interface with external interconnect, such
+  as memory or PL.
+
+properties:
+  compatible:
+    const: xlnx,ai-engine-v1.0
+
+  reg:
+    description: |
+      Physical base address and length of the device registers.
+      The AI engine address space assigned to Linux is defined by Xilinx
+      platform design tool.
+
+  '#address-cells':
+    enum: [2]
+    description: |
+      size of cell to describe AI engine range of tiles address.
+      It is the location of the starting tile of the range.
+      As the AI engine tiles are 2D array, the location of a tile
+      is presented as (column, row), the address cell is 2.
+
+  '#size-cells':
+    enum: [2]
+    description: |
+      size of cell to describe AI engine range of tiles size.
+      As the AI engine tiles are 2D array, the size cell is 2.
+
+  interrupts:
+    maxItems: 3
+
+  interrupt-names:
+    description: |
+      Should be "interrupt1", "interrupt2" or "interrupt3".
+
+required:
+  - compatible
+  - reg
+  - '#address-cells'
+  - '#size-cells'
+  - power-domains
+  - interrupt-parent
+  - interrupts
+  - interrupt-names
+
+patternProperties:
+  "^partition[0-9]@[0-9]+$":
+    type: object
+    description: |
+      AI engine partition which is a group of column based tiles of the AI
+      engine device. Each AI engine partition is isolated from the other
+      AI engine partitions. An AI engine partition is defined by Xilinx
+      platform design tools.
+      AI engine partition driver will create a FPGA bridge when probes the
+      AI engine partition. The AI engine SHIM row of the partition is the
+      bridge to connect AI engine partition, FPGA and PS.
+
+    properties:
+      reg:
+        description: |
+          It describes the group of tiles of the AI engine partition. It needs
+          to include the SHIM row. The format is defined by the parent AI engine
+          device node's '#address-cells' and '#size-cells' properties. e.g. a v1
+          AI engine device has 2D tiles array, the first row is SHIM row. A
+          partition which has 50 columns and 8 rows of core tiles and 1 row of
+          SHIM tiles will be presented as <0 0 50 9>.
+
+      label:
+        maxItems: 1
+
+      xlnx,partition-id:
+        $ref: /schemas/types.yaml#/definitions/uint32
+        description: |
+          AI engine partition ID, which is defined by Xilinx platform design
+          tool to identify the AI engine partition in the system.
+
+    required:
+      - reg
+      - xlnx,partition-id
+
+examples:
+  - |
+    #include <dt-bindings/power/xlnx-versal-power.h>
+    bus {
+      #address-cells = <2>;
+      #size-cells = <2>;
+
+      ai_engine: ai-engine@20000000000 {
+        compatible = "xlnx,ai-engine-v1.0";
+        reg = <0x200 0x0 0x1 0x0>;
+        #address-cells = <2>;
+        #size-cells = <2>;
+        power-domains = <&versal_firmware PM_DEV_AI>;
+        interrupt-parent = <&gic>;
+        interrupts = <0x0 0x94 0x4>,
+                        <0x0 0x95 0x4>,
+                        <0x0 0x96 0x4>;
+        interrupt-names = "interrupt1", "interrupt2", "interrupt3";
+
+        aie_partition0: aie-partition@0 {
+                /* 50 columns and 8 core tile rows + 1 SHIM row */
+                reg = <0 0 50 9>;
+                xlnx,partition-id = <1>;
+        };
+      };
+    };
+
+  - |
+    /*
+     * Device tree overlay example to for FPGA partial configuration which
+     * connects to AI engine partition.
+     */
+    fpga_region0: fpga_region0 {
+      fpga-bridges = <&aie_partition0>;
+    };
+    fragment@0 {
+      target = <&fpga_region0>;
+      #address-cells = <1>;
+      #size-cells = <1>;
+      __overlay__ {
+        firmware-name = "partition.pdi";
+      };
+    };
diff --git a/Documentation/devicetree/bindings/soc/xilinx/xlnx,ai_engine.txt b/Documentation/devicetree/bindings/soc/xilinx/xlnx,ai_engine.txt
new file mode 100644
index 000000000..b7643a138
--- /dev/null
+++ b/Documentation/devicetree/bindings/soc/xilinx/xlnx,ai_engine.txt
@@ -0,0 +1,28 @@
+Xilinx AI Engine
+----------------
+
+The Xilinx AI Engine is a tile processor with many cores (up to 400) that
+can run in parallel. The data routing between cores is configured through
+internal switches, and shim tiles interface with external interconnect, such
+as memory or PL.
+
+Required properties:
+
+- compatible: Must be "xlnx,ai_engine".
+- reg: Physical base address and length of the registers set for the device.
+- interrupt-parent: the phandle to the interrupt controller.
+- interrupts: the interrupt numbers.
+- interrupt-names: Should be "interrupt0", "interrupt1", "interrupt2" or
+  "interrupt3".
+
+Example:
+
+	ai_engine@20000000000 {
+		compatible = "xlnx,ai_engine";
+		reg = <0x200 0x0 0x1 0x0>;
+		interrupt-parent = <&gic>;
+		interrupts = <0x0 0x94 0x1>,
+			     <0x0 0x95 0x1>,
+			     <0x0 0x96 0x1>;
+		interrupt-names = "interrupt1", "interrupt2", "interrupt3";
+	};
diff --git a/Documentation/misc-devices/xilinx_flex.txt b/Documentation/misc-devices/xilinx_flex.txt
new file mode 100644
index 000000000..c07593466
--- /dev/null
+++ b/Documentation/misc-devices/xilinx_flex.txt
@@ -0,0 +1,66 @@
+Kernel driver xilinx_flex
+============================
+
+Supported chips:
+Versal SOC
+
+Author:
+	Shubhrajyoti Datta <shubhrajyoti.datta@xilinx.com>
+
+Description
+-----------
+
+Versal uses the Arteris FlexNoC interconnect instead of the ARM NIC. FlexNoC
+provides the capability to integrate performance counters in the interconnect.
+It has configurable probe points to monitor the packet and forwards it to
+observer for logging. It supports read and write transaction counts for
+request and response.
+
+Features:
+---> Run-time programmable selection of packet probe points.
+---> Recording of traffic and link statistics.
+---> Separate read and write response and request count.
+
+SYSFS:
+
+counteridfpd
+	RW - shows the counter number selected for the FPD Flexnoc.
+
+counterfpd_rdreq
+	RO - shows the read request count for the FPD counters.
+
+counterfpdsrc
+	WO - sets the source of the FPD counter.
+
+counterfpd_wrrsp
+	RO - shows the write response count for the FPD counters.
+
+counterfpd_rdrsp
+	RO - shows the read response count for the FPD counters.
+
+counterfpd_wrreq
+	RO - shows the write request count for the FPD counters.
+
+counterfpdport
+	WO - sets the port number selected for the FPD Flexnoc.
+
+counteridlpd
+	RW - shows the counter number selected for the LPD Flexnoc.
+
+counterlpdport
+	WO - sets the port number selected for the LPD Flexnoc.
+
+counterlpd_rdreq
+	RO - shows the read request count for the LPD counters.
+
+counterlpd_wrreq
+	RO - shows the write request count for the LPD counters.
+
+counterlpd_wrrsp
+	RO - shows the write response count for the LPD counters.
+
+counterlpdsrc
+	WO - sets the source of the LPD counter.
+
+counterlpd_rdrsp
+	RO - shows the read response count for the LPD counters.
diff --git a/Documentation/misc-devices/xilinx_trafgen.txt b/Documentation/misc-devices/xilinx_trafgen.txt
new file mode 100644
index 000000000..dadcdad74
--- /dev/null
+++ b/Documentation/misc-devices/xilinx_trafgen.txt
@@ -0,0 +1,97 @@
+Kernel driver xilinx_trafgen
+============================
+
+Supported chips:
+Zynq SOC, Xilinx 7 series fpga's (Virtex,Kintex,Artix)
+
+Data Sheet:
+	http://www.xilinx.com/support/documentation/ip_documentation/axi_traffic_gen/v2_0/pg125-axi-traffic-gen.pdf
+
+Author:
+        Kedareswara Rao Appana <appanad@xilinx.com>
+
+Description
+-----------
+
+AXI Traffic Generator IP is a core that stresses the AXI4 interconnect and other
+AXI4 peripherals in the system. It generates a wide variety of AXI4 transactions
+based on the core programming.
+
+Features:
+---> Configurable option to generate and accept data according to different
+traffic profiles.
+---> Supports dependent/independent transaction between read/write master port
+with configurable delays.
+---> Programmable repeat count for each transaction with
+constant/increment/random address.
+---> External start/stop to generate traffic without processor intervention.
+---> Generates IP-specific traffic on AXI interface for pre-defined protocols.
+
+SYSFS:
+
+id
+	RO - shows the trafgen id.
+
+resource
+	RO - shows the baseaddress for the trafgen.
+
+master_start_stop
+	RW - monitors the master start logic.
+
+config_slave_status
+	RW - configure and monitors the slave status.
+
+err_sts
+	RW - get the error statistics/clear the errors.
+
+err_en
+	WO - enable the errors.
+
+intr_en
+	WO - enable the interrupts.
+
+last_valid_index
+	RW - gets the last valid index value.
+
+config_sts_show
+	RO - show the config status value.
+
+mram_clear
+	WO - clears the master ram.
+
+cram_clear
+	WO - clears the command ram.
+
+pram_clear
+	WO - clears the parameter ram.
+
+static_enable
+	RO - enables the static mode.
+
+static_burst_len
+	RW - gets/sets the static burst length.
+
+static_transferdone
+	RW - monitors the static transfer done status.
+
+reset_static_transferdone
+	RO - resets the static transferdone bit.
+
+stream_cfg
+	RW - sets the stream configuration parameters like delay.
+
+stream_tktsn
+	RW - TSTRB/TKEEP value for the last beat of the
+transfer set n. N can be 1 to 4.
+
+stream_enable
+	RO - enables the streaming mode.
+
+stream_transferlen
+	RW - get/set the streaming mode transfer length.
+
+stream_transfercnt
+	RW - get/set the streaming mode transfer count.
+
+loop_enable
+	RW - get/set loop enable value.
diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
index fafa8b0d8..edfbc2540 100644
--- a/drivers/misc/Kconfig
+++ b/drivers/misc/Kconfig
@@ -444,6 +444,41 @@ config XILINX_SDFEC
 
 	  If unsure, say N.
 
+config XILINX_FLEX_PM
+	tristate "Xilinx Flexnoc Performance Monitor"
+	help
+	  This option enables support for the Xilinx Flex Noc Performance Monitor driver.
+	  It monitors the read and write transactions. It has counters for the LPD and
+	  FPD domains.
+
+	  If unsure, say N
+
+config XILINX_TRAFGEN
+	tristate "Xilinx Traffic Generator"
+	help
+	  This option enables support for the Xilinx Traffic Generator driver.
+	  It is designed to generate AXI4 traffic which can be used to stress
+	  different modules/interconnect connected in the system. Different
+	  configurable options which are provided through sysfs entries allow
+	  allow the user to generate a wide variety of traffic based on their
+	  their requirements.
+
+	  If unsure, say N
+
+config XILINX_AIE
+	tristate "Xilinx AI engine"
+	depends on ARM64 || COMPILE_TEST
+	depends on ZYNQMP_FIRMWARE
+	select FPGA_BRIDGE
+	help
+	  This option enables support for the Xilinx AI engine driver.
+	  One Xilinx AI engine device can have multiple partitions (groups of
+	  AI engine tiles). Xilinx AI engine device driver instance manages
+	  AI engine partitions. User application access its partitions through
+	  AI engine partition instance file operations.
+
+	  If unsure, say N
+
 config MISC_RTSX
 	tristate
 	default MISC_RTSX_PCI || MISC_RTSX_USB
@@ -466,6 +501,7 @@ config HISI_HIKEY_USB
 	  switching between the dual-role USB-C port and the USB-A host ports
 	  using only one USB controller.
 
+source "drivers/misc/jesd204b/Kconfig"
 source "drivers/misc/c2port/Kconfig"
 source "drivers/misc/eeprom/Kconfig"
 source "drivers/misc/cb710/Kconfig"
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index d23231e73..7bdb21a14 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -57,3 +57,7 @@ obj-$(CONFIG_HABANA_AI)		+= habanalabs/
 obj-$(CONFIG_UACCE)		+= uacce/
 obj-$(CONFIG_XILINX_SDFEC)	+= xilinx_sdfec.o
 obj-$(CONFIG_HISI_HIKEY_USB)	+= hisi_hikey_usb.o
+obj-$(CONFIG_XILINX_FLEX_PM)	+= xilinx_flex_pm.o
+obj-$(CONFIG_XILINX_TRAFGEN)	+= xilinx_trafgen.o
+obj-$(CONFIG_XILINX_JESD204B)	+= jesd204b/
+obj-$(CONFIG_XILINX_AIE)	+= xilinx-ai-engine/
diff --git a/drivers/misc/jesd204b/Kconfig b/drivers/misc/jesd204b/Kconfig
new file mode 100644
index 000000000..aff08cfe8
--- /dev/null
+++ b/drivers/misc/jesd204b/Kconfig
@@ -0,0 +1,28 @@
+#
+# Jesd204b support
+#
+
+config XILINX_JESD204B
+	tristate "Xilinx JESD204B"
+	help
+	  This option enables support for the Xilinx JESD204B driver.
+	  It is designed to allow user to access JESD204B IP registers
+	  with sysfs entries. JESD204B is the protocol used by High-Speed
+	  data converters to transfer data to FPGA/ASIC.
+
+	  If unsure, say N.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called jesd204b.
+
+config XILINX_JESD204B_PHY
+	tristate "JESD Phy Driver"
+	depends on XILINX_JESD204B
+	help
+	  This is JESD204b Phy Interface. It enables support for xilinx jesd204b phy
+	  controller.
+
+	  If unsure, say N.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called jesd204b_phy.
diff --git a/drivers/misc/jesd204b/Makefile b/drivers/misc/jesd204b/Makefile
new file mode 100644
index 000000000..7723fcb00
--- /dev/null
+++ b/drivers/misc/jesd204b/Makefile
@@ -0,0 +1,5 @@
+obj-$(CONFIG_XILINX_JESD204B_PHY) += jesd204b_phy.o
+jesd204b_phy-y += jesd_phy.o gtx7s_cpll_bands.o \
+		gtx7s_qpll_bands.o
+obj-$(CONFIG_XILINX_JESD204B) += jesd204b.o
+jesd204b-y += xilinx_jesd204b.o
diff --git a/drivers/misc/jesd204b/gtx7s_cpll_bands.c b/drivers/misc/jesd204b/gtx7s_cpll_bands.c
new file mode 100644
index 000000000..a9610f7ad
--- /dev/null
+++ b/drivers/misc/jesd204b/gtx7s_cpll_bands.c
@@ -0,0 +1,88 @@
+/*
+ * Copyright (C) 2014 - 2015 Xilinx, Inc.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#include <linux/types.h>
+#include "s7_gtxe2_drp.h"
+#include "gtx7s_cpll_bands.h"
+
+static const u32 gtx7s_cpll_channel_address_lut
+			[GTX7S_CPLL_NUM_CHANNEL_DRP_REGS] = {
+	RXCDR_CFG0_ADDR,
+	RXCDR_CFG1_ADDR,
+	RXCDR_CFG2_ADDR,
+	RXCDR_CFG3_ADDR,
+	RXCDR_CFG4_ADDR,
+	RXOUT_DIV_ADDR,
+	TXOUT_DIV_ADDR,
+	RX_DFE_LPM_CFG_ADDR
+};
+
+static const u32 gtx7s_cpll_channel_offset_lut
+			[GTX7S_CPLL_NUM_CHANNEL_DRP_REGS] = {
+	RXCDR_CFG0_OFFSET,
+	RXCDR_CFG1_OFFSET,
+	RXCDR_CFG2_OFFSET,
+	RXCDR_CFG3_OFFSET,
+	RXCDR_CFG4_OFFSET,
+	RXOUT_DIV_OFFSET,
+	TXOUT_DIV_OFFSET,
+	RX_DFE_LPM_CFG_OFFSET
+};
+
+static const u32 gtx7s_cpll_channel_mask_lut
+			[GTX7S_CPLL_NUM_CHANNEL_DRP_REGS] = {
+	RXCDR_CFG0_MASK,
+	RXCDR_CFG1_MASK,
+	RXCDR_CFG2_MASK,
+	RXCDR_CFG3_MASK,
+	RXCDR_CFG4_MASK,
+	RXOUT_DIV_MASK,
+	TXOUT_DIV_MASK,
+	RX_DFE_LPM_CFG_MASK
+};
+
+/* Note bands run vertically from 1 to 4 */
+static const u16 gtx7s_cpll_channel_param_lut[GTX7S_CPLL_NUM_CHANNEL_DRP_REGS]
+			[GTX7S_CPLL_NUM_LINE_RATE_BANDS] = {
+	{0x20,	 0x20,		0x20,	0x20	},/*	 RXCDR_CFG0 */
+	{0x1010, 0x1020,	0x1040,	0x1040	},/*	 RXCDR_CFG1 */
+	{0x23ff, 0x23ff,	0x23ff,	0x23ff	},/*	 RXCDR_CFG2 */
+	{0x0,	 0x0,		0x0,	0x0	},/*	 RXCDR_CFG3 */
+	{0x3,	0x3,		0x3,	0x3	},/*	 RXCDR_CFG4 */
+	{0x3,	0x2,		0x1,	0x1	},/*	 RXOUT_DIV  */
+	{0x3,	0x2,		0x1,	0x1	},/*	TXOUT_DIV   */
+	{0x904,	0x904,		0x904,	0x104	} /*	RX_DFE_LPM_CFG */
+};
+
+u32 get_gtx7s_cpll_address_lut(u32 lut_address)
+{
+	return gtx7s_cpll_channel_address_lut[lut_address];
+}
+
+u32 get_gtx7s_cpll_offset_lut(u32 lut_address)
+{
+	return gtx7s_cpll_channel_offset_lut[lut_address];
+}
+
+u32 get_gtx7s_cpll_mask_lut(u32 lut_address)
+{
+	return gtx7s_cpll_channel_mask_lut[lut_address];
+}
+
+u16 get_gtx7s_cpll_param_lut(u32 param_address, u32 band_address)
+{
+	return gtx7s_cpll_channel_param_lut[param_address][band_address];
+}
diff --git a/drivers/misc/jesd204b/gtx7s_cpll_bands.h b/drivers/misc/jesd204b/gtx7s_cpll_bands.h
new file mode 100644
index 000000000..f53f20de2
--- /dev/null
+++ b/drivers/misc/jesd204b/gtx7s_cpll_bands.h
@@ -0,0 +1,31 @@
+/*
+ * Copyright (C) 2014 - 2015 Xilinx, Inc.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/types.h>
+
+#ifndef GTX7S_CPLL_BANDS_H_
+#define GTX7S_CPLL_BANDS_H_
+
+#define GTX7S_CPLL_NUM_CHANNEL_DRP_REGS 8
+#define GTX7S_CPLL_NUM_LINE_RATE_BANDS 4
+
+u32 get_gtx7s_cpll_address_lut(u32);
+u32 get_gtx7s_cpll_offset_lut(u32);
+u32 get_gtx7s_cpll_mask_lut(u32);
+u16 get_gtx7s_cpll_param_lut(u32, u32);
+
+#endif /* GTX7S_CPLL_BANDS_H_ */
diff --git a/drivers/misc/jesd204b/gtx7s_qpll_bands.c b/drivers/misc/jesd204b/gtx7s_qpll_bands.c
new file mode 100644
index 000000000..71e70a611
--- /dev/null
+++ b/drivers/misc/jesd204b/gtx7s_qpll_bands.c
@@ -0,0 +1,96 @@
+/*
+ * Copyright (C) 2014 - 2015 Xilinx, Inc.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#include <linux/types.h>
+#include "s7_gtxe2_drp.h"
+#include "gtx7s_qpll_bands.h"
+
+static const u32 gtx7s_qpll_channel_address_lut
+			[GTX7S_QPLL_NUM_CHANNEL_DRP_REGS] = {
+	RXCDR_CFG0_ADDR,
+	RXCDR_CFG1_ADDR,
+	RXCDR_CFG2_ADDR,
+	RXCDR_CFG3_ADDR,
+	RXCDR_CFG4_ADDR,
+	RXOUT_DIV_ADDR,
+	TXOUT_DIV_ADDR,
+	RX_DFE_LPM_CFG_ADDR,
+	QPLL_CFG0_ADDR,
+	QPLL_CFG1_ADDR
+};
+
+static const u32 gtx7s_qpll_channel_offset_lut
+			[GTX7S_QPLL_NUM_CHANNEL_DRP_REGS] = {
+	RXCDR_CFG0_OFFSET,
+	RXCDR_CFG1_OFFSET,
+	RXCDR_CFG2_OFFSET,
+	RXCDR_CFG3_OFFSET,
+	RXCDR_CFG4_OFFSET,
+	RXOUT_DIV_OFFSET,
+	TXOUT_DIV_OFFSET,
+	RX_DFE_LPM_CFG_OFFSET,
+	QPLL_CFG0_OFFSET,
+	QPLL_CFG1_OFFSET
+};
+
+static const u32 gtx7s_qpll_channel_mask_lut
+			[GTX7S_QPLL_NUM_CHANNEL_DRP_REGS] = {
+	RXCDR_CFG0_MASK,
+	RXCDR_CFG1_MASK,
+	RXCDR_CFG2_MASK,
+	RXCDR_CFG3_MASK,
+	RXCDR_CFG4_MASK,
+	RXOUT_DIV_MASK,
+	TXOUT_DIV_MASK,
+	RX_DFE_LPM_CFG_MASK,
+	QPLL_CFG0_MASK,
+	QPLL_CFG1_MASK
+};
+
+/* Note bands run vertically from 1 to 10 */
+static const u16 gtx7s_qpll_channel_param_lut[GTX7S_QPLL_NUM_CHANNEL_DRP_REGS]
+			[GTX7S_QPLL_NUM_LINE_RATE_BANDS] = {
+{0x20,     0x20,   0x20,   0x20,     0x20,    0x20,    0x20,    0x20,    0x20,    0x20},/* RXCDR_CFG0 */
+{0x1008,   0x1010, 0x1020, 0x1010,   0x1020,  0x1040,  0x1020,  0x1040,  0x1040,  0x1040},/* RXCDR_CFG1 */
+{0x23ff,   0x23ff, 0x23ff, 0x23ff,   0x23ff,  0x23ff,  0x23ff,  0x23ff,  0x23ff,  0x23ff},/* RXCDR_CFG2 */
+{0x0,      0x0,    0x0,    0x0,      0x0,     0x0,     0x0,     0x0,     0x0,     0x0}, /* RXCDR_CFG3 */
+{0x3,      0x3,    0x3,    0x3,      0x3,     0x3,     0x3,     0x3,     0x3,     0x3},/* RXCDR_CFG4 */
+{0x3e8,    0x4,    0x2,    0x3,      0x2,     0x1,     0x2,     0x1,     0x1,     0x1},/* RXOUT_DIV  */
+{0x3e8,    0x4,    0x2,    0x3,      0x2,     0x1,     0x2,     0x1,     0x1,     0x1},/* TXOUT_DIV  */
+{0x904,    0x904,  0x904,  0x904,    0x904,   0x904,   0x904,   0x904,   0x104,   0x104},/* RX_DFE_LPM_CFG */
+{0x1c1,    0x1c1,  0x1c1,  0x181,    0x1c1,   0x1c1,   0x181,   0x1c1,   0x1c1,   0x181},/* QPLL_CFG0 */
+{0x68,     0x68,   0x68,   0x68,     0x68,    0x68,    0x68,    0x68,    0x68,    0x68} /* QPLL_CFG1 */
+};
+
+u32 get_gtx7s_qpll_address_lut(u32 lut_address)
+{
+	return gtx7s_qpll_channel_address_lut[lut_address];
+}
+
+u32 get_gtx7s_qpll_offset_lut(u32 lut_address)
+{
+	return gtx7s_qpll_channel_offset_lut[lut_address];
+}
+
+u32 get_gtx7s_qpll_mask_lut(u32 lut_address)
+{
+	return gtx7s_qpll_channel_mask_lut[lut_address];
+}
+
+u16 get_gtx7s_qpll_param_lut(u32 param_address, u32 band_address)
+{
+	return gtx7s_qpll_channel_param_lut[param_address][band_address];
+}
diff --git a/drivers/misc/jesd204b/gtx7s_qpll_bands.h b/drivers/misc/jesd204b/gtx7s_qpll_bands.h
new file mode 100644
index 000000000..8b9f6c24e
--- /dev/null
+++ b/drivers/misc/jesd204b/gtx7s_qpll_bands.h
@@ -0,0 +1,30 @@
+/*
+ * Copyright (C) 2014 - 2015 Xilinx, Inc.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#include <linux/types.h>
+
+#ifndef GTX7S_QPLL_BANDS_H_
+#define GTX7S_QPLL_BANDS_H_
+
+#define GTX7S_QPLL_NUM_CHANNEL_DRP_REGS 10
+#define GTX7S_QPLL_NUM_LINE_RATE_BANDS 10
+
+u32 get_gtx7s_qpll_address_lut(u32);
+u32 get_gtx7s_qpll_offset_lut(u32);
+u32 get_gtx7s_qpll_mask_lut(u32);
+u16 get_gtx7s_qpll_param_lut(u32, u32);
+
+#endif /* GTX7S_QPLL_BANDS_H_ */
diff --git a/drivers/misc/jesd204b/jesd_phy.c b/drivers/misc/jesd204b/jesd_phy.c
new file mode 100644
index 000000000..c35d9433d
--- /dev/null
+++ b/drivers/misc/jesd204b/jesd_phy.c
@@ -0,0 +1,384 @@
+/*
+ * Jesd204b phy support
+ *
+ * Copyright (C) 2014 - 2015 Xilinx, Inc.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program. If not, see <http://www.gnu.org/licenses/>.
+ */
+#include <linux/clk.h>
+#include <linux/io.h>
+#include <linux/delay.h>
+#include "jesd_phy.h"
+#include "gtx7s_cpll_bands.h"
+#include "gtx7s_qpll_bands.h"
+
+#define PLATFORM_JESD204_PHY_ADDR 0x41E10000
+#define JESD_PHY_LOOP_OFF	0
+#define JESD_PHY_LOOP_PCS	1
+#define JESD_PHY_LOOP_PMA	2
+#define JESD_PHY_LOOP_MAX	2
+
+static inline void jesd204b_phy_write(struct jesd204b_phy_state *st,
+				      unsigned reg, unsigned val)
+{
+	iowrite32(val, st->phy + reg);
+}
+
+static inline unsigned int jesd204b_phy_read(struct jesd204b_phy_state *st,
+					     unsigned reg)
+{
+	return ioread32(st->phy + reg);
+}
+
+#define NUM_GT_CHANNELS	8
+
+#define QPLL	0x3 /* QPLL (7 series) QPLL1 (UltraScale) */
+#define QPLL0	0x2 /* (UltraScale Only) */
+#define CPLL	0x0
+
+#define DRPREAD  BIT(30)
+#define DRPWRITE BIT(31)
+
+#define NR_COMMON_DRP_INTERFACES 0x008
+#define NR_TRANS_DRP_INTERFACES 0x00C
+
+#define CHANNEL_DRP_BASE	0x200
+#define CHANNEL_DRP_ADDR	0x204
+#define CHANNEL_DRP_DREAD	0x20C
+#define CHANNEL_DRP_DWRITE	0x208
+#define CHANNEL_DRP_STAT	0x214
+
+#define CHANNEL_XCVR_SEL	0x400
+#define CHANNEL_XCVR_TXPLL	0x40C
+#define CHANNEL_XCVR_RXPLL	0x410
+#define CHANNEL_XCVR_LOOPB	0x41C
+
+static u32 read_channel_drp_reg(struct jesd204b_phy_state *st, u32 addr)
+{
+	u32 temp;
+
+	jesd204b_phy_write(st, CHANNEL_DRP_ADDR, (DRPREAD | addr));
+	temp = jesd204b_phy_read(st, CHANNEL_DRP_DREAD);
+	return temp;
+}
+
+static void write_channel_drp_reg(struct jesd204b_phy_state *st, u32 addr,
+				  u32 data)
+{
+	u32 loop = 10;
+
+	jesd204b_phy_write(st, CHANNEL_DRP_DWRITE, data);
+	jesd204b_phy_write(st, CHANNEL_DRP_ADDR, (DRPWRITE | addr));
+
+	do {
+		if (!jesd204b_phy_read(st, CHANNEL_DRP_STAT))
+			break;
+		msleep(1);
+	} while (loop--);
+
+	if (!loop)
+		dev_err(st->dev, "DRP wait timeout\n");
+}
+
+static void read_plls(struct jesd204b_phy_state *st)
+{
+	int i;
+	int pll = st->pll;
+	u32 no_of_common_drp_interfaces = 1;
+
+	if (st->pll == CPLL)
+		no_of_common_drp_interfaces = jesd204b_phy_read(
+						st, NR_TRANS_DRP_INTERFACES);
+	else
+		no_of_common_drp_interfaces = jesd204b_phy_read(
+						st, NR_COMMON_DRP_INTERFACES);
+
+	for (i = 0; i < no_of_common_drp_interfaces; i++) {
+		jesd204b_phy_write(st, CHANNEL_XCVR_SEL, i);
+		pll = jesd204b_phy_read(st, CHANNEL_XCVR_TXPLL);
+		pll = jesd204b_phy_read(st, CHANNEL_XCVR_RXPLL);
+	}
+}
+
+static void configure_plls(struct jesd204b_phy_state *st, u32 pll)
+{
+	int i;
+	u32 no_of_common_drp_interfaces;
+
+	if (pll == CPLL)
+		no_of_common_drp_interfaces = jesd204b_phy_read(
+						st, NR_TRANS_DRP_INTERFACES);
+	else
+		no_of_common_drp_interfaces = jesd204b_phy_read(
+						st, NR_COMMON_DRP_INTERFACES);
+
+	for (i = 0; i < no_of_common_drp_interfaces; i++) {
+		jesd204b_phy_write(st, CHANNEL_XCVR_SEL, i);
+		jesd204b_phy_write(st, CHANNEL_XCVR_TXPLL, pll);
+		jesd204b_phy_write(st, CHANNEL_XCVR_RXPLL, pll);
+	}
+}
+
+static void configure_channel_drp(struct jesd204b_phy_state *st, u32 setting)
+{
+	u32 i, j, addr, temp, no_of_common_drp_interfaces;
+	u32 no_channel_drp_reg = GTX7S_QPLL_NUM_CHANNEL_DRP_REGS;
+
+	no_of_common_drp_interfaces = jesd204b_phy_read(
+					st, NR_TRANS_DRP_INTERFACES);
+
+	if (st->pll == CPLL)
+		no_channel_drp_reg = GTX7S_CPLL_NUM_CHANNEL_DRP_REGS;
+	for (i = 0; i < no_of_common_drp_interfaces; i++) {
+		jesd204b_phy_write(st, CHANNEL_DRP_BASE, i);
+		for (j = 0; j < no_channel_drp_reg; j++) {
+			/* Get the register address */
+			if (st->pll == QPLL) {
+				addr = get_gtx7s_qpll_address_lut(j);
+
+				/* Read the register */
+				temp = read_channel_drp_reg(st, addr);
+
+				temp &= (0xFFFF ^ (get_gtx7s_qpll_mask_lut(j)));
+				temp |= ((get_gtx7s_qpll_param_lut(j, setting)
+						<< get_gtx7s_qpll_offset_lut(j))
+						& get_gtx7s_qpll_mask_lut(j));
+			} else {
+				addr = get_gtx7s_cpll_address_lut(j);
+
+				temp = read_channel_drp_reg(st, addr);
+
+				temp &= (0xFFFF ^ (get_gtx7s_cpll_mask_lut(j)));
+				temp |= ((get_gtx7s_cpll_param_lut(j, setting)
+						<< get_gtx7s_cpll_offset_lut(j))
+						& get_gtx7s_cpll_mask_lut(j));
+			}
+			write_channel_drp_reg(st, addr, temp);
+		}
+	}
+}
+
+void jesd204_phy_set_speed(struct jesd204b_phy_state *st, u32 band)
+{
+	/* make sure we have the correct PLL's selected. */
+	configure_channel_drp(st, band);
+}
+
+static void jesd204_phy_init(struct jesd204b_phy_state *st, int line_rate)
+{
+	jesd204_phy_set_speed(st, line_rate);
+}
+
+int jesd204_phy_set_loop(struct jesd204b_phy_state *st, u32 loopval)
+{
+	int i;
+	u32 no_of_channels;
+
+	no_of_channels = jesd204b_phy_read(st, NR_COMMON_DRP_INTERFACES);
+
+	if (loopval > JESD_PHY_LOOP_MAX)
+		return -EINVAL;
+
+	for (i = 0; i < no_of_channels ; i++) {
+		jesd204b_phy_write(st, CHANNEL_XCVR_SEL, i);
+		jesd204b_phy_write(st, CHANNEL_XCVR_LOOPB, loopval);
+	}
+	return 0;
+}
+
+static ssize_t jesd204b_pll_read(struct device *dev,
+				 struct device_attribute *attr,
+				 char *buf)
+{
+	struct jesd204b_phy_state *st = dev_get_drvdata(dev);
+
+	read_plls(st);
+	if (st->pll == CPLL)
+		return sprintf(buf, "cpll\n");
+	return sprintf(buf, "qpll\n");
+}
+
+static ssize_t jesd204b_configure_pll(struct device *dev,
+				      struct device_attribute *attr,
+				      const char *buf, size_t count)
+{
+	struct jesd204b_phy_state *st = dev_get_drvdata(dev);
+	unsigned val;
+	int ret;
+
+	ret = kstrtouint(buf, 0, &val);
+	if (!ret)
+		return 0;
+
+	if (val > QPLL) {
+		dev_err(dev, "Setting the pll to %d valid values\n"
+			      "00 = CPLL\n"
+			      "10 = QPLL0 (UltraScale Only)\n"
+			      "11 = QPLL (7 series) QPLL1 (UltraScale)\n", val);
+		return 0;
+	}
+	st->pll = val;
+	configure_plls(st, val);
+
+	return count;
+}
+
+static DEVICE_ATTR(configure_pll, S_IWUSR | S_IRUSR, jesd204b_pll_read,
+			jesd204b_configure_pll);
+
+static ssize_t jesd204b_linerate_read(struct device *dev,
+				      struct device_attribute *attr,
+				      char *buf)
+{
+	struct jesd204b_phy_state *st = dev_get_drvdata(dev);
+
+	return sprintf(buf, "0x%X\n", st->band);
+}
+
+static ssize_t jesd204b_linerate_write(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	struct jesd204b_phy_state *st = dev_get_drvdata(dev);
+	int ret;
+	/* Low frequencies are not supported by qpll */
+
+	ret = kstrtouint(buf, 0, &st->band);
+	if (ret)
+		return ret;
+
+	dev_info(dev, "Setting the line rate to band to %d\n", st->band);
+	/* QPLL - freq options in phy
+	 * 62.5
+	 * 78.125
+	 * 94.697
+	 * 97.656
+	 * 125.000
+	 * 156.25
+	 * 187.5
+	 * 189.394
+	 * 195.313
+	 * 234.375
+	 * 250.000
+	 * 284.091
+	 * 292.969
+	 */
+	if (st->band == 2)
+		clk_set_rate(st->clk,  62500000); /* 2.5G */
+	else if (st->band == 4)
+		clk_set_rate(st->clk,  97656000); /* 3.9G */
+	else if (st->band == 6)
+		clk_set_rate(st->clk, 125000000); /* 5G */
+	else if (st->band == 7)
+		clk_set_rate(st->clk, 156250000); /* 6.25G */
+	else if (st->band == 8)
+		clk_set_rate(st->clk, 195313000); /* 7.812G */
+	else if (st->band == 9)
+		clk_set_rate(st->clk, 250000000);/* 10G */
+
+	jesd204_phy_init(st, st->band);
+
+	return count;
+}
+
+static DEVICE_ATTR(line_rate_band, S_IWUSR | S_IRUSR, jesd204b_linerate_read,
+		   jesd204b_linerate_write);
+
+/* Match table for of_platform binding */
+static const struct of_device_id jesd204b_phy_of_match[] = {
+	{ .compatible = "xlnx,jesd204-phy-2.0", },
+	{ /* end of list */ },
+};
+
+static int jesd204b_phy_probe(struct platform_device *pdev)
+{
+	struct jesd204b_phy_state *st;
+	struct resource *mem; /* IO mem resources */
+	int ret;
+	u32 ref_clk;
+
+	st = devm_kzalloc(&pdev->dev, sizeof(*st), GFP_KERNEL);
+	if (!st)
+		return -ENOMEM;
+
+	st->clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(st->clk))
+		return -EPROBE_DEFER;
+
+	mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	st->phy = devm_ioremap_resource(&pdev->dev, mem);
+	if (IS_ERR(st->phy)) {
+		dev_err(&pdev->dev, "Failed ioremap\n");
+		return PTR_ERR(st->phy);
+	}
+	st->dev = &pdev->dev;
+	platform_set_drvdata(pdev, st);
+
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,lanes",
+				   &st->lanes);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to read required dt property\n");
+		return ret;
+	}
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,pll-selection",
+				   &st->pll);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to read required dt property\n");
+		return ret;
+	}
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,gt-refclk-freq",
+				   &ref_clk);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to read required dt property\n");
+		return ret;
+	}
+
+	clk_set_rate(st->clk, (unsigned long)ref_clk);
+	device_create_file(&pdev->dev, &dev_attr_configure_pll);
+	device_create_file(&pdev->dev, &dev_attr_line_rate_band);
+
+	ret = clk_prepare_enable(st->clk);
+	if (ret) {
+		dev_err(&pdev->dev, "Unable to enable clock.\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static int jesd204b_phy_remove(struct platform_device *pdev)
+{
+	struct jesd204b_phy_state *st = platform_get_drvdata(pdev);
+
+	clk_disable_unprepare(st->clk);
+	clk_put(st->clk);
+	device_remove_file(&pdev->dev, &dev_attr_configure_pll);
+	device_remove_file(&pdev->dev, &dev_attr_line_rate_band);
+	return 0;
+}
+
+static struct platform_driver jesd204b_driver = {
+	.driver = {
+		.name = "jesd204b_phy",
+		.of_match_table = jesd204b_phy_of_match,
+	},
+	.probe		= jesd204b_phy_probe,
+	.remove		= jesd204b_phy_remove,
+};
+
+module_platform_driver(jesd204b_driver);
+
+MODULE_AUTHOR("Shubhrajyoti Datta <shubhraj@xilinx.com>");
+MODULE_DESCRIPTION("AXI-JESD204B Phy Interface Module");
+MODULE_LICENSE("GPL");
diff --git a/drivers/misc/jesd204b/jesd_phy.h b/drivers/misc/jesd204b/jesd_phy.h
new file mode 100644
index 000000000..c15328532
--- /dev/null
+++ b/drivers/misc/jesd204b/jesd_phy.h
@@ -0,0 +1,42 @@
+/*
+ * Copyright (C) 2014 - 2015 Xilinx, Inc.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#ifndef JESD_PHY_H_
+#define JESD_PHY_H_
+
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+
+struct jesd204b_phy_state {
+	struct device	*dev;
+	void __iomem	*phy;
+	struct clk	*clk;
+	u32		vers_id;
+	u32		addr;
+	u32		lanes;
+	u32		band;
+	u32		pll;
+	unsigned long	rate;
+};
+
+int jesd204_phy_set_loop(struct jesd204b_phy_state *st, u32 loopval);
+
+#endif /* JESD_PHY_H_ */
diff --git a/drivers/misc/jesd204b/s7_gtxe2_drp.h b/drivers/misc/jesd204b/s7_gtxe2_drp.h
new file mode 100644
index 000000000..f08a21143
--- /dev/null
+++ b/drivers/misc/jesd204b/s7_gtxe2_drp.h
@@ -0,0 +1,123 @@
+/*
+ * Copyright (C) 2014 - 2015 Xilinx, Inc.
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#define TXOUT_DIV_ADDR					0x88
+#define TXOUT_DIV_MASK					0x70
+#define TXOUT_DIV_OFFSET				0x4
+#define TXOUT_DIV_WIDTH					0x3
+#define TXOUT_DIV_DEFAULT				0x0
+
+#define RXOUT_DIV_ADDR					0x88
+#define RXOUT_DIV_MASK					0x7
+#define RXOUT_DIV_OFFSET				0x0
+#define RXOUT_DIV_WIDTH					0x3
+#define RXOUT_DIV_DEFAULT				0x0
+
+#define RXCDR_CFG0_ADDR					0xa8
+#define RXCDR_CFG0_MASK					0xffff
+#define RXCDR_CFG0_OFFSET				0x0
+#define RXCDR_CFG0_WIDTH				0x10
+#define RXCDR_CFG0_DEFAULT				0x0
+
+#define RXCDR_CFG1_ADDR					0xa9
+#define RXCDR_CFG1_MASK					0xffff
+#define RXCDR_CFG1_OFFSET				0x0
+#define RXCDR_CFG1_WIDTH				0x10
+#define RXCDR_CFG1_DEFAULT				0x0
+
+#define RXCDR_CFG2_ADDR					0xaa
+#define RXCDR_CFG2_MASK					0xffff
+#define RXCDR_CFG2_OFFSET				0x0
+#define RXCDR_CFG2_WIDTH				0x10
+#define RXCDR_CFG2_DEFAULT				0x0
+
+#define RXCDR_CFG3_ADDR					0xab
+#define RXCDR_CFG3_MASK					0xffff
+#define RXCDR_CFG3_OFFSET				0x0
+#define RXCDR_CFG3_WIDTH				0x10
+#define RXCDR_CFG3_DEFAULT				0x0
+
+#define RXCDR_CFG4_ADDR					0xac
+#define RXCDR_CFG4_MASK					0xff
+#define RXCDR_CFG4_OFFSET				0x0
+#define RXCDR_CFG4_WIDTH				0x8
+#define RXCDR_CFG4_DEFAULT				0x0
+
+#define RX_DFE_LPM_CFG_ADDR				0x29
+#define RX_DFE_LPM_CFG_MASK				0xffff
+#define RX_DFE_LPM_CFG_OFFSET				0x0
+#define RX_DFE_LPM_CFG_WIDTH				0x10
+#define RX_DFE_LPM_CFG_DEFAULT				0x0
+
+#define QPLL_CFG0_ADDR					0x32
+#define QPLL_CFG0_MASK					0xffff
+#define QPLL_CFG0_OFFSET				0x0
+#define QPLL_CFG0_WIDTH					0x10
+#define QPLL_CFG0_DEFAULT				0x0
+
+#define QPLL_CFG1_ADDR					0x33
+#define QPLL_CFG1_MASK					0x7ff
+#define QPLL_CFG1_OFFSET				0x0
+#define QPLL_CFG1_WIDTH					0xb
+#define QPLL_CFG1_DEFAULT				0x0
+
+#define QPLL_REFCLK_DIV_M_ADDR				0x33
+#define QPLL_REFCLK_DIV_M_MASK				0xf800
+#define QPLL_REFCLK_DIV_M_OFFSET			0xb
+#define QPLL_REFCLK_DIV_M_WIDTH				0x5
+#define QPLL_REFCLK_DIV_M_DEFAULT			0x0
+
+#define QPLL_FBDIV_N_ADDR				0x36
+#define QPLL_FBDIV_N_MASK				0x3ff
+#define QPLL_FBDIV_N_OFFSET				0x0
+#define QPLL_FBDIV_N_WIDTH				0xa
+#define QPLL_FBDIV_N_DEFAULT				0x0
+
+#define QPLL_FBDIV_RATIO_ADDR				0x37
+#define QPLL_FBDIV_RATIO_MASK				0x40
+#define QPLL_FBDIV_RATIO_OFFSET				0x6
+#define QPLL_FBDIV_RATIO_WIDTH				0x1
+#define QPLL_FBDIV_RATIO_DEFAULT			0x0
+
+#define CPLL_CFG0_ADDR					0x5c
+#define CPLL_CFG0_MASK					0xff00
+#define CPLL_CFG0_OFFSET				0x8
+#define CPLL_CFG0_WIDTH					0x8
+#define CPLL_CFG0_DEFAULT				0x0
+
+#define CPLL_CFG1_ADDR					0x5d
+#define CPLL_CFG1_MASK					0xffff
+#define CPLL_CFG1_OFFSET				0x0
+#define CPLL_CFG1_WIDTH					0x10
+#define CPLL_CFG1_DEFAULT				0x0
+
+#define CPLL_REFCLK_DIV_M_ADDR				0x5e
+#define CPLL_REFCLK_DIV_M_MASK				0x1f00
+#define CPLL_REFCLK_DIV_M_OFFSET			0x8
+#define CPLL_REFCLK_DIV_M_WIDTH				0x5
+#define CPLL_REFCLK_DIV_M_DEFAULT			0x0
+
+#define CPLL_FB_DIV_45_N1_ADDR				0x5e
+#define CPLL_FB_DIV_45_N1_MASK				0x80
+#define CPLL_FB_DIV_45_N1_OFFSET			0x7
+#define CPLL_FB_DIV_45_N1_WIDTH				0x1
+#define CPLL_FB_DIV_45_N1_DEFAULT			0x0
+
+#define CPLL_FBDIV_N2_ADDR				0x5e
+#define CPLL_FBDIV_N2_MASK				0x7f
+#define CPLL_FBDIV_N2_OFFSET				0x0
+#define CPLL_FBDIV_N2_WIDTH				0x7
+#define CPLL_FBDIV_N2_DEFAULT				0x0
diff --git a/drivers/misc/jesd204b/xilinx_jesd204b.c b/drivers/misc/jesd204b/xilinx_jesd204b.c
new file mode 100644
index 000000000..f8d1ddcb6
--- /dev/null
+++ b/drivers/misc/jesd204b/xilinx_jesd204b.c
@@ -0,0 +1,369 @@
+/*
+ * Xilinx AXI-JESD204B Interface Module
+ *
+ * Copyright 2014 Analog Devices Inc.
+ *
+ * Licensed under the GPL-2.
+ *
+ * http://wiki.analog.com/resources/fpga/xilinx/
+ */
+
+#include <linux/clk.h>
+#include <linux/clk-provider.h>
+#include <linux/debugfs.h>
+#include <linux/delay.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/wait.h>
+
+#include "xilinx_jesd204b.h"
+
+struct child_clk {
+	struct clk_hw		hw;
+	struct jesd204b_state	*st;
+	unsigned long		rate;
+	bool			enabled;
+};
+
+#define to_clk_priv(_hw) container_of(_hw, struct child_clk, hw)
+
+static inline void jesd204b_write(struct jesd204b_state *st,
+				  unsigned int reg, unsigned int val)
+{
+	iowrite32(val, st->regs + reg);
+}
+
+static inline unsigned int jesd204b_read(struct jesd204b_state *st,
+					 unsigned int reg)
+{
+	return ioread32(st->regs + reg);
+}
+
+static ssize_t jesd204b_laneinfo_read(struct device *dev,
+				      struct device_attribute *attr,
+				      char *buf, unsigned int lane)
+{
+	struct jesd204b_state *st = dev_get_drvdata(dev);
+	int ret;
+	unsigned int val1, val2, val3;
+
+	val1 = jesd204b_read(st, XLNX_JESD204_REG_ID_L(lane));
+	val2 = jesd204b_read(st, XLNX_JESD204_REG_LANE_F(lane));
+	val3 = jesd204b_read(st, XLNX_JESD204_REG_SCR_S_HD_CF(lane));
+	ret = sprintf(buf,
+		      "DID: %d, BID: %d, LID: %d, L: %d, SCR: %d, F: %d\n",
+		      XLNX_JESD204_LANE_DID(val1),
+		      XLNX_JESD204_LANE_BID(val1),
+		      XLNX_JESD204_LANE_LID(val1),
+		      XLNX_JESD204_LANE_L(val1),
+		      XLNX_JESD204_LANE_SCR(val3),
+		      XLNX_JESD204_LANE_F(val2));
+
+	val1 = jesd204b_read(st, XLNX_JESD204_REG_LANE_K(lane));
+	val2 = jesd204b_read(st, XLNX_JESD204_REG_M_N_ND_CS(lane));
+
+	ret += sprintf(buf + ret,
+		       "K: %d, M: %d, N: %d, CS: %d, S: %d, N': %d, HD: %d\n",
+		       XLNX_JESD204_LANE_K(val1),
+		       XLNX_JESD204_LANE_M(val2),
+		       XLNX_JESD204_LANE_N(val2),
+		       XLNX_JESD204_LANE_CS(val2),
+		       XLNX_JESD204_LANE_S(val3),
+		       XLNX_JESD204_LANE_ND(val2),
+		       XLNX_JESD204_LANE_HD(val3));
+
+	val1 = jesd204b_read(st, XLNX_JESD204_REG_FCHK(lane));
+	ret += sprintf(buf + ret, "FCHK: 0x%X, CF: %d\n",
+		       XLNX_JESD204_LANE_FCHK(val1),
+		       XLNX_JESD204_LANE_CF(val3));
+
+	val1 = jesd204b_read(st, XLNX_JESD204_REG_SC2_ADJ_CTRL(lane));
+	val2 = jesd204b_read(st, XLNX_JESD204_REG_LANE_VERSION(lane));
+	ret += sprintf(buf + ret,
+		"ADJCNT: %d, PHYADJ: %d, ADJDIR: %d, JESDV: %d, SUBCLASS: %d\n",
+		       XLNX_JESD204_LANE_ADJ_CNT(val1),
+		       XLNX_JESD204_LANE_PHASE_ADJ_REQ(val1),
+		       XLNX_JESD204_LANE_ADJ_CNT_DIR(val1),
+		       XLNX_JESD204_LANE_JESDV(val2),
+		       XLNX_JESD204_LANE_SUBCLASS(val2));
+
+	ret += sprintf(buf + ret, "MFCNT : 0x%X\n",
+		       jesd204b_read(st, XLNX_JESD204_REG_TM_MFC_CNT(lane)));
+	ret += sprintf(buf + ret, "ILACNT: 0x%X\n",
+		       jesd204b_read(st, XLNX_JESD204_REG_TM_ILA_CNT(lane)));
+	ret += sprintf(buf + ret, "ERRCNT: 0x%X\n",
+		       jesd204b_read(st, XLNX_JESD204_REG_TM_ERR_CNT(lane)));
+	ret += sprintf(buf + ret, "BUFCNT: 0x%X\n",
+		       jesd204b_read(st, XLNX_JESD204_REG_TM_BUF_ADJ(lane)));
+	ret += sprintf(buf + ret, "LECNT: 0x%X\n",
+		       jesd204b_read(st,
+		       XLNX_JESD204_REG_TM_LINK_ERR_CNT(lane)));
+
+	ret += sprintf(buf + ret, "FC: %lu\n", st->rate);
+
+	return ret;
+}
+
+#define JESD_LANE(_x)							    \
+static ssize_t jesd204b_lane##_x##_info_read(struct device *dev,	    \
+					     struct device_attribute *attr, \
+					     char *buf)			    \
+{									    \
+	return jesd204b_laneinfo_read(dev, attr, buf, _x);		    \
+}									    \
+static DEVICE_ATTR(lane##_x##_info, 0400, jesd204b_lane##_x##_info_read, \
+		   NULL)
+
+JESD_LANE(0);
+JESD_LANE(1);
+JESD_LANE(2);
+JESD_LANE(3);
+JESD_LANE(4);
+JESD_LANE(5);
+JESD_LANE(6);
+JESD_LANE(7);
+
+static ssize_t jesd204b_lane_syscstat_read(struct device *dev,
+			struct device_attribute *attr,
+			char *buf, unsigned int lane)
+{
+	unsigned int stat;
+	struct jesd204b_state *st = dev_get_drvdata(dev);
+
+	stat = jesd204b_read(st, XLNX_JESD204_REG_SYNC_ERR_STAT);
+
+	return sprintf(buf,
+			"NOT_IN_TAB: %d, DISPARITY: %d, UNEXPECTED_K: %d\n",
+			stat & XLNX_JESD204_SYNC_ERR_NOT_IN_TAB(lane),
+			stat & XLNX_JESD204_SYNC_ERR_DISPARITY(lane),
+			stat & XLNX_JESD204_SYNC_ERR_UNEXPECTED_K(lane));
+}
+
+#define JESD_SYNCSTAT_LANE(_x)						       \
+static ssize_t jesd204b_lane##_x##_syncstat_read(struct device *dev,	       \
+						 struct device_attribute *attr,\
+						 char *buf)		       \
+{									       \
+	return jesd204b_lane_syscstat_read(dev, attr, buf, _x);		       \
+}									       \
+static DEVICE_ATTR(lane##_x##_syncstat, 0400,			       \
+		   jesd204b_lane##_x##_syncstat_read, NULL)
+
+JESD_SYNCSTAT_LANE(0);
+JESD_SYNCSTAT_LANE(1);
+JESD_SYNCSTAT_LANE(2);
+JESD_SYNCSTAT_LANE(3);
+JESD_SYNCSTAT_LANE(4);
+JESD_SYNCSTAT_LANE(5);
+JESD_SYNCSTAT_LANE(6);
+JESD_SYNCSTAT_LANE(7);
+
+static ssize_t jesd204b_reg_write(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t count)
+{
+	struct jesd204b_state *st = dev_get_drvdata(dev);
+	unsigned int val;
+	int ret;
+
+	ret = sscanf(buf, "%i %i", &st->addr, &val);
+	if (ret == 2)
+		jesd204b_write(st, st->addr, val);
+
+	return count;
+}
+
+static ssize_t jesd204b_reg_read(struct device *dev,
+				 struct device_attribute *attr,
+				 char *buf)
+{
+	struct jesd204b_state *st = dev_get_drvdata(dev);
+
+	return sprintf(buf, "0x%X\n", jesd204b_read(st, st->addr));
+}
+
+static DEVICE_ATTR(reg_access, 0600, jesd204b_reg_read,
+		   jesd204b_reg_write);
+
+static ssize_t jesd204b_syncreg_read(struct device *dev,
+				     struct device_attribute *attr,
+				     char *buf)
+{
+	struct jesd204b_state *st = dev_get_drvdata(dev);
+
+	return sprintf(buf, "0x%X\n", jesd204b_read(st,
+					XLNX_JESD204_REG_SYNC_STATUS));
+}
+
+static DEVICE_ATTR(sync_status, 0400, jesd204b_syncreg_read, NULL);
+
+/* Match table for of_platform binding */
+static const struct of_device_id jesd204b_of_match[] = {
+	{ .compatible = "xlnx,jesd204-5.1",},
+	{ .compatible = "xlnx,jesd204-5.2",},
+	{ .compatible = "xlnx,jesd204-6.1",},
+	{ /* end of list */ },
+};
+MODULE_DEVICE_TABLE(of, jesd204b_of_match);
+
+static int jesd204b_probe(struct platform_device *pdev)
+{
+	struct jesd204b_state *st;
+	struct resource *mem; /* IO mem resources */
+	struct clk *clk;
+	struct child_clk *clk_priv;
+	struct clk_init_data init;
+	unsigned int val;
+	int ret;
+
+	clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(clk))
+		return -EPROBE_DEFER;
+
+	st = devm_kzalloc(&pdev->dev, sizeof(*st), GFP_KERNEL);
+	if (!st)
+		return -ENOMEM;
+
+	mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	st->regs = devm_ioremap_resource(&pdev->dev, mem);
+	if (IS_ERR(st->regs)) {
+		dev_err(&pdev->dev, "Failed ioremap\n");
+		return PTR_ERR(st->regs);
+	}
+
+	st->dev = &pdev->dev;
+
+	platform_set_drvdata(pdev, st);
+
+	st->clk = clk;
+	clk_set_rate(st->clk, 156250000);
+	st->rate = clk_get_rate(clk);
+
+	of_property_read_u32(pdev->dev.of_node, "xlnx,node-is-transmit",
+			     &st->transmit);
+
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,lanes",
+				   &st->lanes);
+	if (ret)
+		st->lanes = jesd204b_read(st, XLNX_JESD204_REG_LANES) + 1;
+
+	jesd204b_write(st, XLNX_JESD204_REG_RESET, XLNX_JESD204_RESET);
+	while (!jesd204b_read(st, XLNX_JESD204_REG_RESET))
+		msleep(20);
+
+	jesd204b_write(st, XLNX_JESD204_REG_ILA_CTRL,
+		       (of_property_read_bool(pdev->dev.of_node,
+			"xlnx,lanesync-enable") ? XLNX_JESD204_ILA_EN : 0));
+
+	jesd204b_write(st, XLNX_JESD204_REG_SCR_CTRL,
+		       (of_property_read_bool(pdev->dev.of_node,
+			"xlnx,scramble-enable") ? XLNX_JESD204_SCR_EN : 0));
+
+	jesd204b_write(st, XLNX_JESD204_REG_SYSREF_CTRL,
+		       (of_property_read_bool(pdev->dev.of_node,
+			"xlnx,sysref-always-enable") ?
+			XLNX_JESD204_ALWAYS_SYSREF_EN : 0));
+
+	device_create_file(&pdev->dev, &dev_attr_reg_access);
+
+	device_create_file(&pdev->dev, &dev_attr_sync_status);
+	switch (st->lanes) {
+	case 8:
+		device_create_file(&pdev->dev, &dev_attr_lane4_info);
+		device_create_file(&pdev->dev, &dev_attr_lane5_info);
+		device_create_file(&pdev->dev, &dev_attr_lane6_info);
+		device_create_file(&pdev->dev, &dev_attr_lane7_info);
+		if (!st->transmit) {
+			device_create_file(&pdev->dev,
+					   &dev_attr_lane4_syncstat);
+			device_create_file(&pdev->dev,
+					   &dev_attr_lane5_syncstat);
+			device_create_file(&pdev->dev,
+					   &dev_attr_lane6_syncstat);
+			device_create_file(&pdev->dev,
+					   &dev_attr_lane7_syncstat);
+		}
+		/* fall through */
+	case 4:
+		device_create_file(&pdev->dev, &dev_attr_lane2_info);
+		device_create_file(&pdev->dev, &dev_attr_lane3_info);
+		if (!st->transmit) {
+			device_create_file(&pdev->dev,
+					   &dev_attr_lane2_syncstat);
+			device_create_file(&pdev->dev,
+					   &dev_attr_lane3_syncstat);
+		}
+		/* fall through */
+	case 2:
+		device_create_file(&pdev->dev, &dev_attr_lane1_info);
+		if (!st->transmit)
+			device_create_file(&pdev->dev,
+					   &dev_attr_lane1_syncstat);
+		/* fall through */
+	case 1:
+		device_create_file(&pdev->dev, &dev_attr_lane0_info);
+		if (!st->transmit)
+			device_create_file(&pdev->dev,
+					   &dev_attr_lane0_syncstat);
+		break;
+	default:
+
+		break;
+	}
+
+	clk_priv = devm_kzalloc(&pdev->dev, sizeof(*clk_priv), GFP_KERNEL);
+	if (!clk_priv)
+		return -ENOMEM;
+
+	/* struct child_clk assignments */
+	clk_priv->hw.init = &init;
+	clk_priv->rate = st->rate;
+	clk_priv->st = st;
+
+	ret = clk_prepare_enable(clk);
+	if (ret) {
+		dev_err(&pdev->dev, "Unable to enable clock.\n");
+		return ret;
+	}
+	val = jesd204b_read(st, XLNX_JESD204_REG_VERSION);
+
+	dev_info(&pdev->dev,
+		 "AXI-JESD204B %d.%d Rev %d, at 0x%08llX mapped to 0x%p",
+		 XLNX_JESD204_VERSION_MAJOR(val),
+		 XLNX_JESD204_VERSION_MINOR(val),
+		 XLNX_JESD204_VERSION_REV(val),
+		 (unsigned long long)mem->start, st->regs);
+
+	return 0;
+}
+
+static int jesd204b_remove(struct platform_device *pdev)
+{
+	struct jesd204b_state *st = platform_get_drvdata(pdev);
+
+	clk_disable_unprepare(st->clk);
+	clk_put(st->clk);
+
+	return 0;
+}
+
+static struct platform_driver jesd204b_driver = {
+	.driver = {
+		.name = KBUILD_MODNAME,
+		.of_match_table = jesd204b_of_match,
+	},
+	.probe		= jesd204b_probe,
+	.remove		= jesd204b_remove,
+};
+
+module_platform_driver(jesd204b_driver);
+
+MODULE_AUTHOR("Michael Hennerich <michael.hennerich@analog.com>");
+MODULE_DESCRIPTION("Analog Devices AXI-JESD204B Interface Module");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/misc/jesd204b/xilinx_jesd204b.h b/drivers/misc/jesd204b/xilinx_jesd204b.h
new file mode 100644
index 000000000..b9946a723
--- /dev/null
+++ b/drivers/misc/jesd204b/xilinx_jesd204b.h
@@ -0,0 +1,135 @@
+/*
+ * Xilinx AXI-JESD204B v5.1 Interface Module
+ *
+ * Copyright 2014 Analog Devices Inc.
+ *
+ * Licensed under the GPL-2.
+ *
+ * http://wiki.analog.com/resources/fpga/xilinx/
+ */
+
+#ifndef XILINX_JESD204B_H_
+#define XILINX_JESD204B_H_
+
+struct jesd204b_state {
+	struct device	*dev;
+	void __iomem	*regs;
+	void __iomem	*phy;
+	struct clk	*clk;
+	u32		lanes;
+	u32		vers_id;
+	u32		addr;
+	u32		band;
+	u32		transmit;
+	u32		pll;
+	unsigned long	rate;
+};
+
+#define XLNX_JESD204_REG_VERSION		0x000
+#define XLNX_JESD204_VERSION_MAJOR(x)		(((x) >> 24) & 0xFF)
+#define XLNX_JESD204_VERSION_MINOR(x)		(((x) >> 16) & 0xFF)
+#define XLNX_JESD204_VERSION_REV(x)		(((x) >> 8) & 0xFF)
+
+#define XLNX_JESD204_REG_RESET			0x004
+#define XLNX_JESD204_RESET			(1 << 0)
+
+#define XLNX_JESD204_REG_ILA_CTRL		0x008
+#define XLNX_JESD204_ILA_EN			(1 << 0)
+
+#define XLNX_JESD204_REG_SCR_CTRL		0x00C
+#define XLNX_JESD204_SCR_EN			(1 << 0)
+
+#define XLNX_JESD204_REG_SYSREF_CTRL		0x010
+#define XLNX_JESD204_ALWAYS_SYSREF_EN		(1 << 0)
+
+#define XLNX_JESD204_REG_ILA_MFC		0x014
+#define XLNX_JESD204_ILA_MFC(x)			(((x) - 1) & 0xFF)
+						/* TX only 4..256 */
+
+#define XLNX_JESD204_REG_TEST_MODE_SEL		0x018
+#define XLNX_JESD204_TEST_MODE_OFF		0 /* Normal operation */
+#define XLNX_JESD204_TEST_MODE_K28_5		1 /* Send/Receive /K28.5/
+						   * indefinitely
+						   */
+#define XLNX_JESD204_TEST_MODE_ILA		2 /* Synchronize as normal then
+						   * send/receive repeated ILA
+						   * sequences
+						   */
+#define XLNX_JESD204_TEST_MODE_D21_5		3 /* Send/Receive /D21.5/
+						   * indefinitely
+						   */
+#define XLNX_JESD204_TEST_MODE_RPAT		5 /* Send/Receive modified
+						   * random pattern (RPAT)
+						   */
+#define XLNX_JESD204_TEST_MODE_JSPAT		7 /* Send/Receive a scrambled
+						   * jitter pattern (JSPAT)
+						   */
+
+#define XLNX_JESD204_REG_SYNC_STATUS		0x038 /* Link SYNC status */
+#define XLNX_JESD204_REG_SYNC_ERR_STAT		0x01C /* RX only */
+#define XLNX_JESD204_SYNC_ERR_NOT_IN_TAB(lane)		(1 << (0 + (lane) * 3))
+#define XLNX_JESD204_SYNC_ERR_DISPARITY(lane)		(1 << (1 + (lane) * 3))
+#define XLNX_JESD204_SYNC_ERR_UNEXPECTED_K(lane)	(1 << (2 + (lane) * 3))
+
+#define XLNX_JESD204_REG_OCTETS_PER_FRAME	0x020
+#define XLNX_JESD204_OCTETS_PER_FRAME(x)	(((x) - 1) & 0xFF) /* 1..256 */
+
+#define XLNX_JESD204_REG_FRAMES_PER_MFRAME	0x024
+#define XLNX_JESD204_FRAMES_PER_MFRAME(x)	(((x) - 1) & 0x1F) /* 1..32 */
+
+#define XLNX_JESD204_REG_LANES			0x028
+#define XLNX_JESD204_LANES(x)			(((x) - 1) & 0x1F) /* 1..32 */
+
+#define XLNX_JESD204_REG_SUBCLASS		0x02C
+
+#define XLNX_JESD204_REG_RX_BUF_DELAY		0x030 /* RX only */
+#define XLNX_JESD204_RX_BUF_DELAY(x)		((x) & 0x1FFF)
+
+#define XLNX_JESD204_REG_RX_LINK_CTRL		0x034 /* RX only */
+#define XLNX_JESD204_LINK_TEST_EN		(1 << 0)
+#define XLNX_JESD204_SYNC_ERR_REP_DIS		(1 << 8)
+
+/* Per LANE Registers */
+#define XLNX_JESD204_REG_LANE_VERSION(l)	(0x800 + ((l) * 0x40))
+#define XLNX_JESD204_LANE_SUBCLASS(x)		(((x) >> 0) & 0x7)
+#define XLNX_JESD204_LANE_JESDV(x)		(((x) >> 8) & 0x7)
+
+#define XLNX_JESD204_REG_LANE_F(l)		(0x804 + ((l) * 0x40))
+#define XLNX_JESD204_LANE_F(x)			((((x) >> 0) & 0xFF) + 1)
+
+#define XLNX_JESD204_REG_LANE_K(l)		(0x808 + ((l) * 0x40))
+#define XLNX_JESD204_LANE_K(x)			((((x) >> 0) & 0x1F) + 1)
+
+#define XLNX_JESD204_REG_ID_L(l)		(0x80C + ((l) * 0x40))
+#define XLNX_JESD204_LANE_DID(x)		(((x) >> 0) & 0xFF)
+#define XLNX_JESD204_LANE_BID(x)		(((x) >> 8) & 0x1F)
+#define XLNX_JESD204_LANE_LID(x)		(((x) >> 16) & 0x1F)
+#define XLNX_JESD204_LANE_L(x)			((((x) >> 24) & 0x1F) + 1)
+
+#define XLNX_JESD204_REG_M_N_ND_CS(l)		(0x810 + ((l) * 0x40))
+#define XLNX_JESD204_LANE_M(x)			((((x) >> 0) & 0xFF) + 1)
+#define XLNX_JESD204_LANE_N(x)			((((x) >> 8) & 0x1F) + 1)
+#define XLNX_JESD204_LANE_ND(x)			((((x) >> 16) & 0x1F) + 1)
+#define XLNX_JESD204_LANE_CS(x)			(((x) >> 24) & 0x3)
+
+#define XLNX_JESD204_REG_SCR_S_HD_CF(l)		(0x814 + ((l) * 0x40))
+#define XLNX_JESD204_LANE_SCR(x)		(((x) >> 0) & 0x1)
+#define XLNX_JESD204_LANE_S(x)			((((x) >> 8) & 0x1F) + 1)
+#define XLNX_JESD204_LANE_HD(x)			(((x) >> 16) & 0x1)
+#define XLNX_JESD204_LANE_CF(x)			(((x) >> 24) & 0x1F)
+
+#define XLNX_JESD204_REG_FCHK(l)		(0x818 + ((l) * 0x40))
+#define XLNX_JESD204_LANE_FCHK(x)		(((x) >> 16) & 0xFF)
+
+#define XLNX_JESD204_REG_SC2_ADJ_CTRL(l)	(0x81C + ((l) * 0x40))
+#define XLNX_JESD204_LANE_ADJ_CNT(x)		(((x) >> 0) & 0xF)
+#define XLNX_JESD204_LANE_PHASE_ADJ_REQ(x)	(((x) >> 8) & 0x1)
+#define XLNX_JESD204_LANE_ADJ_CNT_DIR(x)		(((x) >> 16) & 0x1)
+
+#define XLNX_JESD204_REG_TM_ERR_CNT(l)		(0x820 + ((l) * 0x40))
+#define XLNX_JESD204_REG_TM_LINK_ERR_CNT(l)	(0x824 + ((l) * 0x40))
+#define XLNX_JESD204_REG_TM_ILA_CNT(l)		(0x828 + ((l) * 0x40))
+#define XLNX_JESD204_REG_TM_MFC_CNT(l)		(0x82C + ((l) * 0x40))
+#define XLNX_JESD204_REG_TM_BUF_ADJ(l)		(0x830 + ((l) * 0x40))
+
+#endif /* ADI_JESD204B_V51_H_ */
diff --git a/drivers/misc/xilinx-ai-engine/Makefile b/drivers/misc/xilinx-ai-engine/Makefile
new file mode 100644
index 000000000..67543bd2f
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/Makefile
@@ -0,0 +1,25 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Makefile for Xilinx AI engine device driver
+#
+
+obj-$(CONFIG_XILINX_AIE)	+= xilinx-aie.o
+
+xilinx-aie-$(CONFIG_XILINX_AIE) := ai-engine-aie.o		\
+				   ai-engine-clock.o		\
+				   ai-engine-dev.o		\
+				   ai-engine-dma.o		\
+				   ai-engine-fpga.o		\
+				   ai-engine-interrupt.o	\
+				   ai-engine-mem.o		\
+				   ai-engine-part.o		\
+				   ai-engine-res.o		\
+				   ai-engine-reset.o		\
+				   ai-engine-rscmgr.o		\
+				   ai-engine-sysfs.o		\
+				   ai-engine-sysfs-core.o	\
+				   ai-engine-sysfs-dma.o	\
+				   ai-engine-sysfs-error.o	\
+				   ai-engine-sysfs-event.o	\
+				   ai-engine-sysfs-lock.o	\
+				   ai-engine-sysfs-status.o
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-aie.c b/drivers/misc/xilinx-ai-engine/ai-engine-aie.c
new file mode 100644
index 000000000..2d422a192
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-aie.c
@@ -0,0 +1,1341 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine driver AIE device specific implementation
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+#define KBYTES(n)	((n) * 1024)
+
+#define AIE_ARRAY_SHIFT		30U
+#define AIE_COL_SHIFT		23U
+#define AIE_ROW_SHIFT		18U
+
+#define NUM_MEMS_PER_TILE	2U
+
+#define NUM_MODS_CORE_TILE	2U
+#define NUM_MODS_SHIMPL_TILE	1U
+
+/*
+ * Number of resources per module
+ */
+#define AIE_NUM_PERF_CORE_MOD		4U
+#define AIE_NUM_USEREVENT_CORE_MOD	4U
+#define AIE_NUM_TRACECONTROL_CORE_MOD	1U
+#define AIE_NUM_PCEVENT_CORE_MOD	4U
+#define AIE_NUM_SSSELECT_CORE_MOD	8U
+#define AIE_NUM_BROADCAST_CORE_MOD	16U
+#define AIE_NUM_COMBOEVENT_CORE_MOD	4U
+#define AIE_NUM_GROUPEVENTS_CORE_MOD	9U
+
+#define AIE_NUM_PERF_MEM_MOD		2U
+#define AIE_NUM_USEREVENT_MEM_MOD	4U
+#define AIE_NUM_TRACECONTROL_MEM_MOD	1U
+#define AIE_NUM_PCEVENT_MEM_MOD		0U
+#define AIE_NUM_SSSELECT_MEM_MOD	0U
+#define AIE_NUM_BROADCAST_MEM_MOD	16U
+#define AIE_NUM_COMBOEVENT_MEM_MOD	4U
+#define AIE_NUM_GROUPEVENTS_MEM_MOD	8U
+
+#define AIE_NUM_PERF_PL_MOD		2U
+#define AIE_NUM_USEREVENT_PL_MOD	4U
+#define AIE_NUM_TRACECONTROL_PL_MOD	1U
+#define AIE_NUM_PCEVENT_PL_MOD		0U
+#define AIE_NUM_SSSELECT_PL_MOD		8U
+#define AIE_NUM_BROADCAST_PL_MOD	16U
+#define AIE_NUM_COMBOEVENT_PL_MOD	4U
+#define AIE_NUM_GROUPEVENTS_PL_MOD	7U
+
+/*
+ * Registers offsets
+ */
+#define AIE_SHIMNOC_L2INTR_MASK_REGOFF		0x00015000U
+#define AIE_SHIMNOC_L2INTR_INTR_REGOFF		0x00015010U
+#define AIE_SHIMNOC_DMA_BD0_ADDRLOW_REGOFF	0x0001d000U
+#define AIE_SHIMNOC_DMA_BD15_PACKET_REGOFF	0x0001d13cU
+#define AIE_SHIMNOC_AXIMM_REGOFF		0x0001e020U
+#define AIE_SHIMPL_L1INTR_MASK_A_REGOFF		0x00035000U
+#define AIE_SHIMPL_L1INTR_BLOCK_NORTH_B_REGOFF	0x00035050U
+#define AIE_SHIMPL_CLKCNTR_REGOFF		0x00036040U
+#define AIE_SHIMPL_COLRESET_REGOFF		0x00036048U
+#define AIE_SHIMPL_RESET_REGOFF			0x0003604cU
+#define AIE_SHIMPL_GROUP_ERROR_REGOFF		0x0003450cU
+#define AIE_TILE_CORE_CLKCNTR_REGOFF		0x00036040U
+#define AIE_TILE_CORE_GROUP_ERROR_REGOFF	0x00034510U
+#define AIE_TILE_MEM_GROUP_ERROR_REGOFF		0x00014514U
+#define AIE_TILE_CORE_R0_REGOFF			0x00030000U
+#define AIE_TILE_CORE_LC_REGOFF			0x00030520U
+#define AIE_TILE_CORE_VRL0_REGOFF		0x00030530U
+#define AIE_TILE_CORE_AMH3_PART3_REGOFF		0x000307a0U
+
+/*
+ * Register masks
+ */
+#define AIE_SHIMPL_SHIMRST_MASK			0x1U
+#define AIE_SHIMPL_COLRST_MASK			0x1U
+#define AIE_SHIMPL_CLKCNTR_COLBUF_MASK		0x1U
+#define AIE_SHIMPL_CLKCNTR_NEXTCLK_MASK		BIT(1)
+#define AIE_TILE_CLKCNTR_COLBUF_MASK		BIT(0)
+#define AIE_TILE_CLKCNTR_NEXTCLK_MASK		BIT(1)
+
+/*
+ * AI engine SHIM reset ID.
+ * TODO: it should follow the Linux reset framework. The ID should be in the
+ * device tree. However, as versal resets is not ready, we hardcode it in the
+ * driver.
+ */
+#define VERSAL_PM_RST_AIE_SHIM_ID			0xc10405fU
+
+/* Macros to define size of a sysfs binary attribute */
+#define AIE_PART_SYSFS_CORE_BINA_SIZE		0x4000		/* 16KB */
+#define AIE_PART_SYSFS_DMA_BINA_SIZE		0xC800		/* 50KB */
+#define AIE_PART_SYSFS_LOCK_BINA_SIZE		0x28000		/* 160KB */
+#define AIE_PART_SYSFS_ERROR_BINA_SIZE		0x4000		/* 16KB */
+#define AIE_PART_SYSFS_STATUS_BINA_SIZE		0x3c000		/* 240KB */
+
+static const struct aie_tile_regs aie_kernel_regs[] = {
+	/* SHIM AXI MM Config */
+	{.attribute = AIE_TILE_TYPE_MASK_SHIMNOC << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMNOC_AXIMM_REGOFF,
+	 .eoff = AIE_SHIMNOC_AXIMM_REGOFF,
+	},
+	/* SHIM DMA ADDRESS range */
+	{.attribute = AIE_TILE_TYPE_MASK_SHIMNOC << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMNOC_DMA_BD0_ADDRLOW_REGOFF,
+	 .eoff = AIE_SHIMNOC_DMA_BD15_PACKET_REGOFF,
+	},
+	/* SHIM 2nd level interrupt controller */
+	{.attribute = AIE_TILE_TYPE_MASK_SHIMNOC << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMNOC_L2INTR_MASK_REGOFF,
+	 .eoff = AIE_SHIMNOC_L2INTR_INTR_REGOFF,
+	},
+	/* SHIM 1st level interrupt controller */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_L1INTR_MASK_A_REGOFF,
+	 .eoff = AIE_SHIMPL_L1INTR_BLOCK_NORTH_B_REGOFF,
+	},
+	/* SHIM column reset */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_COLRESET_REGOFF,
+	 .eoff = AIE_SHIMPL_COLRESET_REGOFF,
+	},
+	/* SHIM reset Enable */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_RESET_REGOFF,
+	 .eoff = AIE_SHIMPL_RESET_REGOFF,
+	},
+	/* SHIM clock control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_CLKCNTR_REGOFF,
+	 .eoff = AIE_SHIMPL_CLKCNTR_REGOFF,
+	},
+	/* SHIM group error enable */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_GROUP_ERROR_REGOFF,
+	 .eoff = AIE_SHIMPL_GROUP_ERROR_REGOFF,
+	},
+	/* Tile clock control */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_TILE_CORE_CLKCNTR_REGOFF,
+	 .eoff = AIE_TILE_CORE_CLKCNTR_REGOFF,
+	},
+	/* Tile group error for core module */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_TILE_CORE_GROUP_ERROR_REGOFF,
+	 .eoff = AIE_TILE_CORE_GROUP_ERROR_REGOFF,
+	},
+	/* Tile group error for memory module */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_TILE_MEM_GROUP_ERROR_REGOFF,
+	 .eoff = AIE_TILE_MEM_GROUP_ERROR_REGOFF,
+	},
+};
+
+static const struct aie_tile_regs aie_core_32bit_regs = {
+	.attribute = AIE_TILE_TYPE_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	.soff = AIE_TILE_CORE_R0_REGOFF,
+	.eoff = AIE_TILE_CORE_LC_REGOFF,
+};
+
+static const struct aie_tile_regs aie_core_128bit_regs = {
+	.attribute = AIE_TILE_TYPE_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	.soff = AIE_TILE_CORE_VRL0_REGOFF,
+	.eoff = AIE_TILE_CORE_AMH3_PART3_REGOFF,
+};
+
+static const struct aie_core_regs_attr aie_core_regs[] = {
+	{.core_regs = &aie_core_32bit_regs,
+	 .width = 1,
+	},
+	{.core_regs = &aie_core_128bit_regs,
+	 .width = 4,
+	},
+};
+
+static const struct aie_single_reg_field aie_col_rst = {
+	.mask = AIE_SHIMPL_COLRST_MASK,
+	.regoff = AIE_SHIMPL_COLRESET_REGOFF,
+};
+
+static const struct aie_single_reg_field aie_col_clkbuf = {
+	.mask = AIE_SHIMPL_CLKCNTR_COLBUF_MASK,
+	.regoff = AIE_SHIMPL_CLKCNTR_REGOFF,
+};
+
+static const struct aie_dma_attr aie_shimdma = {
+	.laddr = {
+		.mask = 0xffffffffU,
+		.regoff = 0U,
+	},
+	.haddr = {
+		.mask = 0xffff0000U,
+		.regoff = 0x8U,
+	},
+	.buflen = {
+		.mask = 0xffffffffU,
+		.regoff = 0x4U,
+	},
+	.sts = {
+		.mask = GENMASK(1, 0),
+		.regoff = 2U,
+	},
+	.stall = {
+		.mask = BIT(4),
+		.regoff = 1U,
+	},
+	.qsize = {
+		.mask = GENMASK(8, 6),
+		.regoff = 3U,
+	},
+	.curbd = {
+		.mask = GENMASK(19, 16),
+		.regoff = 4U,
+	},
+	.qsts = {
+		.mask = BIT(28),
+		.regoff = 1U,
+	},
+	.bd_regoff = 0x0001d000U,
+	.mm2s_sts_regoff = 0x1d164U,
+	.s2mm_sts_regoff = 0x1d160U,
+	.num_bds = 16,
+	.num_mm2s_chan = 2U,
+	.num_s2mm_chan = 2U,
+	.bd_len = 0x14U,
+};
+
+static const struct aie_dma_attr aie_tiledma = {
+	.sts = {
+		.mask = GENMASK(1, 0),
+		.regoff = 2U,
+	},
+	.stall = {
+		.mask = BIT(4),
+		.regoff = 1U,
+	},
+	.qsize = {
+		.mask = GENMASK(8, 6),
+		.regoff = 3U,
+	},
+	.curbd = {
+		.mask = GENMASK(19, 16),
+		.regoff = 4U,
+	},
+	.qsts = {
+		.mask = BIT(28),
+		.regoff = 1U,
+	},
+	.mm2s_sts_regoff = 0x1df10U,
+	.s2mm_sts_regoff = 0x1df00U,
+	.num_bds = 16,
+	.num_mm2s_chan = 2U,
+	.num_s2mm_chan = 2U,
+};
+
+static char *aie_dma_status_str[] = {
+	"idle",
+	"starting",
+	"running",
+	"stalled_on_requesting_lock",
+};
+
+static char *aie_queue_status_str[] = {
+	"okay",
+	"overflow",
+};
+
+static const struct aie_event_attr aie_pl_event = {
+	.bc_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x0U,
+	},
+	.group_error = {
+		.mask = GENMASK(10, 0),
+		.regoff = 0xcU,
+	},
+	.bc_regoff = 0x34010U,
+	.status_regoff = 0x34200U,
+	.group_regoff = 0x34500U,
+	.base_error_event = 62U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 107U,
+	.num_events = 128U,
+};
+
+static const struct aie_event_attr aie_mem_event = {
+	.bc_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x0U,
+	},
+	.group_error = {
+		.mask = GENMASK(13, 0),
+		.regoff = 0x14U,
+	},
+	.bc_regoff = 0x14010U,
+	.status_regoff = 0x14200U,
+	.group_regoff = 0x14500U,
+	.base_error_event = 87U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 107U,
+	.num_events = 128U,
+};
+
+static const struct aie_event_attr aie_core_event = {
+	.bc_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x0U,
+	},
+	.group_error = {
+		.mask = GENMASK(21, 0),
+		.regoff = 0x10U,
+	},
+	.bc_regoff = 0x34010U,
+	.status_regoff = 0x34200U,
+	.group_regoff = 0x34500U,
+	.base_error_event = 48U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 107U,
+	.num_events = 128U,
+};
+
+static const struct aie_l1_intr_ctrl_attr aie_l1_intr_ctrl = {
+	.swa_status = {
+		.mask = GENMASK(19, 0),
+		.regoff = 0xcU,
+	},
+	.swb_status = {
+		.mask = GENMASK(19, 0),
+		.regoff = 0x3cU,
+	},
+	.swa_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x14U,
+	},
+	.swb_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x44U,
+	},
+	.regoff = 0x35000U,
+	.event_lsb = 8,
+	.num_broadcasts = 0x14U,
+};
+
+static const struct aie_l2_intr_ctrl_attr aie_l2_intr_ctrl = {
+	.mask = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x0U,
+	},
+	.enable = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x4U,
+	},
+	.disable = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x8U,
+	},
+	.status = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0xcU,
+	},
+	.regoff = 0x15000U,
+	.num_broadcasts = 0x10U,
+};
+
+static const struct aie_event_prop aie_core_stream_error_prop[] = {
+	{
+		.event = 54U,
+		.event_str = "tlast_in_wss_words_0-2",
+	},
+	{
+		.event = 57U,
+		.event_str = "control_packet_error",
+	},
+	{
+		.event = 56U,
+		.event_str = "stream_packet_parity_error",
+	},
+};
+
+static const struct aie_event_prop aie_core_inst_error_prop[] = {
+	{
+		.event = 59U,
+		.event_str = "instruction_decompression_error",
+	},
+};
+
+static const struct aie_event_prop aie_core_ecc_error_prop[] = {
+	{
+		.event = 64U,
+		.event_str = "pm_ecc_error_2-bit",
+	},
+	{
+		.event = 62U,
+		.event_str = "pm_ecc_error_scrub_2-bit",
+	},
+};
+
+static const struct aie_event_prop aie_core_access_error_prop[] = {
+	{
+		.event = 55U,
+		.event_str = "pm_reg_access_failure",
+	},
+	{
+		.event = 66U,
+		.event_str = "dm_access_to_unavailable",
+	},
+	{
+		.event = 65U,
+		.event_str = "pm_address_out_of_range",
+	},
+	{
+		.event = 60U,
+		.event_str = "dm_address_out_of_range",
+	},
+};
+
+static const struct aie_event_prop aie_core_lock_error_prop[] = {
+	{
+		.event = 67U,
+		.event_str = "lock_access_to_unavailable",
+	},
+};
+
+static const struct aie_event_prop aie_core_bus_error_prop[] = {
+	{
+		.event = 58U,
+		.event_str = "axi-mm_slave_error",
+	},
+};
+
+static const struct aie_event_prop aie_mem_ecc_error_prop[] = {
+	{
+		.event = 88U,
+		.event_str = "dm_ecc_error_scrub_2-bit",
+	},
+	{
+		.event = 90U,
+		.event_str = "dm_ecc_error_2-bit",
+	},
+};
+
+static const struct aie_event_prop aie_mem_parity_error_prop[] = {
+	{
+		.event = 91U,
+		.event_str = "dm_parity_error_bank_2",
+	},
+	{
+		.event = 92U,
+		.event_str = "dm_parity_error_bank_3",
+	},
+	{
+		.event = 93U,
+		.event_str = "dm_parity_error_bank_4",
+	},
+	{
+		.event = 94U,
+		.event_str = "dm_parity_error_bank_5",
+	},
+	{
+		.event = 95U,
+		.event_str = "dm_parity_error_bank_6",
+	},
+	{
+		.event = 96U,
+		.event_str = "dm_parity_error_bank_7",
+	},
+};
+
+static const struct aie_event_prop aie_mem_dma_error_prop[] = {
+	{
+		.event = 97U,
+		.event_str = "dma_s2mm_0_error",
+	},
+	{
+		.event = 98U,
+		.event_str = "dma_s2mm_1_error",
+	},
+	{
+		.event = 99U,
+		.event_str = "dma_mm2s_0_error",
+	},
+	{
+		.event = 100U,
+		.event_str = "dma_mm2s_1_error",
+	},
+};
+
+static const struct aie_event_prop aie_shim_bus_error_prop[] = {
+	{
+		.event = 62U,
+		.event_str = "axi-mm_slave_tile_error",
+	},
+};
+
+static const struct aie_event_prop aie_shim_stream_error_prop[] = {
+	{
+		.event = 63U,
+		.event_str = "control_packet_error",
+	},
+	{
+		.event = 64U,
+		.event_str = "axi-mm_decode_nsu_error",
+	},
+	{
+		.event = 65U,
+		.event_str = "axi-mm_slave_nsu_error",
+	},
+	{
+		.event = 66U,
+		.event_str = "axi-mm_unsupported_traffic",
+	},
+	{
+		.event = 67U,
+		.event_str = "axi-mm_unsecure_access_in_secure_mode",
+	},
+	{
+		.event = 68U,
+		.event_str = "axi-mm_byte_strobe_error",
+	},
+};
+
+static const struct aie_event_prop aie_shim_dma_error_prop[] = {
+	{
+		.event = 69U,
+		.event_str = "dma_s2mm_0_error",
+	},
+	{
+		.event = 70U,
+		.event_str = "dma_s2mm_1_error",
+	},
+	{
+		.event = 71U,
+		.event_str = "dma_mm2s_0_error",
+	},
+	{
+		.event = 72U,
+		.event_str = "dma_mm2s_1_error",
+	},
+};
+
+static const struct aie_err_category aie_core_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_STREAM */
+		.err_category = AIE_ERROR_CATEGORY_STREAM,
+		.num_events = ARRAY_SIZE(aie_core_stream_error_prop),
+		.prop = aie_core_stream_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_ACCESS */
+		.err_category = AIE_ERROR_CATEGORY_ACCESS,
+		.num_events = ARRAY_SIZE(aie_core_access_error_prop),
+		.prop = aie_core_access_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_BUS */
+		.err_category = AIE_ERROR_CATEGORY_BUS,
+		.num_events = ARRAY_SIZE(aie_core_bus_error_prop),
+		.prop = aie_core_bus_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_INSTRUCTION */
+		.err_category = AIE_ERROR_CATEGORY_INSTRUCTION,
+		.num_events = ARRAY_SIZE(aie_core_inst_error_prop),
+		.prop = aie_core_inst_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_ECC */
+		.err_category = AIE_ERROR_CATEGORY_ECC,
+		.num_events = ARRAY_SIZE(aie_core_ecc_error_prop),
+		.prop = aie_core_ecc_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_LOCK */
+		.err_category = AIE_ERROR_CATEGORY_LOCK,
+		.num_events = ARRAY_SIZE(aie_core_lock_error_prop),
+		.prop = aie_core_lock_error_prop,
+	},
+};
+
+static const struct aie_err_category aie_mem_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_ECC */
+		.err_category = AIE_ERROR_CATEGORY_ECC,
+		.num_events = ARRAY_SIZE(aie_mem_ecc_error_prop),
+		.prop = aie_mem_ecc_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_MEM_PARITY */
+		.err_category = AIE_ERROR_CATEGORY_MEM_PARITY,
+		.num_events = ARRAY_SIZE(aie_mem_parity_error_prop),
+		.prop = aie_mem_parity_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_DMA */
+		.err_category = AIE_ERROR_CATEGORY_DMA,
+		.num_events = ARRAY_SIZE(aie_mem_dma_error_prop),
+		.prop = aie_mem_dma_error_prop,
+	},
+};
+
+static const struct aie_err_category aie_shim_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_BUS */
+		.err_category = AIE_ERROR_CATEGORY_BUS,
+		.num_events = ARRAY_SIZE(aie_shim_bus_error_prop),
+		.prop = aie_shim_bus_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_STREAM */
+		.err_category = AIE_ERROR_CATEGORY_STREAM,
+		.num_events = ARRAY_SIZE(aie_shim_stream_error_prop),
+		.prop = aie_shim_stream_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_DMA */
+		.err_category = AIE_ERROR_CATEGORY_DMA,
+		.num_events = ARRAY_SIZE(aie_shim_dma_error_prop),
+		.prop = aie_shim_dma_error_prop,
+	},
+};
+
+static const struct aie_error_attr aie_core_error = {
+	.num_err_categories = ARRAY_SIZE(aie_core_err_category),
+	.err_category = aie_core_err_category,
+};
+
+static const struct aie_error_attr aie_mem_error = {
+	.num_err_categories = ARRAY_SIZE(aie_mem_err_category),
+	.err_category = aie_mem_err_category,
+};
+
+static const struct aie_error_attr aie_shim_error = {
+	.num_err_categories = ARRAY_SIZE(aie_shim_err_category),
+	.err_category = aie_shim_err_category,
+};
+
+/* resource attributes for core tile type */
+static const
+struct aie_tile_rsc_attr aie_core_tile_rscs_attr[AIE_RSCTYPE_MAX] =  {
+	{
+		/* perf counter */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_PERF_MEM_MOD,},
+			{.num_rscs = AIE_NUM_PERF_CORE_MOD,},
+		},
+	},
+	{
+		/* user event */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_USEREVENT_MEM_MOD,},
+			{.num_rscs = AIE_NUM_USEREVENT_CORE_MOD,},
+		},
+	},
+	{
+		/* trace control */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_TRACECONTROL_MEM_MOD,},
+			{.num_rscs = AIE_NUM_TRACECONTROL_CORE_MOD,},
+		},
+	},
+	{
+		/* pc event */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_PCEVENT_MEM_MOD,},
+			{.num_rscs = AIE_NUM_PCEVENT_CORE_MOD,},
+		},
+	},
+	{
+		/* stream switch port select */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_SSSELECT_MEM_MOD,},
+			{.num_rscs = AIE_NUM_SSSELECT_CORE_MOD,},
+		},
+	},
+	{
+		/* broadcast */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_BROADCAST_MEM_MOD,},
+			{.num_rscs = AIE_NUM_BROADCAST_CORE_MOD,},
+		},
+	},
+	{
+		/* combo events */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_COMBOEVENT_MEM_MOD,},
+			{.num_rscs = AIE_NUM_COMBOEVENT_CORE_MOD,},
+		},
+	},
+	{
+		/* group events */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_GROUPEVENTS_MEM_MOD,},
+			{.num_rscs = AIE_NUM_GROUPEVENTS_CORE_MOD,},
+		},
+	},
+};
+
+/* resource attributes for SHIM PL tile type */
+static const
+struct aie_tile_rsc_attr aie_shimpl_tile_rscs_attr[AIE_RSCTYPE_MAX] =  {
+	{
+		/* perf counter */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_PERF_PL_MOD,},
+		},
+	},
+	{
+		/* user event */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_USEREVENT_PL_MOD,},
+		},
+	},
+	{
+		/* trace control */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_TRACECONTROL_PL_MOD},
+		},
+	},
+	{
+		/* pc event */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_PCEVENT_PL_MOD},
+		},
+	},
+	{
+		/* stream switch port select */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_SSSELECT_PL_MOD},
+		},
+	},
+	{
+		/* broadcast */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_BROADCAST_PL_MOD},
+		},
+	},
+	{
+		/* combo events */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_COMBOEVENT_PL_MOD},
+		},
+	},
+	{
+		/* group events */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_GROUPEVENTS_PL_MOD},
+		},
+	},
+};
+
+/* modules types array of CORE tile */
+static const
+enum aie_module_type aie_core_tile_module_types[NUM_MODS_CORE_TILE] = {
+	AIE_MEM_MOD,
+	AIE_CORE_MOD,
+};
+
+/* modules types array of SHIM PL tile */
+static const
+enum aie_module_type aie_shimpl_tile_module_types[NUM_MODS_SHIMPL_TILE] = {
+	AIE_PL_MOD,
+};
+
+static const struct aie_single_reg_field aie_core_sts = {
+	.mask = GENMASK(20, 0),
+	.regoff = 0x32004U,
+};
+
+static const struct aie_single_reg_field aie_core_done = {
+	.mask = BIT(20),
+	.regoff = 0x32004U,
+};
+
+static const struct aie_single_reg_field aie_core_disable_event_sts = {
+	.mask = BIT(15),
+	.regoff = 0x32008U,
+};
+
+static const struct aie_single_reg_field aie_core_pc = {
+	.mask = GENMASK(19, 0),
+	.regoff = 0x30280U,
+};
+
+static const struct aie_single_reg_field aie_core_lr = {
+	.mask = GENMASK(19, 0),
+	.regoff = 0x302B0U,
+};
+
+static const struct aie_single_reg_field aie_core_sp = {
+	.mask = GENMASK(19, 0),
+	.regoff = 0x302A0U,
+};
+
+static char *aie_core_status_str[] = {
+	"enabled",
+	"reset",
+	"south_memory_stall",
+	"west_memory_stall",
+	"north_memory_stall",
+	"east_memory_stall",
+	"south_lock_stall",
+	"west_lock_stall",
+	"north_lock_stall",
+	"east_lock_stall",
+	"stream_stall_ss0",
+	"stream_stall_ss1",
+	"stream_stall_ms0",
+	"stream_stall_ms1",
+	"cascade_stall_scd",
+	"cascade_stall_mcd",
+	"debug_halt",
+	"ecc_error_stall",
+	"ecc_scrubbing_stall",
+	"error_halt",
+	"core_done",
+};
+
+static const struct aie_lock_attr aie_pl_lock = {
+	.sts = {
+		.mask = GENMASK(1, 0),
+		.regoff = 2U,
+	},
+	.sts_regoff = 0x14F00,
+	.num_locks = 16U,
+};
+
+static const struct aie_lock_attr aie_mem_lock = {
+	.sts = {
+		.mask = GENMASK(1, 0),
+		.regoff = 2U,
+	},
+	.sts_regoff = 0x1EF00,
+	.num_locks = 16U,
+};
+
+static char *aie_lock_status_str[] = {
+	"released_for_write",
+	"acquired_for_write",
+	"released_for_read",
+	"acquired_for_read",
+};
+
+static const struct aie_dev_attr aie_tile_dev_attr[] = {
+	AIE_TILE_DEV_ATTR_RO(core, AIE_TILE_TYPE_MASK_TILE),
+	AIE_TILE_DEV_ATTR_RO(dma, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_SHIMNOC),
+	AIE_TILE_DEV_ATTR_RO(error, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_SHIMNOC |
+			     AIE_TILE_TYPE_MASK_SHIMPL),
+	AIE_TILE_DEV_ATTR_RO(event, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_SHIMNOC |
+			     AIE_TILE_TYPE_MASK_SHIMPL),
+	AIE_TILE_DEV_ATTR_RO(lock, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_SHIMNOC),
+};
+
+static const struct aie_dev_attr aie_part_dev_attr[] = {
+	AIE_PART_DEV_ATTR_RO(error_stat),
+};
+
+static const struct aie_bin_attr aie_part_bin_attr[] = {
+	AIE_PART_BIN_ATTR_RO(core, AIE_PART_SYSFS_CORE_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(dma, AIE_PART_SYSFS_DMA_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(error, AIE_PART_SYSFS_ERROR_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(lock, AIE_PART_SYSFS_LOCK_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(status, AIE_PART_SYSFS_STATUS_BINA_SIZE),
+};
+
+static const struct aie_sysfs_attr aie_part_sysfs_attr = {
+	.dev_attr = aie_part_dev_attr,
+	.bin_attr = aie_part_bin_attr,
+	.num_dev_attrs = ARRAY_SIZE(aie_part_dev_attr),
+	.num_bin_attrs = ARRAY_SIZE(aie_part_bin_attr),
+};
+
+static const struct aie_sysfs_attr aie_tile_sysfs_attr = {
+	.dev_attr = aie_tile_dev_attr,
+	.bin_attr = NULL,
+	.num_dev_attrs = ARRAY_SIZE(aie_tile_dev_attr),
+	.num_bin_attrs = 0U,
+};
+
+static u32 aie_get_tile_type(struct aie_location *loc)
+{
+	if (loc->row)
+		return AIE_TILE_TYPE_TILE;
+	/* SHIM row */
+	if ((loc->col % 4) < 2)
+		return AIE_TILE_TYPE_SHIMPL;
+
+	return AIE_TILE_TYPE_SHIMNOC;
+}
+
+static unsigned int aie_get_mem_info(struct aie_range *range,
+				     struct aie_part_mem *pmem)
+{
+	unsigned int i;
+
+	if (range->start.row + range->size.row <= 1) {
+		/* SHIM row only, no memories in this range */
+		return 0;
+	}
+	if (!pmem)
+		return NUM_MEMS_PER_TILE;
+
+	for (i = 0; i < NUM_MEMS_PER_TILE; i++) {
+		struct aie_mem *mem = &pmem[i].mem;
+
+		memcpy(&mem->range, range, sizeof(*range));
+		if (!mem->range.start.row) {
+			mem->range.start.row = 1;
+			mem->range.size.row--;
+		}
+	}
+	/* Setup tile data memory information */
+	pmem[0].mem.offset = 0;
+	pmem[0].mem.size = KBYTES(32);
+	/* Setup program memory information */
+	pmem[1].mem.offset = 0x20000;
+	pmem[1].mem.size = KBYTES(16);
+
+	return NUM_MEMS_PER_TILE;
+}
+
+/**
+ * aie_set_shim_reset() - Set AI engine SHIM reset
+ * @adev: AI engine device
+ * @range: range of AI engine tiles
+ * @assert: true to set reset, false to unset reset
+ */
+static void aie_set_shim_reset(struct aie_device *adev,
+			       struct aie_range *range, bool assert)
+{
+	u32 c;
+	u32 val;
+	struct aie_location loc;
+
+	val = FIELD_PREP(AIE_SHIMPL_SHIMRST_MASK, (assert ? 1 : 0));
+	loc.row = 0;
+	for (c = range->start.col; c < range->start.col + range->size.col;
+	     c++) {
+		u32 regoff;
+
+		loc.col = c;
+		regoff = aie_cal_regoff(adev, loc, AIE_SHIMPL_RESET_REGOFF);
+		iowrite32(val, adev->base + regoff);
+	}
+}
+
+static int aie_reset_shim(struct aie_device *adev, struct aie_range *range)
+{
+	int ret;
+
+	/* Enable shim reset of each column */
+	aie_set_shim_reset(adev, range, true);
+
+	/* Assert shim reset of AI engine array */
+	ret = zynqmp_pm_reset_assert(VERSAL_PM_RST_AIE_SHIM_ID,
+				     PM_RESET_ACTION_ASSERT);
+	if (ret < 0) {
+		dev_err(&adev->dev, "failed to assert SHIM reset.\n");
+		return ret;
+	}
+
+	/* Release shim reset of AI engine array */
+	ret = zynqmp_pm_reset_assert(VERSAL_PM_RST_AIE_SHIM_ID,
+				     PM_RESET_ACTION_RELEASE);
+	if (ret < 0) {
+		dev_err(&adev->dev, "failed to release SHIM reset.\n");
+		return ret;
+	}
+
+	/* Disable shim reset of each column */
+	aie_set_shim_reset(adev, range, false);
+
+	return 0;
+}
+
+static int aie_init_part_clk_state(struct aie_partition *apart)
+{
+	int ret, num_tiles;
+
+	num_tiles = apart->range.size.col * (apart->range.size.row - 1);
+
+	ret = aie_resource_initialize(&apart->cores_clk_state, num_tiles);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize cores clock state resource.\n");
+		return ret;
+	}
+
+	ret = aie_resource_initialize(&apart->tiles_inuse, num_tiles);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize tiles in use resource.\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static int aie_scan_part_clocks(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_range *range = &apart->range;
+	struct aie_location loc;
+
+	/* Clear the bitmap of cores and memories clock state */
+	aie_resource_put_region(&apart->cores_clk_state, 0,
+				apart->cores_clk_state.total);
+
+	for (loc.col = range->start.col;
+	     loc.col < range->start.col + range->size.col;
+	     loc.col++) {
+		for (loc.row = range->start.row;
+		     loc.row < range->start.row + range->size.row - 1;
+		     loc.row++) {
+			void __iomem *va;
+			u32 val, nbitpos;
+
+			/*
+			 * Reading registers of the current tile to see the next
+			 * tile is clock gated.
+			 */
+			nbitpos = loc.col * (range->size.row - 1) + loc.row;
+
+			if (aie_get_tile_type(&loc) != AIE_TILE_TYPE_TILE) {
+				/* Checks shim tile for next core tile */
+				va = adev->base +
+				     aie_cal_regoff(adev, loc,
+						    AIE_SHIMPL_CLKCNTR_REGOFF);
+				val = ioread32(va);
+
+				/*
+				 * check if the clock buffer and the next clock
+				 * tile is set, if one of them is not set, the
+				 * tiles of the column are clock gated.
+				 */
+				if (!(val & AIE_SHIMPL_CLKCNTR_COLBUF_MASK) ||
+				    !(val & AIE_SHIMPL_CLKCNTR_NEXTCLK_MASK))
+					break;
+
+				/* Set next tile in the row clock state on */
+				aie_resource_set(&apart->cores_clk_state,
+						 nbitpos, 1);
+				continue;
+			}
+
+			/* Checks core tile for next tile */
+			va = adev->base +
+			     aie_cal_regoff(adev, loc,
+					    AIE_TILE_CORE_CLKCNTR_REGOFF);
+			val = ioread32(va);
+
+			/*
+			 * If the next tile is gated, skip the rest of the
+			 * column.
+			 */
+			if (!(val & AIE_TILE_CLKCNTR_NEXTCLK_MASK))
+				break;
+
+			aie_resource_set(&apart->cores_clk_state, nbitpos, 1);
+		}
+	}
+
+	/*
+	 * Set the tiles in use bitmap.
+	 * In case of scanning, tiles which are powered on are considered as
+	 * tiles in use.
+	 */
+	bitmap_copy(apart->tiles_inuse.bitmap, apart->cores_clk_state.bitmap,
+		    apart->tiles_inuse.total);
+
+	return 0;
+}
+
+/* aie_set_col_clocks() - set clocks of a range of tiles of a column
+ * @apart: AI engine partition
+ * @range: range of tiles of a column
+ * @enable: true to enable the clock, false to disable
+ * @return: 0 for success, negative value of errors.
+ */
+static int aie_set_col_clocks(struct aie_partition *apart,
+			      struct aie_range *range, bool enable)
+{
+	struct aie_location ploc;
+	u32 startbit;
+
+	/*
+	 * check if the range is of single colum. only single column is allowed.
+	 * check if the start row is tile row, only tile rows are allowed.
+	 */
+	if (range->size.col != 1 || range->start.row < 1)
+		return -EINVAL;
+
+	ploc.col = range->start.col;
+	for (ploc.row = range->start.row - 1;
+	     ploc.row < range->start.row + range->size.row - 1;
+	     ploc.row++) {
+		struct aie_device *adev = apart->adev;
+
+		if (!ploc.row) {
+			void __iomem *va;
+			u32 val = 0;
+
+			/*
+			 * Configure SHIM clock registers to gate or
+			 * ungate next tile.
+			 */
+			if (enable)
+				val = AIE_SHIMPL_CLKCNTR_COLBUF_MASK |
+				      AIE_SHIMPL_CLKCNTR_NEXTCLK_MASK;
+			va = adev->base +
+			     aie_cal_regoff(adev, ploc,
+					    AIE_SHIMPL_CLKCNTR_REGOFF);
+			iowrite32(val, va);
+		} else {
+			void __iomem *va;
+			u32 val = 0;
+
+			/*
+			 * Configure core tile clock registers to gate
+			 * or ungate next tile.
+			 */
+			if (enable)
+				val = AIE_TILE_CLKCNTR_COLBUF_MASK |
+				      AIE_TILE_CLKCNTR_NEXTCLK_MASK;
+			va = adev->base +
+			     aie_cal_regoff(adev, ploc,
+					    AIE_TILE_CORE_CLKCNTR_REGOFF);
+			iowrite32(val, va);
+		}
+
+		/* If the tile clock is not on, jump to next column */
+		if (!enable)
+			break;
+	}
+
+	/* Update clock state bitmap */
+	startbit = range->start.col * (apart->range.size.row - 1) +
+		   range->start.row - 1;
+	if (enable)
+		aie_resource_set(&apart->cores_clk_state, startbit,
+				 range->size.row);
+	else
+		aie_resource_clear(&apart->cores_clk_state, startbit,
+				   range->size.row);
+
+	return 0;
+}
+
+static int aie_set_part_clocks(struct aie_partition *apart)
+{
+	struct aie_range *range = &apart->range, lrange;
+	struct aie_location loc;
+
+	/*
+	 * The tiles below the highest tile whose clock is on, need to have the
+	 * clock on. The first for loop is to scan the clock states bitmap to
+	 * see which tiles are required to be clocked on, and update the bitmap
+	 * to make sure the tiles below are also required to be clocked on.
+	 */
+	for (loc.col = range->start.col;
+	     loc.col < range->start.col + range->size.col;
+	     loc.col++) {
+		u32 startbit, inuse_toprow = 0, clk_toprow = 0;
+
+		startbit = loc.col * (range->size.row - 1);
+
+		for (loc.row = range->start.row + 1;
+		     loc.row < range->start.row + range->size.row;
+		     loc.row++) {
+			u32 bit = startbit + loc.row - 1;
+
+			if (aie_resource_testbit(&apart->tiles_inuse, bit))
+				inuse_toprow = loc.row;
+			if (aie_resource_testbit(&apart->cores_clk_state, bit))
+				clk_toprow = loc.row;
+		}
+
+		/* Update clock states of a column */
+		lrange.start.col = loc.col;
+		lrange.size.col = 1;
+		if (inuse_toprow < clk_toprow) {
+			lrange.start.row = inuse_toprow + 1;
+			lrange.size.row = clk_toprow - inuse_toprow;
+			aie_set_col_clocks(apart, &lrange, false);
+		} else  if (inuse_toprow > clk_toprow) {
+			lrange.start.row = clk_toprow + 1;
+			lrange.size.row = inuse_toprow - clk_toprow;
+			aie_set_col_clocks(apart, &lrange, true);
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_get_core_status() - read the AI engine core status register.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_core_status(struct aie_partition *apart,
+			       struct aie_location *loc)
+{
+	u32 regoff, regvalue, eventval;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, aie_core_sts.regoff);
+	regvalue = ioread32(apart->adev->base + regoff);
+
+	/* Apply core done workaround */
+	if (!FIELD_GET(aie_core_done.mask, regvalue)) {
+		regoff = aie_cal_regoff(apart->adev, *loc,
+					aie_core_disable_event_sts.regoff);
+		eventval = ioread32(apart->adev->base + regoff);
+
+		if (FIELD_GET(aie_core_disable_event_sts.mask, eventval))
+			regvalue |= aie_core_done.mask;
+	}
+	return regvalue;
+}
+
+static const struct aie_tile_operations aie_ops = {
+	.get_tile_type = aie_get_tile_type,
+	.get_mem_info = aie_get_mem_info,
+	.get_core_status = aie_get_core_status,
+	.reset_shim = aie_reset_shim,
+	.init_part_clk_state = aie_init_part_clk_state,
+	.scan_part_clocks = aie_scan_part_clocks,
+	.set_part_clocks = aie_set_part_clocks,
+};
+
+/**
+ * aie_device_init_rscs_attr() - initialize AI engine device resources
+ *				 attributes
+ * @adev: AI engine device
+ */
+static void aie_device_init_rscs_attr(struct aie_device *adev)
+{
+	struct aie_tile_attr *tattr;
+
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_TILE];
+	tattr->start_row = 1;
+	/*
+	 * TODO: number of rows information of the AI engine device should get
+	 * from device tree.
+	 */
+	tattr->num_rows = 0xFF;
+	tattr->num_mods = 2;
+	tattr->rscs_attr = aie_core_tile_rscs_attr;
+	tattr->mods = aie_core_tile_module_types;
+
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_SHIMPL];
+	tattr->start_row = 0;
+	tattr->num_rows = 1;
+	tattr->num_mods = 1;
+	tattr->rscs_attr = aie_shimpl_tile_rscs_attr;
+	tattr->mods = aie_shimpl_tile_module_types;
+
+	/*
+	 * For now, SHIMNOC is the same as SHIMPL as there is
+	 * no SHIMNOC specific resources managed by kernel
+	 * driver yet.
+	 */
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_SHIMNOC];
+	tattr->start_row = 0;
+	tattr->num_rows = 1;
+	tattr->num_mods = 1;
+	tattr->rscs_attr = aie_shimpl_tile_rscs_attr;
+	tattr->mods = aie_shimpl_tile_module_types;
+}
+
+/**
+ * aie_device_init() - Initialize AI engine device struct AIE specific
+ * @adev: AI engine device
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function initialize the AI engine device structure device version
+ * specific elements such as register addressing related array shift,
+ * column shift, and row shift; AIE device specific device operations, device
+ * columns resource.
+ */
+int aie_device_init(struct aie_device *adev)
+{
+	int ret;
+
+	adev->array_shift = AIE_ARRAY_SHIFT;
+	adev->col_shift = AIE_COL_SHIFT;
+	adev->row_shift = AIE_ROW_SHIFT;
+	adev->ops = &aie_ops;
+	adev->num_kernel_regs = ARRAY_SIZE(aie_kernel_regs);
+	adev->kernel_regs = aie_kernel_regs;
+	adev->num_core_regs = ARRAY_SIZE(aie_core_regs);
+	adev->core_regs = aie_core_regs;
+	adev->col_rst = &aie_col_rst;
+	adev->col_clkbuf = &aie_col_clkbuf;
+	adev->shim_dma = &aie_shimdma;
+	adev->tile_dma = &aie_tiledma;
+	adev->pl_events = &aie_pl_event;
+	adev->mem_events = &aie_mem_event;
+	adev->core_events = &aie_core_event;
+	adev->l1_ctrl = &aie_l1_intr_ctrl;
+	adev->l2_ctrl = &aie_l2_intr_ctrl;
+	adev->core_errors = &aie_core_error;
+	adev->mem_errors = &aie_mem_error;
+	adev->shim_errors = &aie_shim_error;
+	adev->part_sysfs_attr = &aie_part_sysfs_attr;
+	adev->tile_sysfs_attr = &aie_tile_sysfs_attr;
+	adev->core_status_str = aie_core_status_str;
+	adev->core_pc = &aie_core_pc;
+	adev->core_lr = &aie_core_lr;
+	adev->core_sp = &aie_core_sp;
+	adev->dma_status_str = aie_dma_status_str;
+	adev->queue_status_str = aie_queue_status_str;
+	adev->pl_lock = &aie_pl_lock;
+	adev->mem_lock = &aie_mem_lock;
+	adev->lock_status_str = aie_lock_status_str;
+
+	aie_device_init_rscs_attr(adev);
+
+	/* Get the columns resource */
+	/* Get number of columns from AI engine memory resource */
+	ret = aie_resource_initialize(&adev->cols_res, 50);
+	if (ret)
+		dev_err(&adev->dev, "failed to initialize columns resource.\n");
+
+	return ret;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-clock.c b/drivers/misc/xilinx-ai-engine/ai-engine-clock.c
new file mode 100644
index 000000000..1888b62fd
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-clock.c
@@ -0,0 +1,316 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include "ai-engine-internal.h"
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+
+/**
+ * aie_part_get_clk_state_bit() - return bit position of the clock state of a
+ *				  tile
+ * @apart: AI engine partition
+ * @loc: AI engine tile location
+ * @return: bit position for success, negative value for failure
+ */
+static int aie_part_get_clk_state_bit(struct aie_partition *apart,
+				      struct aie_location *loc)
+{
+	if (apart->adev->ops->get_tile_type(loc) != AIE_TILE_TYPE_TILE)
+		return -EINVAL;
+
+	return loc->col * (apart->range.size.row - 1) + loc->row - 1;
+}
+
+/**
+ * aie_part_scan_clk_state() - scan the clock states of tiles of the AI engine
+ *			       partition
+ * @apart: AI engine partition
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will scan the clock status of both the memory and core
+ * modules.
+ */
+int aie_part_scan_clk_state(struct aie_partition *apart)
+{
+	return apart->adev->ops->scan_part_clocks(apart);
+}
+
+/**
+ * aie_part_check_clk_enable_loc() - return if clock of a tile is enabled
+ * @apart: AI engine partition
+ * @loc: AI engine tile location
+ * @return: true for enabled, false for disabled
+ */
+bool aie_part_check_clk_enable_loc(struct aie_partition *apart,
+				   struct aie_location *loc)
+{
+	int bit;
+
+	if (apart->adev->ops->get_tile_type(loc) != AIE_TILE_TYPE_TILE)
+		return true;
+
+	bit = aie_part_get_clk_state_bit(apart, loc);
+	return aie_resource_testbit(&apart->cores_clk_state, bit);
+}
+
+/**
+ * aie_part_request_tiles() - request tiles from an AI engine partition.
+ * @apart: AI engine partition
+ * @num_tiles: number of tiles to request. If it is 0, it means all tiles
+ * @locs: the AI engine tiles locations array which will be requested
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will enable clocks of the specified tiles.
+ */
+static int aie_part_request_tiles(struct aie_partition *apart, int num_tiles,
+				  struct aie_location *locs)
+{
+	if (num_tiles == 0) {
+		aie_resource_set(&apart->tiles_inuse, 0,
+				 apart->tiles_inuse.total);
+	} else {
+		u32 n;
+
+		if (!locs)
+			return -EINVAL;
+
+		for (n = 0; n < num_tiles; n++) {
+			int bit = aie_part_get_clk_state_bit(apart, &locs[n]);
+
+			if (bit >= 0)
+				aie_resource_set(&apart->tiles_inuse, bit, 1);
+		}
+	}
+
+	return apart->adev->ops->set_part_clocks(apart);
+}
+
+/**
+ * aie_part_release_tiles() - release tiles from an AI engine partition.
+ * @apart: AI engine partition
+ * @num_tiles: number of tiles to release. If it is 0, it means all tiles
+ * @locs: the AI engine tiles locations array which will be released
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will disable clocks of the specified tiles.
+ */
+static int aie_part_release_tiles(struct aie_partition *apart, int num_tiles,
+				  struct aie_location *locs)
+{
+	if (num_tiles == 0) {
+		aie_resource_clear(&apart->tiles_inuse, 0,
+				   apart->tiles_inuse.total);
+	} else {
+		u32 n;
+
+		if (!locs)
+			return -EINVAL;
+
+		for (n = 0; n < num_tiles; n++) {
+			int bit = aie_part_get_clk_state_bit(apart, &locs[n]);
+
+			if (bit >= 0)
+				aie_resource_clear(&apart->tiles_inuse, bit, 1);
+		}
+	}
+
+	return apart->adev->ops->set_part_clocks(apart);
+}
+
+/**
+ * aie_part_request_tiles_from_user() - request tiles from an AI engine
+ *					partition from user
+ * @apart: AI engine partition
+ * @user_args: user AI engine request tiles argument
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will request tiles from user request.
+ */
+int aie_part_request_tiles_from_user(struct aie_partition *apart,
+				     void __user *user_args)
+{
+	struct aie_tiles_array args;
+	struct aie_location *locs = NULL;
+	int ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	if (args.num_tiles) {
+		u32 i;
+
+		locs = kmalloc_array(args.num_tiles, sizeof(*locs),
+				     GFP_KERNEL);
+		if (!locs)
+			return -ENOMEM;
+
+		if (copy_from_user(locs, (void __user *)args.locs,
+				   args.num_tiles * sizeof(*locs))) {
+			kfree(locs);
+			return -EFAULT;
+		}
+
+		/* update the location to absolute location */
+		for (i = 0; i < args.num_tiles; i++) {
+			if (locs[i].col > apart->range.size.col ||
+			    locs[i].row > apart->range.size.row) {
+				dev_err(&apart->dev,
+					"failed to request tiles, invalid tile(%u,%u).\n",
+					locs[i].col, locs[i].row);
+				kfree(locs);
+				return -EINVAL;
+			}
+			locs[i].col += apart->range.start.col;
+			locs[i].row += apart->range.start.row;
+		}
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(locs);
+		return ret;
+	}
+
+	ret = aie_part_request_tiles(apart, args.num_tiles, locs);
+	mutex_unlock(&apart->mlock);
+
+	kfree(locs);
+	return ret;
+}
+
+/**
+ * aie_part_release_tiles_from_user() - release tiles from an AI engine
+ *					partition from user
+ * @apart: AI engine partition
+ * @user_args: user AI engine request tiles argument
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will release tiles from user request.
+ */
+int aie_part_release_tiles_from_user(struct aie_partition *apart,
+				     void __user *user_args)
+{
+	struct aie_tiles_array args;
+	struct aie_location *locs = NULL;
+	int ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	if (args.num_tiles) {
+		int i;
+
+		locs = kmalloc_array(args.num_tiles, sizeof(*locs),
+				     GFP_KERNEL);
+		if (!locs)
+			return -ENOMEM;
+
+		if (copy_from_user(locs, (void __user *)args.locs,
+				   args.num_tiles * sizeof(*locs))) {
+			kfree(locs);
+			return -EFAULT;
+		}
+
+		/* update the location to absolute location */
+		for (i = 0; i < args.num_tiles; i++) {
+			if (locs[i].col > apart->range.size.col ||
+			    locs[i].row > apart->range.size.row) {
+				dev_err(&apart->dev,
+					"failed to release tiles, invalid tile(%u,%u).\n",
+					locs[i].col, locs[i].row);
+				kfree(locs);
+				return -EINVAL;
+			}
+			locs[i].col += apart->range.start.col;
+			locs[i].row += apart->range.start.row;
+		}
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(locs);
+		return ret;
+	}
+
+	ret = aie_part_release_tiles(apart, args.num_tiles, locs);
+	mutex_unlock(&apart->mlock);
+
+	kfree(locs);
+	return ret;
+}
+
+/**
+ * aie_part_set_freq() - set frequency requirement of an AI engine partition
+ *
+ * @apart: AI engine partition
+ * @freq: required frequency
+ * @return: 0 for success, negative value for failure
+ *
+ * This function sets frequency requirement for the partition.
+ * It will call aie_dev_set_freq() to check the frequency requirements
+ * of all partitions. it will send QoS EEMI request to request the max
+ * frequency of all the partitions.
+ */
+int aie_part_set_freq(struct aie_partition *apart, u64 freq)
+{
+	struct aie_device *adev = apart->adev;
+	unsigned long clk_rate;
+	u32 qos;
+	int ret;
+
+	clk_rate = clk_get_rate(adev->clk);
+	if (freq > (u64)clk_rate) {
+		dev_err(&apart->dev,
+			"Invalid frequency to set, larger than full frequency(%lu).\n",
+			clk_rate);
+		return -EINVAL;
+	}
+
+	apart->freq_req = freq;
+	/* TODO: qos calculation is not defined by PLM yet */
+	qos = clk_rate / (unsigned long)freq;
+	ret = zynqmp_pm_set_requirement(apart->partition_id,
+					ZYNQMP_PM_CAPABILITY_ACCESS, qos,
+					ZYNQMP_PM_REQUEST_ACK_BLOCKING);
+	if (ret < 0)
+		dev_err(&apart->dev, "failed to set frequency requirement.\n");
+	return ret;
+}
+
+/**
+ * aie_part_get_running_freq() - get running frequency of AI engine device.
+ *
+ * @apart: AI engine partition
+ * @freq: return running frequency
+ * @return: 0 for success, negative value for failure
+ *
+ * This function gets clock divider value with EEMI requests, and it gets the
+ * full clock frequency from common clock framework. And then it divides the
+ * full clock frequency by the divider value and returns the result.
+ */
+int aie_part_get_running_freq(struct aie_partition *apart, u64 *freq)
+{
+	unsigned long clk_rate;
+	struct aie_device *adev = apart->adev;
+	u32 divider;
+	int ret;
+
+	if (!freq)
+		return -EINVAL;
+
+	clk_rate = clk_get_rate(adev->clk);
+	/* TODO: AIE clock id is not defined yet */
+	ret = zynqmp_pm_clock_getdivider(adev->clock_id, &divider);
+	if (ret < 0) {
+		dev_err(&apart->dev, "failed to get clock divider.\n");
+		return ret;
+	}
+
+	*freq = (u64)clk_rate / (u64)(divider & 0xFFFFFFFF);
+	return 0;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-dev.c b/drivers/misc/xilinx-ai-engine/ai-engine-dev.c
new file mode 100644
index 000000000..4e8a80eaf
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-dev.c
@@ -0,0 +1,753 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/anon_inodes.h>
+#include <linux/cdev.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/file.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/fs.h>
+#include <linux/idr.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/xlnx-ai-engine.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+#define AIE_DEV_MAX			(MINORMASK + 1)
+#define VERSAL_SILICON_REV_MASK		GENMASK(31, 28)
+
+static dev_t aie_major;
+struct class *aie_class;
+
+static DEFINE_IDA(aie_device_ida);
+static DEFINE_IDA(aie_minor_ida);
+
+/**
+ * aie_partition_fd() - returns a file descriptor for the given AI engine
+ *			partition device
+ * @apart: AI engine partition
+ * @return: file descriptor of the AI engine partition for success,
+ *	    negative value for failure.
+ *
+ * This function allocate a file descriptor for the AI engine partition
+ * export file.
+ */
+static int aie_partition_fd(struct aie_partition *apart)
+{
+	int ret;
+
+	ret = get_unused_fd_flags(O_CLOEXEC);
+	if (ret < 0) {
+		dev_err(&apart->dev,
+			"Failed to get fd for partition %u.\n",
+			apart->partition_id);
+		return ret;
+	}
+	fd_install(ret, apart->filep);
+
+	return ret;
+}
+
+/**
+ * aie_enquire_partitions() - get AI engine partitions information
+ * @adev: AI engine device
+ * @query: data struct to store the partition information
+ * @return: 0 for success, and negative value for failure.
+ */
+static int aie_enquire_partitions(struct aie_device *adev,
+				  struct aie_partition_query *query)
+{
+	struct aie_partition *apart;
+	u32 partition_cnt, i = 0;
+	int ret;
+
+	if (!query->partitions) {
+		/*
+		 * If partitions information buffer is NULL.
+		 * It is to get the number of partitions.
+		 */
+		query->partition_cnt = 0;
+		list_for_each_entry(apart, &adev->partitions, node)
+			query->partition_cnt++;
+		return 0;
+	}
+
+	partition_cnt = query->partition_cnt;
+	if (!partition_cnt)
+		return 0;
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret)
+		return ret;
+
+	list_for_each_entry(apart, &adev->partitions, node) {
+		struct aie_range_args part;
+
+		if (i >= partition_cnt)
+			break;
+		part.partition_id = apart->partition_id;
+		/*
+		 * TBD: check with PLM that if the partition is programmed
+		 * and get the UID of the image which is loaded on the AI
+		 * engine partition.
+		 */
+		part.uid = 0;
+		part.range.start.col = apart->range.start.col;
+		part.range.start.row = apart->range.start.row;
+		part.range.size.col = apart->range.size.col;
+		part.range.size.row = apart->range.size.row;
+		/* Check if partition is in use */
+		part.status = apart->status;
+		if (copy_to_user((void __user *)&query->partitions[i], &part,
+				 sizeof(part))) {
+			mutex_unlock(&adev->mlock);
+			return -EFAULT;
+		}
+		i++;
+	}
+	mutex_unlock(&adev->mlock);
+	query->partition_cnt = i;
+
+	return 0;
+}
+
+/**
+ * aie_get_partition_from_id() - get AI engine partition from id
+ * @adev: AI engine device
+ * @partition_id: partition id to check
+ * @return: partition pointer if partition exists, otherwise, NULL.
+ *
+ * This function checks defined partitions with partition id.
+ * This function expect the caller to lock mlock of @adev.
+ */
+struct aie_partition *aie_get_partition_from_id(struct aie_device *adev,
+						u32 partition_id)
+{
+	struct aie_partition *apart;
+
+	list_for_each_entry(apart, &adev->partitions, node) {
+		if (apart->partition_id == partition_id)
+			return apart;
+	}
+
+	return NULL;
+}
+
+/**
+ * aie_partition_get() - Request the specified AI engine partition
+ *
+ * @apart: AI engine partition
+ * @req: AI engine partition request information which includes image UID.
+ *	 flag to indicate if the partition will cleanup or not when releasing
+ *	 the partition.
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function will check if the specified partition can be requested, it
+ * will check if the partition has been loaded with an image, if no, as long
+ * as it is not in use, the partition request will be granted. If there is
+ * image loaded, it will check if the given UID from @req matches the image UID
+ * loaded on the partition, if they match, the partition request will be
+ * granted. A file will be created for the requested partition.
+ */
+static int aie_partition_get(struct aie_partition *apart,
+			     struct aie_partition_req *req)
+{
+	struct file *filep;
+	int ret;
+
+	(void)req;
+
+	if (apart->status & XAIE_PART_STATUS_INUSE) {
+		dev_err(&apart->dev,
+			"request partition %u failed, partition in use.\n",
+			apart->partition_id);
+		return -EBUSY;
+	}
+	/*
+	 * TODO:
+	 * 1. It will check image UID too to see if the user matches what's
+	 *    loaded in the AI engine partition. And check the meta data to see
+	 *    which resources used by application.
+	 */
+
+	/* Get a file for the partition */
+	filep = anon_inode_getfile(dev_name(&apart->dev), &aie_part_fops,
+				   apart, O_RDWR);
+	if (IS_ERR(filep)) {
+		dev_err(&apart->dev,
+			"Failed to request partition %u, failed to get file.\n",
+			apart->partition_id);
+		return PTR_ERR(filep);
+	}
+
+	filep->f_mode |= (FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE);
+	apart->filep = filep;
+
+	apart->status = XAIE_PART_STATUS_INUSE;
+	apart->cntrflag = req->flag;
+
+	/* open AI engine partition instance to get it ready for use */
+	ret = aie_part_open(apart, (void *)req->meta_data);
+	if (ret) {
+		dev_err(&apart->dev, "Failed to open partition %u instance.\n",
+			apart->partition_id);
+		fput(filep);
+		return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_partition_request_from_adev() - request AI engine partition from AI
+ *				       engine device
+ * @adev: AI engine device
+ * @req: partition request, includes the requested AI engine information
+ *	 such as partition node ID and the UID of the image which is used
+ *	 to describe the partition to request.
+ * @return: pointer to the AI engine partition for success, and negative
+ *	    value for failure.
+ *
+ * This function finds a defined partition which matches the specified
+ * partition id, and request it.
+ */
+static struct aie_partition *
+aie_partition_request_from_adev(struct aie_device *adev,
+				struct aie_partition_req *req)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret)
+		return ERR_PTR(ret);
+
+	apart = aie_get_partition_from_id(adev, req->partition_id);
+	if (!apart) {
+		dev_err(&adev->dev,
+			"request partition %u failed, not exist.\n",
+			req->partition_id);
+		mutex_unlock(&adev->mlock);
+		return ERR_PTR(-EINVAL);
+	}
+	mutex_unlock(&adev->mlock);
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ERR_PTR(ret);
+
+	ret = aie_partition_get(apart, req);
+
+	mutex_unlock(&apart->mlock);
+
+	if (ret)
+		apart = ERR_PTR(ret);
+	return apart;
+}
+
+static long xilinx_ai_engine_ioctl(struct file *filp, unsigned int cmd,
+				   unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct aie_device *adev = cdev_to_aiedev(inode->i_cdev);
+	void __user *argp = (void __user *)arg;
+	int ret;
+
+	switch (cmd) {
+	case AIE_ENQUIRE_PART_IOCTL:
+	{
+		struct aie_partition_query query;
+		struct aie_partition_query  __user *uquery_ptr = argp;
+
+		if (copy_from_user(&query, uquery_ptr, sizeof(query)))
+			return -EFAULT;
+		ret = aie_enquire_partitions(adev, &query);
+		if (ret < 0)
+			return ret;
+		if (copy_to_user((void __user *)&uquery_ptr->partition_cnt,
+				 &query.partition_cnt,
+				 sizeof(query.partition_cnt)))
+			return -EFAULT;
+		break;
+	}
+	case AIE_REQUEST_PART_IOCTL:
+	{
+		struct aie_partition_req req;
+		struct aie_partition *apart;
+
+		if (copy_from_user(&req, argp, sizeof(req)))
+			return -EFAULT;
+
+		apart = aie_partition_request_from_adev(adev, &req);
+		if (IS_ERR(apart))
+			return PTR_ERR(apart);
+
+		/* Allocate fd */
+		ret = aie_partition_fd(apart);
+		if (ret < 0) {
+			fput(apart->filep);
+			break;
+		}
+		break;
+	}
+	default:
+		dev_err(&adev->dev, "Invalid ioctl command %u.\n", cmd);
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+static const struct file_operations aie_device_fops = {
+	.owner		= THIS_MODULE,
+	.unlocked_ioctl	= xilinx_ai_engine_ioctl,
+};
+
+static void xilinx_ai_engine_release_device(struct device *dev)
+{
+	struct aie_device *adev = dev_to_aiedev(dev);
+
+	ida_simple_remove(&aie_device_ida, dev->id);
+	ida_simple_remove(&aie_minor_ida, MINOR(dev->devt));
+	cdev_del(&adev->cdev);
+	aie_resource_uninitialize(&adev->cols_res);
+}
+
+/**
+ * of_xilinx_ai_engine_part_probe() - probes for AI engine partition nodes
+ * @adev: AI engine device
+ *
+ * This function will probe for children AI engine partition nodes and create
+ * an AI engine partition instance for each node.
+ */
+static void of_xilinx_ai_engine_part_probe(struct aie_device *adev)
+{
+	struct device_node *nc;
+
+	for_each_available_child_of_node(adev->dev.of_node, nc) {
+		struct aie_partition *apart;
+
+		if (of_node_test_and_set_flag(nc, OF_POPULATED))
+			continue;
+		apart = of_aie_part_probe(adev, nc);
+		if (IS_ERR(apart)) {
+			dev_err(&adev->dev,
+				"Failed to probe AI engine part for %pOF\n",
+				nc);
+			of_node_clear_flag(nc, OF_POPULATED);
+		}
+	}
+}
+
+static int xilinx_ai_engine_probe(struct platform_device *pdev)
+{
+	struct aie_device *adev;
+	struct device *dev;
+	u32 idcode, version, pm_reg[2];
+	int ret;
+
+	adev = devm_kzalloc(&pdev->dev, sizeof(*adev), GFP_KERNEL);
+	if (!adev)
+		return -ENOMEM;
+	platform_set_drvdata(pdev, adev);
+	INIT_LIST_HEAD(&adev->partitions);
+	mutex_init(&adev->mlock);
+
+	adev->res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!adev->res) {
+		dev_err(&pdev->dev, "No memory resource.\n");
+		return -EINVAL;
+	}
+	adev->base = devm_ioremap_resource(&pdev->dev, adev->res);
+	if (IS_ERR(adev->base)) {
+		dev_err(&pdev->dev, "no io memory resource.\n");
+		return PTR_ERR(adev->base);
+	}
+
+	/* Initialize AIE device specific instance. */
+	ret = aie_device_init(adev);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "failed to initialize device instance.\n");
+		return ret;
+	}
+
+	/*
+	 * AI Engine platform management node ID is required for requesting
+	 * services from firmware driver.
+	 */
+	ret = of_property_read_u32_array(pdev->dev.of_node, "power-domains",
+					 pm_reg, ARRAY_SIZE(pm_reg));
+	if (ret < 0) {
+		dev_err(&pdev->dev,
+			"Failed to read power manangement information\n");
+		return ret;
+	}
+	adev->pm_node_id = pm_reg[1];
+
+	ret = zynqmp_pm_get_chipid(&idcode, &version);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "Failed to get chip ID\n");
+		return ret;
+	}
+	adev->version = FIELD_GET(VERSAL_SILICON_REV_MASK, idcode);
+
+	dev = &adev->dev;
+	device_initialize(dev);
+	dev->class = aie_class;
+	dev->parent = &pdev->dev;
+	dev->of_node = pdev->dev.of_node;
+
+	ret = ida_simple_get(&aie_minor_ida, 0, AIE_DEV_MAX, GFP_KERNEL);
+	if (ret < 0)
+		goto free_dev;
+	dev->devt = MKDEV(MAJOR(aie_major), ret);
+	ret = ida_simple_get(&aie_device_ida, 0, 0, GFP_KERNEL);
+	if (ret < 0)
+		goto free_minor_ida;
+	dev->id = ret;
+	dev_set_name(&adev->dev, "aie%d", dev->id);
+
+	cdev_init(&adev->cdev, &aie_device_fops);
+	adev->cdev.owner = THIS_MODULE;
+	ret = cdev_add(&adev->cdev, dev->devt, 1);
+	if (ret)
+		goto free_ida;
+	/* We can now rely on the release function for cleanup */
+	dev->release = xilinx_ai_engine_release_device;
+
+	ret = device_add(dev);
+	if (ret) {
+		dev_err(&pdev->dev, "device_add failed: %d\n", ret);
+		put_device(dev);
+		return ret;
+	}
+
+	of_xilinx_ai_engine_part_probe(adev);
+	dev_info(&pdev->dev, "Xilinx AI Engine device(cols=%u) probed\n",
+		 adev->cols_res.total);
+
+	INIT_WORK(&adev->backtrack, aie_array_backtrack);
+
+	adev->irq = platform_get_irq_byname(pdev, "interrupt1");
+	if (adev->irq < 0)
+		goto free_ida;
+
+	ret = devm_request_threaded_irq(dev, adev->irq, NULL, aie_interrupt,
+					IRQF_ONESHOT, dev_name(dev), adev);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to request AIE IRQ.\n");
+		goto free_ida;
+	}
+
+	adev->clk = devm_clk_get(&pdev->dev, NULL);
+	if (!adev->clk) {
+		dev_err(&pdev->dev, "Failed to get device clock.\n");
+		goto free_ida;
+	}
+
+	return 0;
+
+free_ida:
+	ida_simple_remove(&aie_device_ida, dev->id);
+free_minor_ida:
+	ida_simple_remove(&aie_minor_ida, MINOR(dev->devt));
+free_dev:
+	put_device(dev);
+
+	return ret;
+}
+
+static int xilinx_ai_engine_remove(struct platform_device *pdev)
+{
+	struct aie_device *adev = platform_get_drvdata(pdev);
+	struct list_head *node, *pos;
+
+	list_for_each_safe(pos, node, &adev->partitions) {
+		struct aie_partition *apart;
+
+		apart = list_entry(pos, struct aie_partition, node);
+		aie_part_remove(apart);
+	}
+
+	device_del(&adev->dev);
+	put_device(&adev->dev);
+
+	return 0;
+}
+
+static const struct of_device_id xilinx_ai_engine_of_match[] = {
+	{ .compatible = "xlnx,ai-engine-v1.0", },
+	{ /* end of table */ },
+};
+MODULE_DEVICE_TABLE(of, xilinx_ai_engine_of_match);
+
+static struct platform_driver xilinx_ai_engine_driver = {
+	.probe			= xilinx_ai_engine_probe,
+	.remove			= xilinx_ai_engine_remove,
+	.driver			= {
+		.name		= "xilinx-ai-engine",
+		.of_match_table	= xilinx_ai_engine_of_match,
+	},
+};
+
+/**
+ * aie_partition_dev_match() - check if AI engine partition matches partition ID
+ *
+ * @dev: pointer to the AI engine partition device
+ * @data: partition_id
+ * @return: 1 if partition matches, otherwise 0.
+ */
+static int aie_partition_dev_match(struct device *dev, const void *data)
+{
+	struct aie_partition *apart;
+	u32 partition_id = (u32)(uintptr_t)data;
+
+	if (strncmp(dev_name(dev), "aiepart", strlen("aiepart")))
+		return 0;
+
+	apart = dev_to_aiepart(dev);
+	if (apart->partition_id == partition_id)
+		return 1;
+	return 0;
+}
+
+/**
+ * aie_class_find_partition_from_id() - Find the AI engine partition whose ID
+ *					matches.
+ * @partition_id: AI engine partition ID
+ * @return: pointer to the AI engine partition if partition is found, otherwise
+ *	    NULL.
+ *
+ * This function looks up all the devices of the AI engine class to check if
+ * the device is AI engine partition device if the partition ID matches.
+ */
+static struct aie_partition *aie_class_find_partition_from_id(u32 partition_id)
+{
+	struct device *dev;
+
+	dev = class_find_device(aie_class, NULL,
+				(void *)(uintptr_t)partition_id,
+				aie_partition_dev_match);
+	if (!dev)
+		return NULL;
+	return dev_to_aiepart(dev);
+}
+
+/**
+ * aie_partition_is_available() - Check if an AI engine partition is available
+ * @req: AI engine partition requesting arguments
+ * @return: true if the AI engine partition is not in use, otherwise, false
+ *
+ * This function looks up the AI engine class devices to find the AI engine
+ * partition whose partition ID matches the given partition ID in @req. If
+ * the partition can be found, if will check if the partition is in use.
+ *
+ * In case the AI engine release function is called from kernel context, the
+ * release() will be scheduled when the AI engine partition reference count is
+ * reduced to 0 instead of get called synchronously, and thus, this is a helper
+ * function for another kernel module to check if the partitions is released
+ * after calling release function from kernel context
+ *
+ * However, if closing the partition is from user context, it will not return
+ * until the release is complete when there is no reference to the AI engine
+ * partition file. In this case, user doesn't need to call this function to
+ * check if the partition is released.
+ */
+bool aie_partition_is_available(struct aie_partition_req *req)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	if (!req)
+		return false;
+
+	apart = aie_class_find_partition_from_id(req->partition_id);
+	if (!apart)
+		return false;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return false;
+
+	if (apart->status & XAIE_PART_STATUS_INUSE) {
+		mutex_unlock(&apart->mlock);
+		return false;
+	}
+
+	mutex_unlock(&apart->mlock);
+	return true;
+}
+EXPORT_SYMBOL_GPL(aie_partition_is_available);
+
+/**
+ * aie_partition_request() - Request an AI engine partition
+ * @req: AI engine partition requesting arguments
+ * @return: pointer to the AI engine partition device, error value for failure.
+ *
+ * This function looks up the AI engine class devices to find the AI engine
+ * partition whose partition ID matches the given partition ID in @req. If
+ * the partition can be found, it will try to request it. It will get a file
+ * for the requested AI engine partition. User can only use the AI engine
+ * partition after it is successfully requested.
+ */
+struct device *aie_partition_request(struct aie_partition_req *req)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	if (!req)
+		return ERR_PTR(-EINVAL);
+
+	apart = aie_class_find_partition_from_id(req->partition_id);
+	if (!apart)
+		return ERR_PTR(-ENODEV);
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ERR_PTR(ret);
+
+	ret = aie_partition_get(apart, req);
+
+	mutex_unlock(&apart->mlock);
+	if (ret)
+		return ERR_PTR(ret);
+
+	if (apart->error_to_report)
+		schedule_work(&apart->adev->backtrack);
+
+	return &apart->dev;
+}
+EXPORT_SYMBOL_GPL(aie_partition_request);
+
+/**
+ * aie_partition_get_fd() - get AI engine partition file descriptor
+ * @dev: AI engine partition device pointer
+ * @return: file descriptor for the AI engine partition for success, and
+ *	    negative value for failure.
+ *
+ * This function allocate a file descriptor for the AI engine requested
+ * partition, and increase the reference count to the AI engine partition file.
+ */
+int aie_partition_get_fd(struct device *dev)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	if (!dev)
+		return -EINVAL;
+
+	apart = dev_to_aiepart(dev);
+
+	ret = aie_partition_fd(apart);
+	if (ret < 0)
+		return ret;
+
+	get_file(apart->filep);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aie_partition_get_fd);
+
+/**
+ * aie_partition_release() - Recrease refcount of the AI engine partition
+ * @dev: AI engine partition device
+ */
+void aie_partition_release(struct device *dev)
+{
+	struct aie_partition *apart;
+
+	if (WARN_ON(!dev))
+		return;
+
+	apart = dev_to_aiepart(dev);
+	fput(apart->filep);
+}
+EXPORT_SYMBOL_GPL(aie_partition_release);
+
+/**
+ * aie_partition_reset() - Reset AI engine partition
+ * @dev: AI engine partition device
+ * @return: 0 for success, negative value for failure
+ */
+int aie_partition_reset(struct device *dev)
+{
+	struct aie_partition *apart;
+
+	if (WARN_ON(!dev))
+		return -EINVAL;
+
+	apart = dev_to_aiepart(dev);
+	return aie_part_reset(apart);
+}
+EXPORT_SYMBOL_GPL(aie_partition_reset);
+
+/**
+ * aie_partition_post_reinit() - Indicate AI engine partition driver the
+ *				 partition has been re-initialized.
+ * @dev: AI engine partition device
+ * @return: 0 for success, negative value for failure
+ *
+ * This function is called after the AI engine partition is reconfigured with
+ * PDI outside the AI engine driver.
+ */
+int aie_partition_post_reinit(struct device *dev)
+{
+	struct aie_partition *apart;
+
+	if (WARN_ON(!dev))
+		return -EINVAL;
+
+	apart = dev_to_aiepart(dev);
+	return aie_part_post_reinit(apart);
+}
+EXPORT_SYMBOL_GPL(aie_partition_post_reinit);
+
+static int __init xilinx_ai_engine_init(void)
+{
+	int ret;
+
+	ret = alloc_chrdev_region(&aie_major, 0, AIE_DEV_MAX, "aie");
+	if (ret < 0) {
+		pr_err("aie: failed to allocate aie region\n");
+		return ret;
+	}
+
+	aie_class = class_create(THIS_MODULE, "aie");
+	if (IS_ERR(aie_class)) {
+		pr_err("failed to create aie class\n");
+		unregister_chrdev_region(aie_major, AIE_DEV_MAX);
+		return PTR_ERR(aie_class);
+	}
+
+	platform_driver_register(&xilinx_ai_engine_driver);
+
+	return 0;
+}
+postcore_initcall(xilinx_ai_engine_init);
+
+static void __exit xilinx_ai_engine_exit(void)
+{
+	platform_driver_unregister(&xilinx_ai_engine_driver);
+	class_destroy(aie_class);
+	unregister_chrdev_region(aie_major, AIE_DEV_MAX);
+}
+module_exit(xilinx_ai_engine_exit);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-dma.c b/drivers/misc/xilinx-ai-engine/ai-engine-dma.c
new file mode 100644
index 000000000..9d5e30b21
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-dma.c
@@ -0,0 +1,667 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine driver DMA implementation
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include "ai-engine-internal.h"
+#include <linux/dma-buf.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/refcount.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/uaccess.h>
+
+/**
+ * struct aie_dmabuf - AI engine dmabuf information
+ * @attach: dmabuf attachment pointer
+ * @sgt: scatter/gather table
+ * @refs: refcount of the attached aie_dmabuf
+ * @node: list node
+ */
+struct aie_dmabuf {
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	refcount_t refs;
+	struct list_head node;
+};
+
+/**
+ * aie_part_find_dmabuf() - find a attached dmabuf
+ * @apart: AI engine partition
+ * @dmabuf: pointer to dmabuf
+ * @return: pointer to AI engine dmabuf struct of the found dmabuf, if dmabuf
+ *	    is not found, returns NULL.
+ *
+ * This function scans all the attached dmabufs to see the input dmabuf is
+ * in the list. if it is attached, return the corresponding struct aie_dmabuf
+ * pointer.
+ */
+static struct aie_dmabuf *
+aie_part_find_dmabuf(struct aie_partition *apart, struct dma_buf *dmabuf)
+{
+	struct aie_dmabuf *adbuf;
+
+	list_for_each_entry(adbuf, &apart->dbufs, node) {
+		if (dmabuf == adbuf->attach->dmabuf)
+			return adbuf;
+	}
+
+	return NULL;
+}
+
+/**
+ * aie_part_find_dmabuf_from_file() - find a attached dmabuf from file
+ * @apart: AI engine partition
+ * @file: file which belongs to a dmabuf
+ * @return: pointer to AI engine dmabuf struct of the found dmabuf, if dmabuf
+ *	    is not found, returns NULL.
+ *
+ * This function scans all the attached dmabufs of the AI engine partition,
+ * it checks the file with the attached dmabufs, if it founds a match, it
+ * returns the aie_dmabuf pointer.
+ */
+static struct aie_dmabuf *
+aie_part_find_dmabuf_from_file(struct aie_partition *apart,
+			       const struct file *file)
+{
+	struct aie_dmabuf *adbuf;
+
+	list_for_each_entry(adbuf, &apart->dbufs, node) {
+		if (file == adbuf->attach->dmabuf->file)
+			return adbuf;
+	}
+
+	return NULL;
+}
+
+/**
+ * aie_part_get_dmabuf_da() -  get DMA address from the va
+ * @apart: AI engine partition
+ * @va: virtual address
+ * @len: memory length
+ * @return: dma address of the specified va, or 0 if va is not valid
+ *
+ * This function returns DMA address if the has been mapped to a dmabuf which
+ * has been attached to the AI engine partition.
+ */
+static dma_addr_t aie_part_get_dmabuf_da(struct aie_partition *apart,
+					 void *va, size_t len)
+{
+	struct vm_area_struct *vma;
+	struct aie_dmabuf *adbuf;
+	unsigned long va_start, va_off;
+
+	va_start = (unsigned long)((uintptr_t)va);
+	if (!current->mm) {
+		dev_err(&apart->dev,
+			"failed to get dma address from va, no process mm.\n");
+		return 0;
+	}
+
+	vma = find_vma(current->mm, va_start);
+	if (!vma) {
+		dev_err(&apart->dev, "failed to find vma for %p, 0x%zx.\n",
+			va, len);
+		return 0;
+	}
+
+	adbuf = aie_part_find_dmabuf_from_file(apart, vma->vm_file);
+	if (!adbuf) {
+		dev_err(&apart->dev,
+			"failed to get dma address for %p, no dma buf is found.\n",
+			va);
+		return 0;
+	}
+
+	va_off = va_start - vma->vm_start;
+	/*
+	 * As we only support continuous DMA memory which is guaranteed from
+	 * dmabuf attachment, we will compared with the size of the dmabuf only
+	 */
+	if (va_off + len >= adbuf->attach->dmabuf->size) {
+		dev_err(&apart->dev,
+			"failed to get dma address for %p, 0x%zx.\n", va, len);
+		return 0;
+	}
+
+	return sg_dma_address(adbuf->sgt->sgl) + va_off;
+}
+
+/**
+ * aie_part_get_dmabuf_da_from_off() - get DMA address from offset to a dmabuf
+ * @apart: AI engine partition
+ * @dmabuf_fd: dmabuf file descriptor
+ * @off: offset to the start of a dmabuf
+ * @len: memory length
+ * @return: dma address, or 0 if @off or @len is invalid, or if @dmabuf_fd is
+ *	    not attached.
+ *
+ * This function returns DMA address if has been mapped to a dmabuf which has
+ * been attached to the AI engine partition.
+ */
+static dma_addr_t
+aie_part_get_dmabuf_da_from_off(struct aie_partition *apart, int dmabuf_fd,
+				u64 off, size_t len)
+{
+	struct dma_buf *dbuf = dma_buf_get(dmabuf_fd);
+	struct aie_dmabuf *adbuf;
+
+	if (IS_ERR(dbuf)) {
+		dev_err(&apart->dev,
+			"failed to get dma address, not able to get dmabuf from %d.\n",
+			dmabuf_fd);
+		return 0;
+	}
+
+	adbuf = aie_part_find_dmabuf(apart, dbuf);
+	dma_buf_put(dbuf);
+	if (!adbuf) {
+		dev_err(&apart->dev,
+			"failed to get dma address, dmabuf %d not attached.\n",
+			dmabuf_fd);
+		return 0;
+	}
+
+	if (off >= dbuf->size || off + len >= dbuf->size) {
+		dev_err(&apart->dev,
+			"failed to get dma address from buf %d, off=0x%llx, len=0x%zx.\n",
+			dmabuf_fd, off, len);
+		return 0;
+	}
+
+	return sg_dma_address(adbuf->sgt->sgl) + off;
+}
+
+/**
+ * aie_part_set_shimdma_bd() - Set the buffer descriptor to AI engine partition
+ *			       hardware
+ * @apart: AI engine partition
+ * @loc: AI engine tile location
+ * @bd_id: buffer descriptor ID
+ * @bd: pointer buffer descriptor content
+ * @return: 0 for success, negative value for failure
+ *
+ * This function sets the specified buffer descriptor content to the
+ * specified buffer descriptor in the specified AI engine SHIM NOC tile.
+ */
+static int aie_part_set_shimdma_bd(struct aie_partition *apart,
+				   struct aie_location loc, u32 bd_id, u32 *bd)
+{
+	const struct aie_dma_attr *shim_dma = apart->adev->shim_dma;
+	struct aie_location loc_adjust;
+	u32 i, regoff, intile_regoff;
+
+	intile_regoff = shim_dma->bd_regoff + shim_dma->bd_len * bd_id;
+	loc_adjust.col = loc.col + apart->range.start.col;
+	loc_adjust.row = loc.row + apart->range.start.row;
+	regoff = aie_cal_regoff(apart->adev, loc_adjust, intile_regoff);
+
+	for (i = 0; i < shim_dma->bd_len / (sizeof(*bd));
+	     i++, regoff += sizeof(*bd))
+		iowrite32(bd[i], apart->adev->base + regoff);
+	return 0;
+}
+
+/**
+ * aie_part_validate_bdloc() - Validate SHIM DMA buffer descriptor location
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @bd_id: buffer descriptor id
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function validate the SHIM DMA buffer descriptor base address.
+ */
+static int aie_part_validate_bdloc(struct aie_partition *apart,
+				   struct aie_location loc, u32 bd_id)
+{
+	const struct aie_dma_attr *shim_dma = apart->adev->shim_dma;
+	struct aie_location loc_adjust;
+	u32 ttype;
+
+	loc_adjust.col = loc.col + apart->range.start.col;
+	loc_adjust.row = loc.row + apart->range.start.row;
+
+	if (aie_validate_location(apart, loc_adjust) < 0) {
+		dev_err(&apart->dev,
+			"invalid loc (%u,%u) in (%u,%u).\n",
+			loc.col, loc.row,
+			apart->range.size.col, apart->range.size.row);
+		return -EINVAL;
+	}
+
+	ttype = apart->adev->ops->get_tile_type(&loc_adjust);
+	if (ttype != AIE_TILE_TYPE_SHIMNOC) {
+		dev_err(&apart->dev,
+			"failed to set bd, (%u,%u) is not SHIM NOC\n",
+			loc.col, loc.row);
+		return -EINVAL;
+	}
+
+	if (bd_id >= shim_dma->num_bds) {
+		dev_err(&apart->dev,
+			"invalid SHIM DMA bd id: %u.\n", bd_id);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_attach_dmabuf() - Attach dmabuf to an AI engine
+ * @apart: AI engine partition
+ * @dbuf: pointer to the DMA buffer to attach
+ * @return: pointer to AI engine dmabuf structure for success, or error value
+ *	    for failure
+ *
+ * This function attaches a dmabuf to the specified AI engine partition.
+ */
+static struct aie_dmabuf *aie_part_attach_dmabuf(struct aie_partition *apart,
+						 struct dma_buf *dbuf)
+{
+	struct aie_dmabuf *adbuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+
+	attach = dma_buf_attach(dbuf, &apart->dev);
+	if (IS_ERR(attach)) {
+		dev_err(&apart->dev, "failed to attach dmabuf\n");
+		return ERR_CAST(attach);
+	}
+
+	sgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);
+	if (IS_ERR(sgt)) {
+		dev_err(&apart->dev, "failed to map dmabuf attachment\n");
+		dma_buf_detach(dbuf, attach);
+		return ERR_CAST(sgt);
+	}
+
+	if (sgt->nents != 1) {
+		dma_addr_t next_sg_addr = sg_dma_address(sgt->sgl);
+		struct scatterlist *s;
+		unsigned int i;
+
+		for_each_sg(sgt->sgl, s, sgt->nents, i) {
+			if (sg_dma_address(s) != next_sg_addr) {
+				dev_err(&apart->dev,
+					"dmabuf not contiguous\n");
+				dma_buf_unmap_attachment(attach, sgt,
+							 attach->dir);
+				dma_buf_detach(dbuf, attach);
+				return ERR_PTR(-EINVAL);
+			}
+
+			next_sg_addr = sg_dma_address(s) + sg_dma_len(s);
+		}
+	}
+
+	adbuf = kmem_cache_alloc(apart->dbufs_cache, GFP_KERNEL);
+	if (!adbuf) {
+		dma_buf_unmap_attachment(attach, sgt, attach->dir);
+		dma_buf_detach(dbuf, attach);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	adbuf->attach = attach;
+	/*
+	 * dmabuf attachment doesn't always include the sgt, store it in
+	 * AI engine dma buf structure.
+	 */
+	adbuf->sgt = sgt;
+
+	refcount_set(&adbuf->refs, 1);
+
+	list_add(&adbuf->node, &apart->dbufs);
+	return adbuf;
+}
+
+/**
+ * aie_part_dmabuf_attach_get() - Get reference to an dmabuf attachment
+ * @adbuf: AI engine partition attached dmabuf
+ *
+ * This call will increase the reference count by 1
+ */
+static void aie_part_dmabuf_attach_get(struct aie_dmabuf *adbuf)
+{
+	refcount_inc(&adbuf->refs);
+}
+
+/**
+ * aie_part_dmabuf_attach_put() - Put reference to an dmabuf attachment
+ * @adbuf: AI engine partition attached dmabuf
+ *
+ * This call will decrease the reference count by 1. If the refcount reaches
+ * 0, it will detach the dmabuf.
+ */
+static void aie_part_dmabuf_attach_put(struct aie_dmabuf *adbuf)
+{
+	struct dma_buf *dbuf;
+	struct aie_partition *apart;
+
+	if (!refcount_dec_and_test(&adbuf->refs))
+		return;
+
+	apart = dev_to_aiepart(adbuf->attach->dev);
+	dbuf = adbuf->attach->dmabuf;
+	dma_buf_unmap_attachment(adbuf->attach, adbuf->sgt, adbuf->attach->dir);
+	dma_buf_detach(dbuf, adbuf->attach);
+	dma_buf_put(dbuf);
+	list_del(&adbuf->node);
+	kmem_cache_free(apart->dbufs_cache, adbuf);
+}
+
+/**
+ * aie_part_release_dmabufs() - detach all the attached dmabufs from partition
+ * @apart: AI engine partition
+ */
+void aie_part_release_dmabufs(struct aie_partition *apart)
+{
+	struct aie_dmabuf *adbuf, *tmpadbuf;
+
+	list_for_each_entry_safe(adbuf, tmpadbuf, &apart->dbufs, node) {
+		struct dma_buf *dbuf = adbuf->attach->dmabuf;
+
+		dma_buf_unmap_attachment(adbuf->attach, adbuf->sgt,
+					 adbuf->attach->dir);
+		dma_buf_detach(dbuf, adbuf->attach);
+		dma_buf_put(dbuf);
+		list_del(&adbuf->node);
+		kmem_cache_free(apart->dbufs_cache, adbuf);
+	}
+}
+
+/**
+ * aie_part_attach_dmabuf_req() - Handle attaching dmabuf to an AI engine
+ *				  partition request
+ * @apart: AI engine partition
+ * @user_args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function attaches a dmabuf to the specified AI engine partition and map
+ * the attachment. It checks if the dmabuf is already attached, if it is not
+ * attached, attach it. It returns the number of entries of the attachment to
+ * the AI engine dmabuf user argument. If user wants to know the sg list, it
+ * can use AI engine get sg ioctl.
+ */
+long aie_part_attach_dmabuf_req(struct aie_partition *apart,
+				void __user *user_args)
+{
+	struct aie_dmabuf *adbuf;
+	struct dma_buf *dbuf;
+	long ret;
+	int dmabuf_fd = (int)(uintptr_t)user_args;
+
+	dbuf = dma_buf_get(dmabuf_fd);
+	if (IS_ERR(dbuf)) {
+		dev_err(&apart->dev, "failed to get dmabuf from %d.\n",
+			dmabuf_fd);
+		return PTR_ERR(dbuf);
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dma_buf_put(dbuf);
+		return ret;
+	}
+
+	adbuf = aie_part_find_dmabuf(apart, dbuf);
+	if (!adbuf)
+		adbuf = aie_part_attach_dmabuf(apart, dbuf);
+	else
+		aie_part_dmabuf_attach_get(adbuf);
+
+	mutex_unlock(&apart->mlock);
+
+	if (IS_ERR(adbuf)) {
+		dev_err(&apart->dev, "failed to attach dmabuf\n");
+		dma_buf_put(dbuf);
+		return PTR_ERR(adbuf);
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_detach_dmabuf_req() - Handle detaching dmabuf from an AI engine
+ *				  partition request
+ * @apart: AI engine partition
+ * @user_args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function unmaps and detaches a dmabuf from the specified AI engine
+ * partition.
+ */
+long aie_part_detach_dmabuf_req(struct aie_partition *apart,
+				void __user *user_args)
+{
+	int dmabuf_fd;
+	struct dma_buf *dbuf;
+	struct aie_dmabuf *adbuf;
+	int ret;
+
+	dmabuf_fd = (int)(uintptr_t)user_args;
+
+	dbuf = dma_buf_get(dmabuf_fd);
+	if (IS_ERR(dbuf)) {
+		dev_err(&apart->dev, "failed to get dmabuf %d.\n", dmabuf_fd);
+		return PTR_ERR(dbuf);
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dma_buf_put(dbuf);
+		return ret;
+	}
+
+	adbuf = aie_part_find_dmabuf(apart, dbuf);
+	dma_buf_put(dbuf);
+	if (!adbuf) {
+		dev_err(&apart->dev, "failed to find dmabuf %d.\n", dmabuf_fd);
+		mutex_unlock(&apart->mlock);
+		return -EINVAL;
+	}
+
+	aie_part_dmabuf_attach_put(adbuf);
+
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_part_set_bd() - Set AI engine SHIM DMA buffer descriptor
+ * @apart: AI engine partition
+ * @user_args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function set the user specified buffer descriptor into the SHIM DMA
+ * buffer descriptor.
+ */
+long aie_part_set_bd(struct aie_partition *apart, void __user *user_args)
+{
+	struct aie_device *adev = apart->adev;
+	const struct aie_dma_attr *shim_dma = adev->shim_dma;
+	struct aie_dma_bd_args args;
+	u32 *bd, *tmpbd, buf_len, laddr, haddr, regval;
+	dma_addr_t addr;
+	int ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	ret = aie_part_validate_bdloc(apart, args.loc, args.bd_id);
+	if (ret) {
+		dev_err(&apart->dev, "invalid SHIM DMA BD reg address.\n");
+		return -EINVAL;
+	}
+
+	bd = memdup_user((void __user *)args.bd, shim_dma->bd_len);
+	if (IS_ERR(bd))
+		return PTR_ERR(bd);
+
+	regval = bd[shim_dma->buflen.regoff / sizeof(u32)];
+	buf_len = aie_get_reg_field(&shim_dma->buflen, regval);
+	if (!buf_len) {
+		dev_err(&apart->dev, "no buf length from shim dma bd.\n");
+		kfree(bd);
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(bd);
+		return ret;
+	}
+
+	/* Get device address from virtual address */
+	addr = aie_part_get_dmabuf_da(apart, (void *)(uintptr_t)args.data_va,
+				      buf_len);
+	if (!addr) {
+		dev_err(&apart->dev, "invalid buffer 0x%llx, 0x%x.\n",
+			args.data_va, buf_len);
+		mutex_unlock(&apart->mlock);
+		kfree(bd);
+		return -EINVAL;
+	}
+
+	/* Set low 32bit address */
+	laddr = lower_32_bits(addr);
+	tmpbd = (u32 *)((char *)bd + shim_dma->laddr.regoff);
+	*tmpbd &= ~shim_dma->laddr.mask;
+	*tmpbd |= aie_get_field_val(&shim_dma->laddr, laddr);
+
+	/* Set high 32bit address */
+	haddr = upper_32_bits(addr);
+	tmpbd = (u32 *)((char *)bd + shim_dma->haddr.regoff);
+	*tmpbd &= ~shim_dma->haddr.mask;
+	*tmpbd |= aie_get_field_val(&shim_dma->haddr, haddr);
+
+	ret = aie_part_set_shimdma_bd(apart, args.loc, args.bd_id, bd);
+	mutex_unlock(&apart->mlock);
+	if (ret)
+		dev_err(&apart->dev, "failed to set to shim dma bd.\n");
+
+	kfree(bd);
+	return ret;
+}
+
+/**
+ * aie_part_set_dmabuf_bd() - Set AI engine SHIM DMA dmabuf buffer descriptor
+ * @apart: AI engine partition
+ * @user_args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function set the user specified buffer descriptor into the SHIM DMA
+ * buffer descriptor. The buffer descriptor contained in the @user_args has the
+ * offset to the start of the buffer descriptor.
+ */
+long aie_part_set_dmabuf_bd(struct aie_partition *apart,
+			    void __user *user_args)
+{
+	struct aie_device *adev = apart->adev;
+	const struct aie_dma_attr *shim_dma = adev->shim_dma;
+	struct aie_dmabuf_bd_args args;
+	u32 *bd, *tmpbd, len, laddr, haddr, regval;
+	u64 off;
+	dma_addr_t addr;
+	int ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	ret = aie_part_validate_bdloc(apart, args.loc, args.bd_id);
+	if (ret) {
+		dev_err(&apart->dev, "invalid SHIM DMA BD reg address.\n");
+		return -EINVAL;
+	}
+
+	bd = memdup_user((void __user *)args.bd, shim_dma->bd_len);
+	if (IS_ERR(bd))
+		return PTR_ERR(bd);
+
+	regval = bd[shim_dma->buflen.regoff / sizeof(u32)];
+	len = aie_get_reg_field(&shim_dma->buflen, regval);
+	if (!len) {
+		dev_err(&apart->dev, "no buf length from shim dma bd.\n");
+		kfree(bd);
+		return -EINVAL;
+	}
+
+	/* Get low 32bit address offset */
+	tmpbd = (u32 *)((char *)bd + shim_dma->laddr.regoff);
+	laddr = *tmpbd & shim_dma->laddr.mask;
+	/* Get high 32bit address offset */
+	tmpbd = (u32 *)((char *)bd + shim_dma->haddr.regoff);
+	haddr = *tmpbd & shim_dma->haddr.mask;
+	off = laddr | ((u64)haddr << 32);
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(bd);
+		return ret;
+	}
+
+	/* Get device address from offset */
+	addr = aie_part_get_dmabuf_da_from_off(apart, args.buf_fd, off, len);
+	if (!addr) {
+		dev_err(&apart->dev, "invalid buffer 0x%llx, 0x%x.\n",
+			off, len);
+		mutex_unlock(&apart->mlock);
+		kfree(bd);
+		return -EINVAL;
+	}
+
+	/* Set low 32bit address */
+	laddr = lower_32_bits(addr);
+	tmpbd = (u32 *)((char *)bd + shim_dma->laddr.regoff);
+	*tmpbd &= ~shim_dma->laddr.mask;
+	*tmpbd |= aie_get_field_val(&shim_dma->laddr, laddr);
+
+	/* Set high 32bit address */
+	haddr = upper_32_bits(addr);
+	tmpbd = (u32 *)((char *)bd + shim_dma->haddr.regoff);
+	*tmpbd &= ~shim_dma->haddr.mask;
+	*tmpbd |= aie_get_field_val(&shim_dma->haddr, haddr);
+
+	ret = aie_part_set_shimdma_bd(apart, args.loc, args.bd_id, bd);
+	mutex_unlock(&apart->mlock);
+	if (ret)
+		dev_err(&apart->dev, "failed to set to shim dma bd.\n");
+
+	kfree(bd);
+	return ret;
+}
+
+/**
+ * aie_part_prealloc_dbufs_cache() - Preallocate dmabuf descriptors memory
+ *
+ * @apart: AI engine partition
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function preallocate memories to save dmabuf descriptors. When dmabuf
+ * is attached to the partition at runtime, it can get the descriptor memory
+ * from this preallocated memory pool.
+ */
+int aie_part_prealloc_dbufs_cache(struct aie_partition *apart)
+{
+	struct kmem_cache *dbufs_cache;
+	char name[64];
+
+	sprintf(name, "%s_dbufs", dev_name(&apart->dev));
+	dbufs_cache = kmem_cache_create(name, sizeof(struct aie_dmabuf),
+					0, 0, NULL);
+	if (!dbufs_cache)
+		return -ENOMEM;
+
+	apart->dbufs_cache = dbufs_cache;
+
+	return 0;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-fpga.c b/drivers/misc/xilinx-ai-engine/ai-engine-fpga.c
new file mode 100644
index 000000000..48cac3827
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-fpga.c
@@ -0,0 +1,94 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine driver FPGA region implementation
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include "ai-engine-internal.h"
+
+static int aie_fpga_bridge_enable_set(struct fpga_bridge *bridge, bool enable)
+{
+	struct aie_partition *apart = bridge->priv;
+	int ret;
+
+	/*
+	 * TBD:
+	 * "Enable" should enable the SHIM tile configuration.
+	 * "Disable" should disable SHIM DMAs, and wait until SHIM DMA stops,
+	 * and disable SHIM to PL streams within partition.
+	 */
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	if (enable)
+		apart->status |= XAIE_PART_STATUS_BRIDGE_ENABLED;
+	else
+		apart->status &= ~XAIE_PART_STATUS_BRIDGE_ENABLED;
+	mutex_unlock(&apart->mlock);
+	return 0;
+}
+
+static int aie_fpga_bridge_enable_show(struct fpga_bridge *bridge)
+{
+	struct aie_partition *apart = bridge->priv;
+	int ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	if (apart->status & XAIE_PART_STATUS_BRIDGE_ENABLED)
+		ret = 1;
+	else
+		ret = 0;
+	mutex_unlock(&apart->mlock);
+	return ret;
+}
+
+static const struct fpga_bridge_ops aie_fpga_bridge_ops = {
+	.enable_set = aie_fpga_bridge_enable_set,
+	.enable_show = aie_fpga_bridge_enable_show,
+};
+
+/**
+ * aie_fpga_create_bridge() - Create FPGA bridge for AI engine partition
+ * @apart: AI engine partition
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will create FPGA bridge for AI engine partition.
+ * FPGA bridge is the presentation of SHIM row of the AI engine partition.
+ * FPGA bridge connects AI engine partition with other FPGA regions.
+ */
+int aie_fpga_create_bridge(struct aie_partition *apart)
+{
+	struct fpga_bridge *br;
+	int ret;
+
+	snprintf(apart->br.name, sizeof(apart->br.name) - 1,
+		 "xlnx-aie-bridge-%u-%u", apart->range.start.col, 0);
+	br = devm_fpga_bridge_create(&apart->dev, apart->br.name,
+				     &aie_fpga_bridge_ops, apart);
+	if (!br)
+		return -ENOMEM;
+	ret = fpga_bridge_register(br);
+	if (ret) {
+		dev_err(&apart->dev, "Failed to register bridge.\n");
+		return ret;
+	}
+	apart->br.br = br;
+	return 0;
+}
+
+/**
+ * aie_fpga_free_bridge() - Free AI engine partition FPGA bridge
+ * @apart: AI engine partition
+ *
+ * This function will free the FPGA bridge for AI engine partition.
+ */
+void aie_fpga_free_bridge(struct aie_partition *apart)
+{
+	if (!WARN_ON(!apart->br.br))
+		fpga_bridge_unregister(apart->br.br);
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-internal.h b/drivers/misc/xilinx-ai-engine/ai-engine-internal.h
new file mode 100644
index 000000000..99ab94564
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-internal.h
@@ -0,0 +1,1106 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx AI Engine driver internal header
+ *
+ * Copyright (C) 2020 - 2021 Xilinx, Inc.
+ */
+
+#ifndef AIE_INTERNAL_H
+#define AIE_INTERNAL_H
+
+#include <linux/bitfield.h>
+#include <linux/bitmap.h>
+#include <linux/bits.h>
+#include <linux/cdev.h>
+#include <linux/clk.h>
+#include <linux/device.h>
+#include <linux/dma-buf.h>
+#include <linux/file.h>
+#include <linux/fpga/fpga-bridge.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/slab.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+
+/*
+ * Macros for AI engine tile type bitmasks
+ */
+enum aie_tile_type {
+	AIE_TILE_TYPE_TILE,
+	AIE_TILE_TYPE_SHIMPL,
+	AIE_TILE_TYPE_SHIMNOC,
+	AIE_TILE_TYPE_MAX
+};
+
+#define AIE_TILE_TYPE_MASK_TILE		BIT(AIE_TILE_TYPE_TILE)
+#define AIE_TILE_TYPE_MASK_SHIMPL	BIT(AIE_TILE_TYPE_SHIMPL)
+/* SHIM NOC tile includes SHIM PL and SHIM NOC modules */
+#define AIE_TILE_TYPE_MASK_SHIMNOC	BIT(AIE_TILE_TYPE_SHIMNOC)
+
+/*
+ * Macros for attribute property of AI engine registers accessed by kernel
+ * 0 - 7 bits: tile type bits
+ * 8 - 15 bits: permission bits. If it is 1, it allows write from userspace
+ */
+#define AIE_REGS_ATTR_TILE_TYPE_SHIFT	0U
+#define AIE_REGS_ATTR_PERM_SHIFT	8U
+#define AIE_REGS_ATTR_TILE_TYPE_MASK	GENMASK(AIE_REGS_ATTR_PERM_SHIFT - 1, \
+						AIE_REGS_ATTR_TILE_TYPE_SHIFT)
+#define AIE_REGS_ATTR_PERM_MASK		GENMASK(15, \
+						AIE_REGS_ATTR_PERM_SHIFT)
+
+#define AIE_PART_STATUS_BRIDGE_DISABLED	0x1U
+
+/* Silicon Engineering Sample(ES) revision ID */
+#define VERSAL_ES1_REV_ID		0x0
+#define VERSAL_ES2_REV_ID		0x1
+
+#define AIE_NPI_ERROR_ID		BIT(1)
+
+/* Macros relevant to interrupts */
+#define AIE_INTR_L2_CTRL_MASK_WIDTH	32
+
+/* Max number of modules per tile */
+#define AIE_MAX_MODS_PER_TILE		2U
+
+/* AIE core registers step size */
+#define AIE_CORE_REGS_STEP		0x10
+
+/*
+ * Macros of AI engine module type index of a tile type
+ * e.g.
+ * id 0 of CORE tile is memory module, and 1 is core module
+ * id 0 of SHIM tile is pl module, and 1 is noc module
+ */
+#define AIE_TILE_MOD_START		AIE_MEM_MOD
+#define AIE_MOD_ID(T, M)		((M) - AIE_##T ## _MOD_START)
+#define AIE_TILE_MEM_MOD_ID		AIE_MOD_ID(TILE, AIE_MEM_MOD)
+#define AIE_TILE_CORE_MOD_ID		AIE_MOD_ID(TILE, AIE_CORE_MOD)
+#define AIE_SHIMPL_MOD_START		AIE_PL_MOD
+#define AIE_SHIMNOC_MOD_START		AIE_PL_MOD
+#define AIE_SHIM_PL_MOD_ID		AIE_MOD_ID(SHIMPL, AIE_PL_MOD)
+#define AIE_SHIM_NOC_MOD_ID		AIE_MOD_ID(SHIMNOC, AIE_NOC_MOD)
+
+/* String delimiter to format sysfs data */
+#define DELIMITER_LEVEL0 "|"
+#define DELIMITER_LEVEL1 ", "
+#define DELIMITER_LEVEL2 "; "
+
+/* Macros to define size of temporary string buffers */
+#define AIE_SYSFS_CORE_STS_SIZE		100U
+#define AIE_SYSFS_CHAN_STS_SIZE		150U
+#define AIE_SYSFS_QUEUE_SIZE_SIZE	40U
+#define AIE_SYSFS_QUEUE_STS_SIZE	60U
+#define AIE_SYSFS_BD_SIZE		40U
+#define AIE_SYSFS_ERROR_SIZE		300U
+#define AIE_SYSFS_ERROR_CATEGORY_SIZE	500U
+#define AIE_SYSFS_LOCK_STS_SIZE		400U
+#define AIE_SYSFS_EVENT_STS_SIZE	550U
+
+/* Helper macros to dynamically create sysfs device attribute */
+#define AIE_PART_DEV_ATTR_RO(_name) {				\
+	.name		= __stringify(_name),			\
+	.mode		= 0444,					\
+	.show		= aie_part_show_##_name,		\
+}
+
+#define AIE_PART_DEV_ATTR_WO(_name) {				\
+	.name		= __stringify(_name),			\
+	.mode		= 0200,					\
+	.store		= aie_part_store_##_name,		\
+}
+
+#define AIE_PART_DEV_ATTR_RW(_name) {				\
+	.name		= __stringify(_name),			\
+	.mode		= 0644,					\
+	.show		= aie_part_show_##_name,		\
+	.store		= aie_part_store_##_name,		\
+}
+
+#define AIE_TILE_DEV_ATTR_RO(_name, _ttype) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0444,					\
+	.tile_type	= _ttype,				\
+	.show		= aie_tile_show_##_name,		\
+}
+
+#define AIE_TILE_DEV_ATTR_WO(_name, _ttype) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0200,					\
+	.tile_type	= _ttype,				\
+	.store		= aie_tile_store_##_name,		\
+}
+
+#define AIE_TILE_DEV_ATTR_RW(_name, _ttype) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0644,					\
+	.tile_type	= _ttype,				\
+	.show		= aie_tile_show_##_name,		\
+	.store		= aie_tile_store_##_name,		\
+}
+
+#define AIE_PART_BIN_ATTR_RO(_name, _size) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0444,					\
+	.size		= _size,				\
+	.read		= aie_sysfs_read_handler,		\
+	.read_callback	= aie_part_read_cb_##_name,		\
+}
+
+#define AIE_PART_BIN_ATTR_WO(_name, _size) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0200,					\
+	.size		= _size,				\
+	.write		= aie_part_write_handler,		\
+	.write_callback	= aie_part_write_cb_##_name,		\
+}
+
+#define AIE_PART_BIN_ATTR_RW(_name, _size) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0644					\
+	.size		= _size,				\
+	.read		= aie_sysfs_read_handler,		\
+	.write		= aie_part_write_handler,		\
+	.read_callback	= aie_part_read_cb_##_name,		\
+	.write_callback	= aie_part_write_cb_##_name,		\
+}
+
+#define AIE_TILE_BIN_ATTR_RO(_name, _size, _ttype) {		\
+	.name		= __stringify(_name),			\
+	.mode		= 0444,					\
+	.size		= _size,				\
+	.tile_type	= _ttype,				\
+	.read		= aie_sysfs_read_handler,		\
+	.read_callback	= aie_tile_read_cb_##_name,		\
+}
+
+#define AIE_TILE_BIN_ATTR_WO(_name, _size, _ttype) {		\
+	.name		= __stringify(_name),			\
+	.mode		= 0200,					\
+	.size		= _size,				\
+	.tile_type	= _ttype,				\
+	.write		= aie_tile_write_handler,		\
+	.write_callback	= aie_tile_write_cb_##_name,		\
+}
+
+#define AIE_TILE_BIN_ATTR_RW(_name, _size, _ttype) {		\
+	.name		= __stringify(_name),			\
+	.mode		= 0644,					\
+	.size		= _size,				\
+	.tile_type	= _ttype,				\
+	.read		= aie_sysfs_read_handler,		\
+	.write		= aie_tile_write_handler,		\
+	.read_callback	= aie_tile_read_cb_##_name,		\
+	.write_callback	= aie_tile_write_cb_##_name,		\
+}
+
+/*
+ * enum aie_shim_switch_type - identifies different switches in shim tile.
+ */
+enum aie_shim_switch_type {
+	AIE_SHIM_SWITCH_A,
+	AIE_SHIM_SWITCH_B
+};
+
+/**
+ * struct aie_tile_regs - contiguous range of AI engine register
+ *			  within an AI engine tile
+ * @soff: start offset of the range
+ * @eoff: end offset of the range
+ * @attribute: registers attribute. It uses AIE_REGS_ATTR_* macros defined
+ *	       above.
+ */
+struct aie_tile_regs {
+	size_t soff;
+	size_t eoff;
+	u32 attribute;
+};
+
+/**
+ * struct aie_single_reg_field - AI engine single field register attribute
+ * @mask: field mask
+ * @regoff: register offset of the field
+ */
+struct aie_single_reg_field {
+	u32 mask;
+	u32 regoff;
+};
+
+struct aie_device;
+struct aie_partition;
+
+/**
+ * struct aie_part_mem - AI engine partition memory information structure
+ * @apart: AI engine partition
+ * @dbuf: dmabuf pointer associated with the memory
+ * @mem: memory information of a type of memory
+ * @size: size of the total memories in the partition
+ *
+ * This structure is to keep the information of a type of memory in a
+ * partition. The memory information will be stored in @mem property.
+ * The following information will be keep:
+ *  * memory start address offset within a tile
+ *  * memory size
+ *  * what tiles contain this type of memory
+ */
+struct aie_part_mem {
+	struct aie_partition *apart;
+	struct dma_buf *dbuf;
+	struct aie_mem mem;
+	size_t size;
+};
+
+/**
+ * struct aie_dma_attr - AI engine DMA attributes structure
+ * @laddr: low address field attributes
+ * @haddr: high address field attributes
+ * @buflen: buffer length field attributes
+ * @sts: channel status field attributes
+ * @stall: queue stall status field attributes
+ * @qsize: queue size field attributes
+ * @curbd: current buffer descriptor field attributes
+ * @qsts: queue status field attributes
+ * @bd_regoff: SHIM DMA buffer descriptors register offset
+ * @mm2s_sts_regoff: MM2S status register offset
+ * @s2mm_sts_regoff: S2MM status register offset
+ * @num_mm2s_chan: number of MM2S channels
+ * @num_s2mm_chan: number of S2MM channels
+ * @num_bds: number of buffer descriptors
+ * @bd_len: length of a buffer descriptor in bytes
+ */
+struct aie_dma_attr {
+	struct aie_single_reg_field laddr;
+	struct aie_single_reg_field haddr;
+	struct aie_single_reg_field buflen;
+	struct aie_single_reg_field sts;
+	struct aie_single_reg_field stall;
+	struct aie_single_reg_field qsize;
+	struct aie_single_reg_field curbd;
+	struct aie_single_reg_field qsts;
+	u32 bd_regoff;
+	u32 mm2s_sts_regoff;
+	u32 s2mm_sts_regoff;
+	u32 num_mm2s_chan;
+	u32 num_s2mm_chan;
+	u32 num_bds;
+	u32 bd_len;
+};
+
+/**
+ * struct aie_core_regs_attr - AI engine core register attributes structure
+ * @core_regs: core registers
+ * @width: number of 32 bit words
+ */
+struct aie_core_regs_attr {
+	const struct aie_tile_regs *core_regs;
+	u32 width;
+};
+
+/**
+ * struct aie_tile_operations - AI engine device operations
+ * @get_tile_type: get type of tile based on tile operation
+ * @get_mem_info: get different types of memories information
+ * @get_core_status: get the status of AIE core.
+ * @reset_shim: reset shim, it will assert and then release SHIM reset
+ * @init_part_clk_state: initialize clock states software structure which is a
+ *			 bitmap for the AI engine partition. The clock states
+ *			 structure is the structure used to keep track of if
+ *			 the modules in the AI engine partition are gated.
+ * @scan_part_clocks: scan partition modules to check whether the modules are
+ *		      clock gated or not, and update the soft clock states
+ *		      structure. It is required to be called when the partition
+ *		      is requested so that the driver knows which modules are
+ *		      clock gated when the partition is requested. This function
+ *		      expects the caller to apply partition lock before calling
+ *		      this function.
+ * @set_part_clocks: set partition modules clocks gate registers based on the
+ *		     partition clock states bitmap. This function expects the
+ *		     caller to apply partition lock before calling this
+ *		     function. The caller function will need to set the bitmap
+ *		     on which tiles are required to be clocked on.
+ *
+ * Different AI engine device version has its own device
+ * operation.
+ */
+struct aie_tile_operations {
+	u32 (*get_tile_type)(struct aie_location *loc);
+	unsigned int (*get_mem_info)(struct aie_range *range,
+				     struct aie_part_mem *pmem);
+	u32 (*get_core_status)(struct aie_partition *apart,
+			       struct aie_location *loc);
+	int (*reset_shim)(struct aie_device *adev, struct aie_range *range);
+	int (*init_part_clk_state)(struct aie_partition *apart);
+	int (*scan_part_clocks)(struct aie_partition *apart);
+	int (*set_part_clocks)(struct aie_partition *apart);
+};
+
+/**
+ * struct aie_resource - AI engine resource structure
+ * @bitmap: resource bitmap
+ * @total: total number of resource
+ */
+struct aie_resource {
+	unsigned long *bitmap;
+	u32 total;
+};
+
+/**
+ * struct aie_event_attr - AI Engine event attributes structure.
+ * @bc_event: broadcast event attribute to capture event mask value and
+ *	      register offset from @bc_regoff.
+ * @group_error: group error attribute to capture error group mask value and
+ *		 register offset value from @group_regoff.
+ * @bc_regoff: base broadcast register offset.
+ * @status_regoff: base status register offset.
+ * @group_regoff: base group error register offset.
+ * @base_error_event: event ID of first error event in a group error.
+ * @num_broadcasts: total number of broadcast events.
+ * @base_bc_event: broadcast 0 vent ID
+ * @num_events: total number of events.
+ */
+struct aie_event_attr {
+	struct aie_single_reg_field bc_event;
+	struct aie_single_reg_field group_error;
+	u32 bc_regoff;
+	u32 status_regoff;
+	u32 group_regoff;
+	u32 base_error_event;
+	u32 num_broadcasts;
+	u32 base_bc_event;
+	u32 num_events;
+};
+
+/**
+ * struct aie_l1_intr_ctrl_attr - AI engine level 1 interrupt controller
+ *				  attributes structure.
+ * @mask: level 1 interrupt controller mask attribute.
+ * @swa_status: switch A level 1 interrupt controller status attribute.
+ * @swb_status: switch A level 1 interrupt controller status attribute.
+ * @swa_event: switch A level 1 interrupt controller event attribute.
+ * @swb_event: switch A level 1 interrupt controller event attribute.
+ * @regoff: base level 1 interrupt controller register offset.
+ * @event_lsb: lsb of IRQ event within IRQ event switch register.
+ * @num_broadcasts: total number of broadcast signals to level 1 interrupt
+ *		    controller.
+ */
+struct aie_l1_intr_ctrl_attr {
+	struct aie_single_reg_field swa_status;
+	struct aie_single_reg_field swb_status;
+	struct aie_single_reg_field swa_event;
+	struct aie_single_reg_field swb_event;
+	u32 regoff;
+	u32 event_lsb;
+	u32 num_broadcasts;
+};
+
+/**
+ * struct aie_l2_intr_ctrl_attr - AI engine level 2 interrupt controller
+ *				  attributes structure.
+ * @mask: level 2 interrupt controller mask attribute.
+ * @enable: level 2 interrupt controller enable attribute.
+ * @disable: level 2 interrupt controller disable attribute.
+ * @status: level 2 interrupt controller status attribute.
+ * @regoff: level 2 interrupt controller register offset.
+ * @num_broadcasts: total number of broadcast signals to level 2 interrupt
+ *		    controller.
+ */
+struct aie_l2_intr_ctrl_attr {
+	struct aie_single_reg_field mask;
+	struct aie_single_reg_field enable;
+	struct aie_single_reg_field disable;
+	struct aie_single_reg_field status;
+	u32 regoff;
+	u32 num_broadcasts;
+};
+
+/**
+ * struct aie_error_cb - AI engine error callback struct.
+ * @cb: pointer to callback function.
+ * @priv: data to be passed to the callback function.
+ */
+struct aie_error_cb {
+	void (*cb)(void *priv);
+	void *priv;
+};
+
+/**
+ * struct aie_event_prop - AI engine event property.
+ * @event: error event ID.
+ * @event_str: error string.
+ */
+struct aie_event_prop {
+	u32 event;
+	char *event_str;
+};
+
+/**
+ * struct aie_err_category - AI engine errors category.
+ * @err_category: category of error.
+ * @num_events: number of event IDs in a category.
+ * @prop: pointer to an array event properties.
+ */
+struct aie_err_category {
+	u32 err_category;
+	u32 num_events;
+	const struct aie_event_prop *prop;
+};
+
+/**
+ * struct aie_error_attr - AI engine error attribute.
+ * @num_err_categories: number of possible error categories valid for a given
+ *			module.
+ * @err_category: pointer to an array of error categories.
+ */
+struct aie_error_attr {
+	u32 num_err_categories;
+	const struct aie_err_category *err_category;
+};
+
+/**
+ * struct aie_rsc_stat - AI engine hardware resource status bitmap of a
+ *			 resource of a module type of a tile type of an AI
+ *			 engine partition
+ * @rbits: runtime allocated resource bitmap
+ * @sbits: static resource bitmap for resources allocated at compilation
+ *	   time
+ */
+struct aie_rsc_stat {
+	struct aie_resource rbits;
+	struct aie_resource sbits;
+};
+
+/**
+ * struct aie_mod_rscs - AI engine hardware resource status bitmaps of
+ *			 a module type of a tile type of an AI engine
+ *			 partition.
+ * @rscs_stat: resource status bitmaps
+ */
+struct aie_mod_rscs {
+	struct aie_rsc_stat *rscs_stat;
+};
+
+/**
+ * struct aie_tile_rscs - AI engine hardware resource status bitmaps of all
+ *			  resources of a tile type of a partition.
+ * @mod_rscs: array of pointers of AI engine resources. Each element is an
+ *	      array of hardware resources of different modules of a particular
+ *	      resource type of a tile type.
+ *	      e.g. if the tile type is TILE. The rscs are an arrary of
+ *	      resources bitmap of all the defined AI engine resources types of
+ *	      TILE type. e.g. the element of AIE_RSCTYPE_PERF. It is an array
+ *	      of perfcounter resources bitmaps for both core module and memory
+ *	      module of TILE type of an AI engine partition.
+ */
+struct aie_tile_rscs {
+	struct aie_mod_rscs *mod_rscs[AIE_RSCTYPE_MAX];
+};
+
+/**
+ * struct aie_mod_rsc_attr - AI engine resource attribute of a module
+ * @num_rscs: number of resource
+ */
+struct aie_mod_rsc_attr {
+	u8 num_rscs;
+};
+
+/**
+ * struct aie_tile_rsc_attr - AI engine resource attributes
+ * @mod_attr: array of resource attribute different modules of a tile type of
+ *	      a particular resource type.
+ */
+struct aie_tile_rsc_attr {
+	struct aie_mod_rsc_attr mod_attr[AIE_MAX_MODS_PER_TILE];
+};
+
+/**
+ * struct aie_lock_attr - AI engine lock attributes
+ * @sts: lock status field attributes
+ * @sts_regoff: lock status register offset
+ * @num_locks: number of locks
+ */
+struct aie_lock_attr {
+	struct aie_single_reg_field sts;
+	u32 sts_regoff;
+	u32 num_locks;
+};
+
+/**
+ * struct aie_tile_attr - AI engine device tile type attributes
+ * @start_row: start row
+ * @num_rows: number of rows
+ * @num_mods: number of modules of this tile type
+ * @mods: array of module types of this tile type
+ * @rscs_attr: resources attributes array. Each element is an array of
+ *	       attributes of a resource type of a tile type.
+ */
+struct aie_tile_attr {
+	u8 start_row;
+	u8 num_rows;
+	u8 num_mods;
+	const enum aie_module_type *mods;
+	const struct aie_tile_rsc_attr *rscs_attr;
+};
+
+/**
+ * struct aie_dev_attr - device attribute properties for AI Engine sysfs nodes.
+ * @name: name of the device attribute
+ * @mode: permissions associated
+ * @tile_type: tile type(s) attribute is valid for. use AIE_TILE_TYPE_MASK_*.
+ * @show: read function handler
+ * @store: write function handler
+ */
+struct aie_dev_attr {
+	const char *name;
+	umode_t mode;
+	u32 tile_type;
+	ssize_t (*show)(struct device *dev, struct device_attribute *attr,
+			char *buf);
+	ssize_t (*store)(struct device *dev, struct device_attribute *attr,
+			 const char *buf, size_t count);
+};
+
+/**
+ * struct aie_sysfs_prop - private data passed to the sysfs read/write handler.
+ * @data: buffer to export sysfs data
+ * @size: size of data exported
+ * @max_size: max size of data that could be exported
+ * @read_callback: callback to fetch data from on read
+ * @write_callback: callback to send data to on write
+ */
+struct aie_sysfs_prop {
+	char *data;
+	ssize_t size;
+	ssize_t max_size;
+	ssize_t (*read_callback)(struct kobject *kobj, char *buffer,
+				 ssize_t size);
+	ssize_t (*write_callback)(struct kobject *kobj, char *buffer,
+				  ssize_t size);
+};
+
+/**
+ * struct aie_bin_attr - binary attribute properties for AI Engine sysfs nodes
+ * @name: name of the binary attribute
+ * @mode: permissions associated
+ * @size: size of the buffer to be allocated
+ * @tile_type: tile type(s) attribute is valid for. use AIE_TILE_TYPE_MASK_*.
+ * @read: read handler
+ * @write: write handler
+ * @read_callback: callback to fetch data from on read
+ * @write_callback:  callback to send data to on write
+ */
+struct aie_bin_attr {
+	const char *name;
+	umode_t mode;
+	ssize_t size;
+	u32 tile_type;
+	ssize_t (*read)(struct file *filp, struct kobject *kobj,
+			struct bin_attribute *attr, char *buf, loff_t offset,
+			size_t max_size);
+	ssize_t (*write)(struct file *filp, struct kobject *kobj,
+			 struct bin_attribute *attr, char *buf, loff_t offset,
+			 size_t max_size);
+	ssize_t (*read_callback)(struct kobject *kobj, char *buffer,
+				 ssize_t size);
+	ssize_t (*write_callback)(struct kobject *kobj, char *buffer,
+				  ssize_t size);
+};
+
+/**
+ * struct aie_sysfs_attr - captures all sysfs attributes defined at
+ *			   partition or tile level.
+ * @dev_attr: pointer to array of device attributes
+ * @bin_attr: pointer to array of binary attributes
+ * @num_dev_attrs: number of device attributes
+ * @num_bin_attrs: number of binary attributes
+ */
+struct aie_sysfs_attr {
+	const struct aie_dev_attr *dev_attr;
+	const struct aie_bin_attr *bin_attr;
+	u32 num_dev_attrs;
+	u32 num_bin_attrs;
+};
+
+/**
+ * struct aie_tile - AI engine tile structure
+ * @loc: tile co-ordinates
+ * @apart: parent partition the tile belongs to
+ * @dev: device for the AI engine tile device
+ * @attr_grp: attribute group
+ */
+struct aie_tile {
+	struct aie_location loc;
+	struct aie_partition *apart;
+	struct device dev;
+	struct attribute_group *attr_grp;
+};
+
+/**
+ * struct aie_device - AI engine device structure
+ * @partitions: list of partitions requested
+ * @cdev: cdev for the AI engine
+ * @dev: device for the AI engine device
+ * @mlock: protection for AI engine device operations
+ * @base: AI engine device base virtual address
+ * @clk: AI enigne device clock
+ * @res: memory resource of AI engine device
+ * @kernel_regs: array of kernel only registers
+ * @core_regs: array of core registers
+ * @ops: tile operations
+ * @col_rst: column reset attribute
+ * @col_clkbuf: column clock buffer attribute
+ * @shim_dma: SHIM DMA attribute
+ * @tile_dma: tile DMA attribute
+ * @pl_events: pl module event attribute
+ * @mem_events: memory module event attribute
+ * @core_events: core module event attribute
+ * @l1_ctrl: level 1 interrupt controller attribute
+ * @l2_ctrl: level 2 interrupt controller attribute
+ * @core_errors: core module error attribute
+ * @mem_errors: memory module error attribute
+ * @shim_errors: shim tile error attribute
+ * @size: size of the AI engine address space
+ * @array_shift: array address shift
+ * @col_shift: column address shift
+ * @row_shift: row address shift
+ * @cols_res: AI engine columns resources to indicate
+ *	      while columns are occupied by partitions.
+ * @num_kernel_regs: number of kernel only registers range
+ * @num_core_regs: number of core registers range
+ * @irq: Linux IRQ number
+ * @backtrack: workqueue to backtrack interrupt
+ * @version: AI engine device version
+ * @pm_node_id: AI Engine platform management node ID
+ * @clock_id: AI Engine clock ID
+ * @ttype_attr: tile type attributes
+ * @part_sysfs_attr: partition level sysfs attributes
+ * @tile_sysfs_attr: tile level sysfs attributes
+ * @core_status_str: core status in string format
+ * @core_pc: program counter attribute
+ * @core_lr: link register attribute
+ * @core_sp: stack pointer attribute
+ * @dma_status_str: DMA channel status in string format
+ * @queue_status_str: DMA queue status in string format
+ * @pl_lock: PL module lock attribute
+ * @mem_lock: memory module lock attribute
+ * @lock_status_str: lock status in string format
+ */
+struct aie_device {
+	struct list_head partitions;
+	struct cdev cdev;
+	struct device dev;
+	struct mutex mlock; /* protection for AI engine partitions */
+	void __iomem *base;
+	struct clk *clk;
+	struct resource *res;
+	const struct aie_tile_regs *kernel_regs;
+	const struct aie_core_regs_attr *core_regs;
+	const struct aie_tile_operations *ops;
+	const struct aie_single_reg_field *col_rst;
+	const struct aie_single_reg_field *col_clkbuf;
+	const struct aie_dma_attr *shim_dma;
+	const struct aie_dma_attr *tile_dma;
+	const struct aie_event_attr *pl_events;
+	const struct aie_event_attr *mem_events;
+	const struct aie_event_attr *core_events;
+	const struct aie_l1_intr_ctrl_attr *l1_ctrl;
+	const struct aie_l2_intr_ctrl_attr *l2_ctrl;
+	const struct aie_error_attr *core_errors;
+	const struct aie_error_attr *mem_errors;
+	const struct aie_error_attr *shim_errors;
+	size_t size;
+	struct aie_resource cols_res;
+	u32 array_shift;
+	u32 col_shift;
+	u32 row_shift;
+	u32 num_kernel_regs;
+	u32 num_core_regs;
+	int irq;
+	struct work_struct backtrack;
+	int version;
+	u32 pm_node_id;
+	u32 clock_id;
+	struct aie_tile_attr ttype_attr[AIE_TILE_TYPE_MAX];
+	const struct aie_sysfs_attr *part_sysfs_attr;
+	const struct aie_sysfs_attr *tile_sysfs_attr;
+	char **core_status_str;
+	const struct aie_single_reg_field *core_pc;
+	const struct aie_single_reg_field *core_lr;
+	const struct aie_single_reg_field *core_sp;
+	char **dma_status_str;
+	char **queue_status_str;
+	const struct aie_lock_attr *pl_lock;
+	const struct aie_lock_attr *mem_lock;
+	char **lock_status_str;
+};
+
+/**
+ * struct aie_part_bridge - AI engine FPGA bridge
+ * @name: name of the FPGA bridge
+ * @br: pointer to FPGA bridge
+ */
+struct aie_part_bridge {
+	char name[32];
+	struct fpga_bridge *br;
+};
+
+/**
+ * struct aie_partition - AI engine partition structure
+ * @node: list node
+ * @dbufs: dmabufs list
+ * @adev: pointer to AI device instance
+ * @filep: pointer to file for refcount on the users of the partition
+ * @pmems: pointer to partition memories types
+ * @dbufs_cache: memory management object for preallocated dmabuf descriptors
+ * @trscs: resources bitmaps for each tile
+ * @freq_req: required frequency
+ * @br: AI engine FPGA bridge
+ * @range: range of partition
+ * @mlock: protection for AI engine partition operations
+ * @dev: device for the AI engine partition
+ * @atiles: pointer to an array of AIE tile structure.
+ * @cores_clk_state: bitmap to indicate the power state of core modules
+ * @tiles_inuse: bitmap to indicate if a tile is in use
+ * @error_cb: error callback
+ * @core_event_status: core module event bitmap
+ * @mem_event_status: memory module event bitmap
+ * @pl_event_status: pl module event bitmap
+ * @l2_mask: level 2 interrupt controller mask bitmap
+ * @attr_grp: attribute group
+ * @partition_id: partition id. Partition ID is the identifier
+ *		  of the AI engine partition in the system.
+ * @status: indicate if the partition is in use
+ * @cntrflag: partition control flag. e.g. whether to reset columns when
+ *	      the partition is released
+ * @error_to_report: indicates if there are errors pending to be reported to
+ *		     the application. This value is set to true if errors are
+ *		     found during backtracking, and error interrupt was
+ *		     received when partition was not requested yet.
+ */
+struct aie_partition {
+	struct list_head node;
+	struct list_head dbufs;
+	struct aie_part_bridge br;
+	struct aie_device *adev;
+	struct file *filep;
+	struct aie_part_mem *pmems;
+	struct kmem_cache *dbufs_cache;
+	struct aie_tile_rscs trscs[AIE_TILE_TYPE_MAX];
+	u64 freq_req;
+	struct aie_range range;
+	struct mutex mlock; /* protection for AI engine partition operations */
+	struct device dev;
+	struct aie_tile *atiles;
+	struct aie_resource cores_clk_state;
+	struct aie_resource tiles_inuse;
+	struct aie_error_cb error_cb;
+	struct aie_resource core_event_status;
+	struct aie_resource mem_event_status;
+	struct aie_resource pl_event_status;
+	struct aie_resource l2_mask;
+	struct attribute_group *attr_grp;
+	u32 partition_id;
+	u32 status;
+	u32 cntrflag;
+	u8 error_to_report;
+};
+
+/**
+ * struct aie_part_pinned_region - AI engine user space pinned region
+ * @user_addr: user space address
+ * @len: length of the user space buffer in bytes
+ * @npages: number of pages of the user space buffer
+ * @pages: array to receive pointers to the pages pinned.
+ *	   should be at least npages long
+ */
+struct aie_part_pinned_region {
+	u64 user_addr;
+	u64 len;
+	struct page **pages;
+	int npages;
+};
+
+extern struct class *aie_class;
+extern const struct file_operations aie_part_fops;
+
+#define cdev_to_aiedev(i_cdev) container_of((i_cdev), struct aie_device, cdev)
+#define dev_to_aiedev(_dev) container_of((_dev), struct aie_device, dev)
+#define dev_to_aiepart(_dev) container_of((_dev), struct aie_partition, dev)
+#define dev_to_aietile(_dev) container_of((_dev), struct aie_tile, dev)
+
+#define aie_col_mask(adev) ({ \
+	struct aie_device *_adev = (adev); \
+	GENMASK_ULL(_adev->array_shift - 1, _adev->col_shift);  \
+	})
+
+#define aie_row_mask(adev) ({ \
+	struct aie_device *_adev = (adev); \
+	GENMASK_ULL(_adev->col_shift - 1, _adev->row_shift);  \
+	})
+
+#define aie_tile_reg_mask(adev) ({ \
+	struct aie_device *_adev = (adev); \
+	GENMASK_ULL(_adev->row_shift - 1, 0);  \
+	})
+
+/*
+ * Need to define field get, as AI engine shift mask is not constant.
+ * Cannot use FIELD_GET()
+ */
+#define aie_tile_reg_field_get(mask, shift, regoff) ( \
+	((regoff) & (mask)) >> (shift))
+
+#define aie_cal_tile_reg(adev, regoff) ( \
+	aie_tile_reg_field_get(aie_tile_reg_mask(adev), 0, regoff))
+
+/**
+ * aie_get_field_val() - calculate value of an AI engine register field
+ * @field: a field in a register
+ * @val: value of the field
+ * @return: value of a register field
+ */
+static inline u32 aie_get_field_val(const struct aie_single_reg_field *field,
+				    u32 val)
+{
+	long long mask = (long long)field->mask & 0x00000000ffffffff;
+
+	return (val << __bf_shf(mask)) & field->mask;
+}
+
+/**
+ * aie_get_reg_field() - get value from a field from a register valuer
+ * @field: a field in a register
+ * @regval: register value
+ * @return: value of a register field
+ */
+static inline u32 aie_get_reg_field(const struct aie_single_reg_field *field,
+				    u32 regval)
+{
+	long long mask64 = (long long)field->mask & 0x00000000ffffffff;
+
+	return (regval & field->mask) >> __bf_shf(mask64);
+}
+
+/**
+ * aie_cal_regoff() - calculate register offset to the whole AI engine
+ *		      device start address
+ * @adev: AI engine device
+ * @loc: AI engine tile location
+ * @regoff_intile: register offset within a tile
+ * @return: register offset to the whole AI engine device start address
+ */
+static inline u32 aie_cal_regoff(struct aie_device *adev,
+				 struct aie_location loc, u32 regoff_intile)
+{
+	return regoff_intile + (loc.col << adev->col_shift) +
+	       (loc.row << adev->row_shift);
+}
+
+/**
+ * aie_validate_location() - validate tile location within an AI engine
+ *			     partition
+ * @apart: AI engine partition
+ * @loc: AI engine tile location
+ * @return: return 0 if it is valid, negative value for errors.
+ *
+ * This function checks if the AI engine location is within the AI engine
+ * partition.
+ */
+static inline int aie_validate_location(struct aie_partition *apart,
+					struct aie_location loc)
+{
+	if (loc.col < apart->range.start.col ||
+	    loc.col >= apart->range.start.col + apart->range.size.col ||
+	    loc.row < apart->range.start.row ||
+	    loc.row >= apart->range.start.row + apart->range.size.row)
+		return -EINVAL;
+
+	return 0;
+}
+
+/**
+ * aie_resource_or_get_valueul() - get unsigned long value of specified
+ *				   number of bits starting from specified
+ *				   start bit of a resource bitmap
+ *
+ * @res: pointer to AI engine resource
+ * @sbit: start bit for OR operation
+ * @nbits: number of bits to OR
+ * @return: or result of @nbits of two bitmaps starting from @sbit
+ *
+ * OR @nbits of two resource bitmaps starting from @sbit
+ */
+static inline
+unsigned long aie_resource_or_get_valueul(struct aie_resource *res,
+					  u32 sbit, u32 nbits)
+{
+	const size_t i = BIT_WORD(sbit);
+	unsigned long bits;
+
+	bits = res->bitmap[i];
+	bits >>= (sbit % BITS_PER_LONG);
+	bits |= BITMAP_FIRST_WORD_MASK(nbits);
+
+	return bits;
+}
+
+int aie_resource_initialize(struct aie_resource *res, int count);
+void aie_resource_uninitialize(struct aie_resource *res);
+int aie_resource_check_region(struct aie_resource *res, u32 start,
+			      u32 count);
+int aie_resource_get_region(struct aie_resource *res, u32 start,
+			    u32 count);
+void aie_resource_put_region(struct aie_resource *res, int start, u32 count);
+int aie_resource_set(struct aie_resource *res, u32 start, u32 count);
+int aie_resource_cpy_from_arr32(struct aie_resource *res, u32 start,
+				const u32 *src, u32 nbits);
+int aie_resource_cpy_to_arr32(struct aie_resource *res, u32 start, u32 *dst,
+			      u32 nbits);
+int aie_resource_clear(struct aie_resource *res, u32 start, u32 count);
+int aie_resource_clear_all(struct aie_resource *res);
+bool aie_resource_testbit(struct aie_resource *res, u32 bit);
+int aie_resource_check_common_avail(struct aie_resource *res0,
+				    struct aie_resource *res1,
+				    u32 sbit, u32 nbits);
+int aie_resource_get_common_avail(struct aie_resource *res0,
+				  struct aie_resource *res1,
+				  u32 sbit, u32 nbits, u32 total,
+				  struct aie_rsc *rscs);
+int aie_resource_check_pattern_region(struct aie_resource *res,
+				      u32 start, u32 end, u32 count);
+int aie_resource_check_common_pattern_region(struct aie_resource *res0,
+					     struct aie_resource *res1,
+					     u32 sbit, u32 nbits, u32 total);
+int aie_resource_get_common_pattern_region(struct aie_resource *res0,
+					   struct aie_resource *res1,
+					   u32 sbit, u32 nbits, u32 total,
+					   struct aie_rsc *rscs);
+
+const struct file_operations *aie_part_get_fops(void);
+u8 aie_part_in_use(struct aie_partition *apart);
+struct aie_partition *aie_get_partition_from_id(struct aie_device *adev,
+						u32 partition_id);
+struct aie_partition *of_aie_part_probe(struct aie_device *adev,
+					struct device_node *nc);
+void aie_part_remove(struct aie_partition *apart);
+int aie_part_clean(struct aie_partition *apart);
+int aie_part_open(struct aie_partition *apart, void *rsc_metadata);
+
+int aie_fpga_create_bridge(struct aie_partition *apart);
+void aie_fpga_free_bridge(struct aie_partition *apart);
+
+int aie_mem_get_info(struct aie_partition *apart, unsigned long arg);
+
+long aie_part_attach_dmabuf_req(struct aie_partition *apart,
+				void __user *user_args);
+long aie_part_detach_dmabuf_req(struct aie_partition *apart,
+				void __user *user_args);
+long aie_part_set_bd(struct aie_partition *apart, void __user *user_args);
+long aie_part_set_dmabuf_bd(struct aie_partition *apart,
+			    void __user *user_args);
+void aie_part_release_dmabufs(struct aie_partition *apart);
+int aie_part_prealloc_dbufs_cache(struct aie_partition *apart);
+
+int aie_part_scan_clk_state(struct aie_partition *apart);
+bool aie_part_check_clk_enable_loc(struct aie_partition *apart,
+				   struct aie_location *loc);
+int aie_part_set_freq(struct aie_partition *apart, u64 freq);
+int aie_part_get_running_freq(struct aie_partition *apart, u64 *freq);
+
+int aie_part_request_tiles_from_user(struct aie_partition *apart,
+				     void __user *user_args);
+int aie_part_release_tiles_from_user(struct aie_partition *apart,
+				     void __user *user_args);
+int aie_device_init(struct aie_device *adev);
+
+void aie_array_backtrack(struct work_struct *work);
+irqreturn_t aie_interrupt(int irq, void *data);
+void aie_part_clear_cached_events(struct aie_partition *apart);
+int aie_part_set_intr_rscs(struct aie_partition *apart);
+
+bool aie_part_has_mem_mmapped(struct aie_partition *apart);
+bool aie_part_has_regs_mmapped(struct aie_partition *apart);
+
+int aie_part_get_tile_rows(struct aie_partition *apart,
+			   enum aie_tile_type ttype);
+
+int aie_part_reset(struct aie_partition *apart);
+int aie_part_post_reinit(struct aie_partition *apart);
+
+int aie_part_rscmgr_init(struct aie_partition *apart);
+void aie_part_rscmgr_finish(struct aie_partition *apart);
+void aie_part_rscmgr_reset(struct aie_partition *apart);
+long aie_part_rscmgr_rsc_req(struct aie_partition *apart,
+			     void __user *user_args);
+long aie_part_rscmgr_rsc_release(struct aie_partition *apart,
+				 void __user *user_args);
+long aie_part_rscmgr_rsc_free(struct aie_partition *apart,
+			      void __user *user_args);
+long aie_part_rscmgr_rsc_req_specific(struct aie_partition *apart,
+				      void __user *user_args);
+long aie_part_rscmgr_rsc_check_avail(struct aie_partition *apart,
+				     void __user *user_args);
+long aie_part_rscmgr_get_broadcast(struct aie_partition *apart,
+				   void __user *user_args);
+int aie_part_rscmgr_set_static(struct aie_partition *apart, void *meta);
+int aie_part_rscmgr_set_tile_broadcast(struct aie_partition *apart,
+				       struct aie_location loc,
+				       enum aie_module_type mod, uint32_t id);
+
+int aie_part_sysfs_create_entries(struct aie_partition *apart);
+void aie_part_sysfs_remove_entries(struct aie_partition *apart);
+int aie_tile_sysfs_create_entries(struct aie_tile *atile);
+void aie_tile_sysfs_remove_entries(struct aie_tile *atile);
+ssize_t aie_sysfs_read_handler(struct file *filp, struct kobject *kobj,
+			       struct bin_attribute *attr, char *buf,
+			       loff_t offset, size_t max_size);
+
+ssize_t aie_sysfs_get_core_status(struct aie_partition *apart,
+				  struct aie_location *loc, char *buffer,
+				  ssize_t size);
+ssize_t aie_tile_show_core(struct device *dev, struct device_attribute *attr,
+			   char *buffer);
+ssize_t aie_part_read_cb_core(struct kobject *kobj, char *buffer, ssize_t size);
+ssize_t aie_sysfs_get_dma_status(struct aie_partition *apart,
+				 struct aie_location *loc, char *buffer,
+				 ssize_t size);
+ssize_t aie_tile_show_dma(struct device *dev, struct device_attribute *attr,
+			  char *buffer);
+ssize_t aie_part_read_cb_dma(struct kobject *kobj, char *buffer, ssize_t size);
+ssize_t aie_tile_show_lock(struct device *dev, struct device_attribute *attr,
+			   char *buffer);
+ssize_t aie_part_read_cb_lock(struct kobject *kobj, char *buffer, ssize_t size);
+ssize_t aie_sysfs_get_lock_status(struct aie_partition *apart,
+				  struct aie_location *loc, char *buffer,
+				  ssize_t size);
+u32 aie_get_module_error_count(struct aie_partition *apart,
+			       struct aie_location loc,
+			       enum aie_module_type module,
+			       const struct aie_error_attr *err_attr);
+bool aie_check_error_bitmap(struct aie_partition *apart,
+			    struct aie_location loc,
+			    enum aie_module_type module, u8 event);
+u32 aie_get_error_count(struct aie_partition *apart);
+ssize_t aie_sysfs_get_errors(struct aie_partition *apart,
+			     struct aie_location *loc, char *buffer,
+			     ssize_t size);
+ssize_t aie_tile_show_error(struct device *dev, struct device_attribute *attr,
+			    char *buffer);
+ssize_t aie_part_show_error_stat(struct device *dev,
+				 struct device_attribute *attr, char *buffer);
+ssize_t aie_part_read_cb_error(struct kobject *kobj, char *buffer,
+			       ssize_t size);
+ssize_t aie_tile_show_event(struct device *dev, struct device_attribute *attr,
+			    char *buffer);
+void aie_read_event_status(struct aie_partition *apart,
+			   struct aie_location *loc,
+			   enum aie_module_type module, u32 *reg);
+ssize_t aie_part_read_cb_status(struct kobject *kobj, char *buffer,
+				ssize_t size);
+long aie_part_rscmgr_get_statistics(struct aie_partition *apart,
+				    void __user *user_args);
+
+#endif /* AIE_INTERNAL_H */
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-interrupt.c b/drivers/misc/xilinx-ai-engine/ai-engine-interrupt.c
new file mode 100644
index 000000000..981e66a19
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-interrupt.c
@@ -0,0 +1,1272 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+#include <linux/bitmap.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <linux/workqueue.h>
+
+#include "ai-engine-internal.h"
+#include "linux/xlnx-ai-engine.h"
+
+#define AIE_ARRAY_TILE_ERROR_BC_ID		0U
+#define AIE_SHIM_TILE_ERROR_IRQ_ID		16U
+#define AIE_SHIM_INTR_BC_MAX			5U
+
+/**
+ * aie_get_broadcast_event() - get event ID being broadcast on given
+ *			       broadcast line.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @bc_id: broadcast ID.
+ * @return: event ID.
+ */
+static u8 aie_get_broadcast_event(struct aie_partition *apart,
+				  struct aie_location *loc,
+				  enum aie_module_type module, u8 bc_id)
+{
+	const struct aie_event_attr *event_mod;
+	u32 bcoff, regoff;
+
+	if (module == AIE_CORE_MOD)
+		event_mod = apart->adev->core_events;
+	else if (module == AIE_MEM_MOD)
+		event_mod = apart->adev->mem_events;
+	else
+		event_mod = apart->adev->pl_events;
+
+	bcoff = event_mod->bc_regoff + event_mod->bc_event.regoff + bc_id * 4U;
+	regoff = aie_cal_regoff(apart->adev, *loc, bcoff);
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_read_event_status() - get the status of event status registers.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @reg: array to store event status register values.
+ */
+void aie_read_event_status(struct aie_partition *apart,
+			   struct aie_location *loc,
+			   enum aie_module_type module, u32 *reg)
+{
+	const struct aie_event_attr *event_mod;
+	u8 offset;
+
+	if (module == AIE_CORE_MOD)
+		event_mod = apart->adev->core_events;
+	else if (module == AIE_MEM_MOD)
+		event_mod = apart->adev->mem_events;
+	else
+		event_mod = apart->adev->pl_events;
+
+	for (offset = 0; offset < (event_mod->num_events / 32); offset++) {
+		u32 status_off = event_mod->status_regoff + offset * 4U;
+		u32 regoff = aie_cal_regoff(apart->adev, *loc, status_off);
+
+		reg[offset] = ioread32(apart->adev->base + regoff);
+	}
+}
+
+/**
+ * aie_clear_event_status() - clears the status of event.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @event: event ID.
+ */
+static void aie_clear_event_status(struct aie_partition *apart,
+				   struct aie_location *loc,
+				   enum aie_module_type module, u8 event)
+{
+	const struct aie_event_attr *event_mod;
+	u32 status_off, regoff;
+
+	if (module == AIE_CORE_MOD)
+		event_mod = apart->adev->core_events;
+	else if (module == AIE_MEM_MOD)
+		event_mod = apart->adev->mem_events;
+	else
+		event_mod = apart->adev->pl_events;
+
+	if (event >= event_mod->num_events)
+		return;
+
+	status_off = event_mod->status_regoff + (event / 32) * 4U;
+	regoff = aie_cal_regoff(apart->adev, *loc, status_off);
+	iowrite32(BIT(event % 32), apart->adev->base + regoff);
+}
+
+/**
+ * aie_check_group_errors_enabled() - get error events enabled in group error.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @return: bitmap of enabled error events.
+ */
+static u32 aie_check_group_errors_enabled(struct aie_partition *apart,
+					  struct aie_location *loc,
+					  enum aie_module_type module)
+{
+	const struct aie_event_attr *event_mod;
+	u32 groff, regoff;
+
+	if (module == AIE_CORE_MOD)
+		event_mod = apart->adev->core_events;
+	else if (module == AIE_MEM_MOD)
+		event_mod = apart->adev->mem_events;
+	else
+		event_mod = apart->adev->pl_events;
+
+	groff = event_mod->group_regoff + event_mod->group_error.regoff;
+	regoff = aie_cal_regoff(apart->adev, *loc, groff);
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_set_error_event() - enable/disable error events in group error.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @bitmap: error event to enable/disable in group errors.
+ */
+static void aie_set_error_event(struct aie_partition *apart,
+				struct aie_location *loc,
+				enum aie_module_type module, u32 bitmap)
+{
+	const struct aie_event_attr *event_mod;
+	u32 groff, regoff;
+
+	if (module == AIE_CORE_MOD)
+		event_mod = apart->adev->core_events;
+	else if (module == AIE_MEM_MOD)
+		event_mod = apart->adev->mem_events;
+	else
+		event_mod = apart->adev->pl_events;
+
+	groff = event_mod->group_regoff + event_mod->group_error.regoff;
+	regoff = aie_cal_regoff(apart->adev, *loc, groff);
+	iowrite32(bitmap, apart->adev->base + regoff);
+}
+
+/**
+ * aie_get_error_event() - map group error status bit to actual error
+ *			   event number.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @index: event index within group errors.
+ * @return: true event ID.
+ */
+static u32 aie_get_error_event(struct aie_partition *apart,
+			       struct aie_location *loc,
+			       enum aie_module_type module, u8 index)
+{
+	const struct aie_event_attr *event_mod;
+
+	if (module == AIE_CORE_MOD)
+		event_mod = apart->adev->core_events;
+	else if (module == AIE_MEM_MOD)
+		event_mod = apart->adev->mem_events;
+	else
+		event_mod = apart->adev->pl_events;
+
+	return event_mod->base_error_event + index;
+}
+
+/**
+ * aie_get_bc_event() - get the broadcast event ID.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @bc_id: broadcast line ID.
+ * @return: broadcast event ID.
+ */
+static u32 aie_get_bc_event(struct aie_partition *apart,
+			    struct aie_location *loc,
+			    enum aie_module_type module, u8 bc_id)
+{
+	const struct aie_event_attr *event_mod;
+
+	if (module == AIE_CORE_MOD)
+		event_mod = apart->adev->core_events;
+	else if (module == AIE_MEM_MOD)
+		event_mod = apart->adev->mem_events;
+	else
+		event_mod = apart->adev->pl_events;
+
+	return event_mod->base_bc_event + bc_id;
+}
+
+/**
+ * aie_get_l1_event() - get event ID being broadcast on level 1 IRQ.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @sw: switch type.
+ * @irq_id: IRQ event ID to be read.
+ * @return: true event ID.
+ */
+static u8 aie_get_l1_event(struct aie_partition *apart,
+			   struct aie_location *loc,
+			   enum aie_shim_switch_type sw, u8 irq_id)
+{
+	const struct aie_l1_intr_ctrl_attr *intr_ctrl = apart->adev->l1_ctrl;
+	u32 l1off, l1mask, regoff, reg_value;
+
+	if (sw == AIE_SHIM_SWITCH_A) {
+		l1off = intr_ctrl->regoff + intr_ctrl->swa_event.regoff;
+		l1mask = intr_ctrl->swa_event.mask;
+	} else {
+		l1off = intr_ctrl->regoff + intr_ctrl->swb_event.regoff;
+		l1mask = intr_ctrl->swb_event.mask;
+	}
+
+	regoff = aie_cal_regoff(apart->adev, *loc, l1off);
+	reg_value = ioread32(apart->adev->base + regoff);
+	reg_value &= l1mask << (irq_id * intr_ctrl->event_lsb);
+	reg_value >>= (irq_id * intr_ctrl->event_lsb);
+	return reg_value;
+}
+
+/**
+ * aie_clear_l1_intr() - clear level 1 interrupt controller status.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @sw: switch type.
+ * @irq_id: IRQ ID to be cleared.
+ */
+static void aie_clear_l1_intr(struct aie_partition *apart,
+			      struct aie_location *loc,
+			      enum aie_shim_switch_type sw, u8 irq_id)
+{
+	const struct aie_l1_intr_ctrl_attr *intr_ctrl = apart->adev->l1_ctrl;
+	u32 l1off, regoff;
+
+	if (sw == AIE_SHIM_SWITCH_A)
+		l1off = intr_ctrl->regoff + intr_ctrl->swa_status.regoff;
+	else
+		l1off = intr_ctrl->regoff + intr_ctrl->swb_status.regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, l1off);
+	iowrite32(BIT(irq_id), apart->adev->base + regoff);
+}
+
+/**
+ * aie_get_l1_status() - get level 1 interrupt controller status value.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @sw: switch type.
+ * @return: status value.
+ */
+static u32 aie_get_l1_status(struct aie_partition *apart,
+			     struct aie_location *loc,
+			     enum aie_shim_switch_type sw)
+{
+	const struct aie_l1_intr_ctrl_attr *intr_ctrl = apart->adev->l1_ctrl;
+	u32 l1off, regoff;
+
+	if (sw == AIE_SHIM_SWITCH_A)
+		l1off = intr_ctrl->regoff + intr_ctrl->swa_status.regoff;
+	else
+		l1off = intr_ctrl->regoff + intr_ctrl->swb_status.regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, l1off);
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_clear_l2_intr() - clear level 2 interrupt controller status.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @bitmap_irq: IRQ bitmap. IRQ lines corresponding to set bits will be
+ *		cleared.
+ */
+static void aie_clear_l2_intr(struct aie_partition *apart,
+			      struct aie_location *loc, u32 bitmap_irq)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = apart->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->status.regoff;
+	u32 regoff = aie_cal_regoff(apart->adev, *loc, l2off);
+
+	iowrite32(bitmap_irq, apart->adev->base + regoff);
+}
+
+/**
+ * aie_get_l2_status() - get level 2 interrupt controller status value.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @return: status value.
+ */
+static u32 aie_get_l2_status(struct aie_partition *apart,
+			     struct aie_location *loc)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = apart->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->status.regoff;
+	u32 regoff = aie_cal_regoff(apart->adev, *loc, l2off);
+
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_get_l2_mask() - get level 2 interrupt controller mask value.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @return: mask value.
+ */
+static u32 aie_get_l2_mask(struct aie_partition *apart,
+			   struct aie_location *loc)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = apart->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->mask.regoff;
+	u32 regoff = aie_cal_regoff(apart->adev, *loc, l2off);
+
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_enable_l2_ctrl() - enable interrupts to level 2 interrupt controller.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @bit_map: bitmap of broadcast lines to enable.
+ */
+static void aie_enable_l2_ctrl(struct aie_partition *apart,
+			       struct aie_location *loc, u32 bit_map)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = apart->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->enable.regoff;
+	u32 regoff = aie_cal_regoff(apart->adev, *loc, l2off);
+
+	bit_map &= intr_ctrl->enable.mask;
+	iowrite32(bit_map, apart->adev->base + regoff);
+}
+
+/**
+ * aie_disable_l2_ctrl() - disable interrupts to level 2 interrupt controller.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @bit_map: bitmap of broadcast lines to disable.
+ */
+static void aie_disable_l2_ctrl(struct aie_partition *apart,
+				struct aie_location *loc, u32 bit_map)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = apart->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->disable.regoff;
+	u32 regoff = aie_cal_regoff(apart->adev, *loc, l2off);
+
+	bit_map &= intr_ctrl->disable.mask;
+	iowrite32(bit_map, apart->adev->base + regoff);
+}
+
+/**
+ * aie_part_set_event_bitmap() - set the status of event in local event
+ *				 bitmap.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @event: event ID to be logged.
+ */
+static void aie_part_set_event_bitmap(struct aie_partition *apart,
+				      struct aie_location loc,
+				      enum aie_module_type module, u8 event)
+{
+	u8 row, col, mod_num_events;
+	struct aie_resource *event_sts;
+	u32 offset;
+
+	if (module == AIE_CORE_MOD) {
+		event_sts = &apart->core_event_status;
+		mod_num_events = apart->adev->core_events->num_events;
+		row = loc.row - apart->range.start.row - 1;
+	} else if (module == AIE_MEM_MOD) {
+		event_sts = &apart->mem_event_status;
+		mod_num_events = apart->adev->mem_events->num_events;
+		row = loc.row - apart->range.start.row - 1;
+	} else {
+		event_sts = &apart->pl_event_status;
+		mod_num_events = apart->adev->pl_events->num_events;
+		row = loc.row;
+	}
+
+	col = loc.col - apart->range.start.col;
+
+	offset = (col + row * apart->range.size.col) * mod_num_events + event;
+	aie_resource_set(event_sts, offset, 1);
+}
+
+/**
+ * aie_check_error_bitmap() - check the status of event in local event bitmap.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @event: event ID to check.
+ * @return: true if event has happened, else false.
+ */
+bool aie_check_error_bitmap(struct aie_partition *apart,
+			    struct aie_location loc,
+			    enum aie_module_type module, u8 event)
+{
+	struct aie_resource *event_sts;
+	u32 offset;
+	u8 row, col, mod_num_events;
+
+	if (module == AIE_CORE_MOD) {
+		event_sts = &apart->core_event_status;
+		mod_num_events = apart->adev->core_events->num_events;
+		row = loc.row - apart->range.start.row - 1;
+	} else if (module == AIE_MEM_MOD) {
+		event_sts = &apart->mem_event_status;
+		mod_num_events = apart->adev->mem_events->num_events;
+		row = loc.row - apart->range.start.row - 1;
+	} else {
+		event_sts = &apart->pl_event_status;
+		mod_num_events = apart->adev->pl_events->num_events;
+		row = loc.row;
+	}
+
+	col = loc.col - apart->range.start.col;
+
+	offset = (col + row * apart->range.size.col) * mod_num_events + event;
+	return aie_resource_testbit(event_sts, offset);
+}
+
+/**
+ * aie_tile_backtrack() - if error was asserted on a broadcast line in
+ *			  the given array tile,
+ *				* disable the error from the group errors
+ *				* record the error event in local bitmap
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @sw: switch type.
+ * @bc_id: broadcast ID.
+ * @return: true if error was asserted, else return false.
+ */
+static bool aie_tile_backtrack(struct aie_partition *apart,
+			       struct aie_location loc,
+			       enum aie_module_type module,
+			       enum aie_shim_switch_type sw, u8 bc_id)
+{
+	unsigned long grenabled;
+	u32 status[4];
+	u8 n, grevent, eevent;
+	bool ret = false;
+
+	if (module == AIE_PL_MOD)
+		grevent = aie_get_l1_event(apart, &loc, sw, bc_id);
+	else
+		grevent = aie_get_broadcast_event(apart, &loc, module, bc_id);
+
+	aie_read_event_status(apart, &loc, module, status);
+
+	if (!(status[grevent / 32] & BIT(grevent % 32)))
+		return ret;
+
+	grenabled = aie_check_group_errors_enabled(apart, &loc, module);
+	for_each_set_bit(n, &grenabled, 32) {
+		eevent = aie_get_error_event(apart, &loc, module, n);
+		if (!(status[eevent / 32] & BIT(eevent % 32)))
+			continue;
+		grenabled &= ~BIT(n);
+		aie_part_set_event_bitmap(apart, loc, module, eevent);
+		ret = true;
+		dev_err_ratelimited(&apart->adev->dev,
+				    "Asserted tile error event %d at col %d row %d\n",
+				    eevent, loc.col, loc.row);
+	}
+	aie_set_error_event(apart, &loc, module, grenabled);
+
+	return ret;
+}
+
+/**
+ * aie_map_l2_to_l1() - map the status bit set in level 2 interrupt controller
+ *		        to a level 1 interrupt controller.
+ * @apart: AIE partition pointer.
+ * @set_pos: position of level 2 set bit.
+ * @l2_col: level 2 interrupt controller column ID.
+ * @l1_col: pointer to return corresponding level 1 column ID.
+ * @sw: pointer to return the level 1 interrupt controller switch ID.
+ *
+ * This API implementation is tightly coupled with the level 2 to level 1
+ * static mapping created when AIE application CDOs are generated.
+ */
+static void aie_map_l2_to_l1(struct aie_partition *apart, u32 set_pos,
+			     u32 l2_col, u32 *l1_col,
+			     enum aie_shim_switch_type *sw)
+{
+	if (l2_col + 3 >= apart->range.start.col + apart->range.size.col) {
+		*l1_col = l2_col + (set_pos % 6) / 2;
+		*sw = (set_pos % 6) % 2;
+	} else if (l2_col % 2 == 0) {
+		/* set bit position could be 0 - 5 */
+		*l1_col = l2_col - (2 - (set_pos % 6) / 2);
+		*sw = (set_pos % 6) % 2;
+	} else {
+		/* set bit position could be 0 - 1 */
+		*l1_col = l2_col;
+		*sw = set_pos;
+	}
+}
+
+/**
+ * aie_l1_backtrack() - backtrack AIE array tiles or shim tile based on
+ *			the level 2 status bit set.
+ * @apart: AIE partition pointer.
+ * @loc: tile location of level 2 interrupt controller.
+ * @set_pos: set bit position in level 2 controller status.
+ * @return: true if error was asserted, else return false.
+ */
+static bool aie_l1_backtrack(struct aie_partition *apart,
+			     struct aie_location loc, u32 set_pos)
+{
+	struct aie_location l1_ctrl;
+	enum aie_shim_switch_type sw;
+	unsigned long status;
+	u32 srow = apart->range.start.row + 1;
+	u32 erow = apart->range.start.row + apart->range.size.row;
+	bool ret = false;
+
+	/*
+	 * Based on the set status bit find which level 1 interrupt
+	 * controller has generated an interrupt
+	 */
+	l1_ctrl.row = 0;
+	aie_map_l2_to_l1(apart, set_pos, loc.col, &l1_ctrl.col, &sw);
+
+	status = aie_get_l1_status(apart, &l1_ctrl, sw);
+
+	/* For now, support error broadcasts only */
+	if (status & BIT(AIE_ARRAY_TILE_ERROR_BC_ID)) {
+		struct aie_location temp;
+		enum aie_module_type module;
+		u32 bc_event;
+
+		if (sw == AIE_SHIM_SWITCH_A)
+			module = AIE_CORE_MOD;
+		else
+			module = AIE_MEM_MOD;
+
+		aie_clear_l1_intr(apart, &l1_ctrl, sw,
+				  AIE_ARRAY_TILE_ERROR_BC_ID);
+
+		temp.row = srow;
+		temp.col = l1_ctrl.col;
+		bc_event = aie_get_bc_event(apart, &temp, module,
+					    AIE_ARRAY_TILE_ERROR_BC_ID);
+		for (; temp.row < erow; temp.row++) {
+			u32 reg[4];
+
+			if (!aie_part_check_clk_enable_loc(apart, &temp))
+				break;
+
+			if (aie_tile_backtrack(apart, temp, module, sw,
+					       AIE_ARRAY_TILE_ERROR_BC_ID))
+				ret = true;
+
+			aie_read_event_status(apart, &temp, module, reg);
+			if (!(reg[bc_event / 32] & BIT(bc_event % 32)))
+				break;
+
+			aie_clear_event_status(apart, &temp, module, bc_event);
+		}
+	}
+
+	if (status & BIT(AIE_SHIM_TILE_ERROR_IRQ_ID)) {
+		aie_clear_l1_intr(apart, &l1_ctrl, sw,
+				  AIE_SHIM_TILE_ERROR_IRQ_ID);
+		if (aie_tile_backtrack(apart, l1_ctrl, AIE_PL_MOD, sw,
+				       AIE_SHIM_TILE_ERROR_IRQ_ID))
+			ret = true;
+	}
+	return ret;
+}
+
+/**
+ * aie_l2_backtrack() - iterate through each level 2 interrupt controller
+ *			in a given partition and backtrack its
+ *			corresponding level 1 interrupt controller.
+ * @apart: AIE partition pointer
+ */
+static void aie_l2_backtrack(struct aie_partition *apart)
+{
+	struct aie_location loc;
+	unsigned long l2_mask = 0;
+	u32 n, ttype, l2_bitmap_offset = 0;
+	int ret;
+	bool sched_work = false;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err_ratelimited(&apart->dev,
+				    "Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return;
+	}
+
+	for (loc.col = apart->range.start.col, loc.row = 0;
+	     loc.col < apart->range.start.col + apart->range.size.col;
+	     loc.col++) {
+		ttype = apart->adev->ops->get_tile_type(&loc);
+		if (ttype != AIE_TILE_TYPE_SHIMNOC)
+			continue;
+
+		aie_resource_cpy_to_arr32(&apart->l2_mask, l2_bitmap_offset *
+					  32, (u32 *)&l2_mask, 32);
+		l2_bitmap_offset++;
+
+		for_each_set_bit(n, &l2_mask,
+				 apart->adev->l2_ctrl->num_broadcasts) {
+			if (aie_l1_backtrack(apart, loc, n))
+				apart->error_to_report = 1;
+		}
+
+		aie_enable_l2_ctrl(apart, &loc, l2_mask);
+	}
+
+	/*
+	 * Level 2 interrupt registers are edge-triggered. As a result,
+	 * re-enabling level 2 won't trigger an interrupt for the already
+	 * latched interrupts at level 1 controller.
+	 */
+	for (loc.col = apart->range.start.col, loc.row = 0;
+	     loc.col < apart->range.start.col + apart->range.size.col;
+	     loc.col++) {
+		if (aie_get_l1_status(apart, &loc, AIE_SHIM_SWITCH_A) ||
+		    aie_get_l1_status(apart, &loc, AIE_SHIM_SWITCH_B)) {
+			mutex_unlock(&apart->mlock);
+			sched_work = true;
+			schedule_work(&apart->adev->backtrack);
+			break;
+		}
+	}
+
+	if (!sched_work)
+		mutex_unlock(&apart->mlock);
+
+	/*
+	 * If error was asserted or there are errors pending to be reported to
+	 * the application, then invoke callback.
+	 */
+	if (apart->error_cb.cb && apart->error_to_report) {
+		apart->error_to_report = 0;
+		apart->error_cb.cb(apart->error_cb.priv);
+	}
+}
+
+/**
+ * aie_part_backtrack() - backtrack a individual.
+ * @apart: AIE partition pointer.
+ */
+static void aie_part_backtrack(struct aie_partition *apart)
+{
+	aie_l2_backtrack(apart);
+}
+
+/**
+ * aie_array_backtrack() - backtrack each partition to find the source of error
+ *			   interrupt.
+ * @work: pointer to the work structure.
+ *
+ * This task will re-enable IRQ after errors in all partitions has been
+ * serviced.
+ */
+void aie_array_backtrack(struct work_struct *work)
+{
+	struct aie_device *adev;
+	struct aie_partition *apart;
+	int ret;
+
+	adev = container_of(work, struct aie_device, backtrack);
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret) {
+		dev_err_ratelimited(&adev->dev,
+				    "Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return;
+	}
+
+	list_for_each_entry(apart, &adev->partitions, node) {
+		/*
+		 * If partition isn't requested yet, then only record the
+		 * occurrence of error interrupt. Such errors can only be
+		 * backtracked when the tiles in-use are known. Based on the
+		 * error_to_report value a task is scheduled in the workqueue
+		 * to backtrack this error interrupt when partition is
+		 * requested.
+		 */
+		if (!apart->status) {
+			apart->error_to_report = 1;
+			continue;
+		}
+		aie_part_backtrack(apart);
+	}
+
+	mutex_unlock(&adev->mlock);
+}
+
+/**
+ * aie_interrupt() - interrupt handler for AIE.
+ * @irq: Interrupt number.
+ * @data: AI engine device structure.
+ * @return: IRQ_HANDLED.
+ *
+ * This thread function disables level 2 interrupt controllers and schedules a
+ * task in workqueue to backtrack the source of error interrupt. Disabled
+ * interrupts are re-enabled after successful completion of bottom half.
+ */
+irqreturn_t aie_interrupt(int irq, void *data)
+{
+	struct aie_device *adev = data;
+	struct aie_partition *apart;
+	int ret;
+	bool sched_work = false;
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret) {
+		dev_err(&adev->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return IRQ_NONE;
+	}
+
+	list_for_each_entry(apart, &adev->partitions, node) {
+		struct aie_location loc;
+		u32 ttype, l2_mask, l2_status, l2_bitmap_offset  = 0;
+
+		ret = mutex_lock_interruptible(&apart->mlock);
+		if (ret) {
+			dev_err(&apart->dev,
+				"Failed to acquire lock. Process was interrupted by fatal signals\n");
+			mutex_unlock(&adev->mlock);
+			return IRQ_NONE;
+		}
+
+		for (loc.col = apart->range.start.col, loc.row = 0;
+		     loc.col < apart->range.start.col + apart->range.size.col;
+		     loc.col++) {
+			ttype = apart->adev->ops->get_tile_type(&loc);
+			if (ttype != AIE_TILE_TYPE_SHIMNOC)
+				continue;
+
+			l2_mask = aie_get_l2_mask(apart, &loc);
+			if (l2_mask) {
+				aie_resource_cpy_from_arr32(&apart->l2_mask,
+							    l2_bitmap_offset  *
+							    32, &l2_mask, 32);
+				aie_disable_l2_ctrl(apart, &loc, l2_mask);
+			}
+			l2_bitmap_offset++;
+
+			l2_status = aie_get_l2_status(apart, &loc);
+			if (l2_status) {
+				aie_clear_l2_intr(apart, &loc, l2_status);
+				sched_work = true;
+			} else {
+				aie_enable_l2_ctrl(apart, &loc, l2_mask);
+			}
+		}
+		mutex_unlock(&apart->mlock);
+	}
+
+	/* For ES1 silicon, interrupts are latched in NPI */
+	if (adev->version == VERSAL_ES1_REV_ID) {
+		ret = zynqmp_pm_clear_aie_npi_isr(adev->pm_node_id,
+						  AIE_NPI_ERROR_ID);
+		if (ret < 0)
+			dev_err(&adev->dev, "Failed to clear NPI ISR\n");
+	}
+
+	mutex_unlock(&adev->mlock);
+
+	if (sched_work)
+		schedule_work(&adev->backtrack);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * aie_get_module_error_count() - get the total count of errors in a module
+ *				  from local bitmap.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @err_attr: error attribute for given module type.
+ * @return: total number of errors found.
+ */
+u32 aie_get_module_error_count(struct aie_partition *apart,
+			       struct aie_location loc,
+			       enum aie_module_type module,
+			       const struct aie_error_attr *err_attr)
+{
+	u32 count = 0;
+	u8 i, j;
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		for (j = 0; j < err_attr->err_category[i].num_events; j++) {
+			u8 event = err_attr->err_category[i].prop[j].event;
+
+			if (aie_check_error_bitmap(apart, loc, module, event))
+				count++;
+		}
+	}
+	return count;
+}
+
+/**
+ * aie_get_error_count() - get the total count of errors in a partition from
+ *			   local bitmap.
+ * @apart: AIE partition pointer.
+ * @return: total number of errors found.
+ */
+u32 aie_get_error_count(struct aie_partition *apart)
+{
+	const struct aie_error_attr *core_errs = apart->adev->core_errors;
+	const struct aie_error_attr *mem_errs = apart->adev->mem_errors;
+	const struct aie_error_attr *shim_errs = apart->adev->shim_errors;
+	struct aie_location loc;
+	u32 ttype, num = 0;
+
+	for (loc.col = apart->range.start.col;
+	     loc.col < apart->range.size.col; loc.col++) {
+		for (loc.row = apart->range.start.row;
+		     loc.row < apart->range.size.row; loc.row++) {
+			ttype = apart->adev->ops->get_tile_type(&loc);
+			if (ttype == AIE_TILE_TYPE_TILE) {
+				num += aie_get_module_error_count(apart, loc,
+								  AIE_CORE_MOD,
+								  core_errs);
+				num += aie_get_module_error_count(apart, loc,
+								  AIE_MEM_MOD,
+								  mem_errs);
+			} else {
+				num += aie_get_module_error_count(apart, loc,
+								  AIE_PL_MOD,
+								  shim_errs);
+			}
+		}
+	}
+
+	return num;
+}
+
+/**
+ * aie_get_errors_from_bitmap() - get status of errors from local bitmap.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @err_attr: error attribute for given module type.
+ * @aie_err: pointer to array of error structure.
+ * @return: total number of errors found.
+ *
+ * This function parses local bitmaps and initializes @aie_err structures with
+ * the tile location of error event, module type and, its event ID.
+ */
+static u32 aie_get_errors_from_bitmap(struct aie_partition *apart,
+				      struct aie_location loc,
+				      enum aie_module_type module,
+				      const struct aie_error_attr *err_attr,
+				      struct aie_error *aie_err)
+{
+	u8 i, j;
+	u32 num_err = 0;
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		const struct aie_err_category *category;
+
+		category = &err_attr->err_category[i];
+		for (j = 0; j < category->num_events; j++) {
+			u8 event = category->prop[j].event;
+
+			if (!aie_check_error_bitmap(apart, loc, module, event))
+				continue;
+
+			aie_err[num_err].loc.col = loc.col;
+			aie_err[num_err].loc.row = loc.row;
+			aie_err[num_err].module = module;
+			aie_err[num_err].error_id = event;
+			aie_err[num_err].category = category->err_category;
+			num_err++;
+		}
+	}
+	return num_err;
+}
+
+/**
+ * aie_get_module_errors() - get errors for a given module type
+ *			     in a partition.
+ * @apart: AIE partition pointer.
+ * @module: module type.
+ * @aie_err: pointer to array of error structure.
+ * @return: total number of errors found.
+ *
+ * This function parses local bitmaps and initializes @aie_err structures.
+ */
+static u32 aie_get_module_errors(struct aie_partition *apart,
+				 enum aie_module_type module,
+				 struct aie_error *aie_err)
+{
+	const struct aie_error_attr *err_attr;
+	struct aie_location loc;
+	u32 srow, erow, scol, ecol, num_err = 0;
+
+	if (module == AIE_CORE_MOD) {
+		err_attr = apart->adev->core_errors;
+		srow = apart->range.start.row + 1;
+		erow = srow + apart->range.size.row - 1;
+	} else if (module == AIE_MEM_MOD) {
+		err_attr = apart->adev->mem_errors;
+		srow = apart->range.start.row + 1;
+		erow = srow + apart->range.size.row - 1;
+	} else {
+		err_attr = apart->adev->shim_errors;
+		srow = 0;
+		erow = 0;
+	}
+
+	scol = apart->range.start.col;
+	ecol = apart->range.start.col + apart->range.size.col - 1;
+
+	for (loc.col = scol; loc.col <= ecol; loc.col++) {
+		for (loc.row = srow; loc.row <= erow; loc.row++) {
+			num_err +=
+				aie_get_errors_from_bitmap(apart, loc,
+							   module, err_attr,
+							   &aie_err[num_err]);
+		}
+	}
+	return num_err;
+}
+
+/**
+ * aie_part_clear_cached_events() - clear cached events in a partition.
+ * @apart: AIE partition pointer
+ *
+ * This function will clear the cached events in a partition.
+ */
+void aie_part_clear_cached_events(struct aie_partition *apart)
+{
+	aie_resource_clear_all(&apart->core_event_status);
+	aie_resource_clear_all(&apart->mem_event_status);
+	aie_resource_clear_all(&apart->pl_event_status);
+}
+
+/**
+ * aie_part_set_intr_rscs() - set broadcast resources used by interrupt
+ * @apart: AIE partition pointer
+ * @return: 0 for sueccess, and negative value for failure
+ *
+ * This function reserves interrupt broadcast channels resources.
+ */
+int aie_part_set_intr_rscs(struct aie_partition *apart)
+{
+	u32 c, r;
+	int ret;
+
+	for (c = 0; c < apart->range.size.col; c++) {
+		u32 b;
+		struct aie_location l = {
+			.col = apart->range.start.col + c,
+			.row = 0,
+		};
+
+		/* reserved broadcast channels 0 - 5 for SHIM */
+		for (b = 0; b <= AIE_SHIM_INTR_BC_MAX; b++) {
+			ret = aie_part_rscmgr_set_tile_broadcast(apart, l,
+								 AIE_PL_MOD,
+								 b);
+			if (ret)
+				return ret;
+		}
+
+		for (r = 1; r < apart->range.size.row; r++) {
+			struct aie_device *adev = apart->adev;
+			struct aie_tile_attr *tattr;
+			u32 m, ttype;
+
+			b = AIE_ARRAY_TILE_ERROR_BC_ID;
+			l.row = apart->range.start.row + r;
+			ttype = adev->ops->get_tile_type(&l);
+
+			if (WARN_ON(ttype >= AIE_TILE_TYPE_MAX))
+				return -EINVAL;
+
+			tattr = &adev->ttype_attr[ttype];
+			for (m = 0; m < tattr->num_mods; m++) {
+				enum aie_module_type mod = tattr->mods[m];
+
+				ret = aie_part_rscmgr_set_tile_broadcast(apart,
+									 l, mod,
+									 b);
+				if (ret)
+					return ret;
+			}
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_register_error_notification() - register a callback for error
+ *				       notification.
+ * @dev: AIE partition device.
+ * @cb: pointer to a function that accepts pointer to private data as an
+ *	argument. callbacks are called in the bottom half without locks.
+ * @priv: private data to be passed to the callback.
+ * @return: 0 for success, and negative value for failure.
+ */
+int aie_register_error_notification(struct device *dev,
+				    void (*cb)(void *priv), void *priv)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	if (!cb || !dev)
+		return -EINVAL;
+
+	apart = container_of(dev, struct aie_partition, dev);
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return ret;
+	}
+
+	if (apart->error_cb.cb) {
+		dev_err(&apart->dev,
+			"Error callback already registered. Unregister the existing callback to register a new one.\n");
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	apart->error_cb.cb = cb;
+	apart->error_cb.priv = priv;
+
+	/*
+	 * Errors during configuration are logged even for the partitions
+	 * which are not requested. Such errors must be reported back to the
+	 * application when a valid callback is registered.
+	 */
+	if (apart->error_to_report) {
+		mutex_unlock(&apart->mlock);
+		schedule_work(&apart->adev->backtrack);
+		return ret;
+	}
+
+exit:
+	mutex_unlock(&apart->mlock);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aie_register_error_notification);
+
+/**
+ * aie_unregister_error_notification() - Unregister the callback for error
+ *					 notification.
+ * @dev: AIE partition device.
+ * @return: 0 for success, and negative value for failure.
+ */
+int aie_unregister_error_notification(struct device *dev)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	if (!dev)
+		return -EINVAL;
+
+	apart = container_of(dev, struct aie_partition, dev);
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return ret;
+	}
+
+	apart->error_cb.cb = NULL;
+	apart->error_cb.priv = NULL;
+
+	mutex_unlock(&apart->mlock);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(aie_unregister_error_notification);
+
+/**
+ * aie_get_errors() - get errors that has happened.
+ * @dev: AIE partition device.
+ * @return: struct pointer of type aie_errors.
+ *
+ * This API allocates and initializes data structures by parsing local
+ * bitmaps. Allocated data structure must be deallocated using
+ * aie_free_errors() function.
+ */
+struct aie_errors *aie_get_errors(struct device *dev)
+{
+	struct aie_partition *apart;
+	struct aie_errors *aie_errs;
+	struct aie_error *error;
+	u32 num_errs, count = 0;
+	int ret;
+
+	if (!dev)
+		return ERR_PTR(-EINVAL);
+
+	apart = container_of(dev, struct aie_partition, dev);
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return ERR_PTR(ret);
+	}
+
+	num_errs = aie_get_error_count(apart);
+	if (!num_errs) {
+		mutex_unlock(&apart->mlock);
+		return NULL;
+	}
+
+	aie_errs = kzalloc(sizeof(*aie_errs), GFP_KERNEL);
+	if (!aie_errs) {
+		mutex_unlock(&apart->mlock);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	error = kcalloc(num_errs, sizeof(*error), GFP_KERNEL);
+	if (!error) {
+		kfree(aie_errs);
+		mutex_unlock(&apart->mlock);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	count += aie_get_module_errors(apart, AIE_MEM_MOD, &error[count]);
+	count += aie_get_module_errors(apart, AIE_CORE_MOD, &error[count]);
+	count += aie_get_module_errors(apart, AIE_PL_MOD, &error[count]);
+
+	aie_errs->dev = dev;
+	aie_errs->errors = error;
+	aie_errs->num_err = count;
+
+	mutex_unlock(&apart->mlock);
+	return aie_errs;
+}
+EXPORT_SYMBOL_GPL(aie_get_errors);
+
+/**
+ * aie_get_error_categories() - Get the error categories. Error information
+ *				returned by aie_get_errors() could be
+ *				abstracted by classifying errors into various
+ *				categories. All DMA channel error are
+ *				classified as AIE_ERROR_CATEGORY_DMA, program
+ *				and data memory ECC errors are classified as
+ *				AIE_ERROR_CATEGORY_ECC, and so on.
+ * @aie_errs: AIE errors structure.
+ * @return: Bitmap of category of error events.
+ */
+u32 aie_get_error_categories(struct aie_errors *aie_errs)
+{
+	u32 e, ret = 0;
+
+	if (!aie_errs || !aie_errs->errors)
+		return 0;
+
+	for (e = 0; e < aie_errs->num_err; e++) {
+		struct aie_error *error = &aie_errs->errors[e];
+
+		ret |= BIT(error->category);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aie_get_error_categories);
+
+/**
+ * aie_get_error_string() - get error string corresponding an error.
+ * @aie_errs: pointer to an array of error structure.
+ * @aie_err: AIE error.
+ * @return: string corresponding to @aie_err.
+ */
+const char *aie_get_error_string(struct aie_errors *aie_errs,
+				 struct aie_error *aie_err)
+{
+	struct aie_partition *apart;
+	const struct aie_error_attr *err_attr;
+	u8 i, j;
+	int ret;
+
+	if (!aie_errs || !aie_errs->dev || !aie_err)
+		return ERR_PTR(-EINVAL);
+
+	apart = container_of(aie_errs->dev, struct aie_partition, dev);
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return ERR_PTR(ret);
+	}
+
+	if (aie_err->module == AIE_CORE_MOD)
+		err_attr = apart->adev->core_errors;
+	else if (aie_err->module == AIE_MEM_MOD)
+		err_attr = apart->adev->mem_errors;
+	else
+		err_attr = apart->adev->shim_errors;
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		for (j = 0; j < err_attr->err_category[i].num_events; j++) {
+			u8 event = err_attr->err_category[i].prop[j].event;
+
+			if (event != aie_err->error_id)
+				continue;
+
+			mutex_unlock(&apart->mlock);
+			return err_attr->err_category[i].prop[j].event_str;
+		}
+	}
+
+	mutex_unlock(&apart->mlock);
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(aie_get_error_string);
+
+/**
+ * aie_flush_errors() - Flush all pending errors.
+ * @dev: AIE partition device.
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function backtracks a given partition, updates local event status
+ * bitmaps and, invokes the registered callback function for any error event.
+ */
+int aie_flush_errors(struct device *dev)
+{
+	struct aie_partition *apart;
+
+	if (!dev)
+		return -EINVAL;
+
+	apart = container_of(dev, struct aie_partition, dev);
+	aie_part_backtrack(apart);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(aie_flush_errors);
+
+/**
+ * aie_free_errors() - Free allocated AIE error structure.
+ * @aie_errs: AIE error structure.
+ */
+void aie_free_errors(struct aie_errors *aie_errs)
+{
+	if (!aie_errs || !aie_errs->errors)
+		return;
+
+	kfree(aie_errs->errors);
+	kfree(aie_errs);
+}
+EXPORT_SYMBOL_GPL(aie_free_errors);
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-mem.c b/drivers/misc/xilinx-ai-engine/ai-engine-mem.c
new file mode 100644
index 000000000..20f96c97a
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-mem.c
@@ -0,0 +1,297 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device memory implementation
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/dma-buf.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/uaccess.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+#define aie_cal_reg_goffset(adev, loc, regoff) ({ \
+	struct aie_device *_adev = (adev); \
+	struct aie_location *_loc = &(loc); \
+	(_loc->col << _adev->col_shift) + \
+	(_loc->row << _adev->row_shift) + (regoff); \
+	})
+
+#define aie_cal_reg_pa(adev, loc, regoff) ({ \
+	struct aie_device *__adev = (adev); \
+	__adev->res->start + aie_cal_reg_goffset(__adev, loc, regoff); \
+	})
+
+static struct sg_table *
+aie_mem_map_dma_buf(struct dma_buf_attachment *attachment,
+		    enum dma_data_direction direction)
+{
+	/*
+	 * TODO: It is mandatory by DMA buf operation. It is used return
+	 * scatterlist table of an attachment. We don't have the implementation
+	 * for now. And thus it has empty implementation.
+	 */
+	(void)attachment;
+	(void)direction;
+	dev_warn(attachment->dev,
+		 "AI engine memory map dma buf is not implemented.\n");
+	return NULL;
+}
+
+static void aie_mem_unmap_dma_buf(struct dma_buf_attachment *attachment,
+				  struct sg_table *table,
+				  enum dma_data_direction direction)
+{
+	/*
+	 * TODO: It is mandatory by DMA buf operation. It is used deallocate
+	 * scatterlist table of an attachment. We don't have the implementation
+	 * for now. And thus it has empty implementation.
+	 */
+	(void)attachment;
+	(void)table;
+	(void)direction;
+	dev_warn(attachment->dev,
+		 "AI engine memory unmap dma buf is not implemented.\n");
+}
+
+static int aie_mem_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)
+{
+	struct aie_part_mem *pmem = dmabuf->priv;
+	struct aie_mem *mem = &pmem->mem;
+	struct aie_partition *apart = pmem->apart;
+	struct aie_location loc;
+	unsigned long addr = vma->vm_start;
+	unsigned long offset = vma->vm_pgoff * PAGE_SIZE, moffset = 0;
+	unsigned long remainder = vma->vm_end - addr;
+	size_t msize = mem->size;
+
+	if (remainder + offset > pmem->size)
+		return -EINVAL;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	for (loc.col = mem->range.start.col;
+	     loc.col < mem->range.start.col + mem->range.size.col; loc.col++) {
+		for (loc.row = mem->range.start.row;
+		     loc.row < mem->range.start.row + mem->range.size.row;
+		     loc.row++) {
+			unsigned long toffset, len;
+			phys_addr_t mempa;
+			int ret;
+
+			remainder = vma->vm_end - addr;
+			if (!remainder)
+				return 0;
+
+			if (moffset + msize < offset) {
+				moffset += msize;
+				continue;
+			}
+			/*
+			 * calculate offset within the tile memory.
+			 * offset is the offset to vma->start.
+			 * moffset is the tile memory start offset to
+			 * vma->start.
+			 */
+			toffset = offset - moffset;
+			len = msize - toffset;
+			if (len > remainder)
+				len = remainder;
+			mempa = aie_cal_reg_pa(apart->adev, loc,
+					       toffset + mem->offset);
+
+			ret = remap_pfn_range(vma, addr, mempa >> PAGE_SHIFT,
+					      len, vma->vm_page_prot);
+			if (ret) {
+				dev_err(&apart->dev,
+					"failed to mmap (%u,%u)memory, remap failed, 0x%pa, 0x%lx.\n",
+					loc.col, loc.row, &mempa, len);
+				return ret;
+			}
+			addr += len;
+			offset += len;
+			moffset += msize;
+		}
+	}
+	return 0;
+}
+
+static void aie_mem_dmabuf_release(struct dma_buf *dmabuf)
+{
+	struct aie_part_mem *pmem = dmabuf->priv;
+
+	pmem->dbuf = NULL;
+}
+
+static const struct dma_buf_ops aie_mem_dma_buf_ops = {
+	.map_dma_buf = aie_mem_map_dma_buf,
+	.unmap_dma_buf = aie_mem_unmap_dma_buf,
+	.mmap = aie_mem_mmap,
+	.release = aie_mem_dmabuf_release,
+};
+
+/**
+ * aie_mem_create_dmabuf() - creates DMA buffer for AI engine partition
+ *			     memories
+ * @apart: AI engine partition
+ * @pmem: pointer to the partition memory information
+ * @mem: pointer to where it store the memory information and DMA buf file
+ *	 descriptor for user.
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will create DMA buffer for the AI engine partition memory
+ * and will store the DMA buffer file descriptor and memory information in
+ * @mem.
+ */
+static int aie_mem_create_dmabuf(struct aie_partition *apart,
+				 struct aie_part_mem *pmem,
+				 struct aie_mem *mem)
+{
+	struct dma_buf *dmabuf;
+	int ret;
+
+	if (!PAGE_ALIGNED(pmem->mem.size)) {
+		dev_warn(&apart->dev,
+			 "no dmabuf for mem(0x%zx, 0x%zx), not aligned with page size.\n",
+			 pmem->mem.offset, pmem->mem.size);
+		return -EINVAL;
+	}
+
+	dmabuf = pmem->dbuf;
+	if (!dmabuf) {
+		DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
+
+		exp_info.ops = &aie_mem_dma_buf_ops;
+		exp_info.size = pmem->size;
+		exp_info.flags = O_RDWR;
+		exp_info.priv = pmem;
+
+		dmabuf = dma_buf_export(&exp_info);
+		if (IS_ERR(dmabuf))
+			return PTR_ERR(dmabuf);
+
+		pmem->dbuf = dmabuf;
+	}
+
+	ret = dma_buf_fd(dmabuf, O_CLOEXEC);
+	if (ret < 0) {
+		dev_err(&apart->dev,
+			"dmabuf creation failed, failed to get fd.\n");
+		return ret;
+	}
+	memcpy(mem, &pmem->mem, sizeof(*mem));
+	mem->fd = ret;
+
+	return 0;
+}
+
+/**
+ * aie_mem_get_info() - get AI engine memories information
+ * @apart: AI engine partition
+ * @arg: argument from user to enquire AI engine partition memory information
+ * @return: 0 for success, and negative value for failure
+ *
+ * This function will get the memories information for the specified AI engine
+ * partition. It will create DMA buf file descriptors for the memories and
+ * return the DMA buf file descriptors to users.
+ * It will create a DMA buffer per type of memories.
+ * e.g. There will be a DMA buffer for all the tile program memories in the
+ * partition, and another DMA buffer for all the tile data memories in the
+ * partition.
+ * User can first pass num_mems as 0 in the @arg to enquire for how many types
+ * of memories in this AI engine partition. And then, user can allocate memory
+ * to keep the information for different types of memories, and then use the
+ * same enqury with non-zero num_mems and none NULL pointer to ask for the
+ * details of the information of all the types of memories in the AI engine
+ * partition.
+ */
+int aie_mem_get_info(struct aie_partition *apart, unsigned long arg)
+{
+	struct aie_mem_args margs;
+	struct aie_mem *mems;
+	unsigned int num_mems, i;
+	int ret;
+
+	if (copy_from_user(&margs, (void __user *)arg, sizeof(margs)))
+		return -EFAULT;
+
+	num_mems = apart->adev->ops->get_mem_info(&apart->range, NULL);
+	if (num_mems <= 0)
+		return -EINVAL;
+
+	if (!margs.num_mems) {
+		struct aie_mem_args __user *umargs_ptr = (void __user *)arg;
+
+		/* This enquiry is to get the number of types of memories. */
+		if (copy_to_user((void __user *)&umargs_ptr->num_mems,
+				 &num_mems, sizeof(num_mems)))
+			return -EFAULT;
+		return 0;
+	}
+
+	if (num_mems != margs.num_mems) {
+		dev_err(&apart->dev,
+			"failed to get mem info, invalid num of mems %d,%d.\n",
+			num_mems, margs.num_mems);
+		return -EINVAL;
+	}
+	if (!margs.mems) {
+		dev_err(&apart->dev,
+			"failed to get mem info, mems pointer is NULL.\n");
+		return -EINVAL;
+	}
+
+	mems = kcalloc(num_mems, sizeof(*mems), GFP_KERNEL);
+	if (!mems)
+		return -ENOMEM;
+
+	/*
+	 * Create DMA buffer for the memories.
+	 * Each type of memory in the partition has its own DMA buf.
+	 */
+	for (i = 0; i < num_mems; i++) {
+		ret = aie_mem_create_dmabuf(apart, &apart->pmems[i], &mems[i]);
+		if (ret)
+			break;
+	}
+	if (!ret) {
+		if (copy_to_user((void __user *)margs.mems, mems,
+				 num_mems * sizeof(mems[0])))
+			ret = -EFAULT;
+	}
+
+	if (ret) {
+		for (i = 0; i < num_mems; i++) {
+			if (mems[i].fd)
+				put_unused_fd(mems[i].fd);
+		}
+	}
+
+	kfree(mems);
+	return ret;
+}
+
+/**
+ * aie_part_has_mem_mmapped() - check if memories in the partition are mapped
+ * @apart: AI engine partition
+ * @return: return true if there are memories mmaped, false otherwise.
+ *
+ * This function checks if there are memories in the partition mmapped in the
+ * partition.
+ */
+bool aie_part_has_mem_mmapped(struct aie_partition *apart)
+{
+	unsigned int num_mems, i;
+
+	num_mems = apart->adev->ops->get_mem_info(&apart->range, NULL);
+	if (!num_mems)
+		return false;
+
+	for (i = 0; i < num_mems; i++) {
+		if (apart->pmems[i].dbuf)
+			return true;
+	}
+	return false;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-part.c b/drivers/misc/xilinx-ai-engine/ai-engine-part.c
new file mode 100644
index 000000000..c06299fb4
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-part.c
@@ -0,0 +1,1198 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine partition driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/cdev.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/fs.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/mmu_context.h>
+#include <linux/mutex.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/uio.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+/**
+ * aie_cal_loc() - calculate tile location from register offset to the AI
+ *		   engine device
+ * @adev: AI engine device
+ * @loc: memory pointer to restore returning location information
+ * @regoff: tile internal register offset
+ *
+ * This function returns the tile location.
+ */
+static void aie_cal_loc(struct aie_device *adev,
+			struct aie_location *loc, u64 regoff)
+{
+	loc->col = (u32)aie_tile_reg_field_get(aie_col_mask(adev),
+					       adev->col_shift, regoff);
+	loc->row = (u32)aie_tile_reg_field_get(aie_row_mask(adev),
+					       adev->row_shift, regoff);
+}
+
+/**
+ * aie_part_reg_validation() - validate AI engine partition register access
+ * @apart: AI engine partition
+ * @offset: AI engine register offset
+ * @len: len of data to write/read
+ * @is_write: is the access to write to register
+ * @return: 0 for success, or negative value for failure.
+ *
+ * This function validate if the register to access is within the AI engine
+ * partition. If it is write access, if the register is writable by user.
+ */
+static int aie_part_reg_validation(struct aie_partition *apart, size_t offset,
+				   size_t len, u8 is_write)
+{
+	struct aie_device *adev;
+	u32 regend32, ttype;
+	u64 regoff, regend64;
+	struct aie_location loc;
+	unsigned int i;
+
+	adev = apart->adev;
+	if (offset % sizeof(u32)) {
+		dev_err(&apart->dev,
+			"Invalid reg off(0x%zx), not 32bit aligned.\n",
+			offset);
+		return -EINVAL;
+	}
+
+	if (len % sizeof(u32)) {
+		dev_err(&apart->dev, "Invalid reg operation len %zu.\n", len);
+		return -EINVAL;
+	}
+
+	regoff = aie_cal_tile_reg(adev, offset);
+	regend64 = regoff + len - 1;
+	if (regend64 >= BIT_ULL(adev->row_shift)) {
+		dev_err(&apart->dev,
+			"Invalid reg operation len %zu.\n", len);
+		return -EINVAL;
+	}
+
+	aie_cal_loc(adev, &loc, offset);
+	if (aie_validate_location(apart, loc)) {
+		dev_err(&apart->dev,
+			"Invalid (%d,%d) out of part(%d,%d),(%d,%d)\n",
+			loc.col, loc.row,
+			apart->range.start.col, apart->range.start.row,
+			apart->range.size.col, apart->range.size.row);
+		return -EINVAL;
+	}
+
+	/*
+	 * We check if a tile is gated before trying to access the tile.
+	 * As we mmap() the registers as read only to enable faster status
+	 * enquiry, and mmap() memories as write/read to faster memory access,
+	 * user can still access the clock gated tiles from userspace by
+	 * accessing the mmapped space.
+	 * Accessing the gated tiles can cause decode error. With PDI flow,
+	 * the PDI sets up the SHIM NOC AXI MM to only generate AI engine error
+	 * even instead of generating the NSU error. but for non PDI flow, as
+	 * the AXI MM register are protected register, until we have EEMI API
+	 * to update the AXI MM register, access the gated tiles can cause NSU
+	 * errors.
+	 * TODO: To solve this, we need to either request EEMI to configure
+	 * AXI MM or split the mmapped space into tiles based lists.
+	 */
+	if (!aie_part_check_clk_enable_loc(apart, &loc)) {
+		dev_err(&apart->dev,
+			"Tile(%u,%d) is gated.\n", loc.col, loc.row);
+		return -EINVAL;
+	}
+
+	if (!is_write)
+		return 0;
+
+	regend32 = lower_32_bits(regend64);
+	ttype = adev->ops->get_tile_type(&loc);
+	for (i = 0; i < adev->num_kernel_regs; i++) {
+		const struct aie_tile_regs *regs;
+		u32 rttype, writable;
+
+		regs = &adev->kernel_regs[i];
+		rttype = (regs->attribute & AIE_REGS_ATTR_TILE_TYPE_MASK) >>
+			 AIE_REGS_ATTR_TILE_TYPE_SHIFT;
+		writable = (regs->attribute & AIE_REGS_ATTR_PERM_MASK) >>
+			   AIE_REGS_ATTR_PERM_SHIFT;
+		if (!(BIT(ttype) & rttype))
+			continue;
+		if ((regoff >= regs->soff && regoff <= regs->eoff) ||
+		    (regend32 >= regs->soff && regend32 <= regs->eoff)) {
+			if (!writable) {
+				dev_err(&apart->dev,
+					"reg 0x%zx,0x%zx not writable.\n",
+					offset, len);
+				return -EINVAL;
+			}
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_write_register() - AI engine partition write register
+ * @apart: AI engine partition
+ * @offset: AI engine register offset
+ * @len: len of data to write
+ * @data: data to write
+ * @mask: mask, if it is non 0, it is mask write.
+ * @return: number of bytes write for success, or negative value for failure.
+ *
+ * This function writes data to the specified registers.
+ * If the mask is non 0, it is mask write.
+ */
+static int aie_part_write_register(struct aie_partition *apart, size_t offset,
+				   size_t len, void *data, u32 mask)
+{
+	u32 i;
+	int ret;
+	void __iomem *va;
+
+	if (mask && len > sizeof(u32)) {
+		/* For mask write, only allow 32bit. */
+		dev_err(&apart->dev,
+			"failed mask write, len is more that 32bit.\n");
+		return -EINVAL;
+	}
+
+	/* offset is expected to be relative to the start of the partition */
+	offset += aie_cal_regoff(apart->adev, apart->range.start, 0);
+	ret = aie_part_reg_validation(apart, offset, len, 1);
+	if (ret < 0) {
+		dev_err(&apart->dev, "failed to write to 0x%zx,0x%zx.\n",
+			offset, len);
+		return ret;
+	}
+
+	va = apart->adev->base + offset;
+	if (!mask) {
+		/*
+		 * TODO: use the burst mode to improve performance when len
+		 * is more than 4. Additional checks have to be made to ensure
+		 * the destination address is 128 bit aligned when burst mode
+		 * is used.
+		 */
+		for (i = 0; i < len; i = i + 4)
+			iowrite32(*((u32 *)(data + i)), va + i);
+	} else {
+		u32 val = ioread32(va);
+
+		val &= ~mask;
+		val |= *((u32 *)data) & mask;
+		iowrite32(val, va);
+	}
+
+	return (int)len;
+}
+
+/**
+ * aie_part_read_register() - AI engine partition read register
+ * @apart: AI engine partition
+ * @offset: AI engine register offset
+ * @len: len of data to read
+ * @data: pointer to the memory to store the read data
+ * @return: number of bytes read for success, or negative value for failure.
+ *
+ * This function reads data from the specified registers.
+ */
+static int aie_part_read_register(struct aie_partition *apart, size_t offset,
+				  size_t len, void *data)
+{
+	void __iomem *va;
+	int ret;
+
+	/* offset is expected to be relative to the start of the partition */
+	offset += aie_cal_regoff(apart->adev, apart->range.start, 0);
+	ret = aie_part_reg_validation(apart, offset, len, 0);
+	if (ret) {
+		dev_err(&apart->dev, "Invalid read request 0x%zx,0x%zx.\n",
+			offset, len);
+		return -EINVAL;
+	}
+
+	va = apart->adev->base + offset;
+	if (len == 4)
+		*((u32 *)data) = ioread32(va);
+	else
+		memcpy_fromio(data, va, len);
+
+	return (int)len;
+}
+
+/**
+ * aie_part_block_set() - AI Engine partition block set registers
+ * @apart: AI engine partition
+ * @args: regestier access arguments
+ * @return: 0 for success, and negative value for failure
+ */
+static int aie_part_block_set(struct aie_partition *apart,
+			      struct aie_reg_args *args)
+{
+	u32 i;
+	int ret;
+
+	for (i = 0; i < args->len; i++) {
+		size_t offset = (size_t)args->offset;
+
+		ret = aie_part_write_register(apart, offset + i * 4,
+					      sizeof(args->val), &args->val,
+					      args->mask);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_pin_user_region() - pin user pages for access
+ * @apart: AI engine partition
+ * @region: user space region to pin. Includes virtual address and size of the
+ *	    user space buffer.
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function pins all the pages of a user space buffer.
+ */
+static int aie_part_pin_user_region(struct aie_partition *apart,
+				    struct aie_part_pinned_region *region)
+{
+	int ret, npages;
+	unsigned long first, last;
+	struct page **pages;
+
+	first = (region->user_addr & PAGE_MASK) >> PAGE_SHIFT;
+	last = ((region->user_addr + region->len - 1) & PAGE_MASK) >>
+		PAGE_SHIFT;
+	npages = last - first + 1;
+
+	pages = kcalloc(npages, sizeof(struct page *), GFP_KERNEL);
+	if (!pages)
+		return -ENOMEM;
+
+	ret = pin_user_pages_fast(region->user_addr, npages, 0, pages);
+	if (ret < 0) {
+		kfree(pages);
+		dev_err(&apart->dev, "Unable to pin user pages\n");
+		return ret;
+	} else if (ret != npages) {
+		unpin_user_pages(pages, ret);
+		kfree(pages);
+		dev_err(&apart->dev, "Unable to pin all user pages\n");
+		return -EFAULT;
+	}
+
+	region->pages = pages;
+	region->npages = npages;
+
+	return 0;
+}
+
+/**
+ * aie_part_unpin_user_region() - unpin user pages
+ * @region: user space region to unpin.
+ *
+ * This function unpins all the pages of a user space buffer. User region passed
+ * to this api must be pinned using aie_part_pin_user_region()
+ */
+static void aie_part_unpin_user_region(struct aie_part_pinned_region *region)
+{
+	unpin_user_pages(region->pages, region->npages);
+	kfree(region->pages);
+}
+
+/**
+ * aie_part_access_regs() - AI engine partition registers access
+ * @apart: AI engine partition
+ * @num_reqs: number of access requests
+ * @reqs: array of registers access
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function executes AI engine partition register access requests.
+ */
+static int aie_part_access_regs(struct aie_partition *apart, u32 num_reqs,
+				struct aie_reg_args *reqs)
+{
+	u32 i;
+
+	for (i = 0; i < num_reqs; i++) {
+		struct aie_reg_args *args = &reqs[i];
+		int ret;
+
+		switch (args->op) {
+		case AIE_REG_WRITE:
+		{
+			ret = aie_part_write_register(apart,
+						      (size_t)args->offset,
+						      sizeof(args->val),
+						      &args->val, args->mask);
+			break;
+		}
+		case AIE_REG_BLOCKWRITE:
+		{
+			struct aie_part_pinned_region region;
+
+			region.user_addr = args->dataptr;
+			region.len = args->len * sizeof(u32);
+			ret = aie_part_pin_user_region(apart, &region);
+			if (ret)
+				break;
+
+			ret = aie_part_write_register(apart,
+						      (size_t)args->offset,
+						      sizeof(u32) * args->len,
+						      (void *)args->dataptr,
+						      args->mask);
+			aie_part_unpin_user_region(&region);
+			break;
+		}
+		case AIE_REG_BLOCKSET:
+		{
+			ret = aie_part_block_set(apart, args);
+			break;
+		}
+		default:
+			dev_err(&apart->dev,
+				"Invalid register command type: %u.\n",
+				args->op);
+			return -EINVAL;
+		}
+
+		if (ret < 0) {
+			dev_err(&apart->dev, "reg op %u failed: 0x%llx.\n",
+				args->op, args->offset);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_execute_transaction_from_user() - AI engine configure registers
+ * @apart: AI engine partition
+ * @user_args: arguments passed by user.
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function executes AI engine register access requests that are part of a
+ * buffer that is populated and passed by user.
+ */
+static int aie_part_execute_transaction_from_user(struct aie_partition *apart,
+						  void __user *user_args)
+{
+	long ret;
+	struct aie_txn_inst txn_inst;
+	struct aie_part_pinned_region region;
+
+	if (copy_from_user(&txn_inst, user_args, sizeof(txn_inst)))
+		return -EFAULT;
+
+	region.user_addr = txn_inst.cmdsptr;
+	region.len = txn_inst.num_cmds * sizeof(struct aie_reg_args);
+	ret = aie_part_pin_user_region(apart, &region);
+	if (ret)
+		return ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		aie_part_unpin_user_region(&region);
+		return ret;
+	}
+
+	ret = aie_part_access_regs(apart, txn_inst.num_cmds,
+				   (struct aie_reg_args *)region.user_addr);
+
+	mutex_unlock(&apart->mlock);
+
+	aie_part_unpin_user_region(&region);
+	return ret;
+}
+
+/**
+ * aie_part_create_event_bitmap() - create event bitmap for all modules in a
+ *				    given partition.
+ * @apart: AI engine partition
+ * @return: 0 for success, and negative value for failure.
+ */
+static int aie_part_create_event_bitmap(struct aie_partition *apart)
+{
+	struct aie_range range = apart->range;
+	u32 bitmap_sz;
+	u32 num_aie_module = range.size.col * (range.size.row - 1);
+	int ret;
+
+	bitmap_sz = num_aie_module * apart->adev->core_events->num_events;
+	ret = aie_resource_initialize(&apart->core_event_status, bitmap_sz);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize event status resource.\n");
+		return -ENOMEM;
+	}
+
+	bitmap_sz = num_aie_module * apart->adev->mem_events->num_events;
+	ret = aie_resource_initialize(&apart->mem_event_status, bitmap_sz);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize event status resource.\n");
+		return -ENOMEM;
+	}
+
+	bitmap_sz = range.size.col * apart->adev->pl_events->num_events;
+	ret = aie_resource_initialize(&apart->pl_event_status, bitmap_sz);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize event status resource.\n");
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+/**
+ * aie_part_create_l2_bitmap() - create bitmaps to record mask and status
+ *				 values for level 2 interrupt controllers.
+ * @apart: AI engine partition
+ * @return: 0 for success, and negative value for failure.
+ */
+static int aie_part_create_l2_bitmap(struct aie_partition *apart)
+{
+	struct aie_location loc;
+	u8 num_l2_ctrls = 0;
+	int ret;
+
+	loc.row = 0;
+	for (loc.col = apart->range.start.col;
+	     loc.col < apart->range.start.col + apart->range.size.col;
+	     loc.col++) {
+		u32 ttype = apart->adev->ops->get_tile_type(&loc);
+
+		if (ttype == AIE_TILE_TYPE_SHIMNOC)
+			num_l2_ctrls++;
+	}
+
+	ret = aie_resource_initialize(&apart->l2_mask, num_l2_ctrls *
+				      AIE_INTR_L2_CTRL_MASK_WIDTH);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize l2 mask resource.\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_release_event_bitmap() - Deallocates event bitmap for all modules
+ *				     in a given partition.
+ * @apart: AI engine partition
+ * @return: 0 for success, and negative value for failure.
+ */
+static void aie_part_release_event_bitmap(struct aie_partition *apart)
+{
+	aie_resource_uninitialize(&apart->core_event_status);
+	aie_resource_uninitialize(&apart->mem_event_status);
+	aie_resource_uninitialize(&apart->pl_event_status);
+}
+
+static int aie_part_release(struct inode *inode, struct file *filp)
+{
+	struct aie_partition *apart = filp->private_data;
+	int ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	aie_part_release_dmabufs(apart);
+	aie_part_clean(apart);
+
+	apart->error_cb.cb = NULL;
+	apart->error_cb.priv = NULL;
+	apart->status = 0;
+	apart->error_to_report = 0;
+
+	aie_part_clear_cached_events(apart);
+	aie_resource_clear_all(&apart->l2_mask);
+
+	aie_part_rscmgr_reset(apart);
+
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+static ssize_t aie_part_write_iter(struct kiocb *iocb, struct iov_iter *from)
+{
+	struct file *filp = iocb->ki_filp;
+	struct aie_partition *apart = filp->private_data;
+	size_t len = iov_iter_count(from);
+	loff_t offset = iocb->ki_pos;
+	void *buf;
+	int ret;
+
+	buf = kzalloc(len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+	if (!copy_from_iter_full(buf, len, from)) {
+		kfree(buf);
+		return -EFAULT;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(buf);
+		return ret;
+	}
+
+	ret = aie_part_write_register(apart, (size_t)offset, len, buf, 0);
+	mutex_unlock(&apart->mlock);
+	kfree(buf);
+
+	return ret;
+}
+
+static ssize_t aie_part_read_iter(struct kiocb *iocb, struct iov_iter *to)
+{
+	struct file *filp = iocb->ki_filp;
+	struct aie_partition *apart = filp->private_data;
+	size_t len = iov_iter_count(to);
+	loff_t offset = iocb->ki_pos;
+	void *buf;
+	int ret;
+
+	buf = kzalloc(len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(buf);
+		return ret;
+	}
+
+	ret = aie_part_read_register(apart, (size_t)offset, len, buf);
+	mutex_unlock(&apart->mlock);
+	if (ret > 0) {
+		if (copy_to_iter(buf, ret, to) != len) {
+			dev_err(&apart->dev, "Failed to copy to read iter.\n");
+			ret = -EFAULT;
+		}
+	}
+	kfree(buf);
+
+	return ret;
+}
+
+static const struct vm_operations_struct aie_part_physical_vm_ops = {
+#ifdef CONFIG_HAVE_IOREMAP_PROT
+	.access = generic_access_phys,
+#endif
+};
+
+static int aie_part_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct aie_partition *apart = fp->private_data;
+	struct aie_device *adev = apart->adev;
+	unsigned long offset = vma->vm_pgoff * PAGE_SIZE;
+	phys_addr_t addr;
+	size_t size;
+
+	if (vma->vm_end < vma->vm_start)
+		return -EINVAL;
+	/* Only allow userspace directly read registers */
+	if (vma->vm_flags & VM_WRITE) {
+		dev_err(&apart->dev, "%s: do not support writable mmap.\n",
+			__func__);
+		return -EINVAL;
+	}
+	vma->vm_private_data = apart;
+	vma->vm_ops = &aie_part_physical_vm_ops;
+	size = apart->range.size.col << adev->col_shift;
+	if ((vma->vm_end - vma->vm_start) > (size - offset)) {
+		dev_err(&apart->dev,
+			"%s: size exceed.\n", __func__);
+		return -EINVAL;
+	}
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	/* Calculate the partition address */
+	addr = adev->res->start;
+	addr += (phys_addr_t)apart->range.start.col << adev->col_shift;
+	addr += (phys_addr_t)apart->range.start.row << adev->row_shift;
+	addr += offset;
+	return remap_pfn_range(vma,
+			       vma->vm_start,
+			       addr >> PAGE_SHIFT,
+			       vma->vm_end - vma->vm_start,
+			       vma->vm_page_prot);
+}
+
+static long aie_part_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct aie_partition *apart = fp->private_data;
+	void __user *argp = (void __user *)arg;
+	long ret;
+
+	switch (cmd) {
+	case AIE_REG_IOCTL:
+	{
+		struct aie_reg_args raccess;
+
+		if (copy_from_user(&raccess, argp, sizeof(raccess)))
+			return -EFAULT;
+
+		ret = mutex_lock_interruptible(&apart->mlock);
+		if (ret)
+			return ret;
+
+		ret = aie_part_access_regs(apart, 1, &raccess);
+		mutex_unlock(&apart->mlock);
+		break;
+	}
+	case AIE_GET_MEM_IOCTL:
+		return aie_mem_get_info(apart, arg);
+	case AIE_ATTACH_DMABUF_IOCTL:
+		return aie_part_attach_dmabuf_req(apart, argp);
+	case AIE_DETACH_DMABUF_IOCTL:
+		return aie_part_detach_dmabuf_req(apart, argp);
+	case AIE_SET_SHIMDMA_BD_IOCTL:
+		return aie_part_set_bd(apart, argp);
+	case AIE_SET_SHIMDMA_DMABUF_BD_IOCTL:
+		return aie_part_set_dmabuf_bd(apart, argp);
+	case AIE_REQUEST_TILES_IOCTL:
+		return aie_part_request_tiles_from_user(apart, argp);
+	case AIE_RELEASE_TILES_IOCTL:
+		return aie_part_release_tiles_from_user(apart, argp);
+	case AIE_TRANSACTION_IOCTL:
+		return aie_part_execute_transaction_from_user(apart, argp);
+	case AIE_SET_FREQUENCY_IOCTL:
+	{
+		u64 freq;
+
+		if (copy_from_user(&freq, argp, sizeof(freq)))
+			return -EFAULT;
+
+		ret = mutex_lock_interruptible(&apart->mlock);
+		if (ret)
+			return ret;
+		ret = aie_part_set_freq(apart, freq);
+		mutex_unlock(&apart->mlock);
+		return ret;
+	}
+	case AIE_GET_FREQUENCY_IOCTL:
+	{
+		u64 freq;
+
+		ret = mutex_lock_interruptible(&apart->mlock);
+		if (ret)
+			return ret;
+		ret = aie_part_get_running_freq(apart, &freq);
+		mutex_unlock(&apart->mlock);
+
+		if (!ret) {
+			if (copy_to_user(argp, &freq, sizeof(freq)))
+				return -EFAULT;
+		}
+		return ret;
+	}
+	case AIE_RSC_REQ_IOCTL:
+		return aie_part_rscmgr_rsc_req(apart, argp);
+	case AIE_RSC_REQ_SPECIFIC_IOCTL:
+		return aie_part_rscmgr_rsc_req_specific(apart, argp);
+	case AIE_RSC_RELEASE_IOCTL:
+		return aie_part_rscmgr_rsc_release(apart, argp);
+	case AIE_RSC_FREE_IOCTL:
+		return aie_part_rscmgr_rsc_free(apart, argp);
+	case AIE_RSC_CHECK_AVAIL_IOCTL:
+		return aie_part_rscmgr_rsc_check_avail(apart, argp);
+	case AIE_RSC_GET_COMMON_BROADCAST_IOCTL:
+		return aie_part_rscmgr_get_broadcast(apart, argp);
+	case AIE_RSC_GET_STAT_IOCTL:
+		return aie_part_rscmgr_get_statistics(apart, argp);
+	default:
+		dev_err(&apart->dev, "Invalid ioctl command %u.\n", cmd);
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+const struct file_operations aie_part_fops = {
+	.owner		= THIS_MODULE,
+	.release	= aie_part_release,
+	.read_iter	= aie_part_read_iter,
+	.write_iter	= aie_part_write_iter,
+	.mmap		= aie_part_mmap,
+	.unlocked_ioctl	= aie_part_ioctl,
+};
+
+/**
+ * aie_part_open() - open the AI engine partition instance to get it ready to
+ *		     be used.
+ * @apart: AI engine partition instance pointer
+ * @rsc_metadata: pointer to static resource metadata
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will make the AI engine partition instance ready to use. It
+ * should be called when the partition is requested.
+ */
+int aie_part_open(struct aie_partition *apart, void *rsc_metadata)
+{
+	int ret;
+
+	/* scan to setup the initial clock state for tiles */
+	ret = aie_part_scan_clk_state(apart);
+	if (ret)
+		return ret;
+
+	/* Sets bitmaps of statically allocated resources */
+	if (rsc_metadata) {
+		ret = aie_part_rscmgr_set_static(apart,
+						 rsc_metadata);
+		if (ret)
+			return ret;
+	}
+
+	/* preallocate memory pool for storing dmabuf descriptors */
+	return aie_part_prealloc_dbufs_cache(apart);
+}
+
+/**
+ * aie_tile_release_device() - release an AI engine tile instance
+ * @dev: AI engine tile device
+ *
+ * It will be called by device driver core when no one holds a valid
+ * pointer to @dev anymore.
+ */
+static void aie_tile_release_device(struct device *dev)
+{
+	(void)dev;
+}
+
+/**
+ * aie_part_release_device() - release an AI engine partition instance
+ * @dev: AI engine partition device
+ *
+ * It will be called by device driver core when no one holds a valid
+ * pointer to @dev anymore.
+ */
+static void aie_part_release_device(struct device *dev)
+{
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_device *adev = apart->adev;
+	int ret;
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret) {
+		dev_warn(&apart->dev,
+			 "getting adev->mlock is interrupted by signal\n");
+	}
+
+	aie_resource_put_region(&adev->cols_res, apart->range.start.col,
+				apart->range.size.col);
+	aie_part_release_event_bitmap(apart);
+	aie_resource_uninitialize(&apart->l2_mask);
+	list_del(&apart->node);
+	mutex_unlock(&adev->mlock);
+	aie_resource_uninitialize(&apart->cores_clk_state);
+	aie_part_rscmgr_finish(apart);
+}
+
+/**
+ * aie_part_create_mems_info() - creates array to store the AI engine partition
+ *				 different memories types information
+ * @apart: AI engine partition
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will create array to store the information of different
+ * memories types in the partition. This array is stored in @apart->pmems.
+ */
+static int aie_part_create_mems_info(struct aie_partition *apart)
+{
+	unsigned int i, num_mems;
+
+	num_mems = apart->adev->ops->get_mem_info(&apart->range, NULL);
+	if (!num_mems)
+		return 0;
+
+	apart->pmems = devm_kcalloc(&apart->dev, num_mems,
+				    sizeof(struct aie_part_mem),
+				    GFP_KERNEL);
+	if (!apart->pmems)
+		return -ENOMEM;
+
+	apart->adev->ops->get_mem_info(&apart->range, apart->pmems);
+	for (i = 0; i < num_mems; i++) {
+		struct aie_mem *mem = &apart->pmems[i].mem;
+
+		apart->pmems[i].apart = apart;
+		apart->pmems[i].size = mem->size *
+				       mem->range.size.col *
+				       mem->range.size.row;
+	}
+	return 0;
+}
+
+/**
+ * aie_create_tiles() - create AI engine tile devices
+ * @apart: AI engine partition
+ * @return: 0 for success, error code on failure
+ *
+ * This function creates AI engine child tile devices for a given partition.
+ */
+static int aie_create_tiles(struct aie_partition *apart)
+{
+	struct aie_tile *atile;
+	u32 row, col, numtiles;
+	int ret = 0;
+
+	numtiles = apart->range.size.col * apart->range.size.row;
+	atile = devm_kzalloc(&apart->dev, numtiles * sizeof(struct aie_tile),
+			     GFP_KERNEL);
+	if (!atile)
+		return -ENOMEM;
+
+	apart->atiles = atile;
+	for (col = 0; col < apart->range.size.col; col++) {
+		for (row = 0; row < apart->range.size.row; row++) {
+			struct device *tdev = &atile->dev;
+			char tdevname[10];
+
+			atile->apart = apart;
+			atile->loc.col = apart->range.start.col + col;
+			atile->loc.row = apart->range.start.row + row;
+			device_initialize(tdev);
+			tdev->parent = &apart->dev;
+			dev_set_drvdata(tdev, atile);
+			snprintf(tdevname, sizeof(tdevname) - 1, "%d_%d",
+				 apart->range.start.col + col,
+				 apart->range.start.row + row);
+			dev_set_name(tdev, tdevname);
+			tdev->release = aie_tile_release_device;
+			ret = device_add(tdev);
+			if (ret) {
+				dev_err(tdev, "tile device_add failed: %d\n",
+					ret);
+				put_device(tdev);
+				return ret;
+			}
+			ret = aie_tile_sysfs_create_entries(atile);
+			if (ret) {
+				dev_err(tdev, "failed to create tile sysfs: %d\n",
+					ret);
+				device_del(tdev);
+				put_device(tdev);
+				return ret;
+			}
+			atile++;
+		}
+	}
+	return ret;
+}
+
+/**
+ * aie_create_partition() - create AI engine partition instance
+ * @adev: AI engine device
+ * @range: AI engine partition range to check. A range describes a group
+ *	   of AI engine tiles.
+ * @return: created AI engine partition pointer for success, and PTR_ERR
+ *	    for failure.
+ *
+ * This function creates an AI engine partition instance.
+ * It creates AI engine partition, the AI engine partition device and
+ * the AI engine partition character device.
+ */
+static struct aie_partition *aie_create_partition(struct aie_device *adev,
+						  struct aie_range *range)
+{
+	struct aie_partition *apart;
+	struct device *dev;
+	char devname[32];
+	int ret;
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret)
+		return ERR_PTR(ret);
+
+	ret = aie_resource_check_region(&adev->cols_res, range->start.col,
+					range->size.col);
+	if (ret != range->start.col) {
+		dev_err(&adev->dev, "invalid partition (%u,%u)(%u,%u).\n",
+			range->start.col, range->start.row,
+			range->size.col, range->size.row);
+		mutex_unlock(&adev->mlock);
+		return ERR_PTR(-EINVAL);
+	}
+	ret = aie_resource_get_region(&adev->cols_res, range->start.col,
+				      range->size.col);
+	if (ret != range->start.col) {
+		dev_err(&adev->dev, "failed to get partition (%u,%u)(%u,%u).\n",
+			range->start.col, range->start.row,
+			range->size.col, range->size.row);
+		mutex_unlock(&adev->mlock);
+		return ERR_PTR(-EFAULT);
+	}
+	mutex_unlock(&adev->mlock);
+
+	apart = devm_kzalloc(&adev->dev, sizeof(*apart), GFP_KERNEL);
+	if (!apart)
+		return ERR_PTR(-ENOMEM);
+
+	apart->adev = adev;
+	INIT_LIST_HEAD(&apart->dbufs);
+	memcpy(&apart->range, range, sizeof(*range));
+	mutex_init(&apart->mlock);
+
+	/* Create AI engine partition device */
+	dev = &apart->dev;
+	device_initialize(dev);
+	dev->parent = &adev->dev;
+	dev->class = aie_class;
+	dev_set_drvdata(dev, apart);
+	snprintf(devname, sizeof(devname) - 1, "aiepart_%d_%d",
+		 apart->range.start.col, apart->range.size.col);
+	dev_set_name(dev, devname);
+	/* We can now rely on the release function for cleanup */
+	dev->release = aie_part_release_device;
+	ret = device_add(dev);
+	if (ret) {
+		dev_err(dev, "device_add failed: %d\n", ret);
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	/* Set up the DMA mask */
+	dev->coherent_dma_mask = DMA_BIT_MASK(48);
+	dev->dma_mask = &dev->coherent_dma_mask;
+
+	/* Create AI Engine tile devices */
+	ret = aie_create_tiles(apart);
+	if (ret) {
+		dev_err(dev, "Failed to create tile devices.\n");
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	/*
+	 * Create array to keep the information of the different types of tile
+	 * memories information of the AI engine partition.
+	 */
+	ret = aie_part_create_mems_info(apart);
+	if (ret) {
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	ret = adev->ops->init_part_clk_state(apart);
+	if (ret) {
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	/*
+	 * Create bitmap to record event status for each module in a
+	 * partition
+	 */
+	ret = aie_part_create_event_bitmap(apart);
+	if (ret < 0) {
+		dev_err(&apart->dev, "Failed to allocate event bitmap.\n");
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	ret = aie_part_create_l2_bitmap(apart);
+	if (ret < 0) {
+		dev_err(&apart->dev, "Failed to allocate l2 bitmap.\n");
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	ret = aie_part_rscmgr_init(apart);
+	if (ret < 0) {
+		dev_err(&apart->dev,
+			"Failed to initialize resources bitmaps.\n");
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	ret = aie_part_sysfs_create_entries(apart);
+	if (ret) {
+		dev_err(&apart->dev, "Failed to create partition sysfs.\n");
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret) {
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+	list_add_tail(&apart->node, &adev->partitions);
+	mutex_unlock(&adev->mlock);
+	dev_dbg(dev, "created AIE partition device.\n");
+
+	return apart;
+}
+
+struct aie_partition *
+of_aie_part_probe(struct aie_device *adev, struct device_node *nc)
+{
+	struct aie_partition *apart;
+	struct aie_range range;
+	u32 partition_id, regs[4];
+	int ret;
+
+	/* Select device driver */
+	ret = of_property_read_u32_array(nc, "reg", regs, ARRAY_SIZE(regs));
+	if (ret < 0) {
+		dev_err(&adev->dev,
+			"probe %pOF failed, no tiles range information.\n",
+			nc);
+		return ERR_PTR(ret);
+	}
+	range.start.col = regs[0];
+	range.start.row = regs[1];
+	range.size.col = regs[2];
+	range.size.row = regs[3];
+
+	ret = of_property_read_u32_index(nc, "xlnx,partition-id", 0,
+					 &partition_id);
+	if (ret < 0) {
+		dev_err(&adev->dev,
+			"probe %pOF failed, no partition id.\n", nc);
+		return ERR_PTR(ret);
+	}
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret)
+		return ERR_PTR(ret);
+
+	apart = aie_get_partition_from_id(adev, partition_id);
+	mutex_unlock(&adev->mlock);
+	if (apart) {
+		dev_err(&adev->dev,
+			"probe failed: partition %u exists.\n",
+			partition_id);
+		return ERR_PTR(-EINVAL);
+	}
+
+	apart = aie_create_partition(adev, &range);
+	if (IS_ERR(apart)) {
+		dev_err(&adev->dev,
+			"%s: failed to create part(%u,%u),(%u,%u).\n",
+			__func__, range.start.col, range.start.row,
+			range.size.col, range.size.row);
+		return apart;
+	}
+
+	of_node_get(nc);
+	apart->dev.of_node = nc;
+	apart->dev.driver = adev->dev.parent->driver;
+	apart->partition_id = partition_id;
+	apart->error_cb.cb = NULL;
+	apart->error_cb.priv = NULL;
+
+	ret = of_dma_configure(&apart->dev, nc, true);
+	if (ret)
+		dev_warn(&apart->dev, "Failed to configure DMA.\n");
+
+	/* Create FPGA bridge for AI engine partition */
+	ret = aie_fpga_create_bridge(apart);
+	if (ret < 0)
+		dev_warn(&apart->dev, "failed to create fpga region.\n");
+
+	dev_info(&adev->dev,
+		 "AI engine part(%u,%u),(%u,%u), id %u is probed successfully.\n",
+		 range.start.col, range.start.row,
+		 range.size.col, range.size.row, apart->partition_id);
+
+	return apart;
+}
+
+/**
+ * aie_tile_remove() - remove AI engine tile device.
+ * @atile: AI engine tile.
+ *
+ * This function will remove AI engine tile device.
+ */
+static void aie_tile_remove(struct aie_tile *atile)
+{
+	aie_tile_sysfs_remove_entries(atile);
+	device_del(&atile->dev);
+	put_device(&atile->dev);
+}
+
+/**
+ * aie_part_remove() - destroy AI engine partition
+ * @apart: AI engine partition
+ *
+ * This function will remove AI engine partition.
+ */
+void aie_part_remove(struct aie_partition *apart)
+{
+	struct aie_tile *atile = apart->atiles;
+	u32 index;
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++)
+		aie_tile_remove(atile);
+
+	aie_fpga_free_bridge(apart);
+	aie_part_sysfs_remove_entries(apart);
+	of_node_clear_flag(apart->dev.of_node, OF_POPULATED);
+	device_del(&apart->dev);
+	put_device(&apart->dev);
+}
+
+/**
+ * aie_part_has_regs_mmapped() - check if registers in the partition are mapped.
+ * @apart: AI engine partition
+ * @return: return true if there are registers mmaped, false otherwise.
+ *
+ * This function checks if there are registerss in the partition mmapped in the
+ * partition.
+ */
+bool aie_part_has_regs_mmapped(struct aie_partition *apart)
+{
+	struct address_space *mapping;
+
+	mapping = apart->filep->f_inode->i_mapping;
+	return mapping_mapped(mapping);
+}
+
+/**
+ * aie_part_get_tile_rows - helper function to get the number of rows of a
+ *			    tile type.
+ *
+ * @apart: AI engine partition
+ * @ttype: tile type
+ * @return: number of rows of a tile type
+ */
+int aie_part_get_tile_rows(struct aie_partition *apart,
+			   enum aie_tile_type ttype)
+{
+	struct aie_tile_attr *tattr = &apart->adev->ttype_attr[ttype];
+
+	/*
+	 * TODO: number of rows information of the AI engine device
+	 * should get from device tree.
+	 */
+	if (tattr->num_rows != 0xFF)
+		return tattr->num_rows;
+	else
+		return (apart->range.size.row - tattr->start_row);
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-res.c b/drivers/misc/xilinx-ai-engine/ai-engine-res.c
new file mode 100644
index 000000000..501f79b91
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-res.c
@@ -0,0 +1,466 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/bitmap.h>
+
+#include "ai-engine-internal.h"
+
+/**
+ * aie_resource_initialize() - initialize AI engine resource
+ * @res: pointer to AI engine resource
+ * @count: total number of element of this resource
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will initialize the data structure for the
+ * resource.
+ */
+int aie_resource_initialize(struct aie_resource *res, int count)
+{
+	if (!res || !count)
+		return -EINVAL;
+	res->bitmap = bitmap_zalloc(count, GFP_KERNEL);
+	if (!res->bitmap)
+		return -ENOMEM;
+	res->total = count;
+
+	return 0;
+}
+
+/**
+ * aie_resource_uninitialize() - uninitialize AI engine resource
+ * @res: pointer to AI engine resource
+ *
+ * This function will release the AI engine resource data members.
+ */
+void aie_resource_uninitialize(struct aie_resource *res)
+{
+	res->total = 0;
+	if (res->bitmap)
+		bitmap_free(res->bitmap);
+}
+
+/**
+ * aie_resource_check_region() - check availability of requested resource
+ * @res: pointer to AI engine resource to check
+ * @start: start index of the required resource, it will only be used if
+ *	   @continuous is 1. It will check the available resource starting from
+ *	   @start
+ * @count: number of requested element
+ * @return: start resource id if the requested number of resources are available
+ *	    It will return negative value of errors.
+ *
+ * This function will check the availability. It will return start resource id
+ * if the requested number of resources are available.
+ */
+int aie_resource_check_region(struct aie_resource *res,
+			      u32 start, u32 count)
+{
+	unsigned long id;
+
+	if (!res || !res->bitmap || !count)
+		return -EINVAL;
+	id = bitmap_find_next_zero_area(res->bitmap, res->total, start,
+					count, 0);
+	if (id >= res->total)
+		return -ERANGE;
+
+	return (int)id;
+}
+
+/**
+ * aie_resource_get_region() - get requested AI engine resource
+ * @res: pointer to AI engine resource to check
+ * @count: number of requested element
+ * @start: start index of the required resource
+ * @return: start resource id for success, and negative value for failure.
+ *
+ * This function check if the requested AI engine resource is available.
+ * If it is available, mark it used and return the start resource id.
+ */
+int aie_resource_get_region(struct aie_resource *res, u32 start, u32 count)
+{
+	unsigned long off;
+
+	if (!res || !res->bitmap || !count)
+		return -EINVAL;
+	off = bitmap_find_next_zero_area(res->bitmap, res->total, start,
+					 count, 0);
+	if (off >= res->total) {
+		pr_err("Failed to get available AI engine resource.\n");
+		return -ERANGE;
+	}
+	bitmap_set(res->bitmap, off, count);
+
+	return (int)off;
+}
+
+/**
+ * aie_resource_put_region() - release requested AI engine resource
+ * @res: pointer to AI engine resource to check
+ * @start: start index of the resource to release
+ * @count: number of elements to release
+ *
+ * This function release the requested AI engine resource.
+ */
+void aie_resource_put_region(struct aie_resource *res, int start, u32 count)
+{
+	if (!res || !count)
+		return;
+	bitmap_clear(res->bitmap, start, count);
+}
+
+/**
+ * aie_resource_set() - set the AI engine resource bits
+ * @res: pointer to AI engine resource
+ * @start: start bit to set
+ * @count: number of bits to set
+ * @return: 0 for success and negative value for failure
+ *
+ * This function sets the specified number bits in the resource.
+ */
+int aie_resource_set(struct aie_resource *res, u32 start, u32 count)
+{
+	if (!res || !res->bitmap || !count || start + count > res->total)
+		return -EINVAL;
+
+	bitmap_set(res->bitmap, start, count);
+	return 0;
+}
+
+/**
+ * aie_resource_cpy_from_arr32() - copies nbits from u32[] to bitmap.
+ * @res: pointer to AI engine resource
+ * @start: start bit in bitmap
+ * @src: source buffer
+ * @nbits: number of bits to copy from u32[]
+ * @return: 0 for success and negative value for failure
+ */
+int aie_resource_cpy_from_arr32(struct aie_resource *res, u32 start,
+				const u32 *src, u32 nbits)
+{
+	if (!res || !res->bitmap || !nbits || start + nbits  > res->total ||
+	    !src)
+		return -EINVAL;
+
+	bitmap_from_arr32(res->bitmap + BIT_WORD(start), src, nbits);
+	return 0;
+}
+
+/**
+ * aie_resource_cpy_to_arr32() - copies nbits to u32[] from bitmap.
+ * @res: pointer to AI engine resource
+ * @start: start bit in bitmap
+ * @dst: destination buffer
+ * @nbits: number of bits to copy to u32[]
+ * @return: 0 for success and negative value for failure
+ */
+int aie_resource_cpy_to_arr32(struct aie_resource *res, u32 start, u32 *dst,
+			      u32 nbits)
+{
+	if (!res || !res->bitmap || !nbits || start + nbits  > res->total ||
+	    !dst)
+		return -EINVAL;
+
+	bitmap_to_arr32(dst, res->bitmap + BIT_WORD(start), nbits);
+	return 0;
+}
+
+/**
+ * aie_resource_clear() - clear the AI engine resource bits
+ * @res: pointer to AI engine resource
+ * @start: start bit to set
+ * @count: number of bits to clear
+ * @return: 0 for success and negative value for failure
+ *
+ * This function clears the specified number bits in the resource.
+ */
+int aie_resource_clear(struct aie_resource *res, u32 start, u32 count)
+{
+	if (!res || !res->bitmap || !count || start + count > res->total)
+		return -EINVAL;
+
+	bitmap_clear(res->bitmap, start, count);
+	return 0;
+}
+
+/**
+ * aie_resource_clear_all() - clear all the AI engine resource bits
+ * @res: pointer to AI engine resource
+ * @return: 0 for success and negative value for failure
+ *
+ * This function clears all the bits in the resource.
+ */
+int aie_resource_clear_all(struct aie_resource *res)
+{
+	if (!res || !res->bitmap)
+		return -EINVAL;
+
+	bitmap_clear(res->bitmap, 0, res->total);
+	return 0;
+}
+
+/**
+ * aie_resource_testbit() - test if a bit is set in a AI engine resource
+ * @res: pointer to AI engine resource
+ * @bit: bit to check
+ * @return: true for set, false for not set
+ */
+bool aie_resource_testbit(struct aie_resource *res, u32 bit)
+{
+	if (!res || !res->bitmap || bit >= res->total)
+		return false;
+
+	/* Locate the unsigned long the required bit belongs to */
+	return test_bit(bit, res->bitmap);
+}
+
+/**
+ * aie_resource_check_common_avail() - check common available bits
+ *				       of two resources table
+ * @res0: pointer to AI engine resource0
+ * @res1: pointer to AI engine resource1
+ * @sbit: start bit to check
+ * @nbits: number of bits to check
+ * @return: number of common bits, or negative value for failure
+ */
+int aie_resource_check_common_avail(struct aie_resource *res0,
+				    struct aie_resource *res1,
+				    u32 sbit, u32 nbits)
+{
+	u32 ebit, avails;
+
+	if (!nbits || !res0 || !res1 || !res0->bitmap || !res1->bitmap ||
+	    (sbit + nbits) > res0->total || (sbit + nbits) > res1->total)
+		return -EINVAL;
+
+	ebit = sbit + nbits - 1;
+	avails = 0;
+	while (sbit <= ebit) {
+		unsigned long  *bitmap0, *bitmap1, tbits;
+		u32 tlbit, lbit = sbit % BITS_PER_LONG;
+		u32 lnbits = ebit - sbit + 1;
+
+		if (lnbits + lbit > BITS_PER_LONG)
+			lnbits = BITS_PER_LONG - lbit;
+
+		bitmap0 = &res0->bitmap[sbit / BITS_PER_LONG];
+		bitmap1 = &res1->bitmap[sbit / BITS_PER_LONG];
+		bitmap_or(&tbits, bitmap0, bitmap1, BITS_PER_LONG);
+		tlbit = lbit;
+		while (tlbit < lbit + lnbits) {
+			u32 b = bitmap_find_next_zero_area(&tbits,
+							   BITS_PER_LONG, tlbit,
+							   1, 0);
+			if (b >= lbit + lnbits)
+				break;
+			avails++;
+			tlbit = b + 1;
+		}
+		sbit += lnbits;
+	};
+
+	return avails;
+}
+
+/**
+ * aie_resource_get_common_avail() - get common available bits
+ *				     of two resources table
+ * @rres: pointer to AI engine runtime resource, runtime resource bitmap will
+ *	  be updated if the required resources are available.
+ * @sres: pointer to AI engine static resource, static resource bitmap will
+ *	  not be updated even if the required resources are available.
+ * @sbit: start bit to check
+ * @nbits: number of bits to get
+ * @total: total number of bits to check
+ * @rscs: resources array to return for resources ids
+ * @return: number of allocated bits for success, negative value for failure
+ */
+int aie_resource_get_common_avail(struct aie_resource *rres,
+				  struct aie_resource *sres,
+				  u32 sbit, u32 nbits, u32 total,
+				  struct aie_rsc *rscs)
+{
+	u32 ebit, tsbit, tnbits;
+
+	if (!nbits || !rres || !sres || !rres->bitmap || !sres->bitmap ||
+	    nbits > total || (sbit + total) > rres->total ||
+	    (sbit + total) > sres->total)
+		return -EINVAL;
+
+	ebit = sbit + total - 1;
+	tsbit = sbit;
+	tnbits = 0;
+	while (tsbit <= ebit && tnbits != nbits) {
+		unsigned long *rbitmap, *sbitmap, tbits;
+		u32 tlbit, lbit = tsbit % BITS_PER_LONG;
+		u32 lnbits = ebit - tsbit + 1;
+
+		if (lnbits + lbit > BITS_PER_LONG)
+			lnbits = BITS_PER_LONG - lbit;
+
+		rbitmap = &rres->bitmap[sbit / BITS_PER_LONG];
+		sbitmap = &sres->bitmap[sbit / BITS_PER_LONG];
+		bitmap_or(&tbits, rbitmap, sbitmap, BITS_PER_LONG);
+		tlbit = lbit;
+		while (tlbit < lbit + lnbits && tnbits != nbits) {
+			u32 b = bitmap_find_next_zero_area(&tbits,
+							   BITS_PER_LONG, tlbit,
+							   1, 0);
+			if (b >= lbit + lnbits)
+				break;
+			rscs[tnbits].id = tsbit - sbit + b - lbit;
+			tnbits++;
+			tlbit = b + 1;
+		}
+		tsbit += lnbits;
+	};
+
+	if (tnbits != nbits)
+		return -EINVAL;
+
+	while (tnbits--)
+		aie_resource_set(rres, sbit + rscs[tnbits].id, 1);
+
+	return nbits;
+}
+
+/**
+ * aie_resource_check_pattern_region() - check availability of requested
+ *					 contiguous resources of a pattern
+ * @res: pointer to AI engine resource to check
+ * @start: start index of the required resource
+ *	   It will check a continuous block of available resource starting from
+ *	   @start.
+ * @end: end index to check
+ * @count: number of requested element
+ * @return: start resource id if the requested number of resources are available
+ *	    It will return negative value of errors.
+ *
+ * This function will check the availability. It will return start resource id
+ * if the requested number of resources are available.
+ * The contiguous resources of a pattern is e.g.
+ * @count is 0, the resources starting from @start needs to be 0,1; or 2,3 and
+ * beyond
+ */
+int aie_resource_check_pattern_region(struct aie_resource *res,
+				      u32 start, u32 end, u32 count)
+{
+	unsigned long id;
+	u32 lstart;
+
+	if (!res || !res->bitmap || !count)
+		return -EINVAL;
+	lstart = start;
+	while (lstart < end) {
+		id = bitmap_find_next_zero_area(res->bitmap, res->total, lstart,
+						count, 0);
+		if (id + count > end + 1)
+			return -ERANGE;
+		else if (!((id - lstart) % count))
+			return (int)id;
+
+		lstart += count;
+	}
+
+	return -ERANGE;
+}
+
+/**
+ * aie_resource_check_common_pattern_region() - check common available region
+ *						of two resources table
+ * @res0: pointer to AI engine resource0
+ * @res1: pointer to AI engine resource1
+ * @sbit: start bit to check
+ * @nbits: number of bits to check
+ * @total: total number of bits to check
+ * @return: start bit of common region if it is found, negative value for
+ *	    failure
+ */
+int aie_resource_check_common_pattern_region(struct aie_resource *res0,
+					     struct aie_resource *res1,
+					     u32 sbit, u32 nbits, u32 total)
+{
+	int sbit0, sbit1;
+
+	if (!nbits || !res0 || !res1 || !res0->bitmap || !res1->bitmap ||
+	    nbits > total || (sbit + total) > res0->total ||
+	    (sbit + total) > res1->total)
+		return -EINVAL;
+
+	sbit0 = aie_resource_check_pattern_region(res0, sbit,
+						  sbit + total - 1, nbits);
+	if (sbit0 < 0)
+		return sbit0;
+
+	if ((u32)sbit0 + nbits > sbit + total)
+		return -EINVAL;
+
+	sbit1 = aie_resource_check_pattern_region(res1, sbit0,
+						  sbit0 + nbits - 1, nbits);
+	if (sbit1 != sbit0)
+		return -EINVAL;
+
+	return sbit1;
+}
+
+/**
+ * aie_resource_get_common_pattern_region() - get common available region
+ *					      of two resources table
+ * @res0: pointer to AI engine resource0
+ * @res1: pointer to AI engine resource1
+ * @sbit: start bit to check
+ * @nbits: number of bits to get
+ * @total: total number of bits to check
+ * @rscs: resources array to return for resources ids
+ * @return: start bit of the common region if it is found, negative value for
+ *	    failure
+ *
+ * The common pattern region is a contiguous block of resources which needs
+ * to be very number of @nbits.
+ * e.g. if nbits is 2, the offset to the start bit @sbit of returned resources
+ * needs to be: 0,1; 2,3 ...
+ */
+int aie_resource_get_common_pattern_region(struct aie_resource *res0,
+					   struct aie_resource *res1,
+					   u32 sbit, u32 nbits, u32 total,
+					   struct aie_rsc *rscs)
+{
+	int rsbit, ret;
+
+	rsbit = aie_resource_check_common_pattern_region(res0, res1, sbit,
+							 nbits, total);
+	if (rsbit < 0)
+		return rsbit;
+
+	ret = aie_resource_get_region(res0, rsbit, nbits);
+	if (ret < 0)
+		return ret;
+
+	if (ret != rsbit) {
+		aie_resource_put_region(res0, ret, nbits);
+		return -EINVAL;
+	}
+
+	ret = aie_resource_get_region(res1, rsbit, nbits);
+	if (ret < 0)
+		return ret;
+
+	if (ret != rsbit) {
+		aie_resource_put_region(res0, rsbit, nbits);
+		aie_resource_put_region(res1, ret, nbits);
+		return -EINVAL;
+	}
+
+	if (rscs) {
+		u32 i;
+
+		for (i = 0; i < nbits; i++, rscs++)
+			rscs->id = rsbit - sbit + i;
+	}
+
+	return rsbit;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-reset.c b/drivers/misc/xilinx-ai-engine/ai-engine-reset.c
new file mode 100644
index 000000000..f87d4c5d5
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-reset.c
@@ -0,0 +1,306 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver resets implementation
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/io.h>
+
+#include "ai-engine-internal.h"
+
+/**
+ * aie_part_set_col_reset() - set AI engine column reset
+ * @apart: AI engine partition
+ * @col: column to reset
+ * @reset: true to assert reset, false to release reset
+ */
+static void aie_part_set_col_reset(struct aie_partition *apart, u32 col,
+				   bool reset)
+{
+	struct aie_device *adev = apart->adev;
+	const struct aie_single_reg_field *col_rst = adev->col_rst;
+	struct aie_location loc;
+	u32 regoff, val;
+
+	loc.row = 0;
+	loc.col = col;
+
+	val = aie_get_field_val(col_rst, (reset ? 1 : 0));
+	regoff = aie_cal_regoff(adev, loc, col_rst->regoff);
+	iowrite32(val, adev->base + regoff);
+}
+
+/**
+ * aie_part_set_col_clkbuf() - set AI engine column clock buffer
+ * @apart: AI engine partition
+ * @col: column to reset
+ * @enable: true to enable, false to disable
+ */
+static void aie_part_set_col_clkbuf(struct aie_partition *apart, u32 col,
+				    bool enable)
+{
+	struct aie_device *adev = apart->adev;
+	const struct aie_single_reg_field *col_clkbuf = adev->col_clkbuf;
+	struct aie_location loc;
+	u32 regoff, val;
+
+	loc.row = 0;
+	loc.col = col;
+
+	val = aie_get_field_val(col_clkbuf, (enable ? 1 : 0));
+	regoff = aie_cal_regoff(adev, loc, col_clkbuf->regoff);
+	iowrite32(val, adev->base + regoff);
+}
+
+/**
+ * aie_part_set_cols_reset() - set column reset of every column in a partition
+ * @apart: AI engine partition
+ * @reset: bool to assert reset, false to release reset
+ */
+static void aie_part_set_cols_reset(struct aie_partition *apart, bool reset)
+{
+	struct aie_range *range = &apart->range;
+	u32 c;
+
+	for (c = range->start.col; c < range->start.col + range->size.col;
+	     c++)
+		aie_part_set_col_reset(apart, c, reset);
+}
+
+/**
+ * aie_part_set_cols_clkbuf() - set column clock buffer of every column in a
+ *				partition
+ * @apart: AI engine partition
+ * @enable: true to enable, false to disable
+ */
+static void aie_part_set_cols_clkbuf(struct aie_partition *apart, bool enable)
+{
+	struct aie_range *range = &apart->range;
+	u32 c;
+
+	for (c = range->start.col; c < range->start.col + range->size.col;
+	     c++)
+		aie_part_set_col_clkbuf(apart, c, enable);
+}
+
+/**
+ * aie_part_clear_mems() - clear memories of every tile in a partition
+ * @apart: AI engine partition
+ */
+static void aie_part_clear_mems(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_part_mem *pmems = apart->pmems;
+	u32 i, num_mems;
+
+	/* Get the number of different types of memories */
+	num_mems = adev->ops->get_mem_info(&apart->range, NULL);
+	if (!num_mems)
+		return;
+
+	/* Clear each type of memories in the partition */
+	for (i = 0; i < num_mems; i++) {
+		struct aie_mem *mem = &pmems[i].mem;
+		struct aie_range *range = &mem->range;
+		u32 c, r;
+
+		for (c = range->start.col;
+		     c < range->start.col + range->size.col; c++) {
+			for (r = range->start.row;
+			     r < range->start.row + range->size.row; r++) {
+				struct aie_location loc;
+				u32 memoff;
+
+				loc.col = c;
+				loc.row = r;
+				memoff = aie_cal_regoff(adev, loc, mem->offset);
+				memset_io(adev->base + memoff, 0, mem->size);
+			}
+		}
+	}
+}
+
+/**
+ * aie_part_clear_core_regs_of_tile() - clear registers of aie core
+ * @apart: AI engine partition
+ * @loc: location of aie tile to clear
+ */
+static void aie_part_clear_core_regs_of_tile(struct aie_partition *apart,
+					     struct aie_location loc)
+{
+	struct aie_device *adev = apart->adev;
+	const struct aie_core_regs_attr *regs = adev->core_regs;
+	u32 i;
+
+	for (i = 0; i < adev->num_core_regs; i++) {
+		u32 j, soff, eoff, reg;
+
+		soff = aie_cal_regoff(adev, loc, regs[i].core_regs->soff);
+		eoff = aie_cal_regoff(adev, loc, regs[i].core_regs->eoff);
+
+		for (reg = soff; reg <= eoff; reg += AIE_CORE_REGS_STEP) {
+			for (j = 0; j < regs[i].width; j++)
+				iowrite32(0, adev->base + reg + j * 4);
+		}
+	}
+}
+
+/**
+ * aie_part_clear_core_regs - clear registers of aie core of a partition
+ * @apart: AI engine partition
+ */
+static void aie_part_clear_core_regs(struct aie_partition *apart)
+{
+	struct aie_range *range = &apart->range;
+	u32 c, r;
+
+	/* clear core registers for each tile in the partition */
+	for (c = range->start.col; c < range->start.col + range->size.col;
+			c++) {
+		for (r = range->start.row;
+				r < range->start.row + range->size.row; r++) {
+			struct aie_location loc;
+			u32 ttype;
+
+			loc.row = r;
+			loc.col = c;
+			ttype = apart->adev->ops->get_tile_type(&loc);
+			if (ttype == AIE_TILE_TYPE_TILE &&
+			    aie_part_check_clk_enable_loc(apart, &loc))
+				aie_part_clear_core_regs_of_tile(apart, loc);
+		}
+	}
+}
+
+/**
+ * aie_part_clean() - reset and clear AI engine partition
+ * @apart: AI engine partition
+ * @return: 0 for success and negative value for failure
+ *
+ * This function will:
+ *  * gate all the columns
+ *  * reset AI engine partition columns
+ *  * reset AI engine shims
+ *  * clear the memories
+ *  * clear core registers
+ *  * gate all the tiles in a partition
+ *  * update clock state bitmap
+ *
+ * This function will not validate the partition, the caller will need to
+ * provide a valid AI engine partition.
+ */
+int aie_part_clean(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	int ret;
+
+	if (apart->cntrflag & XAIE_PART_NOT_RST_ON_RELEASE)
+		return 0;
+
+	aie_part_set_cols_clkbuf(apart, false);
+	aie_part_set_cols_reset(apart, true);
+
+	ret = apart->adev->ops->reset_shim(adev, &apart->range);
+	if (ret < 0)
+		return ret;
+
+	aie_part_clear_mems(apart);
+	aie_part_clear_core_regs(apart);
+	aie_part_set_cols_clkbuf(apart, false);
+	aie_resource_clear_all(&apart->cores_clk_state);
+
+	return 0;
+}
+
+/**
+ * aie_part_reset() - reset AI engine partition
+ * @apart: AI engine partition
+ * @return: 0 for success and negative value for failure
+ *
+ * This function will:
+ * - gate all the columns
+ * - reset AI engine partition columns
+ * - reset AI engine shims
+ * - gate all the tiles in a partition.
+ *
+ * This function will not validate the partition, the caller will need to
+ * provide a valid AI engine partition.
+ */
+int aie_part_reset(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	int ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	/*
+	 * Check if any AI engine memories or registers in the
+	 * partition have been mapped. If yes, don't reset.
+	 */
+	if (aie_part_has_mem_mmapped(apart) ||
+	    aie_part_has_regs_mmapped(apart)) {
+		dev_err(&apart->dev,
+			"failed to reset, there are mmapped memories or registers.\n");
+		mutex_unlock(&apart->mlock);
+		return -EBUSY;
+	}
+
+	/* Clear tiles in use bitmap and clock state bitmap */
+	aie_resource_clear_all(&apart->tiles_inuse);
+	aie_resource_clear_all(&apart->cores_clk_state);
+
+	aie_part_set_cols_clkbuf(apart, false);
+	aie_part_set_cols_reset(apart, true);
+
+	ret = apart->adev->ops->reset_shim(adev, &apart->range);
+	if (ret < 0) {
+		mutex_unlock(&apart->mlock);
+		return ret;
+	}
+
+	aie_part_set_cols_clkbuf(apart, false);
+
+	aie_part_clear_cached_events(apart);
+	aie_resource_clear_all(&apart->l2_mask);
+
+	aie_part_rscmgr_reset(apart);
+
+	mutex_unlock(&apart->mlock);
+	return 0;
+}
+
+/**
+ * aie_part_post_reinit() - AI engine partition has been re-initialized
+ * @apart: AI engine partition
+ * @return: 0 for success and negative value for failure
+ *
+ * This function will:
+ * - scan which tiles are gated
+ * - update memories and registers mapping
+ *
+ * This function will scan which tiles are gated, and update the memories and
+ * registers setting. This function is called after the AI engine partition is
+ * reconfigured with PDI outside the AI engine driver.
+ */
+int aie_part_post_reinit(struct aie_partition *apart)
+{
+	int ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	ret = aie_part_scan_clk_state(apart);
+	mutex_unlock(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to scan clock states after reset is done.\n");
+		return ret;
+	}
+
+	return 0;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-rscmgr.c b/drivers/misc/xilinx-ai-engine/ai-engine-rscmgr.c
new file mode 100644
index 000000000..196229573
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-rscmgr.c
@@ -0,0 +1,1551 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine partition resource manager
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+
+#include "ai-engine-internal.h"
+#include <linux/slab.h>
+
+/*
+ * Macros for the AI engine resource bitmap element header
+ */
+#define AIE_RSC_BITMAP_TILETYPE_BITSHIFT	0U
+#define AIE_RSC_BITMAP_TILETYPE_BITWIDTH	4U
+#define AIE_RSC_BITMAP_MODTYPE_BITSHIFT		4U
+#define AIE_RSC_BITMAP_MODTYPE_BITWIDTH		4U
+#define AIE_RSC_BITMAP_RSCTYPE_BITSHIFT		8U
+#define AIE_RSC_BITMAP_RSCTYPE_BITWIDTH		8U
+#define AIE_RSC_BITMAP_LENU64_BITSHIFT		16U
+#define AIE_RSC_BITMAP_LENU64_BITWIDTH		32U
+
+#define AIE_RSC_BITMAP_GEN_MASK(N) \
+	GENMASK_ULL((AIE_RSC_BITMAP_##N ##_BITSHIFT + \
+		     AIE_RSC_BITMAP_##N ##_BITWIDTH - 1), \
+		    AIE_RSC_BITMAP_##N ##_BITSHIFT)
+#define AIE_RSC_BITMAP_TILETYPE_MASK	AIE_RSC_BITMAP_GEN_MASK(TILETYPE)
+#define AIE_RSC_BITMAP_MODTYPE_MASK	AIE_RSC_BITMAP_GEN_MASK(MODTYPE)
+#define AIE_RSC_BITMAP_RSCTYPE_MASK	AIE_RSC_BITMAP_GEN_MASK(RSCTYPE)
+#define AIE_RSC_BITMAP_LENU64_MASK	AIE_RSC_BITMAP_GEN_MASK(LENU64)
+
+#define AIE_RSC_BITMAP_HEAD_VAL(N, v) \
+	(((v) & AIE_RSC_BITMAP_##N ##_MASK) >> AIE_RSC_BITMAP_##N ##_BITSHIFT)
+
+/*
+ * enum for AI engine resource bitmap allocation types
+ */
+enum aie_rsc_alloc_type {
+	AIE_RSC_ALLOC_STATIC = 0,
+	AIE_RSC_ALLOC_AVAIL = 1,
+	AIE_RSC_ALLOC_MAX = 2
+};
+
+/**
+ * struct aie_rsc_meta_header - struct of a resource bitmaps meta data header
+ * @stat: statistics information of the bitmaps, such as number of bitmaps
+ * @bitmap_off: offset to the start of the binary of the first bitmap element
+ */
+struct aie_rsc_meta_header {
+	u64 stat;
+	u64 bitmap_off;
+};
+
+/**
+ * struct aie_rsc_bitmap - struct of a resource bitmap element
+ * @header: bitmap header, it contains the following information:
+ *	    tile type, module type, resource type, and the bitmap
+ *	    length.
+ * @bitmap: the pointer of bitmap
+ */
+struct aie_rsc_bitmap {
+	u64 header;
+	u64 bitmap[0];
+};
+
+/**
+ * aie_dev_get_tile_attr - helper function to get tile attributes
+ * @adev: AI engine device
+ * @ttype: tile type
+ * @return: attributes of an AI engine tile type
+ */
+static inline
+struct aie_tile_attr *aie_dev_get_tile_attr(struct aie_device *adev,
+					    enum aie_tile_type ttype)
+{
+	return &adev->ttype_attr[ttype];
+}
+
+/**
+ * aie_dev_get_tile_rsc_attr - helper function to get resource attribute of a
+ *			       tile type of an AI engine device
+ * @adev: AI engine device
+ * @ttype: tile type
+ * @rtype: resource type
+ * @return: attributes of an AI engine resource type of a tile type
+ */
+static inline const
+struct aie_tile_rsc_attr *aie_dev_get_tile_rsc_attr(struct aie_device *adev,
+						    enum aie_tile_type ttype,
+						    enum aie_rsc_type rtype)
+{
+	return &adev->ttype_attr[ttype].rscs_attr[rtype];
+}
+
+/**
+ * aie_dev_get_mod_id - helper function to get module id of a module of a tile
+ *			type. The module ID can be used to indexing the resource
+ *			attributes of a module type of a tile type or indexing
+ *			resource status bitmaps.
+ *
+ * @adev: AI engine device
+ * @ttype: tile type
+ * @mod: module type
+ * @return: module type index
+ */
+static int aie_dev_get_mod_id(struct aie_device *adev,
+			      enum aie_tile_type ttype,
+			      enum aie_module_type mod)
+{
+	struct aie_tile_attr *tattr = &adev->ttype_attr[ttype];
+
+	int ret;
+
+	if (ttype == AIE_TILE_TYPE_TILE)
+		ret = AIE_MOD_ID(TILE, mod);
+	else if (ttype == AIE_TILE_TYPE_SHIMPL)
+		ret = AIE_MOD_ID(SHIMPL, mod);
+	else
+		ret = AIE_MOD_ID(SHIMNOC, mod);
+
+	if (ret < 0 || ret > tattr->num_mods)
+		return -EINVAL;
+
+	return ret;
+}
+
+/**
+ * aie_dev_get_mod_rsc_attr - helper function to get resource attribute of a
+ *			      module of an AI engine device
+ * @adev: AI engine device
+ * @ttype: tile type
+ * @mod: module type
+ * @rtype: resource type
+ * @return: module type resource attributes
+ */
+static const
+struct aie_mod_rsc_attr *aie_dev_get_mod_rsc_attr(struct aie_device *adev,
+						  enum aie_tile_type ttype,
+						  enum aie_module_type mod,
+						  enum aie_rsc_type rtype)
+{
+	const struct aie_tile_rsc_attr *rsc = aie_dev_get_tile_rsc_attr(adev,
+									ttype,
+									rtype);
+	const struct aie_mod_rsc_attr *mrsc = NULL;
+	int mod_id = aie_dev_get_mod_id(adev, ttype, mod);
+
+	if (mod_id < 0)
+		return NULL;
+
+	mrsc = &rsc->mod_attr[mod_id];
+	if (mrsc && !mrsc->num_rscs)
+		return NULL;
+
+	return mrsc;
+}
+
+/**
+ * aie_part_get_ttype_rsc_bitmaps - helper function to get bitmap of a resource
+ *				    with tile type, module type, and resource
+ *				    type
+ *
+ * @apart: AI engine partition
+ * @ttype: tile type
+ * @mod: module type
+ * @rtype: resource type
+ * @return: pointer to AI engine resource status bitmaps if resource is found,
+ *	    otherwise NULL
+ */
+static
+struct aie_rsc_stat *aie_part_get_ttype_rsc_bitmaps(struct aie_partition *apart,
+						    enum aie_tile_type ttype,
+						    enum aie_module_type mod,
+						    enum aie_rsc_type rtype)
+{
+	int mod_id;
+	struct aie_mod_rscs *mrscs;
+
+	if (ttype >= AIE_TILE_TYPE_MAX)
+		return NULL;
+
+	mod_id = aie_dev_get_mod_id(apart->adev, ttype, mod);
+	if (mod_id < 0)
+		return NULL;
+
+	if (rtype >= AIE_RSCTYPE_MAX)
+		return NULL;
+
+	mrscs = apart->trscs[ttype].mod_rscs[rtype];
+	if (!mrscs)
+		return NULL;
+
+	return mrscs[mod_id].rscs_stat;
+}
+
+/**
+ * aie_part_get_rsc_bitmaps - helper function to get bitmap of a resource
+ *
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @mod: module type
+ * @rtype: resource type
+ * @return: pointer to AI engine resource status bitmaps if resource is found,
+ *	    otherwise NULL
+ */
+static
+struct aie_rsc_stat *aie_part_get_rsc_bitmaps(struct aie_partition *apart,
+					      struct aie_location loc,
+					      enum aie_module_type mod,
+					      enum aie_rsc_type rtype)
+{
+	u32 ttype = apart->adev->ops->get_tile_type(&loc);
+
+	return aie_part_get_ttype_rsc_bitmaps(apart, ttype, mod, rtype);
+}
+
+/**
+ * aie_part_get_mod_num_rscs - helper function to get number of resources
+ *			       of a module of a tile
+ *
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @mod: module type
+ * @rtype: resource type
+ * @return: number of max resources of a module of a tile
+ */
+static
+int aie_part_get_mod_num_rscs(struct aie_partition *apart,
+			      struct aie_location loc,
+			      enum aie_module_type mod,
+			      enum aie_rsc_type rtype)
+{
+	u32 ttype = apart->adev->ops->get_tile_type(&loc);
+	const struct aie_mod_rsc_attr *mattr;
+
+	mattr = aie_dev_get_mod_rsc_attr(apart->adev, ttype, mod, rtype);
+	if (!mattr)
+		return 0;
+
+	return mattr->num_rscs;
+}
+
+/**
+ * aie_part_get_rsc_startbit - helper function to get the start bit of a
+ *			       resource of a module of a tile.
+ *
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @mod: module type
+ * @rtype: resource type
+ * @return: pointer to AI engine resource status bitmaps if resource is found,
+ *	    otherwise NULL
+ *
+ */
+static
+int aie_part_get_rsc_startbit(struct aie_partition *apart,
+			      struct aie_location loc,
+			      enum aie_module_type mod,
+			      enum aie_rsc_type rtype)
+{
+	struct aie_device *adev = apart->adev;
+	u32 ttype;
+	const struct aie_mod_rsc_attr *mattr;
+	int num_rows;
+	struct aie_tile_attr *tattr;
+
+	ttype = adev->ops->get_tile_type(&loc);
+
+	mattr = aie_dev_get_mod_rsc_attr(adev, ttype, mod, rtype);
+	if (!mattr)
+		return -EINVAL;
+
+	num_rows = aie_part_get_tile_rows(apart, ttype);
+	tattr = &adev->ttype_attr[ttype];
+	return mattr->num_rscs *
+	       ((loc.col - apart->range.start.col) * num_rows +
+		loc.row - tattr->start_row);
+}
+
+/**
+ * aie_part_adjust_loc - adjust relative tile location to partition to
+ *				absolute location in AI engine device
+ * @apart: AI engine partition
+ * @rloc: relative location in AI engine partition
+ * @loc: returns absolute location in AI engine device
+ * @return: 0 for success, negative value for failure
+ */
+static
+int aie_part_adjust_loc(struct aie_partition *apart,
+			struct aie_location rloc, struct aie_location *loc)
+{
+	loc->col = rloc.col + apart->range.start.col;
+	loc->row = rloc.row + apart->range.start.row;
+
+	if (aie_validate_location(apart, *loc) < 0) {
+		dev_err(&apart->dev,
+			"invalid loc (%u,%u) in (%u,%u).\n",
+			rloc.col, rloc.row,
+			apart->range.size.col, apart->range.size.row);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_init() - initialize AI engine partition resource status
+ *			    bitmaps
+ * @apart: AI engine partition
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will create the hardware resources status bitmaps for the whole
+ * partition.
+ * Each partition contains an array of hardware resources status bitmaps of all
+ * defined tiles types.:
+ * aie_partition
+ *   |- trscs[<all_tile_types>]
+ *       |-mod_rscs[<all_resources_types>]
+ *         |-rscs_stat - resources status bitmaps of a module type of a tile
+ *			 type of the AI engine partition.
+ */
+int aie_part_rscmgr_init(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	u32 t;
+
+	for (t = AIE_TILE_TYPE_TILE; t < AIE_TILE_TYPE_MAX; t++) {
+		struct aie_tile_rscs *trscs = &apart->trscs[t];
+		struct aie_tile_attr *tattr;
+		int num_rows, num_cols;
+		u32 r;
+
+		/*
+		 * SHIMNOC tile resource bitmaps reuse the SHIMPL resource
+		 * bitmaps. In future, for DMA resources, SHIMNOC tile will
+		 * have DMA resources bitmap which will be the unique to
+		 * SHIMNOC tiles.
+		 */
+		if (t == AIE_TILE_TYPE_SHIMNOC) {
+			*trscs = apart->trscs[AIE_TILE_TYPE_SHIMPL];
+			continue;
+		}
+
+		/*
+		 * Get the number of rows of a tile type and the number
+		 * of columns of the partition, which will be used to
+		 * calculate the size of the bitmap is a resource.
+		 */
+		tattr = aie_dev_get_tile_attr(adev, t);
+		num_rows = aie_part_get_tile_rows(apart, t);
+		num_cols = apart->range.size.col;
+
+		for (r = AIE_RSCTYPE_PERF; r < AIE_RSCTYPE_MAX; r++) {
+			const struct aie_tile_rsc_attr *trsc_attr;
+			struct aie_mod_rscs *mod_rscs;
+			u32 m;
+
+			trsc_attr = aie_dev_get_tile_rsc_attr(adev, t, r);
+			if (!trsc_attr)
+				continue;
+
+			mod_rscs = kcalloc(tattr->num_mods,
+					   sizeof(*mod_rscs), GFP_KERNEL);
+			if (!mod_rscs) {
+				aie_part_rscmgr_finish(apart);
+				return -ENOMEM;
+			}
+
+			trscs->mod_rscs[r] = mod_rscs;
+			for (m = 0 ; m < tattr->num_mods; m++) {
+				struct aie_rsc_stat *rscs_stat;
+				int num_mrscs = trsc_attr->mod_attr[m].num_rscs;
+				int ret, total_rscs;
+
+				/*
+				 * if number of resources of this module type in
+				 * this tile type is 0, skip allocating bitmap
+				 * for the resource of this module type.
+				 */
+				if (!num_mrscs)
+					continue;
+
+				rscs_stat = kzalloc(sizeof(*rscs_stat),
+						    GFP_KERNEL);
+				if (!rscs_stat) {
+					aie_part_rscmgr_finish(apart);
+					return -ENOMEM;
+				}
+
+				mod_rscs[m].rscs_stat = rscs_stat;
+				total_rscs = num_mrscs * num_rows * num_cols;
+				/*
+				 * initialize bitmaps for static resources and
+				 * runtime allocated resources.
+				 */
+				ret = aie_resource_initialize(&rscs_stat->rbits,
+							      total_rscs);
+				if (ret) {
+					aie_part_rscmgr_finish(apart);
+					return ret;
+				}
+				ret = aie_resource_initialize(&rscs_stat->sbits,
+							      total_rscs);
+				if (ret) {
+					aie_part_rscmgr_finish(apart);
+					return ret;
+				}
+			}
+		}
+	}
+
+	/* Reserve resources for interrupts */
+	return aie_part_set_intr_rscs(apart);
+}
+
+/**
+ * aie_part_rscmgr_finish() - uninitialize AI engine partition resource status
+ *			      bitmaps.
+ * @apart: AI engine partition
+ */
+void aie_part_rscmgr_finish(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	u32 t;
+
+	for (t = AIE_TILE_TYPE_TILE; t < AIE_TILE_TYPE_MAX; t++) {
+		struct aie_tile_rscs *trscs = &apart->trscs[t];
+		struct aie_tile_attr *tattr;
+		u32 r;
+
+		/* SHIMNOC reuses SHIMPL resources bitmap */
+		if (t == AIE_TILE_TYPE_SHIMNOC)
+			continue;
+
+		tattr = aie_dev_get_tile_attr(adev, t);
+		for (r = AIE_RSCTYPE_PERF; r < AIE_RSCTYPE_MAX; r++) {
+			struct aie_mod_rscs *mod_rscs;
+			u32 m;
+
+			mod_rscs = trscs->mod_rscs[r];
+			if (!mod_rscs)
+				continue;
+
+			for (m = 0 ; m < tattr->num_mods; m++) {
+				struct aie_rsc_stat *rscs_stat;
+
+				rscs_stat = mod_rscs[m].rscs_stat;
+				if (!rscs_stat)
+					continue;
+
+				aie_resource_uninitialize(&rscs_stat->rbits);
+				aie_resource_uninitialize(&rscs_stat->sbits);
+			}
+
+			kfree(mod_rscs);
+			trscs->mod_rscs[r] = NULL;
+		}
+	}
+}
+
+/**
+ * aie_part_rscmgr_reset() - reset AI engine partition resource status bitmaps
+ *
+ * @apart: AI engine partition
+ *
+ * This function expect caller to lock the partition before calling this
+ * function.
+ */
+void aie_part_rscmgr_reset(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	u32 t;
+
+	for (t = AIE_TILE_TYPE_TILE; t < AIE_TILE_TYPE_MAX; t++) {
+		struct aie_tile_rscs *trscs = &apart->trscs[t];
+		struct aie_tile_attr *tattr;
+		u32 r;
+
+		/* SHIMNOC reuses SHIMPL resources bitmap */
+		if (t == AIE_TILE_TYPE_SHIMNOC)
+			continue;
+
+		tattr = aie_dev_get_tile_attr(adev, t);
+		for (r = AIE_RSCTYPE_PERF; r < AIE_RSCTYPE_MAX; r++) {
+			struct aie_mod_rscs *mod_rscs;
+			u32 m;
+
+			mod_rscs = trscs->mod_rscs[r];
+			if (!mod_rscs)
+				continue;
+
+			for (m = 0 ; m < tattr->num_mods; m++) {
+				struct aie_rsc_stat *rscs_stat;
+
+				rscs_stat = mod_rscs[m].rscs_stat;
+				if (!rscs_stat)
+					continue;
+
+				aie_resource_clear_all(&rscs_stat->rbits);
+				aie_resource_clear_all(&rscs_stat->sbits);
+			}
+		}
+	}
+
+	/* Always reserve resources for interrupt */
+	(void)aie_part_set_intr_rscs(apart);
+}
+
+/**
+ * aie_part_rscmgr_rsc_req() - request a type of resource from a module of a
+ *			       tile of an AI engine partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource request arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will check if there the specified number of free resources
+ * available. If yes, allocated the specified number of resources.
+ */
+long aie_part_rscmgr_rsc_req(struct aie_partition *apart,
+			     void __user *user_args)
+{
+	struct aie_rsc_req_rsp args;
+	struct aie_location loc;
+	struct aie_rsc_stat *rstat;
+	long ret;
+	int mod_num_rscs, start_bit;
+	struct aie_rsc *rscs;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	if (!args.rscs) {
+		dev_err(&apart->dev,
+			"invalid resource request, empty resources list.\n");
+		return -EINVAL;
+	}
+
+	ret = aie_part_adjust_loc(apart, args.req.loc, &loc);
+	if (ret < 0)
+		return ret;
+
+	if (args.req.type > AIE_RSCTYPE_MAX) {
+		dev_err(&apart->dev,
+			"invalid resource request, invalid resource type %d.\n",
+			args.req.type);
+		return -EINVAL;
+	}
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, args.req.mod,
+					 args.req.type);
+	start_bit = aie_part_get_rsc_startbit(apart, loc, args.req.mod,
+					      args.req.type);
+	if (!rstat || start_bit < 0) {
+		dev_err(&apart->dev,
+			"invalid resource request(%u,%u), mod:%u, rsc:%u.\n",
+			args.req.loc.col, args.req.loc.row, args.req.mod,
+			args.req.type);
+		return -EINVAL;
+	}
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, loc, args.req.mod,
+						 args.req.type);
+	if (!args.req.num_rscs || args.req.num_rscs > mod_num_rscs) {
+		dev_err(&apart->dev,
+			"invalid resource req(%u,%u),mod:%u,rsc:%u,expect=%u,max=%u.\n",
+			args.req.loc.col, args.req.loc.row, args.req.mod,
+			args.req.type, args.req.num_rscs, mod_num_rscs);
+		return -EINVAL;
+	}
+
+	rscs = kmalloc_array(args.req.num_rscs, sizeof(*rscs), GFP_KERNEL);
+	if (!rscs)
+		return -ENOMEM;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(rscs);
+		return ret;
+	}
+
+	/*
+	 * There can be some resources needs to be contiguous, such as combo events.
+	 * It needs to be 0,1; 2,3; or 0,1,2; or 0,1,2,3
+	 */
+	if (!(args.req.flag & XAIE_RSC_PATTERN_BLOCK)) {
+		ret = aie_resource_get_common_avail(&rstat->rbits, &rstat->sbits,
+						    start_bit,
+						    args.req.num_rscs,
+						    mod_num_rscs, rscs);
+	} else {
+		ret = aie_resource_get_common_pattern_region(&rstat->rbits,
+							     &rstat->sbits,
+							     start_bit,
+							     args.req.num_rscs,
+							     mod_num_rscs,
+							     rscs);
+	}
+	mutex_unlock(&apart->mlock);
+
+	if (ret < 0) {
+		if (!(args.req.flag & XAIE_RSC_PATTERN_BLOCK)) {
+			dev_warn(&apart->dev,
+				 "invalid resource req(%u,%u),mod:%u,rsc:%u,expect=%u not avail.\n",
+				args.req.loc.col, args.req.loc.row,
+				args.req.mod, args.req.type,
+				args.req.num_rscs);
+		} else {
+			dev_warn(&apart->dev,
+				 "invalid contiguous resource req(%u,%u),mod:%u,rsc:%u,expect=%u not avail.\n",
+				args.req.loc.col, args.req.loc.row,
+				args.req.mod, args.req.type, args.req.num_rscs);
+		}
+		kfree(rscs);
+		return ret;
+	}
+
+	if (copy_to_user((void __user *)args.rscs, rscs,
+			 sizeof(*rscs) * args.req.num_rscs))
+		ret = -EFAULT;
+	else
+		ret = 0;
+
+	kfree(rscs);
+	return ret;
+}
+
+/**
+ * aie_part_rscmgr_rsc_clearbit() - clear resource status of a module of a
+ *				    tile of an AI engine partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource release arguments
+ * @is_release: true to clear the status from both runtime and static bitmaps
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will clear the status of a resource of both runtime status
+ * bitmap and static status bitmap or both based on the @is_release setting.
+ */
+static long aie_part_rscmgr_rsc_clearbit(struct aie_partition *apart,
+					 void __user *user_args,
+					 bool is_release)
+{
+	struct aie_rsc args;
+	struct aie_location loc, rloc;
+	struct aie_rsc_stat *rstat;
+	long ret;
+	int mod_num_rscs, start_bit;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	rloc.col = (u32)(args.loc.col & 0xFF);
+	rloc.row = (u32)(args.loc.row & 0xFF);
+	ret = aie_part_adjust_loc(apart, rloc, &loc);
+	if (ret < 0)
+		return ret;
+
+	if (args.type > AIE_RSCTYPE_MAX) {
+		dev_err(&apart->dev,
+			"invalid resource to release, invalid resource type %d.\n",
+			args.type);
+		return -EINVAL;
+	}
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, args.mod, args.type);
+	start_bit = aie_part_get_rsc_startbit(apart, loc, args.mod,
+					      args.type);
+	if (!rstat || start_bit < 0) {
+		dev_err(&apart->dev,
+			"invalid resource to release(%u,%u),mod:%u,rsc:%u.\n",
+			rloc.col, rloc.row, args.mod, args.type);
+		return -EINVAL;
+	}
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, loc, args.mod,
+						 args.type);
+	if (args.id > mod_num_rscs) {
+		dev_err(&apart->dev,
+			"invalid resource to release(%u,%u),mod:%u,rsc:%u,id=%u.\n",
+			rloc.col, rloc.row, args.mod, args.type, args.id);
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	if (!aie_resource_testbit(&rstat->rbits, start_bit + args.id)) {
+		dev_err(&apart->dev,
+			"invalid resource to release(%u,%u),mod:%u,rsc:%u,id=%u. not requested\n",
+			rloc.col, rloc.row, args.mod, args.type, args.id);
+		mutex_unlock(&apart->mlock);
+		return -EINVAL;
+	}
+
+	aie_resource_clear(&rstat->rbits, start_bit + args.id, 1);
+	if (is_release)
+		aie_resource_clear(&rstat->sbits, start_bit + args.id, 1);
+
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_rsc_release() - release resource of a module of a tile of
+ *				   an AI engine partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource releasearguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will clear the bit of the resource runtime and static status
+ * bitmap.
+ */
+long aie_part_rscmgr_rsc_release(struct aie_partition *apart,
+				 void __user *user_args)
+{
+	return aie_part_rscmgr_rsc_clearbit(apart, user_args, true);
+}
+
+/**
+ * aie_part_rscmgr_rsc_free() - free resource of a module of a tile of an AI
+ *				engine partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource free arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will clear the bit of the resource runtime status bitmap.
+ */
+long aie_part_rscmgr_rsc_free(struct aie_partition *apart,
+			      void __user *user_args)
+{
+	return aie_part_rscmgr_rsc_clearbit(apart, user_args, false);
+}
+
+/**
+ * aie_part_rscmgr_rsc_req_specific() - request for specific resource of a
+ *					module of a tile of an AI engine
+ *					partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource free arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function requires the specified resource is set in the static
+ * status bitmap
+ */
+long aie_part_rscmgr_rsc_req_specific(struct aie_partition *apart,
+				      void __user *user_args)
+{
+	struct aie_rsc args;
+	struct aie_location loc, rloc;
+	struct aie_rsc_stat *rstat;
+	long ret;
+	int mod_num_rscs, start_bit;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	rloc.col = (u32)(args.loc.col & 0xFF);
+	rloc.row = (u32)(args.loc.row & 0xFF);
+	ret = aie_part_adjust_loc(apart, rloc, &loc);
+	if (ret < 0)
+		return ret;
+
+	if (args.type > AIE_RSCTYPE_MAX) {
+		dev_err(&apart->dev,
+			"invalid resource to request, invalid resource type %d.\n",
+			args.type);
+		return -EINVAL;
+	}
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, args.mod, args.type);
+	start_bit = aie_part_get_rsc_startbit(apart, loc, args.mod,
+					      args.type);
+	if (!rstat || start_bit < 0) {
+		dev_err(&apart->dev,
+			"invalid resource to request(%u,%u),mod:%u,rsc:%u.\n",
+			rloc.col, rloc.row, args.mod, args.type);
+		return -EINVAL;
+	}
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, loc, args.mod,
+						 args.type);
+	if (args.id > mod_num_rscs) {
+		dev_err(&apart->dev,
+			"invalid resource to request(%u,%u),mod:%u, rsc:%u,id=%u.\n",
+			rloc.col, rloc.row, args.mod, args.type, args.id);
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	/* Check if the resource is in the runtime status bitmap */
+	if (aie_resource_testbit(&rstat->rbits, start_bit + args.id)) {
+		dev_err(&apart->dev,
+			"invalid resource to request(%u,%u),mod:%u,rsc:%u,id=%u, resource in use.\n",
+			rloc.col, rloc.row, args.mod, args.type, args.id);
+		mutex_unlock(&apart->mlock);
+		return -EBUSY;
+	}
+
+	aie_resource_set(&rstat->rbits, start_bit + args.id, 1);
+
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_rsc_check_avail() - check how many resources vailable for
+ *				       the specified resource type
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource free arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function requires the specified resource is set in the static
+ * status bitmap
+ */
+long aie_part_rscmgr_rsc_check_avail(struct aie_partition *apart,
+				     void __user *user_args)
+{
+	struct aie_rsc_stat *rstat;
+	struct aie_location loc;
+	long ret;
+	int mod_num_rscs, start_bit;
+	struct aie_rsc_req args;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	ret = aie_part_adjust_loc(apart, args.loc, &loc);
+	if (ret < 0)
+		return ret;
+
+	if (args.type > AIE_RSCTYPE_MAX) {
+		dev_err(&apart->dev,
+			"invalid resource to request, invalid resource type %d.\n",
+			args.type);
+		return -EINVAL;
+	}
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, args.mod, args.type);
+	start_bit = aie_part_get_rsc_startbit(apart, loc, args.mod,
+					      args.type);
+	if (!rstat || start_bit < 0) {
+		dev_err(&apart->dev,
+			"invalid resource to request(%u,%u),mod:%u,rsc:%u.\n",
+			args.loc.col, args.loc.row, args.mod, args.type);
+		return -EINVAL;
+	}
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, loc, args.mod,
+						 args.type);
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	args.num_rscs = aie_resource_check_common_avail(&rstat->rbits,
+							&rstat->sbits,
+							start_bit,
+							mod_num_rscs);
+	mutex_unlock(&apart->mlock);
+
+	if (copy_to_user(user_args, &args, sizeof(args)))
+		return -EFAULT;
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_get_ungated_bc_mods() - find the ungated modules of the full
+ *					   partition and fill in the locations
+ *					   information to the resources array.
+ * @apart: AI engine partition
+ * @num_rscs: number of broadcast resources, each module of a tile has a
+ *	      broadcast resource in this array.
+ * @onum_rscs: returns the number of actual ungated broadcast resources of the
+ *	       whole partition.
+ *
+ * @rscs: broadcast resources array
+ * @return: 0 for success, negative value for failure
+ */
+static int aie_part_rscmgr_get_ungated_bc_mods(struct aie_partition *apart,
+					       u32 num_rscs, u32 *onum_rscs,
+					       struct aie_rsc *rscs)
+{
+	struct aie_device *adev = apart->adev;
+	u32 c, r, i = 0;
+
+	for (c = 0; c < apart->range.size.col; c++) {
+		for (r = 0; r < apart->range.size.row; r++) {
+			struct aie_location l;
+			u32 ttype, m;
+			const struct aie_tile_attr *tattr;
+			const struct aie_tile_rsc_attr *rattr;
+			enum aie_rsc_type rtype = AIE_RSCTYPE_BROADCAST;
+
+			l.col = apart->range.start.col + c;
+			l.row = r;
+			ttype = adev->ops->get_tile_type(&l);
+			tattr = &adev->ttype_attr[ttype];
+			rattr = &tattr->rscs_attr[rtype];
+			for (m = 0; m < tattr->num_mods; m++) {
+				/*
+				 * if module doesn't have broadcast channel,
+				 * skipped. This is not the case today.
+				 */
+				if (!rattr->mod_attr[m].num_rscs)
+					continue;
+				/* Check if the broadcast resource is gated */
+				if (aie_part_check_clk_enable_loc(apart, &l)) {
+					if (i >= num_rscs) {
+						dev_err(&apart->dev,
+							"failed to returns all ungated tiles, not enough resource elements.\n");
+						return -EINVAL;
+					}
+					rscs[i].loc.col = (u8)(c & 0xFF);
+					rscs[i].loc.row = (u8)(r & 0xFF);
+					rscs[i].mod = tattr->mods[m];
+					i++;
+				}
+			}
+		}
+	}
+
+	*onum_rscs = i;
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_get_or_bc_stat() - get OR the broadcast resources stat of
+ *				      specified modules in the specified
+ *				      resources array.
+ *
+ * @apart: AI engine partition
+ * @num_rscs: number of broadcast resources, every module has one broadcast
+ *	      resource
+ * @rscs: array of broadcast resources, each element contains the tile
+ *	  location and module information of the broadcast channel
+ * @runtime_only: true to only check the runtime allocated resources bitmap,
+ *		  false to check both runtime and statically allocated resource
+ *		  bitmaps.
+ * @or_stat: returns result of OR all the modules broadcast resources status
+ * @return: 0 for success, negative value for failure
+ */
+static int aie_part_rscmgr_get_or_bc_stat(struct aie_partition *apart,
+					  u32 num_rscs, struct aie_rsc *rscs,
+					  bool runtime_only,
+					  unsigned long *or_stat)
+{
+	u32 i;
+
+	*or_stat = 0;
+	for (i = 0; i < num_rscs; i++) {
+		struct aie_location l;
+		struct aie_rsc_stat *rstat;
+		int mod_num_rscs, start_bit;
+
+		l.col = apart->range.start.col + rscs[i].loc.col;
+		l.row = rscs[i].loc.row;
+		rstat = aie_part_get_rsc_bitmaps(apart, l, rscs[i].mod,
+						 AIE_RSCTYPE_BROADCAST);
+		start_bit = aie_part_get_rsc_startbit(apart, l, rscs[i].mod,
+						      AIE_RSCTYPE_BROADCAST);
+		if (!rstat || start_bit < 0) {
+			dev_err(&apart->dev,
+				"failed to get broadcast bitmap for[%u]:tile(%u,%u), mod=%u.\n",
+				i, rscs[i].loc.col, rscs[i].loc.row,
+				rscs[i].mod);
+			return -EINVAL;
+		}
+		mod_num_rscs = aie_part_get_mod_num_rscs(apart, l, rscs[i].mod,
+							 AIE_RSCTYPE_BROADCAST);
+		*or_stat |= aie_resource_or_get_valueul(&rstat->rbits,
+							start_bit,
+							mod_num_rscs);
+		if (!runtime_only)
+			*or_stat |= aie_resource_or_get_valueul(&rstat->sbits,
+								start_bit,
+								mod_num_rscs);
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_get_common_bc() - get common broadcast id of specified
+ *				     modules in the specified resources array.
+ *
+ * @apart: AI engine partition
+ * @num_rscs: number of broadcast resources, every module has one broadcast
+ *	      resource
+ * @rscs: array of broadcast resources, each element contains the tile
+ *	  location and module information of the broadcast channel
+ * @return: common broadcast channel id for success, negative value for failure
+ *
+ * This function checks both runtime and static allocated resources bitmap.
+ */
+static int aie_part_rscmgr_get_common_bc(struct aie_partition *apart,
+					 u32 num_rscs, struct aie_rsc *rscs)
+{
+	unsigned long or_stat, b;
+	int ret;
+	struct aie_location l;
+	int mod_num_rscs;
+
+	l.col = apart->range.start.col + (u32)rscs[0].loc.row;
+	l.row = (u32)rscs[0].loc.row;
+
+	ret = aie_part_rscmgr_get_or_bc_stat(apart, num_rscs, rscs, false,
+					     &or_stat);
+	if (ret)
+		return ret;
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, l, rscs[0].mod,
+						 AIE_RSCTYPE_BROADCAST);
+	b = bitmap_find_next_zero_area(&or_stat, mod_num_rscs, 0, 1, 0);
+	if (b >= mod_num_rscs)
+		return -EINVAL;
+
+	return (int)b;
+}
+
+/**
+ * aie_part_rscmgr_check_common_bc() - validate the specified common broadcast
+ *				       id in the specified modules in the
+ *				       specified resources array.
+ *
+ * @apart: AI engine partition
+ * @bc: broadcast channel id to check
+ * @num_rscs: number of broadcast resources, every module has one broadcast
+ *	      resource
+ * @rscs: array of broadcast resources, each element contains the tile
+ *	  location and module information of the broadcast channel
+ * @return: 0 if the specified broadcast channel id is available for all the
+ *	    specified modules, negative value for failure
+ *
+ * This function only checks runtime allocated resources bitmap.
+ */
+static int aie_part_rscmgr_check_common_bc(struct aie_partition *apart,
+					   u32 bc, u32 num_rscs,
+					   struct aie_rsc *rscs)
+{
+	unsigned long or_stat;
+	int ret;
+	struct aie_location l;
+	int mod_num_rscs;
+
+	l.col = apart->range.start.col + (u32)rscs[0].loc.row;
+	l.row = (u32)rscs[0].loc.row;
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, l, rscs[0].mod,
+						 AIE_RSCTYPE_BROADCAST);
+	if (bc > mod_num_rscs) {
+		dev_err(&apart->dev,
+			"invalid specified broadcast id %u, max is %u.\n",
+			bc, mod_num_rscs);
+		return -EINVAL;
+	}
+
+	ret = aie_part_rscmgr_get_or_bc_stat(apart, num_rscs, rscs, true,
+					     &or_stat);
+	if (ret)
+		return ret;
+
+	if (test_bit(bc, &or_stat)) {
+		dev_err(&apart->dev,
+			"specified broadcast id %u is occupied.\n", bc);
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_check_rscs_modules() - validate the modules of the array of
+ *					input resources
+ *
+ * @apart: AI engine partition
+ * @num_rscs: number of resources
+ * @rscs: array of resources, each element contains the tile
+ *	  location and module information of the resource
+ * @return: 0 if the modules of all the resources are valid, negative value
+ *	    for failure
+ *
+ * This function validate the modules and the tiles of the resources, and
+ * check if resource module is gated.
+ */
+static int aie_part_rscmgr_check_rscs_modules(struct aie_partition *apart,
+					      u32 num_rscs,
+					      struct aie_rsc *rscs)
+{
+	struct aie_device *adev = apart->adev;
+	u32 i;
+
+	for (i = 0; i < num_rscs; i++) {
+		struct aie_location l;
+
+		l.col = apart->range.start.col + rscs[i].loc.col;
+		l.row = rscs[i].loc.row;
+		/* validate tile location */
+		if (aie_validate_location(apart, l)) {
+			dev_err(&apart->dev,
+				"failed resource check tile(%u,%u) invalid.\n",
+					rscs[i].loc.col, rscs[i].loc.row);
+			return -EINVAL;
+		}
+
+		/* validate module */
+		if (aie_dev_get_mod_id(adev, adev->ops->get_tile_type(&l),
+				       rscs[i].mod) < 0) {
+			dev_err(&apart->dev,
+				"failed resource check, tile(%u,%u) mod %u invalid.\n",
+					rscs[i].loc.col, rscs[i].loc.row,
+					rscs[i].mod);
+			return -EINVAL;
+		}
+
+		/* check if the resource module is gated */
+		if (!aie_part_check_clk_enable_loc(apart, &l)) {
+			dev_err(&apart->dev,
+				"failed resource check, tile(%u,%u) mod=%u is gated.\n",
+				rscs[i].loc.col, rscs[i].loc.row,
+				rscs[i].mod);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_set_tile_broadcast() - set broadcast channel in use
+ *					  of a module of a tile
+ *
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @mod: module
+ * @id: broadcast channel id
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will set the bit of the specified broadcast channel in the
+ * runtime broadcast bitmap of the specified module of the specified tile.
+ */
+int aie_part_rscmgr_set_tile_broadcast(struct aie_partition *apart,
+				       struct aie_location loc,
+				       enum aie_module_type mod, uint32_t id)
+{
+	struct aie_rsc_stat *rstat;
+	int start_bit;
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, mod,
+					 AIE_RSCTYPE_BROADCAST);
+	/* bitmap pointer cannot be NULL. */
+	if (WARN_ON(!rstat || !rstat->rbits.bitmap))
+		return -EFAULT;
+
+	start_bit = aie_part_get_rsc_startbit(apart, loc, mod,
+					      AIE_RSCTYPE_BROADCAST);
+	aie_resource_set(&rstat->rbits, start_bit + id, 1);
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_get_broadcast() - get common broadcast channel of
+ *				     the specified modules or the whole
+ *				     partition.
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource free arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function get a common broadcast channel for the specified set
+ * of AI engine modules in the resources array. If the any of the input set of
+ * tiles is gated, it will return failure. This ioctl will not check the
+ * connection of the input modules set.
+ * The driver will fill in the resource ID with the assigned broadcast channel
+ * ID of the resources array.
+ * If the XAIE_BROADCAST_ALL is set in the request flag, it will get the
+ * broadcast channel for all the ungated tiles of the partition.
+ * If a particular broadcast channel id is specified in the request, if will
+ * check if the channel is available for the specified modules, or the whole
+ * partition depends on if XAIE_BROADCAST_ALL is set.
+ */
+long aie_part_rscmgr_get_broadcast(struct aie_partition *apart,
+				   void __user *user_args)
+{
+	struct aie_rsc_bc_req args;
+	struct aie_rsc *rscs;
+	u32 i;
+	long ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	rscs = kmalloc_array(args.num_rscs, sizeof(*rscs), GFP_KERNEL);
+	if (!rscs)
+		return -ENOMEM;
+
+	if (!(args.flag & XAIE_BROADCAST_ALL)) {
+		if (copy_from_user(rscs, (void __user *)args.rscs,
+				   sizeof(*rscs) * args.num_rscs)) {
+			kfree(rscs);
+			return -EFAULT;
+		}
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(rscs);
+		return ret;
+	}
+
+	if (args.flag & XAIE_BROADCAST_ALL)
+		/*
+		 * It is to broadcast to the whole partition.
+		 * Get all the ungated modules.
+		 */
+		ret = aie_part_rscmgr_get_ungated_bc_mods(apart, args.num_rscs,
+							  &args.num_rscs,
+							  rscs);
+	else
+		/*
+		 * validate tiles and modules, and check if there are modules
+		 * gated
+		 */
+		ret = aie_part_rscmgr_check_rscs_modules(apart, args.num_rscs,
+							 rscs);
+	if (ret)
+		goto error;
+
+	/* find the common broadcast signal among the specified modules */
+	if (args.id == XAIE_BROADCAST_ID_ANY) {
+		ret = aie_part_rscmgr_get_common_bc(apart, args.num_rscs, rscs);
+		if (ret >= 0) {
+			args.id = (u32)ret;
+			ret = 0;
+		} else {
+			dev_warn(&apart->dev, "no available broadcast channel.\n");
+		}
+	} else {
+		ret = aie_part_rscmgr_check_common_bc(apart, args.id,
+						      args.num_rscs,
+						      rscs);
+	}
+	if (ret)
+		goto error;
+
+	/* set the broadcast channel resource runtime status bit */
+	for (i = 0; i < args.num_rscs; i++) {
+		struct aie_location l;
+
+		l.col = apart->range.start.col + rscs[i].loc.col;
+		l.row = rscs[i].loc.row;
+		ret = aie_part_rscmgr_set_tile_broadcast(apart, l, rscs[i].mod,
+							 args.id);
+		if (ret)
+			goto error;
+
+		rscs[i].id = args.id;
+	}
+
+	mutex_unlock(&apart->mlock);
+
+	if (copy_to_user((void __user *)args.rscs, rscs,
+			 sizeof(*rscs) * args.num_rscs)) {
+		kfree(rscs);
+		return -EFAULT;
+	}
+
+	/*
+	 * If it is required to broadcast to whole partition, it needs to
+	 * return the actual number of broadcast resources as some tiles
+	 * can be gated
+	 */
+	if (args.flag & XAIE_BROADCAST_ALL) {
+		struct aie_rsc_bc_req __user *uargs = user_args;
+
+		if (copy_to_user((void __user *)&uargs->num_rscs,
+				 &args.num_rscs, sizeof(args.num_rscs))) {
+			kfree(rscs);
+			return -EFAULT;
+		}
+	}
+
+	kfree(rscs);
+	return 0;
+error:
+	mutex_unlock(&apart->mlock);
+	kfree(rscs);
+	return ret;
+}
+
+/**
+ * aie_part_rscmgr_set_static() - sets statically allocated resources bitmaps
+ *
+ * @apart: AI engine partition
+ * @meta: meta data which contains the statically allocated resources bitmaps
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function takes the static bitmap information from meta data and fill
+ * in the static bitmap.
+ */
+int aie_part_rscmgr_set_static(struct aie_partition *apart, void *meta)
+{
+	struct aie_rsc_meta_header *header = meta;
+	struct aie_rsc_bitmap *bitmap;
+	u64 i, num_bitmaps, offset;
+
+	if (!header) {
+		dev_err(&apart->dev,
+			"failed to get static resources, meta data is NULL.\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * For now, the stat field of the header only contains the number of
+	 * bitmaps.
+	 */
+	num_bitmaps = header->stat;
+	offset = header->bitmap_off;
+	if (!num_bitmaps || offset < sizeof(*header)) {
+		dev_err(&apart->dev,
+			"failed to get static resources, invalid header.\n");
+		return -EINVAL;
+	}
+
+	bitmap = (struct aie_rsc_bitmap *)(meta + offset);
+	for (i = 0; i < num_bitmaps; i++) {
+		struct aie_rsc_stat *rstat;
+		const struct aie_mod_rsc_attr *mrattr;
+		u64 header = bitmap->header;
+		u32 lrlen, rlen, ttype, mtype, rtype, total;
+
+		ttype = AIE_RSC_BITMAP_HEAD_VAL(TILETYPE, header);
+		mtype = AIE_RSC_BITMAP_HEAD_VAL(MODTYPE, header);
+		rtype = AIE_RSC_BITMAP_HEAD_VAL(RSCTYPE, header);
+		rlen = AIE_RSC_BITMAP_HEAD_VAL(LENU64, header);
+
+		if (!rlen) {
+			dev_err(&apart->dev,
+				"invalid static bitmap[%llu], length is 0.\n",
+				i);
+			return -EINVAL;
+		}
+
+		mrattr = aie_dev_get_mod_rsc_attr(apart->adev, ttype, mtype,
+						  rtype);
+		if (!mrattr) {
+			dev_err(&apart->dev,
+				"invalid static bitmap[%llu], invalid tile(%u)/module(%u)/rsce(%u) types combination.\n",
+				i, ttype, mtype, rtype);
+			return -EINVAL;
+		}
+
+		total = mrattr->num_rscs * apart->range.size.col *
+			aie_part_get_tile_rows(apart, ttype);
+		lrlen = BITS_TO_LONGS(total);
+		if (rlen != lrlen) {
+			dev_err(&apart->dev,
+				"invalid static bitmap[%llu], tile(%u)/module(%u)/rscs(%u), expect len(%u), actual(%u).\n",
+				i, ttype, mtype, rtype, lrlen, rlen);
+			return -EINVAL;
+		}
+
+		rstat = aie_part_get_ttype_rsc_bitmaps(apart, ttype, mtype,
+						       rtype);
+		/* if bitmap length is not 0, bitmap pointer cannot be NULL. */
+		if (WARN_ON(!rstat || !rstat->sbits.bitmap))
+			return -EFAULT;
+
+		/* copy the bitmap from meta data */
+		bitmap_copy(rstat->sbits.bitmap,
+			    (unsigned long *)bitmap->bitmap, total);
+
+		bitmap = (struct aie_rsc_bitmap *)((void *)bitmap +
+						   sizeof(header) +
+						   rlen * sizeof(u64));
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_check_static() - check the number of static resources
+ *
+ * @rstat: resource statistics structure which contains bitmaps of a resource
+ *	   type of a module type of a tile type.
+ * @sbit: start bit of the resource bitmap of a tile of a module
+ * @total: number of total resources bits to check
+ *
+ * @return: number of static resources
+ *
+ * This function returns the number of static resources of a resource
+ * bitmap.
+ */
+static int aie_part_rscmgr_check_static(struct aie_rsc_stat *rstat,
+					u32 sbit, u32 total)
+{
+	u32 i;
+	int num_static = 0;
+
+	for (i = sbit; i < sbit + total; i++) {
+		if (aie_resource_testbit(&rstat->sbits, i))
+			num_static++;
+	}
+
+	return num_static;
+}
+
+/**
+ * aie_part_rscmgr_check_avail() - check the number of available resources
+ *
+ * @rstat: resource statistics structure which contains bitmaps of a resource
+ *	   type of a module type of a tile type.
+ * @sbit: start bit of the resource bitmap of a tile of a module
+ * @total: number of total resources bits to check
+ *
+ * @return: number of available resources for success, negative value for
+ *	    failure
+ *
+ * This function returns the number of available resources of a resource
+ * bitmap.
+ */
+static int aie_part_rscmgr_check_avail(struct aie_rsc_stat *rstat,
+				       u32 sbit, u32 total)
+{
+	return aie_resource_check_common_avail(&rstat->rbits,
+					       &rstat->sbits,
+					       sbit, total);
+}
+
+/**
+ * aie_part_rscmgr_get_statistics() - get resource statistics based on user
+ *				      request
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource statistics request. it contains the number of
+ *	       resource statistics wants to get followed by the statistics
+ *	       array and the statistics type to specify if it is for static
+ *	       allocated resources or available resources. Each statistics
+ *	       element contains the tile location, module type and the resource
+ *	       type.
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function returns the resource statistics based on the user request.
+ * If user requests for available resource statistics, it returns the number
+ * of available resources of each resource statistics entry. If user requests
+ * for static resources statistics, it returns the number of static resources
+ * of each resource statistics entry.
+ */
+long aie_part_rscmgr_get_statistics(struct aie_partition *apart,
+				    void __user *user_args)
+{
+	struct aie_rsc_user_stat_array args;
+	struct aie_rsc_user_stat __user *ustat_ptr;
+	u32 i;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	if (args.stats_type >= AIE_RSC_STAT_TYPE_MAX) {
+		dev_err(&apart->dev,
+			"get rsc statistics failed, invalid rsc stat type %u.\n",
+			args.stats_type);
+		return -EINVAL;
+	}
+
+	ustat_ptr = (struct aie_rsc_user_stat __user *)args.stats;
+	for (i = 0; i < args.num_stats; i++) {
+		struct aie_rsc_user_stat ustat;
+		struct aie_rsc_stat *rstat;
+		struct aie_location rloc, loc;
+		long ret;
+		int max_rscs, start_bit;
+
+		if (copy_from_user(&ustat, (void __user *)ustat_ptr,
+				   sizeof(ustat)))
+			return -EFAULT;
+
+		/* convert user tile loc to kernel tile loc format */
+		rloc.col = (u32)(ustat.loc.col & 0xFF);
+		rloc.row = (u32)(ustat.loc.row & 0xFF);
+		ret = aie_part_adjust_loc(apart, rloc, &loc);
+		if (ret < 0)
+			return ret;
+
+		if (ustat.type > AIE_RSCTYPE_MAX) {
+			dev_err(&apart->dev,
+				"get rsc statistics failed, invalid resource type %d.\n",
+				ustat.type);
+			return -EINVAL;
+		}
+
+		rstat = aie_part_get_rsc_bitmaps(apart, loc, ustat.mod,
+						 ustat.type);
+		start_bit = aie_part_get_rsc_startbit(apart, loc, ustat.mod,
+						      ustat.type);
+		if (!rstat || start_bit < 0) {
+			dev_err(&apart->dev,
+				"get rsc statistics failed, invalid resource(%u,%u),mod:%u,rsc:%u.\n",
+				loc.col, loc.row, ustat.mod, ustat.type);
+			return -EINVAL;
+		}
+
+		max_rscs = aie_part_get_mod_num_rscs(apart, loc, ustat.mod,
+						     ustat.type);
+		ret = mutex_lock_interruptible(&apart->mlock);
+		if (ret)
+			return ret;
+
+		if (args.stats_type == AIE_RSC_STAT_TYPE_STATIC)
+			ustat.num_rscs = aie_part_rscmgr_check_static(rstat,
+								      start_bit,
+								      max_rscs);
+		else
+			ustat.num_rscs = aie_part_rscmgr_check_avail(rstat,
+								     start_bit,
+								     max_rscs);
+
+		mutex_unlock(&apart->mlock);
+		if (WARN_ON(ustat.num_rscs < 0))
+			return -EFAULT;
+
+		/* copy the information back to userspace */
+		if (copy_to_user((void __user *)ustat_ptr, &ustat,
+				 sizeof(ustat)))
+			return -EFAULT;
+
+		ustat_ptr++;
+	}
+
+	return 0;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-core.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-core.c
new file mode 100644
index 000000000..110dbf8a6
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-core.c
@@ -0,0 +1,190 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_get_core_pc() - reads the AI engine core program counter value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_core_pc(struct aie_partition *apart,
+			   struct aie_location *loc)
+{
+	u32 regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc,
+				apart->adev->core_pc->regoff);
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_get_core_lr() - reads the AI engine core link register value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_core_lr(struct aie_partition *apart,
+			   struct aie_location *loc)
+{
+	u32 regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc,
+				apart->adev->core_lr->regoff);
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_get_core_sp() - reads the AI engine core stack pointer value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_core_sp(struct aie_partition *apart,
+			   struct aie_location *loc)
+{
+	u32 regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc,
+				apart->adev->core_sp->regoff);
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_sysfs_get_core_status() - returns the status of core in string format
+ *				 with each status value separated by a '|'
+ *				 symbol.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @buffer: location to return core status string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_sysfs_get_core_status(struct aie_partition *apart,
+				  struct aie_location *loc, char *buffer,
+				  ssize_t size)
+{
+	ssize_t len = 0;
+	u32 ttype;
+	unsigned long status;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		return len;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(buffer, max(0L, size), "clock_gated");
+		return len;
+	}
+
+	status = apart->adev->ops->get_core_status(apart, loc);
+	if (!status) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "disabled");
+	} else {
+		u32 n;
+		bool is_delimit_req = false;
+		char **str = apart->adev->core_status_str;
+
+		for_each_set_bit(n, &status, 32) {
+			if (is_delimit_req) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 DELIMITER_LEVEL0);
+			}
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 str[n]);
+			is_delimit_req = true;
+		}
+	}
+	return len;
+}
+
+/**
+ * aie_tile_show_core() - exports AI engine core status, value of program
+ *			  counter, stack pointer, and link register to a tile
+ *			  level sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_core(struct device *dev, struct device_attribute *attr,
+			   char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	ssize_t len = 0, size = PAGE_SIZE;
+	u32 pc = 0, lr = 0, sp = 0;
+	char sts_buf[AIE_SYSFS_CORE_STS_SIZE];
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	if (!aie_part_check_clk_enable_loc(apart, &atile->loc)) {
+		scnprintf(sts_buf, AIE_SYSFS_CORE_STS_SIZE, "clock_gated");
+		goto out;
+	}
+
+	aie_sysfs_get_core_status(apart, &atile->loc, sts_buf,
+				  AIE_SYSFS_CORE_STS_SIZE);
+	pc = aie_get_core_pc(apart, &atile->loc);
+	lr = aie_get_core_lr(apart, &atile->loc);
+	sp = aie_get_core_sp(apart, &atile->loc);
+
+out:
+	mutex_unlock(&apart->mlock);
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "status: %s\n",
+			 sts_buf);
+	len += scnprintf(&buffer[len], max(0L, size - len), "pc: %#.8x\n", pc);
+	len += scnprintf(&buffer[len], max(0L, size - len), "lr: %#.8x\n", lr);
+	len += scnprintf(&buffer[len], max(0L, size - len), "sp: %#.8x\n", sp);
+	return len;
+}
+
+/**
+ * aie_part_read_cb_core() - exports status of all cores within a given
+ *			     partition to partition level node.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_core(struct kobject *kobj, char *buffer, ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	ssize_t len = 0;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		u32 ttype = apart->adev->ops->get_tile_type(&atile->loc);
+
+		if (ttype != AIE_TILE_TYPE_TILE)
+			continue;
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d_%d: ",
+				 atile->loc.col, atile->loc.row);
+		len += aie_sysfs_get_core_status(apart, &atile->loc,
+						 &buffer[len], size - len);
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-dma.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-dma.c
new file mode 100644
index 000000000..0aa8487be
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-dma.c
@@ -0,0 +1,413 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_get_dma_s2mm_status() - reads the DMA stream to memory map status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_dma_s2mm_status(struct aie_partition *apart,
+				   struct aie_location *loc)
+{
+	u32 stsoff, regoff, ttype;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		stsoff = apart->adev->shim_dma->s2mm_sts_regoff;
+	else
+		stsoff = apart->adev->tile_dma->s2mm_sts_regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_get_dma_mm2s_status() - reads the DMA memory map to stream status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_dma_mm2s_status(struct aie_partition *apart,
+				   struct aie_location *loc)
+{
+	u32 stsoff, regoff, ttype;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		stsoff = apart->adev->shim_dma->mm2s_sts_regoff;
+	else
+		stsoff = apart->adev->tile_dma->mm2s_sts_regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_get_chan_status() - reads the DMA channel status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @chanid: DMA channel ID.
+ * @return: 8-bit status value.
+ */
+static u8 aie_get_chan_status(struct aie_partition *apart,
+			      struct aie_location *loc, u32 status, u8 chanid)
+{
+	const struct aie_single_reg_field *sts, *stall;
+	u32 mask, chan_shift, shift, value, ttype;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype != AIE_TILE_TYPE_TILE) {
+		sts = &apart->adev->shim_dma->sts;
+		stall = &apart->adev->shim_dma->stall;
+	} else {
+		sts = &apart->adev->tile_dma->sts;
+		stall = &apart->adev->tile_dma->stall;
+	}
+
+	chan_shift = sts->regoff;
+	mask = sts->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+	value = (status & mask) >> shift;
+
+	chan_shift = stall->regoff;
+	mask = stall->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+	value |= (status & mask) >> shift;
+	return value;
+}
+
+/**
+ * aie_get_queue_size() - reads the DMA queue size.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @chanid: DMA channel ID.
+ * @return: 8-bit value.
+ */
+static u8 aie_get_queue_size(struct aie_partition *apart,
+			     struct aie_location *loc, u32 status, u8 chanid)
+{
+	const struct aie_single_reg_field *qsize;
+	u32 mask, chan_shift, shift, ttype;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		qsize = &apart->adev->shim_dma->qsize;
+	else
+		qsize = &apart->adev->tile_dma->qsize;
+
+	chan_shift = qsize->regoff;
+	mask = qsize->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+	return (status & mask) >> shift;
+}
+
+/**
+ * aie_get_queue_status() - reads the DMA queue status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @chanid: DMA channel ID.
+ * @return: 8-bit status value.
+ */
+static u8 aie_get_queue_status(struct aie_partition *apart,
+			       struct aie_location *loc, u32 status, u8 chanid)
+{
+	const struct aie_single_reg_field *qsts;
+	u32 mask, chan_shift, shift, ttype;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		qsts = &apart->adev->shim_dma->qsts;
+	else
+		qsts = &apart->adev->tile_dma->qsts;
+
+	chan_shift = qsts->regoff;
+	mask = qsts->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+	return (status & mask) >> shift;
+}
+
+/**
+ * aie_get_current_bd() - reads the current buffer descriptor being processed
+ *			  by DMA channel.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @chanid: DMA channel ID.
+ * @return: 8-bit buffer descriptor value.
+ */
+static u8 aie_get_current_bd(struct aie_partition *apart,
+			     struct aie_location *loc, u32 status, u8 chanid)
+{
+	const struct aie_single_reg_field *curbd;
+	u32 mask, chan_shift, shift, ttype;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		curbd = &apart->adev->shim_dma->curbd;
+	else
+		curbd = &apart->adev->tile_dma->curbd;
+
+	chan_shift = curbd->regoff;
+	mask = curbd->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+	return (status & mask) >> shift;
+}
+
+/**
+ * aie_sysfs_get_dma_status() - returns the status of DMA in string format with
+ *				MM2S and S2MM type channel separated by a ','
+ *				symbol. Channels with a given type are
+ *				separated by a '|' symbol.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @buffer: location to return DMA status string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_sysfs_get_dma_status(struct aie_partition *apart,
+				 struct aie_location *loc, char *buffer,
+				 ssize_t size)
+{
+	u32 i, ttype, num_s2mm_chan, num_mm2s_chan;
+	ssize_t len = 0;
+	unsigned long status;
+	bool is_delimit_req = false;
+	char **str = apart->adev->dma_status_str;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype == AIE_TILE_TYPE_SHIMPL)
+		return len;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(buffer, max(0L, size - len),
+				 "mm2s: clock_gated%ss2mm: clock_gated",
+				 DELIMITER_LEVEL1);
+		return len;
+	}
+
+	if (ttype != AIE_TILE_TYPE_TILE) {
+		num_mm2s_chan = apart->adev->shim_dma->num_mm2s_chan;
+		num_s2mm_chan = apart->adev->shim_dma->num_s2mm_chan;
+	} else {
+		num_mm2s_chan = apart->adev->tile_dma->num_mm2s_chan;
+		num_s2mm_chan = apart->adev->tile_dma->num_s2mm_chan;
+	}
+
+	/* MM2S */
+	len += scnprintf(&buffer[len], max(0L, size - len), "mm2s: ");
+	status = aie_get_dma_mm2s_status(apart, loc);
+	for (i = 0; i < num_mm2s_chan; i++) {
+		u32 value = aie_get_chan_status(apart, loc, status, i);
+
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+		}
+		len += scnprintf(&buffer[len], max(0L, size - len), str[value]);
+		is_delimit_req = true;
+	}
+
+	/* S2MM */
+	is_delimit_req = false;
+	len += scnprintf(&buffer[len], max(0L, size - len), "%ss2mm: ",
+			 DELIMITER_LEVEL1);
+	status = aie_get_dma_s2mm_status(apart, loc);
+	for (i = 0; i < num_s2mm_chan; i++) {
+		u32 value = aie_get_chan_status(apart, loc, status, i);
+
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+		}
+		len += scnprintf(&buffer[len], max(0L, size - len), str[value]);
+		is_delimit_req = true;
+	}
+	return len;
+}
+
+/**
+ * aie_tile_show_dma() - exports AI engine DMA channel status, queue size,
+ *			 queue status, and current buffer descriptor ID being
+ *			 processed by DMA channel to a tile level sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_dma(struct device *dev, struct device_attribute *attr,
+			  char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	u32 ttype, i, num_s2mm_chan, num_mm2s_chan;
+	unsigned long status;
+	bool is_delimit_req = false;
+	ssize_t len = 0, size = PAGE_SIZE, l0 = 0, l1 = 0, l2 = 0;
+	char **qsts_str = apart->adev->queue_status_str;
+	char ch_buf[AIE_SYSFS_CHAN_STS_SIZE],
+	     qsz_mm2s_buf[AIE_SYSFS_QUEUE_SIZE_SIZE],
+	     qsz_s2mm_buf[AIE_SYSFS_QUEUE_SIZE_SIZE],
+	     qsts_mm2s_buf[AIE_SYSFS_QUEUE_STS_SIZE],
+	     qsts_s2mm_buf[AIE_SYSFS_QUEUE_STS_SIZE],
+	     bd_mm2s_buf[AIE_SYSFS_BD_SIZE],
+	     bd_s2mm_buf[AIE_SYSFS_BD_SIZE];
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	if (!aie_part_check_clk_enable_loc(apart, &atile->loc)) {
+		scnprintf(ch_buf, AIE_SYSFS_CHAN_STS_SIZE,
+			  "mm2s: clock_gated%ss2mm: clock_gated",
+			  DELIMITER_LEVEL1);
+		scnprintf(qsz_mm2s_buf, AIE_SYSFS_QUEUE_SIZE_SIZE,
+			  "clock_gated");
+		scnprintf(qsts_mm2s_buf, AIE_SYSFS_QUEUE_STS_SIZE,
+			  "clock_gated");
+		scnprintf(bd_mm2s_buf, AIE_SYSFS_BD_SIZE, "clock_gated");
+		scnprintf(qsz_s2mm_buf, AIE_SYSFS_QUEUE_SIZE_SIZE,
+			  "clock_gated");
+		scnprintf(qsts_s2mm_buf, AIE_SYSFS_QUEUE_STS_SIZE,
+			  "clock_gated");
+		scnprintf(bd_s2mm_buf, AIE_SYSFS_BD_SIZE, "clock_gated");
+		goto print;
+	}
+
+	aie_sysfs_get_dma_status(apart, &atile->loc, ch_buf,
+				 AIE_SYSFS_CHAN_STS_SIZE);
+
+	ttype = apart->adev->ops->get_tile_type(&atile->loc);
+	if (ttype != AIE_TILE_TYPE_TILE) {
+		num_mm2s_chan = apart->adev->shim_dma->num_mm2s_chan;
+		num_s2mm_chan = apart->adev->shim_dma->num_s2mm_chan;
+	} else {
+		num_mm2s_chan = apart->adev->tile_dma->num_mm2s_chan;
+		num_s2mm_chan = apart->adev->tile_dma->num_s2mm_chan;
+	}
+
+	/* MM2S */
+	status = aie_get_dma_mm2s_status(apart, &atile->loc);
+	for (i = 0; i < num_mm2s_chan; i++) {
+		u8 qsize = aie_get_queue_size(apart, &atile->loc, status, i);
+		u8 qsts = aie_get_queue_status(apart, &atile->loc, status, i);
+		u8 curbd = aie_get_current_bd(apart, &atile->loc, status, i);
+
+		if (is_delimit_req) {
+			l0 += scnprintf(&qsz_mm2s_buf[l0],
+					max(0L, AIE_SYSFS_QUEUE_SIZE_SIZE - l0),
+					DELIMITER_LEVEL0);
+			l1 += scnprintf(&qsts_mm2s_buf[l1],
+					max(0L, AIE_SYSFS_QUEUE_STS_SIZE - l1),
+					DELIMITER_LEVEL0);
+			l2 += scnprintf(&bd_mm2s_buf[l2],
+					max(0L, AIE_SYSFS_BD_SIZE - l2),
+					DELIMITER_LEVEL0);
+		}
+		l0 += scnprintf(&qsz_mm2s_buf[l0],
+				max(0L, AIE_SYSFS_QUEUE_SIZE_SIZE - l0), "%d",
+				qsize);
+		l1 += scnprintf(&qsts_mm2s_buf[l1],
+				max(0L, AIE_SYSFS_QUEUE_STS_SIZE - l1),
+				qsts_str[qsts]);
+		l2 += scnprintf(&bd_mm2s_buf[l2],
+				max(0L, AIE_SYSFS_BD_SIZE - l2), "%d", curbd);
+		is_delimit_req = true;
+	}
+
+	/* S2MM */
+	is_delimit_req = false;
+	l0 = 0; l1 = 0; l2 = 0;
+	status = aie_get_dma_s2mm_status(apart, &atile->loc);
+	for (i = 0; i < num_s2mm_chan; i++) {
+		u8 qsize = aie_get_queue_size(apart, &atile->loc, status, i);
+		u8 qsts = aie_get_queue_status(apart, &atile->loc, status, i);
+		u8 curbd = aie_get_current_bd(apart, &atile->loc, status, i);
+
+		if (is_delimit_req) {
+			l0 += scnprintf(&qsz_s2mm_buf[l0],
+					max(0L, AIE_SYSFS_QUEUE_SIZE_SIZE - l0),
+					DELIMITER_LEVEL0);
+			l1 += scnprintf(&qsts_s2mm_buf[l1],
+					max(0L, AIE_SYSFS_QUEUE_STS_SIZE - l1),
+					DELIMITER_LEVEL0);
+			l2 += scnprintf(&bd_s2mm_buf[l2],
+					max(0L, AIE_SYSFS_BD_SIZE - l2),
+					DELIMITER_LEVEL0);
+		}
+		l0 += scnprintf(&qsz_s2mm_buf[l0],
+				max(0L, AIE_SYSFS_QUEUE_SIZE_SIZE - l0), "%d",
+				qsize);
+		l1 += scnprintf(&qsts_s2mm_buf[l1],
+				max(0L, AIE_SYSFS_QUEUE_STS_SIZE - l1),
+				qsts_str[qsts]);
+		l2 += scnprintf(&bd_s2mm_buf[l2],
+				max(0L, AIE_SYSFS_BD_SIZE - l2), "%d", curbd);
+		is_delimit_req = true;
+	}
+
+print:
+	mutex_unlock(&apart->mlock);
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "channel_status: %s\n", ch_buf);
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "queue_size: mm2s: %s%ss2mm: %s\n", qsz_mm2s_buf,
+			 DELIMITER_LEVEL1, qsz_s2mm_buf);
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "queue_status: mm2s: %s%ss2mm: %s\n", qsts_mm2s_buf,
+			 DELIMITER_LEVEL1, qsts_s2mm_buf);
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "current_bd: mm2s: %s%ss2mm: %s\n", bd_mm2s_buf,
+			 DELIMITER_LEVEL1, bd_s2mm_buf);
+	return len;
+}
+
+/**
+ * aie_part_read_cb_dma() - exports status of all DMAs within a given
+ *			    partition to partition level node.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_dma(struct kobject *kobj, char *buffer, ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	ssize_t len = 0;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		u32 ttype = apart->adev->ops->get_tile_type(&atile->loc);
+
+		if (ttype == AIE_TILE_TYPE_SHIMPL)
+			continue;
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d_%d: ",
+				 atile->loc.col, atile->loc.row);
+		len += aie_sysfs_get_dma_status(apart, &atile->loc,
+						&buffer[len], size - len);
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-error.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-error.c
new file mode 100644
index 000000000..8f2dcd708
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-error.c
@@ -0,0 +1,349 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+#include "linux/xlnx-ai-engine.h"
+
+static char *aie_error_category_str[] = {
+	"saturation",
+	"floating_point",
+	"stream_switch",
+	"access",
+	"bus",
+	"instruction",
+	"ecc",
+	"lock",
+	"dma",
+	"memory_parity",
+};
+
+/**
+ * aie_get_errors_str() - returns errors in string format. Errors of the same
+ *			  category are separated by a '|' symbol with the error
+ *			  category sting as a label prefix.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine tile.
+ * @module: module type.
+ * @err_attr: error attribute for given module type.
+ * @buffer: location to return error string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_get_errors_str(struct aie_partition *apart,
+				  struct aie_location loc,
+				  enum aie_module_type module,
+				  const struct aie_error_attr *err_attr,
+				  char *buffer, ssize_t size)
+{
+	ssize_t len = 0;
+	u32 i, j;
+	char *mod;
+
+	if (module == AIE_CORE_MOD)
+		mod = "core";
+	else if (module == AIE_MEM_MOD)
+		mod = "memory";
+	else
+		mod = "pl";
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		const struct aie_err_category *category;
+		char errstr[AIE_SYSFS_ERROR_SIZE];
+		bool is_delimit_req = false;
+		bool err = false;
+		ssize_t l = 0;
+		u8 index;
+
+		category = &err_attr->err_category[i];
+		index = category->err_category;
+		for (j = 0; j < category->num_events; j++) {
+			u8 event = category->prop[j].event;
+			char *str = category->prop[j].event_str;
+
+			if (!aie_check_error_bitmap(apart, loc, module, event))
+				continue;
+
+			if (is_delimit_req) {
+				l += scnprintf(&errstr[l],
+					       max(0L, AIE_SYSFS_ERROR_SIZE - l),
+					       DELIMITER_LEVEL0);
+			}
+
+			l += scnprintf(&errstr[l],
+				       max(0L, AIE_SYSFS_ERROR_SIZE - l), str);
+			err = true;
+			is_delimit_req = true;
+		}
+
+		if (err) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%s: %s: %s\n", mod,
+					 aie_error_category_str[index], errstr);
+		}
+	}
+	return len;
+}
+
+/**
+ * aie_get_error_category_str() - returns error categories in string format.
+ *				  Errors categories are separated by a '|'
+ *				  symbol.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine tile.
+ * @module: module type.
+ * @err_attr: error attribute for given module type.
+ * @buffer: location to return error string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_get_error_category_str(struct aie_partition *apart,
+					  struct aie_location loc,
+					  enum aie_module_type module,
+					  const struct aie_error_attr *err_attr,
+					  char *buffer, ssize_t size)
+{
+	ssize_t len = 0;
+	u32 i, j;
+	bool is_delimit_req = false;
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		const struct aie_err_category *category;
+
+		category = &err_attr->err_category[i];
+		for (j = 0; j < category->num_events; j++) {
+			u8 event = category->prop[j].event;
+			u8 index = category->err_category;
+
+			if (!aie_check_error_bitmap(apart, loc, module, event))
+				continue;
+
+			if (is_delimit_req) {
+				len += scnprintf(&buffer[len], max(0L, size - len),
+						 DELIMITER_LEVEL0);
+			}
+
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					aie_error_category_str[index]);
+			is_delimit_req = true;
+			break;
+		}
+	}
+	return len;
+}
+
+/**
+ * aie_tile_show_error() - exports detailed error information to a tile level
+ *			   sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_error(struct device *dev, struct device_attribute *attr,
+			    char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	const struct aie_error_attr *core_attr, *mem_attr, *pl_attr;
+	ssize_t len = 0, size = PAGE_SIZE;
+	u32 ttype, core_count = 0, mem_count = 0, pl_count = 0;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	ttype = apart->adev->ops->get_tile_type(&atile->loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		core_attr = apart->adev->core_errors;
+		mem_attr = apart->adev->mem_errors;
+		core_count = aie_get_module_error_count(apart, atile->loc,
+							AIE_CORE_MOD,
+							core_attr);
+		mem_count = aie_get_module_error_count(apart, atile->loc,
+						       AIE_MEM_MOD, mem_attr);
+	} else {
+		pl_attr = apart->adev->shim_errors;
+		pl_count = aie_get_module_error_count(apart, atile->loc,
+						      AIE_PL_MOD, pl_attr);
+	}
+
+	if (!(core_count || mem_count || pl_count)) {
+		mutex_unlock(&apart->mlock);
+		return len;
+	}
+
+	if (core_count) {
+		len += aie_get_errors_str(apart, atile->loc, AIE_CORE_MOD,
+					  core_attr, &buffer[len], size - len);
+	}
+
+	if (mem_count) {
+		len += aie_get_errors_str(apart, atile->loc, AIE_MEM_MOD,
+					  mem_attr, &buffer[len], size - len);
+	}
+
+	if (pl_count) {
+		len += aie_get_errors_str(apart, atile->loc, AIE_PL_MOD,
+					  pl_attr, &buffer[len], size - len);
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
+
+/**
+ * aie_part_show_error_stat() - exports error count in a partition to a
+ *				partition level sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_show_error_stat(struct device *dev,
+				 struct device_attribute *attr, char *buffer)
+{
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	const struct aie_error_attr *core_attr, *mem_attr, *pl_attr;
+	ssize_t len = 0, size = PAGE_SIZE;
+	u32 index, core = 0, mem = 0, pl = 0;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		u32 ttype = apart->adev->ops->get_tile_type(&atile->loc);
+
+		if (ttype == AIE_TILE_TYPE_TILE) {
+			core_attr = apart->adev->core_errors;
+			mem_attr = apart->adev->mem_errors;
+			core += aie_get_module_error_count(apart, atile->loc,
+							   AIE_CORE_MOD,
+							   core_attr);
+			mem += aie_get_module_error_count(apart, atile->loc,
+							  AIE_MEM_MOD,
+							  mem_attr);
+		} else {
+			pl_attr = apart->adev->shim_errors;
+			pl += aie_get_module_error_count(apart, atile->loc,
+							 AIE_PL_MOD, pl_attr);
+		}
+	}
+
+	mutex_unlock(&apart->mlock);
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "core: %d\n", core);
+	len += scnprintf(&buffer[len], max(0L, size - len), "memory: %d\n",
+			 mem);
+	len += scnprintf(&buffer[len], max(0L, size - len), "pl: %d\n", pl);
+	return len;
+}
+
+/**
+ * aie_sysfs_get_errors() - returns all asserted error categories in string
+ *			    format. Error categories within module label,
+ *			    are separated by a '|' symbol.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine tile.
+ * @buffer: location to return error string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_sysfs_get_errors(struct aie_partition *apart,
+			     struct aie_location *loc, char *buffer,
+			     ssize_t size)
+{
+	u32 ttype, core_count = 0, mem_count = 0, pl_count = 0;
+	const struct aie_error_attr *core_attr, *mem_attr, *pl_attr;
+	ssize_t len = 0;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		core_attr = apart->adev->core_errors;
+		mem_attr = apart->adev->mem_errors;
+		core_count = aie_get_module_error_count(apart, *loc,
+							AIE_CORE_MOD,
+							core_attr);
+		mem_count  = aie_get_module_error_count(apart, *loc,
+							AIE_MEM_MOD, mem_attr);
+	} else {
+		pl_attr = apart->adev->shim_errors;
+		pl_count = aie_get_module_error_count(apart, *loc, AIE_PL_MOD,
+						      pl_attr);
+	}
+
+	if (!(core_count || mem_count || pl_count))
+		return len;
+
+	if (core_count) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "core: ");
+		len += aie_get_error_category_str(apart, *loc, AIE_CORE_MOD,
+						  core_attr, &buffer[len],
+						  size - len);
+	}
+
+	if (mem_count) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "%smemory: ",
+				 core_count ? DELIMITER_LEVEL1 : "");
+		len += aie_get_error_category_str(apart, *loc, AIE_MEM_MOD,
+						  mem_attr, &buffer[len],
+						  size - len);
+	}
+
+	if (pl_count) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "pl: ");
+		len += aie_get_error_category_str(apart, *loc, AIE_PL_MOD,
+						  pl_attr, &buffer[len],
+						  size - len);
+	}
+	return len;
+}
+
+/**
+ * aie_part_read_cb_error() - exports errors with a given partition to
+ *			      partition level node.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_error(struct kobject *kobj, char *buffer, ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	ssize_t len = 0;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	if (!(aie_get_error_count(apart))) {
+		mutex_unlock(&apart->mlock);
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d_%d: ",
+				 atile->loc.col, atile->loc.row);
+		len += aie_sysfs_get_errors(apart, &atile->loc, &buffer[len],
+					    size - len);
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-event.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-event.c
new file mode 100644
index 000000000..b7683b690
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-event.c
@@ -0,0 +1,125 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_tile_print_event() - formats events strings from each module into a
+ *			    single buffer.
+ * @atile: AI engine tile.
+ * @buffer: export buffer.
+ * @core: core module event string
+ * @mem: memory module event string
+ * @pl: pl module event string
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_tile_print_event(struct aie_tile *atile, char *buffer,
+				    char *core, char *mem, char *pl)
+{
+	ssize_t len = 0, size = PAGE_SIZE;
+	u32 ttype;
+
+	ttype = atile->apart->adev->ops->get_tile_type(&atile->loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "core: %s\n", core);
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "memory: %s\n", mem);
+	} else {
+		len += scnprintf(&buffer[len], max(0L, size - len), "pl: %s\n",
+				 pl);
+	}
+	return len;
+}
+
+/**
+ * aie_tile_show_event() - exports all active events in a given tile to a
+ *			   tile level sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_event(struct device *dev, struct device_attribute *attr,
+			    char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	ssize_t l = 0;
+	unsigned long cs[4] = {0}, ms[4] = {0}, ps[4] = {0};
+	u32 ttype, n;
+	char core_buf[AIE_SYSFS_EVENT_STS_SIZE],
+	     mem_buf[AIE_SYSFS_EVENT_STS_SIZE],
+	     pl_buf[AIE_SYSFS_EVENT_STS_SIZE];
+	bool is_delimit_req = false;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return 0;
+	}
+
+	ttype = apart->adev->ops->get_tile_type(&atile->loc);
+
+	if (!aie_part_check_clk_enable_loc(apart, &atile->loc)) {
+		mutex_unlock(&apart->mlock);
+		return aie_tile_print_event(atile, buffer, "clock_gated",
+					    "clock_gated", "clock_gated");
+	}
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		aie_read_event_status(apart, &atile->loc, AIE_CORE_MOD,
+				      (u32 *)cs);
+		aie_read_event_status(apart, &atile->loc, AIE_MEM_MOD,
+				      (u32 *)ms);
+	} else {
+		aie_read_event_status(apart, &atile->loc, AIE_PL_MOD,
+				      (u32 *)ps);
+	}
+
+	for_each_set_bit(n, cs, 128) {
+		if (is_delimit_req) {
+			l += scnprintf(&core_buf[l],
+				       max(0L, AIE_SYSFS_EVENT_STS_SIZE - l),
+				       DELIMITER_LEVEL0);
+		}
+
+		l += scnprintf(&core_buf[l],
+			       max(0L, AIE_SYSFS_EVENT_STS_SIZE - l), "%d", n);
+		is_delimit_req = true;
+	}
+
+	l = 0;
+	is_delimit_req = false;
+	for_each_set_bit(n, ms, 128) {
+		if (is_delimit_req) {
+			l += scnprintf(&mem_buf[l],
+				       max(0L, AIE_SYSFS_EVENT_STS_SIZE - l),
+				       DELIMITER_LEVEL0);
+		}
+
+		l += scnprintf(&mem_buf[l],
+			       max(0L, AIE_SYSFS_EVENT_STS_SIZE - l), "%d", n);
+		is_delimit_req = true;
+	}
+
+	l = 0;
+	is_delimit_req = false;
+	for_each_set_bit(n, ps, 128) {
+		if (is_delimit_req) {
+			l += scnprintf(&pl_buf[l],
+				       max(0L, AIE_SYSFS_EVENT_STS_SIZE - l),
+				       DELIMITER_LEVEL0);
+		}
+
+		l += scnprintf(&pl_buf[l],
+			       max(0L, AIE_SYSFS_EVENT_STS_SIZE - l), "%d", n);
+		is_delimit_req = true;
+	}
+
+	mutex_unlock(&apart->mlock);
+	return aie_tile_print_event(atile, buffer, core_buf, mem_buf, pl_buf);
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-lock.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-lock.c
new file mode 100644
index 000000000..db77b1b09
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-lock.c
@@ -0,0 +1,183 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_get_lock_status() - reads the lock status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_lock_status(struct aie_partition *apart,
+			       struct aie_location *loc)
+{
+	u32 ttype, stsoff, regoff;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		stsoff = apart->adev->pl_lock->sts_regoff;
+	else
+		stsoff = apart->adev->mem_lock->sts_regoff;
+	regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+	return ioread32(apart->adev->base + regoff);
+}
+
+/**
+ * aie_get_lock_status_str() - returns the string value corresponding to
+ *			       lock status value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine lock.
+ * @status: status value of lock.
+ * @lock: lock ID.
+ * @buffer: location to return lock status string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_get_lock_status_str(struct aie_partition *apart,
+				       struct aie_location *loc, u32 status,
+				       u32 lock, char *buffer, ssize_t size)
+{
+	char **str = apart->adev->lock_status_str;
+	u32 ttype, mask;
+	u8 value, shift;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype != AIE_TILE_TYPE_TILE) {
+		shift = lock * apart->adev->pl_lock->sts.regoff;
+		mask = (apart->adev->pl_lock->sts.mask) << shift;
+	} else {
+		shift = lock * apart->adev->mem_lock->sts.regoff;
+		mask = (apart->adev->mem_lock->sts.mask) << shift;
+	}
+
+	value = (status & mask) >> shift;
+	return scnprintf(buffer, max(0L, size), str[value]);
+}
+
+/**
+ * aie_tile_show_lock() - exports AI engine lock status to a tile level sysfs
+ *			  node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_lock(struct device *dev, struct device_attribute *attr,
+			   char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	ssize_t len = 0, size = PAGE_SIZE;
+	unsigned long status;
+	u32 ttype, i, num_locks;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	ttype = apart->adev->ops->get_tile_type(&atile->loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		num_locks = apart->adev->pl_lock->num_locks;
+	else
+		num_locks = apart->adev->mem_lock->num_locks;
+
+	if (!aie_part_check_clk_enable_loc(apart, &atile->loc)) {
+		for (i = 0; i < num_locks; i++) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%d: clock_gated\n", i);
+		}
+		mutex_unlock(&apart->mlock);
+		return len;
+	}
+
+	status = aie_get_lock_status(apart, &atile->loc);
+	for (i = 0; i < num_locks; i++) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d: ", i);
+		len += aie_get_lock_status_str(apart, &atile->loc, status, i,
+					       &buffer[len], size - len);
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+	mutex_unlock(&apart->mlock);
+	return len;
+}
+
+ssize_t aie_sysfs_get_lock_status(struct aie_partition *apart,
+				  struct aie_location *loc, char *buffer,
+				  ssize_t size)
+{
+	u32 i, ttype, num_locks;
+	unsigned long status;
+	ssize_t len = 0;
+
+	ttype = apart->adev->ops->get_tile_type(loc);
+	if (ttype == AIE_TILE_TYPE_SHIMPL)
+		return len;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "clock_gated");
+		return len;
+	}
+
+	if (ttype != AIE_TILE_TYPE_TILE)
+		num_locks = apart->adev->pl_lock->num_locks;
+	else
+		num_locks = apart->adev->mem_lock->num_locks;
+
+	status = aie_get_lock_status(apart, loc);
+	for (i = 0; i < num_locks; i++) {
+		len += aie_get_lock_status_str(apart, loc, status, i,
+					       &buffer[len], size - len);
+		if (i < num_locks - 1) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+		}
+	}
+	return len;
+}
+
+/**
+ * aie_part_read_cb_lock() - exports status of all lock modules within a given
+ *			     partition to partition level node.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_lock(struct kobject *kobj, char *buffer, ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	ssize_t len = 0;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		u32 ttype = apart->adev->ops->get_tile_type(&atile->loc);
+
+		if (ttype == AIE_TILE_TYPE_SHIMPL)
+			continue;
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d_%d: ",
+				 atile->loc.col, atile->loc.row);
+		len += aie_sysfs_get_lock_status(apart, &atile->loc,
+						 &buffer[len], size - len);
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-status.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-status.c
new file mode 100644
index 000000000..6f3378ac4
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-status.c
@@ -0,0 +1,103 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine driver AIE device specific implementation
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include <linux/slab.h>
+
+#include "ai-engine-internal.h"
+
+/**
+ * aie_part_read_cb_status() - exports status of cores, DMAs, errors, and locks
+ *			       within a partition at a partition level node.
+ *			       this node serves as a single access point to
+ *			       query the status of a partition by a
+ *			       script/tool. For a given tile location, core
+ *			       status, DMAs, etc are separated by a ';' symbol.
+ *			       Core status information is captured under 'cs'
+ *			       label, DMA under 'ds', errors under 'es', and
+ *			       lock status under 'ls'.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_status(struct kobject *kobj, char *buffer,
+				ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	ssize_t len = 0;
+	size_t temp_size = AIE_SYSFS_CORE_STS_SIZE + AIE_SYSFS_CHAN_STS_SIZE +
+			   AIE_SYSFS_ERROR_CATEGORY_SIZE +
+			   AIE_SYSFS_LOCK_STS_SIZE;
+	char *cs_buf, *ds_buf, *es_buf, *ls_buf;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	cs_buf = kmalloc(temp_size, GFP_KERNEL);
+	if (!cs_buf) {
+		mutex_unlock(&apart->mlock);
+		return len;
+	}
+
+	ds_buf = cs_buf + AIE_SYSFS_CORE_STS_SIZE;
+	es_buf = ds_buf + AIE_SYSFS_CHAN_STS_SIZE;
+	ls_buf = es_buf + AIE_SYSFS_ERROR_CATEGORY_SIZE;
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		u32 cs = 0, ds = 0, es = 0, ls = 0;
+
+		cs = aie_sysfs_get_core_status(apart, &atile->loc, cs_buf,
+					       AIE_SYSFS_CORE_STS_SIZE);
+		ds = aie_sysfs_get_dma_status(apart, &atile->loc, ds_buf,
+					      AIE_SYSFS_CHAN_STS_SIZE);
+		es = aie_sysfs_get_errors(apart, &atile->loc, es_buf,
+					  AIE_SYSFS_ERROR_CATEGORY_SIZE);
+		ls = aie_sysfs_get_lock_status(apart, &atile->loc, ls_buf,
+					       AIE_SYSFS_LOCK_STS_SIZE);
+
+		if (!(cs || ds || es || ls))
+			continue;
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d_%d: ",
+				 atile->loc.col, atile->loc.row);
+
+		if (cs) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "cs: %s", cs_buf);
+		}
+
+		if (ds) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%sds: %s", cs ?
+					 DELIMITER_LEVEL2 : "", ds_buf);
+		}
+
+		if (es) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%ses: %s", (cs || ds) ?
+					 DELIMITER_LEVEL2 : "", es_buf);
+		}
+
+		if (ls) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%sls: %s", (cs || ds || es) ?
+					 DELIMITER_LEVEL2 : "", ls_buf);
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	mutex_unlock(&apart->mlock);
+	kfree(cs_buf);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs.c
new file mode 100644
index 000000000..637087d5f
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs.c
@@ -0,0 +1,334 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_sysfs_read_handler() - sysfs binary attribute read handler.
+ * @filp: file pointer.
+ * @kobj: pointer to the kobject.
+ * @attr: sysfs binary attribute.
+ * @buf: buffer to copy the data to.
+ * @offset: offset into the sysfs file.
+ * @max_size: maximum length of data that could be copied to buf.
+ * @return: length of data actually copied to buf.
+ */
+ssize_t aie_sysfs_read_handler(struct file *filp, struct kobject *kobj,
+			       struct bin_attribute *attr, char *buf,
+			       loff_t offset, size_t max_size)
+{
+	ssize_t len = max_size;
+	struct aie_sysfs_prop *prop;
+
+	prop = attr->private;
+	if (!prop->data)
+		return 0;
+
+	if (!offset)
+		prop->size = prop->read_callback(kobj, prop->data,
+						 prop->max_size);
+
+	if (offset >= prop->size)
+		return 0;
+
+	if (offset + max_size > prop->size)
+		len = prop->size - offset;
+
+	memcpy(buf,  prop->data + offset, len);
+	return len;
+}
+
+/**
+ * aie_sysfs_create_dev_attr() - dynamically allocates and initialize a device
+ *				 attribute
+ * @dev: device to allocate attribute for.
+ * @attr: AI engine device attribute.
+ * @return: pointer to the allocated device attribute.
+ */
+static struct device_attribute *
+aie_sysfs_create_dev_attr(struct device *dev, const struct aie_dev_attr *attr)
+{
+	struct device_attribute *node;
+
+	node = devm_kzalloc(dev, sizeof(struct device_attribute), GFP_KERNEL);
+	if (!node)
+		return ERR_PTR(-ENOMEM);
+
+	sysfs_attr_init(&node->attr);
+
+	node->attr.name = attr->name;
+	node->attr.mode = attr->mode;
+	node->show = attr->show;
+	return node;
+}
+
+/**
+ * aie_sysfs_create_bin_attr() - dynamically allocates and initialize a binary
+ *				 attribute
+ * @dev: device to allocate attribute for.
+ * @attr: AI engine binary attribute.
+ * @return: pointer to the allocated binary attribute.
+ */
+static struct bin_attribute *
+aie_sysfs_create_bin_attr(struct device *dev, const struct aie_bin_attr *attr)
+{
+	struct bin_attribute *node;
+	struct aie_sysfs_prop *prop;
+
+	node = devm_kzalloc(dev, sizeof(struct bin_attribute), GFP_KERNEL);
+	if (!node)
+		return ERR_PTR(-ENOMEM);
+
+	sysfs_bin_attr_init(node);
+
+	node->attr.name = attr->name;
+	node->attr.mode = attr->mode;
+	node->size = attr->size;
+	node->read = attr->read;
+
+	prop = devm_kzalloc(dev, sizeof(struct aie_sysfs_prop), GFP_KERNEL);
+	if (!prop)
+		return ERR_PTR(-ENOMEM);
+
+	prop->data = devm_kzalloc(dev, node->size, GFP_KERNEL);
+	if (!prop->data)
+		return ERR_PTR(-ENOMEM);
+
+	prop->max_size = node->size;
+	prop->read_callback = attr->read_callback;
+	node->private = prop;
+	return node;
+}
+
+/**
+ * aie_tile_sysfs_create() - creates sysfs nodes at the tile level.
+ * @atile: AI engine tile.
+ * @return: 0 for success, error code for failure.
+ */
+static int aie_tile_sysfs_create(struct aie_tile *atile)
+{
+	struct attribute_group *attr_grp;
+	struct bin_attribute **bin_attrs;
+	struct attribute **dev_attrs;
+	const struct aie_sysfs_attr *attr;
+	int ret = 0;
+	u32 ttype;
+	u32 index, i = 0, j = 0;
+
+	attr = atile->apart->adev->tile_sysfs_attr;
+	ttype = atile->apart->adev->ops->get_tile_type(&atile->loc);
+
+	if (attr->num_dev_attrs) {
+		dev_attrs = devm_kzalloc(&atile->dev, sizeof(*dev_attrs) *
+					 (attr->num_dev_attrs + 1), GFP_KERNEL);
+		if (!dev_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_dev_attrs; index++) {
+			struct device_attribute *node;
+			const struct aie_dev_attr *dev_attr;
+
+			dev_attr = &attr->dev_attr[index];
+
+			if (!(BIT(ttype) & attr->dev_attr[index].tile_type))
+				continue;
+
+			node = aie_sysfs_create_dev_attr(&atile->dev, dev_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			dev_attrs[i++] = &node->attr;
+		}
+	}
+
+	if (attr->num_bin_attrs) {
+		bin_attrs = devm_kzalloc(&atile->dev, sizeof(*bin_attrs) *
+					 (attr->num_bin_attrs + 1), GFP_KERNEL);
+		if (!bin_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_bin_attrs; index++) {
+			struct bin_attribute *node;
+			const struct aie_bin_attr *bin_attr;
+
+			bin_attr = &attr->bin_attr[index];
+
+			if (!(BIT(ttype) & attr->bin_attr[index].tile_type))
+				continue;
+
+			node = aie_sysfs_create_bin_attr(&atile->dev, bin_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			bin_attrs[j++] = node;
+		}
+	}
+
+	if (attr->num_dev_attrs || attr->num_bin_attrs) {
+		attr_grp = devm_kzalloc(&atile->dev,
+					sizeof(struct attribute_group),
+					GFP_KERNEL);
+		if (!attr_grp)
+			return -ENOMEM;
+
+		atile->attr_grp = attr_grp;
+
+		if (attr->num_dev_attrs)
+			attr_grp->attrs = dev_attrs;
+
+		if (attr->num_bin_attrs)
+			attr_grp->bin_attrs = bin_attrs;
+
+		/* TODO - use the non managed api to create sysfs group.
+		 * This workaround solves an issue where the device_del()
+		 * removes the SYSFS files before the managed create group.
+		 * This results in an error where files cannot be found.
+		 */
+		ret = sysfs_create_group(&atile->dev.kobj, attr_grp);
+		if (ret) {
+			dev_err(&atile->dev,
+				"Failed to add sysfs attributes group\n");
+		}
+	}
+	return ret;
+}
+
+/**
+ * aie_part_sysfs_create() - creates sysfs nodes at the partition level.
+ * @apart: AI engine partition.
+ * @return: 0 for success, error code for failure.
+ */
+static int aie_part_sysfs_create(struct aie_partition *apart)
+{
+	const struct aie_sysfs_attr *attr;
+	struct attribute_group *attr_grp;
+	struct bin_attribute **bin_attrs;
+	struct attribute **dev_attrs;
+	int ret = 0;
+	u32 index;
+
+	attr = apart->adev->part_sysfs_attr;
+
+	if (attr->num_dev_attrs) {
+		dev_attrs = devm_kzalloc(&apart->dev, sizeof(*dev_attrs) *
+					 (attr->num_dev_attrs + 1), GFP_KERNEL);
+		if (!dev_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_dev_attrs; index++) {
+			struct device_attribute *node;
+			const struct aie_dev_attr *dev_attr;
+
+			dev_attr = &attr->dev_attr[index];
+
+			node = aie_sysfs_create_dev_attr(&apart->dev, dev_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			dev_attrs[index] = &node->attr;
+		}
+	}
+
+	if (attr->num_bin_attrs) {
+		bin_attrs = devm_kzalloc(&apart->dev, sizeof(*bin_attrs) *
+					 (attr->num_bin_attrs + 1), GFP_KERNEL);
+		if (!bin_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_bin_attrs; index++) {
+			struct bin_attribute *node;
+			const struct aie_bin_attr *bin_attr;
+
+			bin_attr = &attr->bin_attr[index];
+
+			node = aie_sysfs_create_bin_attr(&apart->dev, bin_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			bin_attrs[index] = node;
+		}
+	}
+
+	if (attr->num_dev_attrs || attr->num_bin_attrs) {
+		attr_grp = devm_kzalloc(&apart->dev,
+					sizeof(struct attribute_group),
+					GFP_KERNEL);
+		if (!attr_grp)
+			return -ENOMEM;
+
+		apart->attr_grp = attr_grp;
+
+		if (attr->num_dev_attrs)
+			attr_grp->attrs = dev_attrs;
+
+		if (attr->num_bin_attrs)
+			attr_grp->bin_attrs = bin_attrs;
+
+		/* TODO - use the non managed api to create sysfs group.
+		 * This workaround solves an issue where the device_del()
+		 * removes the SYSFS files before the managed create group.
+		 * This results in an error where files cannot be found.
+		 */
+		ret = sysfs_create_group(&apart->dev.kobj, attr_grp);
+		if (ret) {
+			dev_err(&apart->dev,
+				"Failed to add sysfs attributes group\n");
+		}
+	}
+	return ret;
+}
+
+/**
+ * aie_part_sysfs_create_entries() - creates sysfs group for partition device.
+ * @apart: AI engine partition.
+ * @return: 0 for success, error code for failure.
+ */
+int aie_part_sysfs_create_entries(struct aie_partition *apart)
+{
+	int ret;
+
+	ret = aie_part_sysfs_create(apart);
+	if (ret < 0) {
+		dev_err(&apart->dev, "Failed to create sysfs partition\n");
+		return ret;
+	}
+	return ret;
+}
+
+/**
+ * aie_tile_sysfs_create_entries() - creates sysfs group for tile device.
+ * @atile: AI engine tile.
+ * @return: 0 for success, error code for failure.
+ */
+int aie_tile_sysfs_create_entries(struct aie_tile *atile)
+{
+	int ret;
+
+	ret = aie_tile_sysfs_create(atile);
+	if (ret < 0) {
+		dev_err(&atile->dev, "Failed to create sysfs tile\n");
+		return ret;
+	}
+	return ret;
+}
+
+/**
+ * aie_part_sysfs_remove_entries() - removes sysfs group from partition device.
+ * @apart: AI engine partition.
+ */
+void aie_part_sysfs_remove_entries(struct aie_partition *apart)
+{
+	sysfs_remove_group(&apart->dev.kobj, apart->attr_grp);
+}
+
+/**
+ * aie_tile_sysfs_remove_entries() - removes sysfs group from tile device.
+ * @atile: AI engine tile.
+ */
+void aie_tile_sysfs_remove_entries(struct aie_tile *atile)
+{
+	sysfs_remove_group(&atile->dev.kobj, atile->attr_grp);
+}
diff --git a/drivers/misc/xilinx_flex_pm.c b/drivers/misc/xilinx_flex_pm.c
new file mode 100644
index 000000000..ad7b6ef9c
--- /dev/null
+++ b/drivers/misc/xilinx_flex_pm.c
@@ -0,0 +1,605 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx Flex Noc Performance Monitor driver.
+ * Copyright (c) 2019 Xilinx Inc.
+ */
+
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+
+#define to_xflex_dev_info(n)	((struct xflex_dev_info *)dev_get_drvdata(n))
+
+#define FPM_LAR_OFFSET			0xFB0
+#define FPM_UNLOCK			0xC5ACCE55
+
+#define FPM_RD_REQ_OFFSET		0x1000
+#define FPM_RD_RES_OFFSET		0x2000
+#define FPM_WR_REQ_OFFSET		0x3000
+#define FPM_WR_RES_OFFSET		0x4000
+
+#define FPM_PORT_SEL_OFFSET		0x134
+#define FPM_MAIN_CTRL_OFFSET		0x008
+#define FPM_SRC_SEL_OFFSET		0x138
+#define FPM_STATPERIOD			0x24
+#define FPM_CFGCTRL			0x0C
+#define FPM_LPD				0x4210002
+#define FPM_FPD				0x420c003
+
+#define FPM_VAL				0x300
+#define FPM_SRC				0x200
+#define FPM_WRRSP_L			0x70000
+#define FPM_WRREQ_L			0x60000
+#define FPM_RDRSP_L			0x50000
+#define FPM_RDREQ_L			0x40000
+#define FPM_PROBE_SHIFT			16
+#define FPM_COUNTER_OFFSET		0x14
+#define FPM_GLOBALEN			BIT(0)
+#define FPM_STATEN			BIT(3)
+#define FPM_STATCOND_DUMP		BIT(5)
+#define FPM_NUM_COUNTERS		4
+#define FPM_MAINCTL_DIS			0
+
+#define FPM_SRC_OFF			0x0
+#define FPM_SRC_CYCLE			0x1
+#define FPM_SRC_IDLE			0x2
+#define FPM_SRC_XFER			0x3
+#define FPM_SRC_BUSY			0x4
+#define FPM_SRC_WAIT			0x5
+#define FPM_SRC_PACKET			0x6
+
+/* Port values */
+#define FPM_PORT_LPD_AFIFS_AXI		0x0
+#define FPM_PORT_LPD_OCM		0x1
+#define FPM_PORT_LPD_OCMEXT		0x2
+#define FPM_PORT_PMC_RPU_AXI0		0x3
+
+#define FPM_PORT_FPDAXI			0x1
+#define FPM_PORT_PROTXPPU		0x2
+
+/**
+ * struct xflex_dev_info - Global Driver structure
+ * @dev: Device structure
+ * @baselpd: Iomapped LPD base address
+ * @basefpd: Iomapped FPD base address
+ * @funnel: Iomapped funnel register base address
+ * @counterid_lpd: LPD counter id
+ * @counterid_fpd: FPD counter id
+ * @mutex: avoid parallel access to device
+ */
+struct xflex_dev_info {
+	struct device *dev;
+	void __iomem *baselpd;
+	void __iomem *basefpd;
+	void __iomem *funnel;
+	u32 counterid_fpd;
+	u32 counterid_lpd;
+	struct mutex mutex; /* avoid parallel access to device */
+};
+
+/**
+ * enum xflex_sysfs_cmd_codes - sysfs command codes
+ * @XFLEX_GET_COUNTER_FPD: get the FPD counter value
+ * @XFLEX_SET_COUNTER_FPD: set the FPD counter value
+ * @XFLEX_GET_COUNTER_FPD_RDREQ: get the FPD read request count
+ * @XFLEX_GET_COUNTER_FPD_RDRSP: get the FPD read response count
+ * @XFLEX_GET_COUNTER_FPD_WRREQ: get the FPD write request count
+ * @XFLEX_GET_COUNTER_FPD_WRRSP: get the FPD write response count
+ * @XFLEX_GET_COUNTER_LPD_RDREQ: get the LPD read request count
+ * @XFLEX_GET_COUNTER_LPD_RDRSP: get the LPD read response count
+ * @XFLEX_GET_COUNTER_LPD_WRREQ: get the LPD write request count
+ * @XFLEX_GET_COUNTER_LPD_WRRSP: get the LPD write response count
+ * @XFLEX_SET_COUNTER_LPD: set the LPD counter value
+ * @XFLEX_SET_SRC_COUNTER_LPD: set the LPD source
+ * @XFLEX_SET_SRC_COUNTER_FPD: set the FPD source
+ * @XFLEX_SET_PORT_COUNTER_LPD: set the LPD port
+ * @XFLEX_SET_PORT_COUNTER_FPD: set the FPD port
+ */
+enum xflex_sysfs_cmd_codes {
+	XFLEX_GET_COUNTER_FPD = 0,
+	XFLEX_SET_COUNTER_FPD,
+	XFLEX_GET_COUNTER_FPD_RDREQ,
+	XFLEX_GET_COUNTER_FPD_RDRSP,
+	XFLEX_GET_COUNTER_FPD_WRREQ,
+	XFLEX_GET_COUNTER_FPD_WRRSP,
+	XFLEX_GET_COUNTER_LPD_RDREQ,
+	XFLEX_GET_COUNTER_LPD_RDRSP,
+	XFLEX_GET_COUNTER_LPD_WRREQ,
+	XFLEX_GET_COUNTER_LPD_WRRSP,
+	XFLEX_SET_COUNTER_LPD,
+	XFLEX_SET_SRC_COUNTER_LPD,
+	XFLEX_SET_SRC_COUNTER_FPD,
+	XFLEX_SET_PORT_COUNTER_LPD,
+	XFLEX_SET_PORT_COUNTER_FPD,
+};
+
+static inline void fpm_reg(void __iomem *base, u32 val, u32 offset)
+{
+	writel(val, base + FPM_RD_REQ_OFFSET + offset);
+	writel(val, base + FPM_RD_RES_OFFSET + offset);
+	writel(val, base + FPM_WR_REQ_OFFSET + offset);
+	writel(val, base + FPM_WR_RES_OFFSET + offset);
+}
+
+static void reset_default(struct device *dev, u32 counter, u32 domain)
+{
+	struct xflex_dev_info *flexpm = to_xflex_dev_info(dev);
+	void __iomem *base = flexpm->basefpd;
+	u32 offset;
+
+	if (domain == FPM_LPD)
+		base = flexpm->baselpd;
+
+	fpm_reg(base, FPM_MAINCTL_DIS, FPM_MAIN_CTRL_OFFSET);
+	fpm_reg(base, FPM_STATEN | FPM_STATCOND_DUMP, FPM_MAIN_CTRL_OFFSET);
+	fpm_reg(base, FPM_STATEN | FPM_STATCOND_DUMP, FPM_MAIN_CTRL_OFFSET);
+
+	offset = FPM_PORT_SEL_OFFSET + counter * FPM_COUNTER_OFFSET;
+	fpm_reg(base, FPM_PORT_LPD_OCM, offset);
+	offset = FPM_SRC_SEL_OFFSET + counter * FPM_COUNTER_OFFSET;
+	fpm_reg(base, FPM_SRC_PACKET, offset);
+
+	fpm_reg(base, 0, FPM_STATPERIOD);
+	fpm_reg(base, FPM_GLOBALEN, FPM_CFGCTRL);
+}
+
+/**
+ * xflex_sysfs_cmd - Implements sysfs operations
+ * @dev: Device structure
+ * @buf: Value to write
+ * @cmd: sysfs cmd
+ *
+ * Return: value read from the sysfs cmd on success and negative error code
+ *		otherwise.
+ */
+static int xflex_sysfs_cmd(struct device *dev, const char *buf,
+			   enum xflex_sysfs_cmd_codes cmd)
+{
+	struct xflex_dev_info *flexpm = to_xflex_dev_info(dev);
+	u32 domain, src, offset, reg, val, counter;
+	int ret;
+	u32 pm_api_ret[4] = {0, 0, 0, 0};
+
+	mutex_lock(&flexpm->mutex);
+
+	switch (cmd) {
+	case XFLEX_GET_COUNTER_LPD_WRRSP:
+		reg = flexpm->counterid_lpd | FPM_WRRSP_L | FPM_VAL;
+		domain = FPM_LPD;
+
+		break;
+
+	case XFLEX_GET_COUNTER_LPD_WRREQ:
+		reg = flexpm->counterid_lpd | FPM_WRRSP_L | FPM_VAL;
+		domain = FPM_LPD;
+
+		break;
+
+	case XFLEX_GET_COUNTER_LPD_RDRSP:
+		reg = flexpm->counterid_lpd | FPM_RDRSP_L | FPM_VAL;
+		domain = FPM_LPD;
+
+		break;
+
+	case XFLEX_GET_COUNTER_LPD_RDREQ:
+		reg = flexpm->counterid_lpd | FPM_RDREQ_L | FPM_VAL;
+		domain = FPM_LPD;
+
+		break;
+
+	case XFLEX_SET_COUNTER_LPD:
+		ret = kstrtou32(buf, 0, &val);
+		if (ret < 0)
+			goto exit_unlock;
+
+		flexpm->counterid_lpd = val;
+		reset_default(dev, val, FPM_LPD);
+		break;
+
+	case XFLEX_SET_PORT_COUNTER_FPD:
+		ret = kstrtou32(buf, 0, &val);
+		if (ret < 0)
+			goto exit_unlock;
+
+		counter = flexpm->counterid_fpd * FPM_COUNTER_OFFSET;
+		offset = FPM_PORT_SEL_OFFSET + counter * FPM_COUNTER_OFFSET;
+		fpm_reg(flexpm->basefpd, val, offset);
+		break;
+
+	case XFLEX_SET_PORT_COUNTER_LPD:
+		ret = kstrtou32(buf, 0, &val);
+		if (ret < 0)
+			goto exit_unlock;
+
+		counter = flexpm->counterid_lpd * FPM_COUNTER_OFFSET;
+		offset = FPM_PORT_SEL_OFFSET + counter * FPM_COUNTER_OFFSET;
+		fpm_reg(flexpm->baselpd, val, offset);
+		break;
+
+	case XFLEX_SET_SRC_COUNTER_LPD:
+		reg = flexpm->counterid_lpd;
+		domain = FPM_LPD;
+		ret = kstrtou32(buf, 0, &val);
+		if (ret < 0)
+			goto exit_unlock;
+
+		for (src = 0; src < FPM_NUM_COUNTERS; src++) {
+			reg = reg | FPM_SRC | (src << FPM_PROBE_SHIFT);
+			ret = zynqmp_pm_probe_counter_write(domain, reg, val);
+			if (ret < 0) {
+				dev_err(dev, "Counter write error %d\n", ret);
+				goto exit_unlock;
+			}
+		}
+		break;
+
+	case XFLEX_SET_SRC_COUNTER_FPD:
+		reg = flexpm->counterid_fpd;
+		domain = FPM_FPD;
+		ret = kstrtou32(buf, 0, &val);
+		if (ret < 0)
+			goto exit_unlock;
+
+		for (src = 0; src < FPM_NUM_COUNTERS; src++) {
+			reg = reg | FPM_SRC | (src << FPM_PROBE_SHIFT);
+			ret = zynqmp_pm_probe_counter_write(domain, reg, val);
+			if (ret < 0) {
+				dev_err(dev, "Counter write error %d\n", ret);
+				goto exit_unlock;
+			}
+		}
+		break;
+
+	case XFLEX_SET_COUNTER_FPD:
+		ret = kstrtou32(buf, 0, &val);
+		if (ret < 0)
+			goto exit_unlock;
+
+		flexpm->counterid_fpd = val;
+		reset_default(dev, val, FPM_FPD);
+		break;
+
+	case XFLEX_GET_COUNTER_FPD_WRRSP:
+		reg = flexpm->counterid_fpd | FPM_WRRSP_L | FPM_VAL;
+		domain = FPM_FPD;
+
+		break;
+
+	case XFLEX_GET_COUNTER_FPD_WRREQ:
+		reg = flexpm->counterid_fpd | FPM_WRREQ_L | FPM_VAL;
+		domain = FPM_FPD;
+
+		break;
+
+	case XFLEX_GET_COUNTER_FPD_RDRSP:
+		reg = flexpm->counterid_fpd | FPM_RDRSP_L | FPM_VAL;
+		domain = FPM_FPD;
+
+		break;
+
+	case XFLEX_GET_COUNTER_FPD_RDREQ:
+		reg = flexpm->counterid_fpd | FPM_RDREQ_L | FPM_VAL;
+		domain = FPM_FPD;
+
+		break;
+
+	default:
+		dev_err(dev, "Invalid option\n");
+		break;
+	}
+
+	ret = zynqmp_pm_probe_counter_read(domain, reg, &pm_api_ret[0]);
+
+	if (ret < 0) {
+		dev_err(dev, "Counter read error %d\n", ret);
+		return ret;
+	}
+	mutex_unlock(&flexpm->mutex);
+	return pm_api_ret[1];
+
+exit_unlock:
+	mutex_unlock(&flexpm->mutex);
+	return ret;
+}
+
+/* Sysfs functions */
+
+static ssize_t counterfpd_wrreq_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	int rdval = xflex_sysfs_cmd(dev, buf, XFLEX_GET_COUNTER_FPD_WRREQ);
+
+	if (rdval < 0)
+		return 0;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", rdval);
+}
+static DEVICE_ATTR_RO(counterfpd_wrreq);
+
+static ssize_t counterfpd_wrrsp_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	int rdval = xflex_sysfs_cmd(dev, buf, XFLEX_GET_COUNTER_FPD_WRRSP);
+
+	if (rdval < 0)
+		return 0;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", rdval);
+}
+static DEVICE_ATTR_RO(counterfpd_wrrsp);
+
+static ssize_t counterfpd_rdreq_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	int rdval = xflex_sysfs_cmd(dev, buf, XFLEX_GET_COUNTER_FPD_RDREQ);
+
+	if (rdval < 0)
+		return 0;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", rdval);
+}
+static DEVICE_ATTR_RO(counterfpd_rdreq);
+
+static ssize_t counterfpd_rdrsp_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	int rdval = xflex_sysfs_cmd(dev, buf, XFLEX_GET_COUNTER_FPD_RDRSP);
+
+	if (rdval < 0)
+		return 0;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", rdval);
+}
+static DEVICE_ATTR_RO(counterfpd_rdrsp);
+
+static ssize_t counterlpd_wrreq_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	int rdval = xflex_sysfs_cmd(dev, buf, XFLEX_GET_COUNTER_LPD_WRREQ);
+
+	if (rdval < 0)
+		return 0;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", rdval);
+}
+static DEVICE_ATTR_RO(counterlpd_wrreq);
+
+static ssize_t counterlpd_wrrsp_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	int rdval = xflex_sysfs_cmd(dev, buf, XFLEX_GET_COUNTER_LPD_WRRSP);
+
+	if (rdval < 0)
+		return 0;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", rdval);
+}
+static DEVICE_ATTR_RO(counterlpd_wrrsp);
+
+static ssize_t counterlpd_rdreq_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	int rdval = xflex_sysfs_cmd(dev, buf, XFLEX_GET_COUNTER_LPD_RDREQ);
+
+	if (rdval < 0)
+		return 0;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", rdval);
+}
+static DEVICE_ATTR_RO(counterlpd_rdreq);
+
+static ssize_t counterlpd_rdrsp_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	int rdval = xflex_sysfs_cmd(dev, buf, XFLEX_GET_COUNTER_LPD_RDRSP);
+
+	if (rdval < 0)
+		return 0;
+
+	return snprintf(buf, PAGE_SIZE, "%d\n", rdval);
+}
+static DEVICE_ATTR_RO(counterlpd_rdrsp);
+
+static ssize_t counterlpdsrc_store(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t size)
+{
+	xflex_sysfs_cmd(dev, buf, XFLEX_SET_SRC_COUNTER_LPD);
+
+	return size;
+}
+static DEVICE_ATTR_WO(counterlpdsrc);
+
+static ssize_t counterfpdsrc_store(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t size)
+{
+	xflex_sysfs_cmd(dev, buf, XFLEX_SET_SRC_COUNTER_FPD);
+
+	return size;
+}
+static DEVICE_ATTR_WO(counterfpdsrc);
+
+static ssize_t counterlpdport_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t size)
+{
+	xflex_sysfs_cmd(dev, buf, XFLEX_SET_PORT_COUNTER_LPD);
+
+	return size;
+}
+static DEVICE_ATTR_WO(counterlpdport);
+
+static ssize_t counterfpdport_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t size)
+{
+	xflex_sysfs_cmd(dev, buf, XFLEX_SET_PORT_COUNTER_FPD);
+
+	return size;
+}
+static DEVICE_ATTR_WO(counterfpdport);
+
+static ssize_t counteridlpd_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	struct xflex_dev_info *flexpm = to_xflex_dev_info(dev);
+
+	return snprintf(buf, PAGE_SIZE, "%08d\n", flexpm->counterid_lpd);
+}
+
+static ssize_t counteridlpd_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t size)
+{
+	int ret;
+	struct xflex_dev_info *flexpm = to_xflex_dev_info(dev);
+
+	ret = kstrtou32(buf, 0, &flexpm->counterid_lpd);
+	if (ret < 0)
+		return ret;
+
+	reset_default(dev, flexpm->counterid_lpd, FPM_LPD);
+
+	return size;
+}
+static DEVICE_ATTR_RW(counteridlpd);
+
+static ssize_t counteridfpd_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	struct xflex_dev_info *flexpm = to_xflex_dev_info(dev);
+
+	return snprintf(buf, PAGE_SIZE, "%08d\n", flexpm->counterid_fpd);
+}
+
+static ssize_t counteridfpd_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t size)
+{
+	int ret;
+	struct xflex_dev_info *flexpm = to_xflex_dev_info(dev);
+
+	ret = kstrtou32(buf, 0, &flexpm->counterid_fpd);
+	if (ret < 0)
+		return ret;
+
+	return size;
+}
+static DEVICE_ATTR_RW(counteridfpd);
+
+static struct attribute *xflex_attrs[] = {
+	&dev_attr_counterlpdsrc.attr,
+	&dev_attr_counterlpdport.attr,
+	&dev_attr_counterfpdsrc.attr,
+	&dev_attr_counterfpdport.attr,
+
+	&dev_attr_counterlpd_rdreq.attr,
+	&dev_attr_counterlpd_wrreq.attr,
+	&dev_attr_counterlpd_rdrsp.attr,
+	&dev_attr_counterlpd_wrrsp.attr,
+
+	&dev_attr_counterfpd_rdreq.attr,
+	&dev_attr_counterfpd_wrreq.attr,
+	&dev_attr_counterfpd_rdrsp.attr,
+	&dev_attr_counterfpd_wrrsp.attr,
+
+	&dev_attr_counteridlpd.attr,
+	&dev_attr_counteridfpd.attr,
+	NULL,
+};
+
+ATTRIBUTE_GROUPS(xflex);
+
+/**
+ * xflex_probe - Driver probe function
+ * @pdev: Pointer to the platform_device structure
+ *
+ * This is the driver probe routine. It does all the memory
+ * allocation and creates sysfs entries for the device.
+ *
+ * Return: 0 on success and failure value on error
+ */
+static int xflex_probe(struct platform_device *pdev)
+{
+	struct xflex_dev_info *flexpm;
+	struct resource *res;
+	int err;
+	struct device *dev = &pdev->dev;
+
+	flexpm = devm_kzalloc(dev, sizeof(*flexpm), GFP_KERNEL);
+	if (!flexpm)
+		return -ENOMEM;
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "baselpd");
+	flexpm->baselpd = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(flexpm->baselpd))
+		return PTR_ERR(flexpm->baselpd);
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "basefpd");
+	flexpm->basefpd = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(flexpm->basefpd))
+		return PTR_ERR(flexpm->basefpd);
+
+	res = platform_get_resource_byname(pdev, IORESOURCE_MEM, "funnel");
+	flexpm->funnel = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(flexpm->funnel))
+		return PTR_ERR(flexpm->funnel);
+
+	mutex_init(&flexpm->mutex);
+	writel(FPM_UNLOCK, flexpm->funnel + FPM_LAR_OFFSET);
+	writel(FPM_UNLOCK, flexpm->baselpd + FPM_LAR_OFFSET);
+
+	/* Create sysfs file entries for the device */
+	err = sysfs_create_groups(&dev->kobj, xflex_groups);
+	if (err < 0) {
+		dev_err(dev, "unable to create sysfs entries\n");
+		return err;
+	}
+
+	dev_set_drvdata(dev, flexpm);
+
+	return 0;
+}
+
+/**
+ * xflex_remove - Driver remove function
+ * @pdev: Pointer to the platform_device structure
+ *
+ * This function frees all the resources allocated to the device.
+ *
+ * Return: 0 always
+ */
+static int xflex_remove(struct platform_device *pdev)
+{
+	sysfs_remove_groups(&pdev->dev.kobj, xflex_groups);
+	return 0;
+}
+
+static const struct of_device_id xflex_of_match[] = {
+	{ .compatible = "xlnx,flexnoc-pm-2.7", },
+	{ /* end of table */ }
+};
+MODULE_DEVICE_TABLE(of, xflex_of_match);
+
+static struct platform_driver xflex_driver = {
+	.driver = {
+		.name = "xilinx-flex",
+		.of_match_table = xflex_of_match,
+	},
+	.probe = xflex_probe,
+	.remove = xflex_remove,
+};
+
+module_platform_driver(xflex_driver);
+
+MODULE_AUTHOR("Shubhrajyoti Datta <shubhrajyoti.datta@xilinx.com>");
+MODULE_DESCRIPTION("Xilinx Flexnoc performance monitor driver");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/misc/xilinx_trafgen.c b/drivers/misc/xilinx_trafgen.c
new file mode 100644
index 000000000..0c0c5c408
--- /dev/null
+++ b/drivers/misc/xilinx_trafgen.c
@@ -0,0 +1,1654 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AXI Traffic Generator driver.
+ *
+ * Copyright (c) 2013 - 2019 Xilinx Inc.
+ *
+ * Description:
+ * This driver is developed for AXI Traffic Generator IP, which is
+ * designed to generate AXI4 traffic which can be used to stress
+ * different modules/interconnect connected in the system. Different
+ * configurable options which are provided through sysfs entries
+ * allow the user to generate a wide variety of traffic based on
+ * their requirements.
+ */
+
+#include <linux/clk.h>
+#include <linux/dma-mapping.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+
+/* Hw specific definitions */
+
+/* Internal RAM Offsets */
+#define XTG_PARAM_RAM_OFFSET	   0x1000  /* Parameter RAM offset */
+#define XTG_COMMAND_RAM_OFFSET	   0x8000  /* Command RAM offset */
+#define XTG_COMMAND_RAM_MSB_OFFSET 0xa000	/**< Command RAM MSB Offset */
+#define XTG_MASTER_RAM_INIT_OFFSET 0x10000 /* Master RAM initial offset(v1.0) */
+#define XTG_MASTER_RAM_OFFSET	   0xc000  /* Master RAM offset */
+#define XTG_WRITE_COMMAND_RAM_OFFSET	0x9000  /* Write Command RAM offset */
+
+/* Register Offsets */
+#define XTG_MCNTL_OFFSET	0x00	/* Master control */
+#define XTG_SCNTL_OFFSET	0x04	/* Slave control */
+#define XTG_ERR_STS_OFFSET	0x08	/* Error status  */
+#define XTG_ERR_EN_OFFSET	0x0C	/* Error enable */
+#define XTG_MSTERR_INTR_OFFSET	0x10	/* Master error interrupt enable */
+#define XTG_CFG_STS_OFFSET	0x14	/* Config status */
+#define XTG_STREAM_CNTL_OFFSET	0x30	/* Streaming Control */
+#define XTG_STREAM_CFG_OFFSET	0x34	/* Streaming Control */
+#define XTG_STREAM_TL_OFFSET	0x38    /* Streaming Transfer Length */
+#define XTG_STREAM_TKTS1_OFFSET	0x40    /* Streaming tkeep tstrb set1*/
+#define XTG_STREAM_TKTS2_OFFSET	0x44    /* Streaming tkeep tstrb set2*/
+#define XTG_STREAM_TKTS3_OFFSET	0x48    /* Streaming tkeep tstrb set3*/
+#define XTG_STREAM_TKTS4_OFFSET	0x4C    /* Streaming tkeep tstrb set4*/
+#define XTG_STATIC_CNTL_OFFSET	0x60	/* Static Control */
+#define XTG_STATIC_LEN_OFFSET	0x64	/* Static Length */
+
+/* Register Bitmasks/shifts */
+
+/* Master logic enable */
+#define XTG_MCNTL_MSTEN_MASK		0x00100000
+/* Loop enable */
+#define XTG_MCNTL_LOOPEN_MASK		0x00080000
+/* Slave error interrupt enable */
+#define XTG_SCNTL_ERREN_MASK		0x00008000
+/* Master complete interrupt enable */
+#define XTG_ERR_EN_MSTIRQEN_MASK	0x80000000
+/* Master error interrupt enable */
+#define XTG_MSTERR_INTR_MINTREN_MASK	0x00008000
+/* Master complete done status */
+#define XTG_ERR_STS_MSTDONE_MASK	0x80000000
+/* Error mask for error status/enable registers */
+#define XTG_ERR_ALL_ERRS_MASK		0x801F0003
+/* Core Revision shift */
+#define XTG_MCNTL_REV_SHIFT		24
+
+/* Axi Traffic Generator Command RAM Entry field mask/shifts */
+
+/* Command RAM entry masks */
+#define XTG_LEN_MASK		0xFF		/* Driven to a*_len line  */
+#define XTG_LOCK_MASK		0x1		/* Driven to a*_lock line */
+#define XTG_BURST_MASK		0x3		/* Driven to a*_burst line */
+#define XTG_SIZE_MASK		0x7		/* Driven to a*_size line */
+#define XTG_ID_MASK		0x1F		/* Driven to a*_id line */
+#define XTG_PROT_MASK		0x7		/* Driven to a*_prot line */
+#define XTG_LAST_ADDR_MASK	0x7		/* Last address */
+#define XTG_VALID_CMD_MASK	0x1		/* Valid Command */
+#define XTG_MSTRAM_INDEX_MASK	0x1FFF		/* Master RAM Index */
+#define XTG_OTHER_DEPEND_MASK	0x1FF		/* Other depend Command no */
+#define XTG_MY_DEPEND_MASK	0x1FF		/* My depend command no */
+#define XTG_QOS_MASK		0xF		/* Driven to a*_qos line */
+#define XTG_USER_MASK		0xFF		/* Driven to a*_user line */
+#define XTG_CACHE_MASK		0xF		/* Driven to a*_cache line */
+#define XTG_EXPECTED_RESP_MASK	0x7		/* Expected response */
+
+/* Command RAM entry shift values */
+#define XTG_LEN_SHIFT		0		/* Driven to a*_len line  */
+#define XTG_LOCK_SHIFT		8		/* Driven to a*_lock line */
+#define XTG_BURST_SHIFT		10		/* Driven to a*_burst line */
+#define XTG_SIZE_SHIFT		12		/* Driven to a*_size line */
+#define XTG_ID_SHIFT		15		/* Driven to a*_id line */
+#define XTG_PROT_SHIFT		21		/* Driven to a*_prot line */
+#define XTG_LAST_ADDR_SHIFT	28		/* Last address */
+#define XTG_VALID_CMD_SHIFT	31		/* Valid Command */
+#define XTG_MSTRAM_INDEX_SHIFT	0		/* Master RAM Index */
+#define XTG_OTHER_DEPEND_SHIFT	13		/* Other depend cmd num */
+#define XTG_MY_DEPEND_SHIFT	22		/* My depend cmd num */
+#define XTG_QOS_SHIFT		16		/* Driven to a*_qos line */
+#define XTG_USER_SHIFT		5		/* Driven to a*_user line */
+#define XTG_CACHE_SHIFT		4		/* Driven to a*_cache line */
+#define XTG_EXPECTED_RESP_SHIFT	0		/* Expected response */
+
+/* Axi Traffic Generator Parameter RAM Entry field mask/shifts */
+
+/* Parameter RAM Entry field shift values */
+#define XTG_PARAM_ADDRMODE_SHIFT	24	/* Address mode */
+#define XTG_PARAM_INTERVALMODE_SHIFT	26	/* Interval mode */
+#define XTG_PARAM_IDMODE_SHIFT		28	/* Id mode */
+#define XTG_PARAM_OP_SHIFT		29	/* Opcode */
+
+/* PARAM RAM Opcode shift values */
+#define XTG_PARAM_COUNT_SHIFT		0	/* Repeat/Delay count */
+#define XTG_PARAM_DELAYRANGE_SHIFT	0	/* Delay range */
+#define XTG_PARAM_DELAY_SHIFT		8	/* FIXED RPT delay count */
+#define XTG_PARAM_ADDRRANGE_SHIFT	20	/* Address range */
+
+/* Parameter RAM Entry field mask values */
+#define XTG_PARAM_ADDRMODE_MASK		0x3	/* Address mode */
+#define XTG_PARAM_INTERVALMODE_MASK	0x3	/* Interval mode */
+#define XTG_PARAM_IDMODE_MASK		0x1	/* Id mode */
+#define XTG_PARAM_OP_MASK		0x7	/* Opcode */
+
+/* PARAM RAM Opcode mask values */
+#define XTG_PARAM_COUNT_MASK		0xFFFFFF/* Repeat/Delay count */
+#define XTG_PARAM_DELAYRANGE_MASK	0xFF	/* Delay range */
+#define XTG_PARAM_DELAY_MASK		0xFFF	/* FIXED RPT delay count */
+#define XTG_PARAM_ADDRRANGE_MASK	0xF	/* Address range */
+
+/* PARAM RAM Opcode values */
+#define XTG_PARAM_OP_NOP		0x0	/* NOP mode */
+#define XTG_PARAM_OP_RPT		0x1	/* Repeat mode */
+#define XTG_PARAM_OP_DELAY		0x2	/* Delay mode */
+#define XTG_PARAM_OP_FIXEDRPT		0x3	/* Fixed repeat delay */
+
+/* Axi Traffic Generator Static Mode masks */
+#define XTG_STATIC_CNTL_TD_MASK		0x00000002	/* Transfer Done Mask */
+#define XTG_STATIC_CNTL_STEN_MASK	0x00000001	/* Static Enable Mask */
+#define XTG_STATIC_CNTL_RESET_MASK	0x00000000	/* Static Reset Mask */
+
+/* Axi Traffic Generator Stream Mode mask/shifts */
+#define XTG_STREAM_CNTL_STEN_MASK   0x00000001	/* Stream Enable Mask */
+#define XTG_STREAM_TL_TCNT_MASK	    0xFFFF0000	/* Transfer Count Mask */
+#define XTG_STREAM_TL_TLEN_MASK	    0x0000FFFF	/* Transfer Length Mask */
+#define XTG_STREAM_TL_TCNT_SHIFT    16		/* Transfer Count Shift */
+
+/* Driver Specific Definitions */
+
+#define MAX_NUM_ENTRIES	256	/* Number of command entries per region */
+
+#define VALID_SIG	0xa5a5a5a5	/* Valid unique identifier */
+
+/* Internal RAM Sizes */
+#define XTG_PRM_RAM_BLOCK_SIZE	0x400	/* PRAM Block size (1KB) */
+#define XTG_CMD_RAM_BLOCK_SIZE	0x1000	/* CRAM Block size (4KB) */
+#define XTG_EXTCMD_RAM_BLOCK_SIZE 0x400	/**< Extended CMDRAM Block Size (1KB) */
+#define XTG_PARAM_RAM_SIZE	0x800	/* Parameter RAM (2KB) */
+#define XTG_COMMAND_RAM_SIZE	0x2000	/* Command RAM (8KB) */
+#define XTG_EXTCMD_RAM_SIZE	0x800	/* Command RAM (2KB) */
+#define XTG_MASTER_RAM_SIZE	0x2000	/* Master RAM (8KB) */
+
+/* RAM Access Flags */
+#define XTG_READ_RAM		0x0	/* Read RAM flag */
+#define XTG_WRITE_RAM		0x1	/* Write RAM flag */
+#define XTG_WRITE_RAM_ZERO	0x2	/* Write Zero flag */
+
+/* Bytes per entry */
+#define XTG_CRAM_BYTES_PER_ENTRY	16 /* CRAM bytes per entry */
+#define XTG_PRAM_BYTES_PER_ENTRY	4  /* PRAM bytes per entry */
+
+/* Interrupt Definitions */
+#define XTG_MASTER_CMP_INTR	0x1	/* Master complete intr flag */
+#define XTG_MASTER_ERR_INTR	0x2	/* Master error intr flag */
+#define XTG_SLAVE_ERR_INTR	0x4	/* Slave error intr flag */
+
+/*
+ * Version value of the trafgen core.
+ * For the initial IP release the version(v1.0) value is 0x47
+ * From the v2.0 IP and onwards the value starts from  0x20.
+ * For eg:
+ * v2.1 -> 0x21
+ * v2.2 -> 0x22 ... so on.
+ *
+ */
+#define XTG_INIT_VERSION	0x47	/* Trafgen initial version(v1.0) */
+
+/* Macro */
+#define to_xtg_dev_info(n)	((struct xtg_dev_info *)dev_get_drvdata(n))
+
+#define CMD_WDS	0x4	/* No of words in command ram per command */
+#define EXT_WDS	0x1	/* No of words in extended ram per command */
+#define MSB_INDEX	0x4
+/**
+ * struct xtg_cram - Command RAM structure
+ * @addr: Address Driven to a*_addr line
+ * @valid_cmd: Valid Command
+ * @last_addr: Last address
+ * @prot: Driven to a*_prot line
+ * @id: Driven to a*_id line
+ * @size: Driven to a*_size line
+ * @burst: Driven to a*_burst line
+ * @lock: Driven to a*_lock line
+ * @length: Driven to a*_len line
+ * @my_dpnd: My Depend command number
+ * @other_dpnd: Other depend command number
+ * @mram_idx: Master RAM index
+ * @qos: Driven to a*_qos line
+ * @user: Driven to a*_user line
+ * @cache: Driven to a*_cache line
+ * @expected_resp: Expected response
+ * @index: Command Index
+ * @is_write_block: Write/Read block
+ * @is_valid_req: Unique signature
+ *
+ * FIXME: This structure is shared with the user application and
+ * hence need to be synchronized. We know these kind of structures
+ * should not be defined in the driver and this need to be fixed
+ * if found a proper placeholder (in uapi/).
+ */
+struct xtg_cram {
+	phys_addr_t addr;
+	u32 valid_cmd;
+	u32 last_addr;
+	u32 prot;
+	u32 id;
+	u32 size;
+	u32 burst;
+	u32 lock;
+	u32 length;
+	u32 my_dpnd;
+	u32 other_dpnd;
+	u32 mram_idx;
+	u32 qos;
+	u32 user;
+	u32 cache;
+	u32 expected_resp;
+	u16 index;
+	bool is_write_block;
+	u32 is_valid_req;
+};
+
+/**
+ * struct xtg_pram - Parameter RAM structure
+ * @op_cntl0: Control field 0
+ * @op_cntl1: Control field 1
+ * @op_cntl2: Control field 2
+ * @addr_mode: Address mode
+ * @interval_mode: Interval mode
+ * @id_mode: Id mode
+ * @opcode: Opcode
+ * @index: Command Index
+ * @is_write_block: Write/Read block
+ * @is_valid_req: Unique signature
+ *
+ * FIXME: This structure is shared with the user application and
+ * hence need to be synchronized. We know these kind of structures
+ * should not be defined in the driver and this need to be fixed
+ * if found a proper placeholder (in uapi/).
+ */
+struct xtg_pram {
+	u32 op_cntl0;
+	u32 op_cntl1;
+	u32 op_cntl2;
+	u32 addr_mode;
+	u32 interval_mode;
+	u32 id_mode;
+	u32 opcode;
+	u16 index;
+	bool is_write_block;
+	u32 is_valid_req;
+};
+
+/**
+ * struct xtg_dev_info - Global Driver structure
+ * @regs: Iomapped base address
+ * @dev: Device structure
+ * @phys_base_addr: Physical base address
+ * @last_rd_valid_idx: Last Read Valid Command Index
+ * @last_wr_valid_idx: Last Write Valid Command Index
+ * @id: Device instance id
+ * @xtg_mram_offset: MasterRam offset
+ * @clk: Input clock
+ */
+struct xtg_dev_info {
+	void __iomem *regs;
+	struct device *dev;
+	phys_addr_t phys_base_addr;
+	s16 last_rd_valid_idx;
+	s16 last_wr_valid_idx;
+	u32 id;
+	u32 xtg_mram_offset;
+	struct clk *clk;
+};
+
+/**
+ * enum xtg_sysfs_ioctl - Ioctl opcodes
+ * @XTG_GET_MASTER_CMP_STS: get master complete status
+ * @XTG_GET_SLV_CTRL_REG: get slave control reg status
+ * @XTG_GET_ERR_STS: get error status
+ * @XTG_GET_CFG_STS: get config status
+ * @XTG_GET_LAST_VALID_INDEX: get last valid index
+ * @XTG_GET_DEVICE_ID: get device id
+ * @XTG_GET_RESOURCE: get resource
+ * @XTG_GET_STATIC_ENABLE: get staic mode traffic genration state
+ * @XTG_GET_STATIC_BURSTLEN: get static mode burst length
+ * @XTG_GET_STATIC_TRANSFERDONE: get static transfer done
+ * @XTG_GET_STREAM_ENABLE : get strean mode traffic genration state
+ * @XTG_GET_STREAM_TRANSFERLEN: get streaming mode transfer length
+ * @XTG_GET_STREAM_TRANSFERCNT: get streaming mode transfer count
+ * @XTG_GET_MASTER_LOOP_EN: get master loop enable status
+ * @XTG_GET_STREAM_TKTS1: get stream tstrb and tkeep set1 values
+ * @XTG_GET_STREAM_TKTS2: get stream tstrb and tkeep set2 values
+ * @XTG_GET_STREAM_TKTS3: get stream tstrb and tkeep set3 values
+ * @XTG_GET_STREAM_TKTS4: get stream tstrb and tkeep set4 values
+ * @XTG_GET_STREAM_CFG: get stream configuration values
+ * @XTG_START_MASTER_LOGIC: start master logic
+ * @XTG_SET_SLV_CTRL_REG: set slave control
+ * @XTG_CLEAR_ERRORS: clear errors
+ * @XTG_ENABLE_ERRORS: enable errors
+ * @XTG_ENABLE_INTRS: enable interrupts
+ * @XTG_CLEAR_MRAM: clear master ram
+ * @XTG_CLEAR_CRAM: clear command ram
+ * @XTG_CLEAR_PRAM: clear parameter ram
+ * @XTG_SET_STATIC_ENABLE: enable static mode traffic genration
+ * @XTG_SET_STATIC_DISABLE: disable static mode traffic genration
+ * @XTG_SET_STATIC_BURSTLEN: set static mode burst length
+ * @XTG_SET_STATIC_TRANSFERDONE: set static transfer done
+ * @XTG_SET_STREAM_ENABLE: enable streaming mode traffic genration
+ * @XTG_SET_STREAM_DISABLE: disable streaming mode traffic genration
+ * @XTG_SET_STREAM_TRANSFERLEN: set streaming mode transfer length
+ * @XTG_SET_STREAM_TRANSFERCNT: set streaming mode transfer count
+ * @XTG_SET_STREAM_TKTS1: set stream tstrb and tkeep set1 values
+ * @XTG_SET_STREAM_TKTS2: set stream tstrb and tkeep set2 values
+ * @XTG_SET_STREAM_TKTS3: set stream tstrb and tkeep set3 values
+ * @XTG_SET_STREAM_TKTS4: set stream tstrb and tkeep set4 values
+ * @XTG_SET_STREAM_CFG: set stream configuration values
+ * @XTG_MASTER_LOOP_EN: enable master loop
+ */
+enum xtg_sysfs_ioctl_opcode {
+	XTG_GET_MASTER_CMP_STS,
+	XTG_GET_SLV_CTRL_REG,
+	XTG_GET_ERR_STS,
+	XTG_GET_CFG_STS,
+	XTG_GET_LAST_VALID_INDEX,
+	XTG_GET_DEVICE_ID,
+	XTG_GET_RESOURCE,
+	XTG_GET_STATIC_ENABLE,
+	XTG_GET_STATIC_BURSTLEN,
+	XTG_GET_STATIC_TRANSFERDONE,
+	XTG_GET_STREAM_ENABLE,
+	XTG_GET_STREAM_TRANSFERLEN,
+	XTG_GET_MASTER_LOOP_EN,
+	XTG_GET_STREAM_TKTS1,
+	XTG_GET_STREAM_TKTS2,
+	XTG_GET_STREAM_TKTS3,
+	XTG_GET_STREAM_TKTS4,
+	XTG_GET_STREAM_CFG,
+	XTG_GET_STREAM_TRANSFERCNT,
+	XTG_START_MASTER_LOGIC,
+	XTG_SET_SLV_CTRL_REG,
+	XTG_CLEAR_ERRORS,
+	XTG_ENABLE_ERRORS,
+	XTG_ENABLE_INTRS,
+	XTG_CLEAR_MRAM,
+	XTG_CLEAR_CRAM,
+	XTG_CLEAR_PRAM,
+	XTG_SET_STATIC_ENABLE,
+	XTG_SET_STATIC_DISABLE,
+	XTG_SET_STATIC_BURSTLEN,
+	XTG_SET_STATIC_TRANSFERDONE,
+	XTG_SET_STREAM_ENABLE,
+	XTG_SET_STREAM_DISABLE,
+	XTG_SET_STREAM_TRANSFERLEN,
+	XTG_SET_STREAM_TRANSFERCNT,
+	XTG_SET_STREAM_TKTS1,
+	XTG_SET_STREAM_TKTS2,
+	XTG_SET_STREAM_TKTS3,
+	XTG_SET_STREAM_TKTS4,
+	XTG_SET_STREAM_CFG,
+	XTG_MASTER_LOOP_EN
+};
+
+/**
+ * xtg_access_rams - Write/Read Master/Command/Parameter RAM
+ * @tg: Pointer to xtg_dev_info structure
+ * @where: Offset from base
+ * @count: Number of bytes to write/read
+ * @flags: Read/Write/Write Zero
+ * @data: Data pointer
+ */
+static void xtg_access_rams(struct xtg_dev_info *tg, int where,
+			    int count, int flags, u32 *data)
+{
+	u32 index;
+
+	switch (flags) {
+	case XTG_WRITE_RAM_ZERO:
+		memset_io(tg->regs + where, 0, count);
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+		writel(0x0, tg->regs + where +
+			(XTG_COMMAND_RAM_MSB_OFFSET - XTG_COMMAND_RAM_OFFSET) +
+			XTG_EXTCMD_RAM_BLOCK_SIZE - XTG_CMD_RAM_BLOCK_SIZE);
+#endif
+		break;
+	case XTG_WRITE_RAM:
+		for (index = 0; count > 0; index++, count -= 4)
+			writel(data[index], tg->regs + where + index * 4);
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	/*
+	 * This additional logic is required only for command ram.
+	 * when writing to READ Command RAM write higher address to READ addr
+	 * RAM
+	 */
+	if (where >= XTG_COMMAND_RAM_OFFSET &&
+	    where < XTG_WRITE_COMMAND_RAM_OFFSET)
+		writel(data[MSB_INDEX],	tg->regs + XTG_COMMAND_RAM_OFFSET +
+			(where - XTG_COMMAND_RAM_OFFSET) / 4 +
+			(XTG_COMMAND_RAM_MSB_OFFSET - XTG_COMMAND_RAM_OFFSET));
+	/*
+	 * Writing to WRITE Command RAM write higher address to WRITE addr RAM
+	 */
+	if (where >=  XTG_WRITE_COMMAND_RAM_OFFSET &&
+	    where < XTG_COMMAND_RAM_MSB_OFFSET)
+		writel(data[MSB_INDEX],	tg->regs +
+			XTG_WRITE_COMMAND_RAM_OFFSET +
+			(where - XTG_WRITE_COMMAND_RAM_OFFSET) / 4 +
+			(XTG_COMMAND_RAM_MSB_OFFSET - XTG_COMMAND_RAM_OFFSET) +
+			XTG_EXTCMD_RAM_BLOCK_SIZE - XTG_CMD_RAM_BLOCK_SIZE);
+#endif
+		break;
+	case XTG_READ_RAM:
+		for (index = 0; count > 0; index++, count -= 4)
+			data[index] = readl(tg->regs + where + index * 4);
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	if (where >= XTG_COMMAND_RAM_OFFSET &&
+	    where < XTG_WRITE_COMMAND_RAM_OFFSET)
+		data[MSB_INDEX] = readl(tg->regs + XTG_COMMAND_RAM_OFFSET +
+			(where - XTG_COMMAND_RAM_OFFSET) / 4 +
+			(XTG_COMMAND_RAM_MSB_OFFSET - XTG_COMMAND_RAM_OFFSET));
+
+	if (where >=  XTG_WRITE_COMMAND_RAM_OFFSET &&
+	    where < XTG_COMMAND_RAM_MSB_OFFSET)
+		data[MSB_INDEX] = readl(tg->regs +
+			XTG_WRITE_COMMAND_RAM_OFFSET +
+			(where - XTG_WRITE_COMMAND_RAM_OFFSET) / 4 +
+			(XTG_COMMAND_RAM_MSB_OFFSET - XTG_COMMAND_RAM_OFFSET) +
+			XTG_EXTCMD_RAM_BLOCK_SIZE - XTG_CMD_RAM_BLOCK_SIZE);
+#endif
+		break;
+	}
+}
+
+/**
+ * xtg_prepare_cmd_words - Prepares all four Command RAM words
+ * @tg: Pointer to xtg_dev_info structure
+ * @cmdp: Pointer to xtg_cram structure
+ * @cmd_words: Pointer to Command Words that needs to be prepared
+ */
+static void xtg_prepare_cmd_words(struct xtg_dev_info *tg,
+				  const struct xtg_cram *cmdp, u32 *cmd_words)
+{
+	/* Command Word 0 */
+	cmd_words[0] = lower_32_bits(cmdp->addr);
+
+	/* Command Word 4 */
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	cmd_words[MSB_INDEX] = upper_32_bits(cmdp->addr);
+#endif
+
+	/* Command Word 1 */
+	cmd_words[1] = 0;
+	cmd_words[1] |= (cmdp->length & XTG_LEN_MASK) << XTG_LEN_SHIFT;
+	cmd_words[1] |= (cmdp->lock & XTG_LOCK_MASK) << XTG_LOCK_SHIFT;
+	cmd_words[1] |= (cmdp->burst & XTG_BURST_MASK) << XTG_BURST_SHIFT;
+	cmd_words[1] |= (cmdp->size & XTG_SIZE_MASK) << XTG_SIZE_SHIFT;
+	cmd_words[1] |= (cmdp->id & XTG_ID_MASK) << XTG_ID_SHIFT;
+	cmd_words[1] |= (cmdp->prot & XTG_PROT_MASK) << XTG_PROT_SHIFT;
+	cmd_words[1] |= (cmdp->last_addr & XTG_LAST_ADDR_MASK) <<
+					XTG_LAST_ADDR_SHIFT;
+	cmd_words[1] |= (cmdp->valid_cmd & XTG_VALID_CMD_MASK) <<
+					XTG_VALID_CMD_SHIFT;
+
+	/* Command Word 2 */
+	cmd_words[2] = 0;
+	cmd_words[2] |= (cmdp->mram_idx & XTG_MSTRAM_INDEX_MASK) <<
+					XTG_MSTRAM_INDEX_SHIFT;
+	cmd_words[2] |= (cmdp->other_dpnd & XTG_OTHER_DEPEND_MASK) <<
+					XTG_OTHER_DEPEND_SHIFT;
+	cmd_words[2] |= (cmdp->my_dpnd & XTG_MY_DEPEND_MASK) <<
+					XTG_MY_DEPEND_SHIFT;
+
+	/* Command Word 3 */
+	cmd_words[3] = 0;
+	cmd_words[3] |= (cmdp->qos & XTG_QOS_MASK) << XTG_QOS_SHIFT;
+	cmd_words[3] |= (cmdp->user & XTG_USER_MASK) << XTG_USER_SHIFT;
+	cmd_words[3] |= (cmdp->cache & XTG_CACHE_MASK) << XTG_CACHE_SHIFT;
+	cmd_words[3] |= (cmdp->expected_resp & XTG_EXPECTED_RESP_MASK) <<
+					XTG_EXPECTED_RESP_SHIFT;
+}
+
+/**
+ * xtg_prepare_param_words - Prepares Parameter RAM word
+ * @tg: Pointer to xtg_dev_info structure
+ * @cmdp: Pointer to xtg_pram structure
+ * @param_word: Pointer to Param Word that needs to be prepared
+ */
+static void xtg_prepare_param_word(struct xtg_dev_info *tg,
+				   const struct xtg_pram *cmdp, u32 *param_word)
+{
+	*param_word = 0;
+	*param_word |= (cmdp->opcode & XTG_PARAM_OP_MASK) << XTG_PARAM_OP_SHIFT;
+	*param_word |= (cmdp->addr_mode & XTG_PARAM_ADDRMODE_MASK) <<
+					XTG_PARAM_ADDRMODE_SHIFT;
+	*param_word |= (cmdp->id_mode & XTG_PARAM_IDMODE_MASK) <<
+					XTG_PARAM_IDMODE_SHIFT;
+	*param_word |= (cmdp->interval_mode & XTG_PARAM_INTERVALMODE_MASK) <<
+					XTG_PARAM_INTERVALMODE_SHIFT;
+
+	switch (cmdp->opcode) {
+	case XTG_PARAM_OP_RPT:
+	case XTG_PARAM_OP_DELAY:
+		*param_word |= (cmdp->op_cntl0 & XTG_PARAM_COUNT_MASK) <<
+					XTG_PARAM_COUNT_SHIFT;
+		break;
+
+	case XTG_PARAM_OP_FIXEDRPT:
+		*param_word |= (cmdp->op_cntl0 & XTG_PARAM_ADDRRANGE_MASK) <<
+					XTG_PARAM_ADDRRANGE_SHIFT;
+		*param_word |= (cmdp->op_cntl1 & XTG_PARAM_DELAY_MASK) <<
+					XTG_PARAM_DELAY_SHIFT;
+		*param_word |= (cmdp->op_cntl2 & XTG_PARAM_DELAYRANGE_MASK) <<
+					XTG_PARAM_DELAYRANGE_SHIFT;
+		break;
+
+	case XTG_PARAM_OP_NOP:
+		*param_word = 0;
+		break;
+	}
+}
+
+/**
+ * xtg_sysfs_ioctl - Implements sysfs operations
+ * @dev: Device structure
+ * @buf: Value to write
+ * @opcode: Ioctl opcode
+ *
+ * Return: value read from the sysfs opcode.
+ */
+static ssize_t xtg_sysfs_ioctl(struct device *dev, const char *buf,
+			       enum xtg_sysfs_ioctl_opcode opcode)
+{
+	struct xtg_dev_info *tg = to_xtg_dev_info(dev);
+	unsigned long wrval;
+	ssize_t status, rdval = 0;
+
+	if (opcode > XTG_GET_STREAM_TRANSFERCNT) {
+		status = kstrtoul(buf, 0, &wrval);
+		if (status < 0)
+			return status;
+	}
+
+	switch (opcode) {
+	case XTG_GET_MASTER_CMP_STS:
+		rdval = (readl(tg->regs + XTG_MCNTL_OFFSET) &
+				XTG_MCNTL_MSTEN_MASK) ? 1 : 0;
+		break;
+
+	case XTG_GET_MASTER_LOOP_EN:
+		rdval = (readl(tg->regs + XTG_MCNTL_OFFSET) &
+				XTG_MCNTL_LOOPEN_MASK) ? 1 : 0;
+		break;
+
+	case XTG_GET_SLV_CTRL_REG:
+		rdval = readl(tg->regs + XTG_SCNTL_OFFSET);
+		break;
+
+	case XTG_GET_ERR_STS:
+		rdval = readl(tg->regs + XTG_ERR_STS_OFFSET) &
+				XTG_ERR_ALL_ERRS_MASK;
+		break;
+
+	case XTG_GET_CFG_STS:
+		rdval = readl(tg->regs + XTG_CFG_STS_OFFSET);
+		break;
+
+	case XTG_GET_LAST_VALID_INDEX:
+		rdval = (((tg->last_wr_valid_idx << 16) & 0xffff0000) |
+				(tg->last_rd_valid_idx & 0xffff));
+		break;
+
+	case XTG_GET_DEVICE_ID:
+		rdval = tg->id;
+		break;
+
+	case XTG_GET_RESOURCE:
+		rdval = (unsigned long)tg->regs;
+		break;
+
+	case XTG_GET_STATIC_ENABLE:
+		rdval = readl(tg->regs + XTG_STATIC_CNTL_OFFSET);
+		break;
+
+	case XTG_GET_STATIC_BURSTLEN:
+		rdval = readl(tg->regs + XTG_STATIC_LEN_OFFSET);
+		break;
+
+	case XTG_GET_STATIC_TRANSFERDONE:
+		rdval = (readl(tg->regs + XTG_STATIC_CNTL_OFFSET) &
+				XTG_STATIC_CNTL_TD_MASK);
+		break;
+
+	case XTG_GET_STREAM_ENABLE:
+		rdval = readl(tg->regs + XTG_STREAM_CNTL_OFFSET);
+		break;
+
+	case XTG_GET_STREAM_TRANSFERLEN:
+		rdval = (readl(tg->regs + XTG_STREAM_TL_OFFSET) &
+				XTG_STREAM_TL_TLEN_MASK);
+		break;
+
+	case XTG_GET_STREAM_TRANSFERCNT:
+		rdval = ((readl(tg->regs + XTG_STREAM_TL_OFFSET) &
+				XTG_STREAM_TL_TCNT_MASK) >>
+				XTG_STREAM_TL_TCNT_SHIFT);
+		break;
+
+	case XTG_GET_STREAM_TKTS1:
+		rdval = readl(tg->regs + XTG_STREAM_TKTS1_OFFSET);
+		break;
+	case XTG_GET_STREAM_TKTS2:
+		rdval = readl(tg->regs + XTG_STREAM_TKTS2_OFFSET);
+		break;
+	case XTG_GET_STREAM_TKTS3:
+		rdval = readl(tg->regs + XTG_STREAM_TKTS3_OFFSET);
+		break;
+	case XTG_GET_STREAM_TKTS4:
+		rdval = readl(tg->regs + XTG_STREAM_TKTS4_OFFSET);
+		break;
+
+	case XTG_GET_STREAM_CFG:
+		rdval = (readl(tg->regs + XTG_STREAM_CFG_OFFSET));
+		break;
+
+	case XTG_START_MASTER_LOGIC:
+		if (wrval)
+			writel(readl(tg->regs + XTG_MCNTL_OFFSET) |
+					XTG_MCNTL_MSTEN_MASK,
+				tg->regs + XTG_MCNTL_OFFSET);
+		break;
+
+	case XTG_MASTER_LOOP_EN:
+		if (wrval)
+			writel(readl(tg->regs + XTG_MCNTL_OFFSET) |
+					XTG_MCNTL_LOOPEN_MASK,
+				tg->regs + XTG_MCNTL_OFFSET);
+		else
+			writel(readl(tg->regs + XTG_MCNTL_OFFSET) &
+					~XTG_MCNTL_LOOPEN_MASK,
+				tg->regs + XTG_MCNTL_OFFSET);
+		break;
+
+	case XTG_SET_SLV_CTRL_REG:
+		writel(wrval, tg->regs + XTG_SCNTL_OFFSET);
+		break;
+
+	case XTG_ENABLE_ERRORS:
+		wrval &= XTG_ERR_ALL_ERRS_MASK;
+		writel(wrval, tg->regs + XTG_ERR_EN_OFFSET);
+		break;
+
+	case XTG_CLEAR_ERRORS:
+		wrval &= XTG_ERR_ALL_ERRS_MASK;
+		writel(readl(tg->regs + XTG_ERR_STS_OFFSET) | wrval,
+		       tg->regs + XTG_ERR_STS_OFFSET);
+		break;
+
+	case XTG_ENABLE_INTRS:
+		if (wrval & XTG_MASTER_CMP_INTR) {
+			pr_info("Enabling Master Complete Interrupt\n");
+			writel(readl(tg->regs + XTG_ERR_EN_OFFSET) |
+					XTG_ERR_EN_MSTIRQEN_MASK,
+				tg->regs + XTG_ERR_EN_OFFSET);
+		}
+		if (wrval & XTG_MASTER_ERR_INTR) {
+			pr_info("Enabling Interrupt on Master Errors\n");
+			writel(readl(tg->regs + XTG_MSTERR_INTR_OFFSET) |
+					XTG_MSTERR_INTR_MINTREN_MASK,
+				tg->regs + XTG_MSTERR_INTR_OFFSET);
+		}
+		if (wrval & XTG_SLAVE_ERR_INTR) {
+			pr_info("Enabling Interrupt on Slave Errors\n");
+			writel(readl(tg->regs + XTG_SCNTL_OFFSET) |
+					XTG_SCNTL_ERREN_MASK,
+				tg->regs + XTG_SCNTL_OFFSET);
+		}
+		break;
+
+	case XTG_CLEAR_MRAM:
+		xtg_access_rams(tg, tg->xtg_mram_offset,
+				XTG_MASTER_RAM_SIZE,
+				XTG_WRITE_RAM_ZERO, NULL);
+		break;
+
+	case XTG_CLEAR_CRAM:
+		xtg_access_rams(tg, XTG_COMMAND_RAM_OFFSET,
+				XTG_COMMAND_RAM_SIZE,
+				XTG_WRITE_RAM_ZERO, NULL);
+		break;
+
+	case XTG_CLEAR_PRAM:
+		xtg_access_rams(tg, XTG_PARAM_RAM_OFFSET,
+				XTG_PARAM_RAM_SIZE,
+				XTG_WRITE_RAM_ZERO, NULL);
+		break;
+
+	case XTG_SET_STATIC_ENABLE:
+		if (wrval) {
+			wrval &= XTG_STATIC_CNTL_STEN_MASK;
+			writel(readl(tg->regs + XTG_STATIC_CNTL_OFFSET) | wrval,
+			       tg->regs + XTG_STATIC_CNTL_OFFSET);
+		} else {
+			writel(readl(tg->regs + XTG_STATIC_CNTL_OFFSET) &
+				~XTG_STATIC_CNTL_STEN_MASK,
+				tg->regs + XTG_STATIC_CNTL_OFFSET);
+		}
+		break;
+
+	case XTG_SET_STATIC_BURSTLEN:
+		writel(wrval, tg->regs + XTG_STATIC_LEN_OFFSET);
+		break;
+
+	case XTG_SET_STATIC_TRANSFERDONE:
+		wrval |= XTG_STATIC_CNTL_TD_MASK;
+		writel(readl(tg->regs + XTG_STATIC_CNTL_OFFSET) | wrval,
+		       tg->regs + XTG_STATIC_CNTL_OFFSET);
+		break;
+
+	case XTG_SET_STREAM_ENABLE:
+		if (wrval) {
+			rdval = readl(tg->regs + XTG_STREAM_CNTL_OFFSET);
+			rdval |= XTG_STREAM_CNTL_STEN_MASK,
+			writel(rdval,
+			       tg->regs + XTG_STREAM_CNTL_OFFSET);
+		} else {
+			writel(readl(tg->regs + XTG_STREAM_CNTL_OFFSET) &
+			       ~XTG_STREAM_CNTL_STEN_MASK,
+			       tg->regs + XTG_STREAM_CNTL_OFFSET);
+		}
+		break;
+
+	case XTG_SET_STREAM_TRANSFERLEN:
+		wrval &= XTG_STREAM_TL_TLEN_MASK;
+		rdval = readl(tg->regs + XTG_STREAM_TL_OFFSET);
+		rdval &= ~XTG_STREAM_TL_TLEN_MASK;
+		writel(rdval | wrval,
+		       tg->regs + XTG_STREAM_TL_OFFSET);
+		break;
+
+	case XTG_SET_STREAM_TRANSFERCNT:
+		wrval = ((wrval << XTG_STREAM_TL_TCNT_SHIFT) &
+				XTG_STREAM_TL_TCNT_MASK);
+		rdval = readl(tg->regs + XTG_STREAM_TL_OFFSET);
+		rdval = rdval & ~XTG_STREAM_TL_TCNT_MASK;
+		writel(rdval | wrval,
+		       tg->regs + XTG_STREAM_TL_OFFSET);
+		break;
+
+	case XTG_SET_STREAM_TKTS1:
+		writel(wrval, tg->regs + XTG_STREAM_TKTS1_OFFSET);
+		break;
+	case XTG_SET_STREAM_TKTS2:
+		writel(wrval, tg->regs + XTG_STREAM_TKTS2_OFFSET);
+		break;
+	case XTG_SET_STREAM_TKTS3:
+		writel(wrval, tg->regs + XTG_STREAM_TKTS3_OFFSET);
+		break;
+	case XTG_SET_STREAM_TKTS4:
+		writel(wrval, tg->regs + XTG_STREAM_TKTS4_OFFSET);
+		break;
+
+	case XTG_SET_STREAM_CFG:
+		writel(wrval, tg->regs + XTG_STREAM_CFG_OFFSET);
+		break;
+
+	default:
+		break;
+	}
+
+	return rdval;
+}
+
+/* Sysfs functions */
+
+static ssize_t id_show(struct device *dev,
+		       struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_DEVICE_ID);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+static DEVICE_ATTR_RO(id);
+
+static ssize_t resource_show(struct device *dev,
+			     struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_RESOURCE);
+
+	return snprintf(buf, PAGE_SIZE, "0x%08zx\n", rdval);
+}
+static DEVICE_ATTR_RO(resource);
+
+static ssize_t master_start_stop_show(struct device *dev,
+				      struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_MASTER_CMP_STS);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t master_start_stop_store(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_START_MASTER_LOGIC);
+
+	return size;
+}
+static DEVICE_ATTR_RW(master_start_stop);
+
+static ssize_t config_slave_status_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_SLV_CTRL_REG);
+
+	return snprintf(buf, PAGE_SIZE, "0x%08zx\n", rdval);
+}
+
+static ssize_t config_slave_status_store(struct device *dev,
+					 struct device_attribute *attr,
+					 const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_SLV_CTRL_REG);
+
+	return size;
+}
+static DEVICE_ATTR_RW(config_slave_status);
+
+static ssize_t err_sts_show(struct device *dev,
+			    struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_ERR_STS);
+
+	return snprintf(buf, PAGE_SIZE, "0x%08zx\n", rdval);
+}
+
+static ssize_t err_sts_store(struct device *dev,
+			     struct device_attribute *attr,
+			     const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_CLEAR_ERRORS);
+
+	return size;
+}
+static DEVICE_ATTR_RW(err_sts);
+
+static ssize_t err_en_store(struct device *dev,
+			    struct device_attribute *attr, const char *buf,
+			    size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_ENABLE_ERRORS);
+
+	return size;
+}
+static DEVICE_ATTR_WO(err_en);
+
+static ssize_t intr_en_store(struct device *dev,
+			     struct device_attribute *attr,
+			     const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_ENABLE_INTRS);
+
+	return size;
+}
+static DEVICE_ATTR_WO(intr_en);
+
+static ssize_t last_valid_index_show(struct device *dev,
+				     struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_LAST_VALID_INDEX);
+
+	return snprintf(buf, PAGE_SIZE, "0x%08zx\n", rdval);
+}
+static DEVICE_ATTR_RO(last_valid_index);
+
+static ssize_t config_sts_show(struct device *dev,
+			       struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_CFG_STS);
+
+	return snprintf(buf, PAGE_SIZE, "0x%08zx\n", rdval);
+}
+static DEVICE_ATTR_RO(config_sts);
+
+static ssize_t mram_clear_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_CLEAR_MRAM);
+
+	return size;
+}
+static DEVICE_ATTR_WO(mram_clear);
+
+static ssize_t cram_clear_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_CLEAR_CRAM);
+
+	return size;
+}
+static DEVICE_ATTR_WO(cram_clear);
+
+static ssize_t pram_clear_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_CLEAR_CRAM);
+
+	return size;
+}
+static DEVICE_ATTR_WO(pram_clear);
+
+static ssize_t static_enable_show(struct device *dev,
+				  struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STATIC_ENABLE);
+
+	return snprintf(buf, PAGE_SIZE, "0x%08zx\n", rdval);
+}
+
+static ssize_t static_enable_store(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STATIC_ENABLE);
+
+	return size;
+}
+static DEVICE_ATTR_RW(static_enable);
+
+static ssize_t static_burstlen_show(struct device *dev,
+				    struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STATIC_BURSTLEN);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t static_burstlen_store(struct device *dev,
+				     struct device_attribute *attr,
+				     const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STATIC_BURSTLEN);
+
+	return size;
+}
+static DEVICE_ATTR_RW(static_burstlen);
+
+static ssize_t stream_cfg_show(struct device *dev,
+			       struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STREAM_CFG);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t stream_cfg_store(struct device *dev,
+				struct device_attribute *attr, const char *buf,
+				size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STREAM_CFG);
+
+	return size;
+}
+static DEVICE_ATTR_RW(stream_cfg);
+
+static ssize_t stream_tkts4_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STREAM_TKTS4);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t stream_tkts4_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STREAM_TKTS4);
+
+	return size;
+}
+static DEVICE_ATTR_RW(stream_tkts4);
+
+static ssize_t stream_tkts3_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STREAM_TKTS3);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t stream_tkts3_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STREAM_TKTS3);
+
+	return size;
+}
+static DEVICE_ATTR_RW(stream_tkts3);
+
+static ssize_t stream_tkts2_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STREAM_TKTS2);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t stream_tkts2_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STREAM_TKTS2);
+
+	return size;
+}
+static DEVICE_ATTR_RW(stream_tkts2);
+
+static ssize_t stream_tkts1_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STREAM_TKTS1);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t stream_tkts1_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STREAM_TKTS1);
+
+	return size;
+}
+static DEVICE_ATTR_RW(stream_tkts1);
+
+static ssize_t static_transferdone_show(struct device *dev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STATIC_TRANSFERDONE);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t static_transferdone_store(struct device *dev,
+					 struct device_attribute *attr,
+					 const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STATIC_TRANSFERDONE);
+
+	return size;
+}
+static DEVICE_ATTR_RW(static_transferdone);
+
+static ssize_t reset_static_transferdone_show(struct device *dev,
+					      struct device_attribute *attr,
+					      char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STATIC_TRANSFERDONE);
+
+	if (rdval == XTG_STATIC_CNTL_RESET_MASK)
+		rdval = 1;
+	else
+		rdval = 0;
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+static DEVICE_ATTR_RO(reset_static_transferdone);
+
+static ssize_t stream_enable_show(struct device *dev,
+				  struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STREAM_ENABLE);
+
+	return snprintf(buf, PAGE_SIZE, "0x%08zx\n", rdval);
+}
+
+static ssize_t stream_enable_store(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STREAM_ENABLE);
+
+	return size;
+}
+static DEVICE_ATTR_RW(stream_enable);
+
+static ssize_t stream_transferlen_show(struct device *dev,
+				       struct device_attribute *attr,
+				       char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STREAM_TRANSFERLEN);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t stream_transferlen_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STREAM_TRANSFERLEN);
+
+	return size;
+}
+static DEVICE_ATTR_RW(stream_transferlen);
+
+static ssize_t stream_transfercnt_show(struct device *dev,
+				       struct device_attribute *attr,
+				       char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_STREAM_TRANSFERCNT);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t stream_transfercnt_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_SET_STREAM_TRANSFERCNT);
+
+	return size;
+}
+static DEVICE_ATTR_RW(stream_transfercnt);
+
+static ssize_t loop_enable_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	ssize_t rdval = xtg_sysfs_ioctl(dev, buf, XTG_GET_MASTER_LOOP_EN);
+
+	return snprintf(buf, PAGE_SIZE, "%zd\n", rdval);
+}
+
+static ssize_t loop_enable_store(struct device *dev,
+				 struct device_attribute *attr,
+				 const char *buf, size_t size)
+{
+	xtg_sysfs_ioctl(dev, buf, XTG_MASTER_LOOP_EN);
+
+	return size;
+}
+static DEVICE_ATTR_RW(loop_enable);
+
+static ssize_t xtg_pram_read(struct file *filp, struct kobject *kobj,
+			     struct bin_attribute *bin_attr,
+			     char *buf, loff_t off, size_t count)
+{
+	pr_info("No read access to Parameter RAM\n");
+
+	return 0;
+}
+
+static ssize_t xtg_pram_write(struct file *filp, struct kobject *kobj,
+			      struct bin_attribute *bin_attr,
+			      char *buf, loff_t off, size_t count)
+{
+	struct xtg_dev_info *tg =
+		to_xtg_dev_info(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *)buf;
+	struct xtg_pram *cmdp = (struct xtg_pram *)buf;
+	u32 param_word;
+
+	if (off >= XTG_PARAM_RAM_SIZE) {
+		pr_err("Requested Write len exceeds 2K PRAM size\n");
+		return -ENOMEM;
+	}
+
+	if (count >= XTG_PARAM_RAM_SIZE)
+		count = XTG_PARAM_RAM_SIZE;
+
+	/* Program each command */
+	if (count == sizeof(struct xtg_pram)) {
+
+		if (!cmdp)
+			return -EINVAL;
+
+		if (cmdp->is_valid_req == VALID_SIG) {
+			/* Prepare parameter word */
+			xtg_prepare_param_word(tg, cmdp, &param_word);
+
+			count = XTG_PRAM_BYTES_PER_ENTRY;
+			data = &param_word;
+
+			/* Maximum command entries are 256 */
+			if (cmdp->index > MAX_NUM_ENTRIES)
+				return -EINVAL;
+
+			/* Calculate the block index */
+			if (cmdp->is_write_block)
+				off = XTG_PRM_RAM_BLOCK_SIZE +
+						cmdp->index * count;
+			else
+				off = cmdp->index * count;
+		}
+	}
+
+	off += XTG_PARAM_RAM_OFFSET;
+	xtg_access_rams(tg, off, count, XTG_WRITE_RAM, data);
+
+	return count;
+}
+
+static int xtg_pram_mmap(struct file *filp, struct kobject *kobj,
+			 struct bin_attribute *attr,
+			 struct vm_area_struct *vma)
+{
+	struct xtg_dev_info *tg =
+		to_xtg_dev_info(container_of(kobj, struct device, kobj));
+	int ret;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	vma->vm_flags |= VM_IO;
+
+	ret = remap_pfn_range(vma, vma->vm_start, (tg->phys_base_addr +
+			XTG_PARAM_RAM_OFFSET) >> PAGE_SHIFT,
+			XTG_PARAM_RAM_SIZE, vma->vm_page_prot);
+	return ret;
+}
+
+static struct bin_attribute xtg_pram_attr = {
+	.attr =	{
+		.name = "parameter_ram",
+		.mode = 0644,
+	},
+	.size = XTG_PARAM_RAM_SIZE,
+	.read = xtg_pram_read,
+	.write = xtg_pram_write,
+	.mmap = xtg_pram_mmap,
+};
+
+static ssize_t xtg_cram_read(struct file *filp, struct kobject *kobj,
+			     struct bin_attribute *bin_attr,
+			     char *buf, loff_t off, size_t count)
+{
+	struct xtg_dev_info *tg =
+		to_xtg_dev_info(container_of(kobj, struct device, kobj));
+
+	off += XTG_COMMAND_RAM_OFFSET;
+	xtg_access_rams(tg, off, count, XTG_READ_RAM, (u32 *)buf);
+
+	return count;
+}
+
+static ssize_t xtg_cram_write(struct file *filp, struct kobject *kobj,
+			      struct bin_attribute *bin_attr,
+			      char *buf, loff_t off, size_t count)
+{
+	struct xtg_dev_info *tg =
+		to_xtg_dev_info(container_of(kobj, struct device, kobj));
+	u32 *data = (u32 *)buf;
+	struct xtg_cram *cmdp = (struct xtg_cram *)buf;
+	u32 cmd_words[CMD_WDS + EXT_WDS];
+
+	if (off >= XTG_COMMAND_RAM_SIZE) {
+		pr_err("Requested Write len exceeds 8K CRAM size\n");
+		return -ENOMEM;
+	}
+
+	/* Program each command */
+	if (count == sizeof(struct xtg_cram)) {
+
+		if (!cmdp)
+			return -EINVAL;
+
+		if (cmdp->is_valid_req == VALID_SIG) {
+			/* Prepare command words */
+			xtg_prepare_cmd_words(tg, cmdp, cmd_words);
+			count = XTG_CRAM_BYTES_PER_ENTRY;
+			data = cmd_words;
+
+			/* Maximum command entries are 256 */
+			if (cmdp->index > MAX_NUM_ENTRIES)
+				return -EINVAL;
+
+			/* Calculate the block index */
+			if (cmdp->is_write_block)
+				off = XTG_CMD_RAM_BLOCK_SIZE +
+						cmdp->index * count;
+			else
+				off = cmdp->index * count;
+
+			/* Store the valid command index */
+			if (cmdp->valid_cmd) {
+				if (cmdp->is_write_block)
+					tg->last_wr_valid_idx =
+							cmdp->index;
+				else
+					tg->last_rd_valid_idx =
+							cmdp->index;
+			}
+		}
+	}
+
+	off += XTG_COMMAND_RAM_OFFSET;
+	xtg_access_rams(tg, off, count, XTG_WRITE_RAM, data);
+
+	return count;
+}
+
+static int xtg_cram_mmap(struct file *filp, struct kobject *kobj,
+			 struct bin_attribute *attr,
+			 struct vm_area_struct *vma)
+{
+	struct xtg_dev_info *tg =
+		to_xtg_dev_info(container_of(kobj, struct device, kobj));
+	int ret;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	vma->vm_flags |= VM_IO;
+
+	ret = remap_pfn_range(vma, vma->vm_start, (tg->phys_base_addr +
+			XTG_COMMAND_RAM_OFFSET) >> PAGE_SHIFT,
+			XTG_COMMAND_RAM_SIZE + XTG_EXTCMD_RAM_SIZE,
+			vma->vm_page_prot);
+	return ret;
+}
+
+static struct bin_attribute xtg_cram_attr = {
+	.attr =	{
+		.name = "command_ram",
+		.mode = 0644,
+	},
+	.size = XTG_COMMAND_RAM_SIZE,
+	.read = xtg_cram_read,
+	.write = xtg_cram_write,
+	.mmap = xtg_cram_mmap,
+};
+
+static ssize_t xtg_mram_read(struct file *filp, struct kobject *kobj,
+			     struct bin_attribute *bin_attr,
+			     char *buf, loff_t off, size_t count)
+{
+	struct xtg_dev_info *tg =
+		to_xtg_dev_info(container_of(kobj, struct device, kobj));
+
+	off += tg->xtg_mram_offset;
+	xtg_access_rams(tg, off, count, XTG_READ_RAM, (u32 *)buf);
+
+	return count;
+}
+
+static ssize_t xtg_mram_write(struct file *filp, struct kobject *kobj,
+			      struct bin_attribute *bin_attr,
+			      char *buf, loff_t off, size_t count)
+{
+	struct xtg_dev_info *tg =
+		to_xtg_dev_info(container_of(kobj, struct device, kobj));
+
+	if (off >= XTG_MASTER_RAM_SIZE) {
+		pr_err("Requested Write len exceeds 8K MRAM size\n");
+		return -ENOMEM;
+	}
+
+	off += tg->xtg_mram_offset;
+	xtg_access_rams(tg, off, count, XTG_WRITE_RAM, (u32 *)buf);
+
+	return count;
+}
+
+static int xtg_mram_mmap(struct file *filp, struct kobject *kobj,
+			 struct bin_attribute *attr,
+			 struct vm_area_struct *vma)
+{
+	struct xtg_dev_info *tg =
+		to_xtg_dev_info(container_of(kobj, struct device, kobj));
+	int ret;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	vma->vm_flags |= VM_IO;
+
+	ret = remap_pfn_range(vma, vma->vm_start, (tg->phys_base_addr +
+			tg->xtg_mram_offset) >> PAGE_SHIFT,
+			XTG_MASTER_RAM_SIZE,
+			vma->vm_page_prot);
+	return ret;
+}
+
+static struct bin_attribute xtg_mram_attr = {
+	.attr =	{
+		.name = "master_ram",
+		.mode = 0644,
+	},
+	.size = XTG_MASTER_RAM_SIZE,
+	.read = xtg_mram_read,
+	.write = xtg_mram_write,
+	.mmap = xtg_mram_mmap,
+};
+
+static struct bin_attribute *xtg_bin_attrs[] = {
+	&xtg_mram_attr,
+	&xtg_pram_attr,
+	&xtg_cram_attr,
+	NULL,
+};
+
+static const struct attribute *xtg_attrs[] = {
+	&dev_attr_id.attr,
+	&dev_attr_resource.attr,
+	&dev_attr_master_start_stop.attr,
+	&dev_attr_config_slave_status.attr,
+	&dev_attr_err_en.attr,
+	&dev_attr_err_sts.attr,
+	&dev_attr_intr_en.attr,
+	&dev_attr_last_valid_index.attr,
+	&dev_attr_config_sts.attr,
+	&dev_attr_mram_clear.attr,
+	&dev_attr_cram_clear.attr,
+	&dev_attr_pram_clear.attr,
+	&dev_attr_static_enable.attr,
+	&dev_attr_static_burstlen.attr,
+	&dev_attr_static_transferdone.attr,
+	&dev_attr_stream_transfercnt.attr,
+	&dev_attr_stream_transferlen.attr,
+	&dev_attr_stream_tkts1.attr,
+	&dev_attr_stream_tkts2.attr,
+	&dev_attr_stream_tkts3.attr,
+	&dev_attr_stream_tkts4.attr,
+	&dev_attr_stream_cfg.attr,
+	&dev_attr_stream_enable.attr,
+	&dev_attr_reset_static_transferdone.attr,
+	&dev_attr_loop_enable.attr,
+	NULL,
+};
+
+static const struct attribute_group xtg_attributes = {
+	.attrs = (struct attribute **)xtg_attrs,
+	.bin_attrs = xtg_bin_attrs,
+};
+
+/**
+ * xtg_cmp_intr_handler - Master Complete Interrupt handler
+ * @irq: IRQ number
+ * @data: Pointer to the xtg_dev_info structure
+ *
+ * Return: IRQ_HANDLED always
+ */
+static irqreturn_t xtg_cmp_intr_handler(int irq, void *data)
+{
+	struct xtg_dev_info *tg = (struct xtg_dev_info *)data;
+
+	writel(readl(tg->regs + XTG_ERR_STS_OFFSET) |
+	       XTG_ERR_STS_MSTDONE_MASK, tg->regs + XTG_ERR_STS_OFFSET);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * xtg_err_intr_handler - Master/Slave Error Interrupt handler
+ * @irq: IRQ number
+ * @data: Pointer to the xtg_dev_info structure
+ *
+ * Return: IRQ_HANDLED always
+ */
+static irqreturn_t xtg_err_intr_handler(int irq, void *data)
+{
+	struct xtg_dev_info *tg = (struct xtg_dev_info *)data;
+	u32 value;
+
+	value = readl(tg->regs + XTG_ERR_STS_OFFSET) &
+		      XTG_ERR_ALL_ERRS_MASK;
+
+	if (value) {
+		dev_err(tg->dev, "Found errors 0x%08x\n", value);
+		writel(readl(tg->regs + XTG_ERR_STS_OFFSET) | value,
+		       tg->regs + XTG_ERR_STS_OFFSET);
+	}
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * xtg_probe - Driver probe function
+ * @pdev: Pointer to the platform_device structure
+ *
+ * This is the driver probe routine. It does all the memory
+ * allocation and creates sysfs entries for the device.
+ *
+ * Return: 0 on success and failure value on error
+ */
+static int xtg_probe(struct platform_device *pdev)
+{
+	struct xtg_dev_info *tg;
+	struct device_node *node;
+	struct resource *res;
+	struct device *dev;
+	int err, irq, var;
+
+	tg = devm_kzalloc(&pdev->dev, sizeof(*tg), GFP_KERNEL);
+	if (!tg)
+		return -ENOMEM;
+
+	tg->dev = &pdev->dev;
+	dev = tg->dev;
+	node = pdev->dev.of_node;
+
+	/* Map the registers */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	tg->regs = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(tg->regs))
+		return PTR_ERR(tg->regs);
+
+	/* Save physical base address */
+	tg->phys_base_addr = res->start;
+
+	/* Get the device instance id */
+	err = of_property_read_u32(node, "xlnx,device-id", &tg->id);
+	if (err < 0) {
+		dev_err(&pdev->dev, "unable to read property");
+		return err;
+	}
+
+	/* Map the error interrupt, if it exists in the device tree. */
+	irq = platform_get_irq_byname(pdev, "err-out");
+	if (irq < 0) {
+		dev_dbg(&pdev->dev, "unable to get err irq");
+	} else {
+		err = devm_request_irq(&pdev->dev, irq, xtg_err_intr_handler,
+				       0, dev_name(&pdev->dev), tg);
+		if (err < 0) {
+			dev_err(&pdev->dev, "unable to request irq %d", irq);
+			return err;
+		}
+	}
+
+	/* Map the completion interrupt, if it exists in the device tree. */
+	irq = platform_get_irq_byname(pdev, "irq-out");
+	if (irq < 0) {
+		dev_dbg(&pdev->dev, "unable to get cmp irq");
+	} else {
+		err = devm_request_irq(&pdev->dev, irq, xtg_cmp_intr_handler,
+				       0, dev_name(&pdev->dev), tg);
+		if (err < 0) {
+			dev_err(&pdev->dev, "unable to request irq %d", irq);
+			return err;
+		}
+	}
+
+	tg->clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(tg->clk)) {
+		if (PTR_ERR(tg->clk) != -ENOENT) {
+			if (PTR_ERR(tg->clk) != -EPROBE_DEFER)
+				dev_err(&pdev->dev, "input clock not found\n");
+			return PTR_ERR(tg->clk);
+		}
+		tg->clk = NULL;
+	}
+
+	err = clk_prepare_enable(tg->clk);
+	if (err) {
+		dev_err(&pdev->dev, "Unable to enable clock.\n");
+		return err;
+	}
+
+	/*
+	 * Create sysfs file entries for the device
+	 */
+	err = sysfs_create_group(&dev->kobj, &xtg_attributes);
+	if (err < 0) {
+		dev_err(tg->dev, "unable to create sysfs entries\n");
+		clk_disable_unprepare(tg->clk);
+		return err;
+	}
+
+	/*
+	 * Initialize the write and read valid index values.
+	 * Possible range of values for these variables is <0 255>.
+	 */
+	tg->last_wr_valid_idx = -1;
+	tg->last_rd_valid_idx = -1;
+
+	dev_set_drvdata(&pdev->dev, tg);
+
+	/* Update the Proper MasterRam offset */
+	tg->xtg_mram_offset = XTG_MASTER_RAM_OFFSET;
+	var = readl(tg->regs + XTG_MCNTL_OFFSET) >> XTG_MCNTL_REV_SHIFT;
+	if (var == XTG_INIT_VERSION)
+		tg->xtg_mram_offset = XTG_MASTER_RAM_INIT_OFFSET;
+
+	dev_info(&pdev->dev, "Probing xilinx traffic generator success\n");
+
+	return 0;
+}
+
+/**
+ * xtg_remove - Driver remove function
+ * @pdev: Pointer to the platform_device structure
+ *
+ * This function frees all the resources allocated to the device.
+ *
+ * Return: 0 always
+ */
+static int xtg_remove(struct platform_device *pdev)
+{
+	struct xtg_dev_info *tg;
+	struct device *dev;
+
+	tg = dev_get_drvdata(&pdev->dev);
+	dev = tg->dev;
+	sysfs_remove_group(&dev->kobj, &xtg_attributes);
+	clk_disable_unprepare(tg->clk);
+
+	return 0;
+}
+
+static const struct of_device_id xtg_of_match[] = {
+	{ .compatible = "xlnx,axi-traffic-gen", },
+	{ /* end of table */ }
+};
+MODULE_DEVICE_TABLE(of, xtg_of_match);
+
+static struct platform_driver xtg_driver = {
+	.driver = {
+		.name = "xilinx-trafgen",
+		.of_match_table = xtg_of_match,
+	},
+	.probe = xtg_probe,
+	.remove = xtg_remove,
+};
+
+module_platform_driver(xtg_driver);
+
+MODULE_AUTHOR("Xilinx Inc.");
+MODULE_DESCRIPTION("Xilinx Traffic Generator driver");
+MODULE_LICENSE("GPL");
diff --git a/include/linux/xlnx-ai-engine.h b/include/linux/xlnx-ai-engine.h
new file mode 100644
index 000000000..3f9fadf74
--- /dev/null
+++ b/include/linux/xlnx-ai-engine.h
@@ -0,0 +1,163 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * xlnx-ai-engine.h - Xilinx AI engine external interface
+ *
+ * Copyright (c) 2020, Xilinx Inc.
+ */
+
+#ifndef _XLNX_AI_ENGINE_H_
+#define _XLNX_AI_ENGINE_H_
+
+#if !IS_ENABLED(CONFIG_XILINX_AIE)
+#include <linux/errno.h>
+#endif
+#include <uapi/linux/xlnx-ai-engine.h>
+
+/*
+ * Macro to classify errors into categories to provide higher-level error
+ * event abstraction.
+ */
+#define AIE_ERROR_CATEGORY_SATURATION		0U
+#define AIE_ERROR_CATEGORY_FP			1U
+#define AIE_ERROR_CATEGORY_STREAM		2U
+#define AIE_ERROR_CATEGORY_ACCESS		3U
+#define AIE_ERROR_CATEGORY_BUS			4U
+#define AIE_ERROR_CATEGORY_INSTRUCTION		5U
+#define AIE_ERROR_CATEGORY_ECC			6U
+#define AIE_ERROR_CATEGORY_LOCK			7U
+#define AIE_ERROR_CATEGORY_DMA			8U
+#define AIE_ERROR_CATEGORY_MEM_PARITY		9U
+
+/* AIE error category bit mask */
+#define AIE_ERROR_CATMASK(c)			BIT(AIE_ERROR_CATEGORY_##c)
+#define AIE_ERROR_CATEGORY_MASK_SATURATION	AIE_ERROR_CATMASK(SATURATION)
+#define AIE_ERROR_CATEGORY_MASK_FP		AIE_ERROR_CATMASK(FP)
+#define AIE_ERROR_CATEGORY_MASK_STREAM		AIE_ERROR_CATMASK(STREAM)
+#define AIE_ERROR_CATEGORY_MASK_ACCESS		AIE_ERROR_CATMASK(ACCESS)
+#define AIE_ERROR_CATEGORY_MASK_BUS		AIE_ERROR_CATMASK(BUS)
+#define AIE_ERROR_CATEGORY_MASK_INSTRUCTION	AIE_ERROR_CATMASK(INSTRUCTION)
+#define AIE_ERROR_CATEGORY_MASK_ECC		AIE_ERROR_CATMASK(ECC)
+#define AIE_ERROR_CATEGORY_MASK_LOCK		AIE_ERROR_CATMASK(LOCK)
+#define AIE_ERROR_CATEGORY_MASK_DMA		AIE_ERROR_CATMASK(DMA)
+#define AIE_ERROR_CATEGORY_MASK_MEM_PARITY	AIE_ERROR_CATMASK(MEM_PARITY)
+
+struct device;
+
+/**
+ * struct aie_error - AI engine error
+ * @loc: AI engine tile location of which the error is from
+ * @module: AI engine module type of which the error is from
+ * @error_id: AI engine hardware event ID
+ * @category: AI engine error category of the error
+ */
+struct aie_error {
+	struct aie_location loc;
+	enum aie_module_type module;
+	u32 error_id;
+	u32 category;
+};
+
+struct aie_errors {
+	struct device *dev;
+	struct aie_error *errors;
+	u32 num_err;
+};
+
+#if IS_ENABLED(CONFIG_XILINX_AIE)
+bool aie_partition_is_available(struct aie_partition_req *req);
+struct device *aie_partition_request(struct aie_partition_req *req);
+int aie_partition_get_fd(struct device *dev);
+void aie_partition_release(struct device *dev);
+int aie_partition_reset(struct device *dev);
+int aie_partition_post_reinit(struct device *dev);
+
+int aie_register_error_notification(struct device *dev,
+				    void (*cb)(void *priv), void *priv);
+int aie_unregister_error_notification(struct device *dev);
+struct aie_errors *aie_get_errors(struct device *dev);
+u32 aie_get_error_categories(struct aie_errors *aie_errs);
+const char *aie_get_error_string(struct aie_errors *aie_errs,
+				 struct aie_error *aie_err);
+int aie_flush_errors(struct device *dev);
+void aie_free_errors(struct aie_errors *aie_errs);
+
+/**
+ * aie_get_error_category() - Get the category of an AIE error
+ * @err: AI engine hardware error
+ * @return: category of the error
+ */
+static inline u32 aie_get_error_category(struct aie_error *err)
+{
+	return err->category;
+}
+
+#else
+static inline bool aie_partition_is_available(struct aie_partition_req *req)
+{
+	return false;
+}
+
+static inline struct device *
+aie_partition_request(struct aie_partition_req *req)
+{
+	return NULL;
+}
+
+static inline int aie_partition_get_fd(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline void aie_partition_release(struct device *dev) {}
+
+static inline int aie_partition_reset(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline int aie_partition_post_reinit(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline int
+aie_register_error_notification(struct device *dev, void (*cb)(void *priv),
+				void *priv)
+{
+	return -EINVAL;
+}
+
+static inline int aie_unregister_error_notification(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline struct aie_errors *aie_get_errors(struct device *dev)
+{
+	return NULL;
+}
+
+static inline u32 aie_get_error_categories(struct aie_errors *aie_errs)
+{
+	return 0;
+}
+
+static inline const char *aie_get_error_string(struct aie_errors *aie_errs,
+					       struct aie_error *aie_err)
+{
+	return NULL;
+}
+
+static inline int aie_flush_errors(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline void aie_free_errors(struct aie_errors *aie_errs) {}
+
+static inline u32 aie_get_error_category(struct aie_error *err)
+{
+	return 0;
+}
+#endif /* CONFIG_XILINX_AIE */
+#endif
diff --git a/include/uapi/linux/xlnx-ai-engine.h b/include/uapi/linux/xlnx-ai-engine.h
new file mode 100644
index 000000000..a80e8555d
--- /dev/null
+++ b/include/uapi/linux/xlnx-ai-engine.h
@@ -0,0 +1,576 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * Copyright (c) 2020, Xilinx Inc.
+ */
+
+#ifndef _UAPI_AI_ENGINE_H_
+#define _UAPI_AI_ENGINE_H_
+
+#ifndef __KERNEL__
+#include <stdlib.h>
+#endif
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+enum aie_reg_op {
+	AIE_REG_WRITE,
+	AIE_REG_BLOCKWRITE,
+	AIE_REG_BLOCKSET,
+};
+
+/**
+ * enum aie_module_type - identifies different hardware modules within a
+ *			  tile type. AIE tile may have memory and core
+ *			  module. While a PL or shim tile may have PL module.
+ * @AIE_MEM_MOD: comprises of the following sub-modules,
+ *			* data memory.
+ *			* tile DMA.
+ *			* lock module.
+ *			* events, event broadcast and event actions.
+ *			* tracing and profiling.
+ * @AIE_CORE_MOD: comprises of the following sub-modules,
+ *			* AIE core.
+ *			* program Memory.
+ *			* events, event broadcast and event actions.
+ *			* tracing and profiling.
+ *			* AXI-MM and AXI-S tile interconnects.
+ * @AIE_PL_MOD: comprises of the following sub-modules,
+ *			* PL interface.
+ *			* AXI-MM and AXI-S tile interconnects.
+ *			* Level 1 interrupt controllers.
+ *			* events, event broadcast and event actions.
+ *			* tracing and profiling.
+ * @AIE_NOC_MOD: comprises of the following sub-modules,
+ *			* interface from NoC Slave Unit (NSU)
+ *			  (bridge to AXI-MM switch)
+ *			* interfaces to NoC NoC Master Unit (NMU)
+ *				* shim DMA & locks
+ *				* NoC stream interface
+ */
+enum aie_module_type {
+	AIE_MEM_MOD,
+	AIE_CORE_MOD,
+	AIE_PL_MOD,
+	AIE_NOC_MOD,
+};
+
+/**
+ * enum aie_rsc_type - defines AI engine hardware resource types
+ * @AIE_RSCTYPE_PERF: perfcounter resource
+ * @AIE_RSCTYPE_USEREVENT: user events resource
+ * @AIE_RSCTYPE_TRACECONTROL: trace controller resource
+ * @AIE_RSCTYPE_PCEVENT: PC events resource
+ * @AIE_RSCTYPE_SSSELECT: stream switch port select resource
+ * @AIE_RSCTYPE_BROADCAST: broadcast events resource
+ * @AIE_RSCTYPE_COMBOEVENT: combo events resource
+ * @AIE_RSCTYPE_GROUPEVENTS: group events resource
+ * @AIE_RSCTYPE_MAX: total number of resources
+ */
+enum aie_rsc_type {
+	AIE_RSCTYPE_PERF,
+	AIE_RSCTYPE_USEREVENT,
+	AIE_RSCTYPE_TRACECONTROL,
+	AIE_RSCTYPE_PCEVENT,
+	AIE_RSCTYPE_SSSELECT,
+	AIE_RSCTYPE_BROADCAST,
+	AIE_RSCTYPE_COMBOEVENT,
+	AIE_RSCTYPE_GROUPEVENTS,
+	AIE_RSCTYPE_MAX
+};
+
+/* AI engine partition is in use */
+#define XAIE_PART_STATUS_INUSE		(1U << 0)
+/* AI engine partition bridge is enabled */
+#define XAIE_PART_STATUS_BRIDGE_ENABLED	(1U << 1)
+
+/*
+ * AI engine partition control flags
+ */
+/* Not reset when release AI engine partition */
+#define XAIE_PART_NOT_RST_ON_RELEASE	0x00000001U
+
+/*
+ * AI engine resource property flags
+ */
+/*
+ * For resources which needs to be allocated contiguous
+ * such as combo events, it needs to be 0, 1; 2, 3;
+ * or 0, 1, 2, 3
+ */
+#define XAIE_RSC_PATTERN_BLOCK		(1U << 0)
+
+/* Any broadcast channel id */
+#define XAIE_BROADCAST_ID_ANY		0xFFFFFFFFU
+
+/* request a channel to broadcast to the whole partition */
+#define XAIE_BROADCAST_ALL		(1U << 0)
+
+/**
+ * struct aie_location - AIE location information
+ * @col: column id
+ * @row: row id
+ */
+struct aie_location {
+	__u32 col;
+	__u32 row;
+};
+
+/**
+ * struct aie_location_byte - AIE location information with single byte for
+ *			      column and row
+ * @row: row id
+ * @col: column id
+ *
+ * This structure follows the SSW AIE row and col sequence.
+ */
+struct aie_location_byte {
+	__u8 row;
+	__u8 col;
+};
+
+/**
+ * struct aie_range - AIE range information
+ * @start: start tile location
+ * @size: size of the range, number of columns and rows
+ */
+struct aie_range {
+	struct aie_location start;
+	struct aie_location size;
+};
+
+/**
+ * struct aie_mem - AIE memory information
+ * @range: range of tiles of the memory
+ * @offset: register offset within a tile of the memory
+ * @size: of a the memory in one tile
+ * @fd: file descriptor of the memory
+ */
+struct aie_mem {
+	struct aie_range range;
+	size_t offset;
+	size_t size;
+	int fd;
+};
+
+/**
+ * struct aie_mem_args - AIE memory enquiry arguments
+ * @num_mems: number of "struct aie_mem" elements
+ *	      e.g. two memory information elements, one for tile core memory,
+ *	      and the other for tile data memory.
+ * @mems: array of AI engine memory information elements
+ */
+struct aie_mem_args {
+	unsigned int num_mems;
+	struct aie_mem *mems;
+};
+
+/**
+ * struct aie_reg_args - AIE access register arguments
+ * @op: if this request is to read, write or poll register
+ * @mask: mask for mask write, 0 for not mask write
+ * @offset: offset of register to the start of an AI engine partition
+ * @val: value to write or get
+ * @dataptr: pointer to data buffer for block write
+ * @len: length of the buffer pointed by data
+ */
+struct aie_reg_args {
+	enum aie_reg_op op;
+	__u32 mask;
+	__u64 offset;
+	__u32 val;
+	__u64 dataptr;
+	__u32 len;
+};
+
+/**
+ * struct aie_range_args - AIE range request arguments
+ * @partition_id: partition id. It is used to identify the
+ *		  AI engine partition in the system.
+ * @uid: image identifier loaded on the AI engine partition
+ * @range: range of AIE tiles
+ * @status: indicate if the AI engine is in use.
+ *	    0 means not in used, otherwise, in use.
+ */
+struct aie_range_args {
+	__u32 partition_id;
+	__u32 uid;
+	struct aie_range range;
+	__u32 status;
+};
+
+/**
+ * struct aie_partition_query - AIE partition query arguments
+ * @partition_cnt: number of defined partitions in the system
+ * @partitions: buffer to store defined partitions information.
+ */
+struct aie_partition_query {
+	struct aie_range_args *partitions;
+	__u32 partition_cnt;
+};
+
+/**
+ * struct aie_partition_req - AIE request partition arguments
+ * @partition_id: partition node id. It is used to identify the AI engine
+ *		  partition in the system.
+ * @uid: image identifier loaded on the AI engine partition
+ * @meta_data: meta data to indicate which resources used by application.
+ * @flag: used for application to indicate particular driver requirements
+ *	  application wants to have for the partition. e.g. do not clean
+ *	  resource when closing the partition.
+ */
+struct aie_partition_req {
+	__u32 partition_id;
+	__u32 uid;
+	__u64 meta_data;
+	__u32 flag;
+};
+
+/**
+ * struct aie_dma_bd_args - AIE DMA buffer descriptor information
+ * @bd: DMA buffer descriptor
+ * @data_va: virtual address of the data
+ * @loc: Tile location relative to the start of a partition
+ * @bd_id: buffer descriptor id
+ */
+struct aie_dma_bd_args {
+	__u32 *bd;
+	__u64 data_va;
+	struct aie_location loc;
+	__u32 bd_id;
+};
+
+/**
+ * struct aie_dmabuf_bd_args - AIE dmabuf buffer descriptor information
+ * @bd: DMA buffer descriptor, within the buffer descriptor, the address field
+ *	will be the offset to the start of the dmabuf
+ * @buf_fd: DMA buffer handler which is dmabuf file descriptor
+ * @loc: Tile location relative to the start of a partition
+ * @bd_id: buffer descriptor id
+ */
+struct aie_dmabuf_bd_args {
+	__u32 *bd;
+	struct aie_location loc;
+	int buf_fd;
+	__u32 bd_id;
+};
+
+/**
+ * struct aie_tiles_array - AIE tiles array
+ * @locs: tiles locations array
+ * @num_tiles: number of tiles in the tiles locations array
+ */
+struct aie_tiles_array {
+	struct aie_location *locs;
+	__u32 num_tiles;
+};
+
+/**
+ * struct aie_txn_inst - AIE transaction instance
+ * @num_cmds: number commands containing register ops
+ * @cmdsptr: pointer to the buffer containing register ops
+ */
+struct aie_txn_inst {
+	__u32 num_cmds;
+	__u64 cmdsptr;
+};
+
+/**
+ * struct aie_rsc_req - AIE resource request
+ * @loc: tile location
+ * @mod: module type
+ * @type: resource type
+ * @num_rscs: number of resource per request
+ * @flag: resource property, such as it needs to be in pattern block such as
+ *	  if @num_rscs is 2, it needs to be 0,1; 2,3, or 4,5
+ */
+struct aie_rsc_req {
+	struct aie_location loc;
+	__u32 mod;
+	__u32 type;
+	__u32 num_rscs;
+	__u8 flag;
+};
+
+/**
+ * struct aie_rsc - AIE resource properties
+ * @loc: tile location, single byte for column and row each
+ * @mod: module type
+ * @type: resource type
+ * @id: resource id
+ */
+struct aie_rsc {
+	struct aie_location_byte loc;
+	__u32 mod;
+	__u32 type;
+	__u32 id;
+};
+
+/**
+ * struct aie_rsc_req_rsp - AIE resource request and response structure
+ * @req: resource request per tile module
+ * @rscs: allocated resources array of `struct aie_rsc`
+ */
+struct aie_rsc_req_rsp {
+	struct aie_rsc_req req;
+	__u64 rscs;
+};
+
+/**
+ * struct aie_rsc_bc_req - AIE broadcast channel request
+ * @rscs: broadcast channel resource array for every module and every tile
+ *	  of the channel
+ * @num_rscs: number of expected broadcast channel resources on the path,
+ *	      it also indicates the number of expected modules on the path.
+ * @flag: user flag to indicate if it is to get a broadcast channel for the
+ *	  whole partition.
+ * @id: broadcast channel ID. XAIE_BROADCAST_ID_ANY, it means not particular
+ *	id is specified, driver will allocate a free one.
+ */
+struct aie_rsc_bc_req {
+	__u64 rscs;
+	__u32 num_rscs;
+	__u32 flag;
+	__u32 id;
+};
+
+/* AI engine resource statistics types */
+#define AIE_RSC_STAT_TYPE_STATIC	0U
+#define AIE_RSC_STAT_TYPE_AVAIL		1U
+#define AIE_RSC_STAT_TYPE_MAX		2U
+
+/**
+ * struct aie_rsc_user_stat - AIE user requested resource statistics
+ * @loc: tile location, single byte for column and row each
+ * @mod: module type
+ * @type: resource type
+ * @num_rscs: number of resources
+ */
+struct aie_rsc_user_stat {
+	struct aie_location_byte loc;
+	__u8 mod;
+	__u8 type;
+	__u8 num_rscs;
+} __attribute__((packed, aligned(4)));
+
+/**
+ * struct aie_rsc_user_stat_array - AIE user requested resource statistics array
+ * @stats: resource statistics array
+ * @num_stats: number of resource statistics elements
+ * @stats_type: resource statistics type
+ */
+struct aie_rsc_user_stat_array {
+	__u64 stats;
+	__u32 num_stats;
+	__u32 stats_type;
+};
+
+#define AIE_IOCTL_BASE 'A'
+
+/* AI engine device IOCTL operations */
+#define AIE_ENQUIRE_PART_IOCTL		_IOWR(AIE_IOCTL_BASE, 0x1, \
+					      struct aie_partition_query)
+#define AIE_REQUEST_PART_IOCTL		_IOR(AIE_IOCTL_BASE, 0x2, \
+					     struct aie_partition_req)
+
+/* AI engine partition IOCTL operations */
+#define AIE_REG_IOCTL			_IOWR(AIE_IOCTL_BASE, 0x8, \
+					      struct aie_reg_args)
+/**
+ * DOC: AIE_GET_MEM_IOCTL - enquire information of memories in the AI engine
+ *			    partition
+ * This ioctl is used to get the information of all the different types of
+ * memories in the AI engine partition. Application can get the memories
+ * information in two steps:
+ * 1. passing 0 as @num_mems in struct aie_mem_args to enquire the number of
+ *    different memories in the partition, the value will be returned in
+ *    @num_mems.
+ * 2. passing the number of memories in @num_mems and valid pointer as @mems of
+ *    struct aie_mem_args to store the details information of different
+ *    memories. The driver will create DMA buf for each type of memories, and
+ *    will return the memory addressing information along with the DMA buf file
+ *    descriptors in @mems.
+ * After getting the memories information, user can use mmap() with the DMA buf
+ * file descriptor to enable access the memories from userspace.
+ */
+#define AIE_GET_MEM_IOCTL		_IOWR(AIE_IOCTL_BASE, 0x9, \
+					      struct aie_mem_args)
+/**
+ * DOC: AIE_ATTACH_DMABUF_IOCTL - attach a dmabuf to AI engine partition
+ *
+ * This ioctl is used to attach a dmabuf to the AI engine partition. AI engine
+ * partition will return the number of scatter gather list elements of the
+ * dmabuf.
+ */
+#define AIE_ATTACH_DMABUF_IOCTL		_IOR(AIE_IOCTL_BASE, 0xa, int)
+
+/**
+ * DOC: AIE_DETACH_DMABUF_IOCTL - dettach a dmabuf from AI engine partition
+ *
+ * This ioctl is used to detach a dmabuf from the AI engine partition
+ */
+#define AIE_DETACH_DMABUF_IOCTL		_IOR(AIE_IOCTL_BASE, 0xb, int)
+
+/**
+ * DOC: AIE_SET_DMABUF_BD_IOCTL - set buffer descriptor to SHIM DMA
+ *
+ * This ioctl is used to set the buffer descriptor to SHIM DMA
+ */
+#define AIE_SET_SHIMDMA_BD_IOCTL	_IOW(AIE_IOCTL_BASE, 0xd, \
+					     struct aie_dma_bd_args)
+
+/**
+ * DOC: AIE_REQUEST_TILES_IOCTL - request AI engine tiles
+ *
+ * This ioctl is used to request tiles.
+ * When requested the AI engine partition, the kernel driver will scan the
+ * partition to track which tiles are enabled or not. After that, if user
+ * want to request for more tiles, it will use this ioctl to request more
+ * tiles.
+ * If the aie_tiles_array is empty, it means it will request for all tiles
+ * in the partition.
+ */
+#define AIE_REQUEST_TILES_IOCTL		_IOW(AIE_IOCTL_BASE, 0xe, \
+					     struct aie_tiles_array)
+
+/**
+ * DOC: AIE_RELEASE_TILES_IOCTL - release AI engine tiles
+ *
+ * This ioctl is used to release tiles
+ */
+#define AIE_RELEASE_TILES_IOCTL		_IOW(AIE_IOCTL_BASE, 0xf, \
+					     struct aie_tiles_array)
+
+/**
+ * DOC: AIE_SET_SHIMDMA_DMABUF_BD_IOCTL - set buffer descriptor which contains
+ *					  dmabuf to SHIM DMA
+ *
+ * This ioctl is used to set the buffer descriptor to SHIM DMA. The
+ * aie_dmabuf_bd_args contains the dmabuf fd and the buffer descriptor contents.
+ * The address field in the buffer descriptor contents should be the offset to
+ * the start of the dmabuf.
+ */
+#define AIE_SET_SHIMDMA_DMABUF_BD_IOCTL	_IOW(AIE_IOCTL_BASE, 0x10, \
+					     struct aie_dmabuf_bd_args)
+
+/**
+ * DOC: AIE_TRANSACTION_IOCTL - execute the register operations to
+ *					configure AIE partition
+ *
+ * This ioctl is used to perform multiple register operations like write,
+ * mask write, block set and block write on AIE partition. The aie_txn_inst
+ * contains the buffer with all the register operations required by the
+ * application.
+ */
+#define AIE_TRANSACTION_IOCTL		_IOWR(AIE_IOCTL_BASE, 0x11, \
+					     struct aie_txn_inst)
+
+/**
+ * DOC: AIE_SET_FREQUENCY_IOCTL - set AI engine partition clock frequency
+ *
+ * This ioctl is used to set AI engine partition clock frequency.
+ * AI engine partition driver converts the required clock frequency to QoS
+ * based on the full frequency. And then it sends the set QoS request to
+ * firmware. As AI engine device can have multiple users but there is only
+ * one clock for the whole device, the firmware will check all the QoS
+ * requirements from all users, and set the AI engine device to run on the
+ * max required frequency.
+ */
+#define AIE_SET_FREQUENCY_IOCTL	_IOW(AIE_IOCTL_BASE, 0x12, __u64)
+
+/**
+ * DOC: AIE_GET_FREQUENCY_IOCTL - get AI engine partition running clock
+ *				  frequency
+ *
+ * This ioctl is used to get AI engine partition running clock frequency.
+ * AI engine partition driver sends get divider requests to the firmware.
+ * And then the driver calculates the running frequency with the full frequency
+ * and the divider, and returns the running clock frequency.
+ */
+#define AIE_GET_FREQUENCY_IOCTL	_IOR(AIE_IOCTL_BASE, 0x13, __u64)
+
+/**
+ * DOC: AIE_RSC_REQ_IOCTL - request a type of resources of a tile
+ *
+ * This ioctl is used to request a type of resources of a tile of an AI engine
+ * partition.
+ * AI engine partitition driver will check if there are the requested number
+ * of resources available. If yes, fill in the allcoated resource IDs in the
+ * resources array provided by user.
+ */
+#define AIE_RSC_REQ_IOCTL		_IOW(AIE_IOCTL_BASE, 0x14, \
+					     struct aie_rsc_req_rsp)
+
+/**
+ * DOC: AIE_RSC_REQ_SPECIFIC_IOCTL - request statically allocated resource
+ *
+ * This ioctl is used to request to use a specified allcoated resource
+ * AI engine partitition driver will check if the resource has been allocated
+ * at compilation time. If yes, and no one else has requested it, it returns
+ * success.
+ */
+#define AIE_RSC_REQ_SPECIFIC_IOCTL	_IOW(AIE_IOCTL_BASE, 0x15, \
+					     struct aie_rsc)
+
+/**
+ * DOC: AIE_RSC_RELEASE_IOCTL - release allocated resource
+ *
+ * This ioctl is used to release a resource and returns it to the resource
+ * pool, so that next time if user want to request for a resource, it is
+ * available
+ */
+#define AIE_RSC_RELEASE_IOCTL		_IOW(AIE_IOCTL_BASE, 0x16, \
+					     struct aie_rsc)
+
+/**
+ * DOC: AIE_RSC_FREE_IOCTL - free allocated resource
+ *
+ * This ioctl is used to free an allocated resource. It will unmark the
+ * resource from runtime used. If the resource is allocated at compilation
+ * time, it will not be returned back to the resource pool.
+ */
+#define AIE_RSC_FREE_IOCTL		_IOW(AIE_IOCTL_BASE, 0x17, \
+					     struct aie_rsc)
+
+/**
+ * DOC: AIE_RSC_CHECK_AVAIL_IOCTL - check if resource is available
+ *
+ * This ioctl is used to check how many resources are available for a specified
+ * type of resource.
+ */
+#define AIE_RSC_CHECK_AVAIL_IOCTL	_IOW(AIE_IOCTL_BASE, 0x18, \
+					     struct aie_rsc_req)
+
+/**
+ * DOC: AIE_RSC_GET_COMMON_BROADCAST_IOCTL - get a common broadcast channel for
+ *					     the specified set of AI engine
+ *					     modules.
+ *
+ * This ioctl is used to get a common broadcast channel for the specified set
+ * of AI engine modules in the resources array. If the any of the input set of
+ * tiles is gated, it will return failure. This ioctl will not check the
+ * connection of the input modules set.
+ * The driver will fill in the resource ID with the assigned broadcast channel
+ * ID of the resources array.
+ * If the XAIE_BROADCAST_ALL is set in the request flag, it will get the
+ * broadcast channel for all the ungated tiles of the partition.
+ * If a particular broadcast channel id is specified in the request, if will
+ * check if the channel is available for the specified modules, or the whole
+ * partition depends on if XAIE_BROADCAST_ALL is set.
+ */
+#define AIE_RSC_GET_COMMON_BROADCAST_IOCTL	_IOW(AIE_IOCTL_BASE, 0x19, \
+						struct aie_rsc_bc_req)
+
+/**
+ * DOC: AIE_RSC_GET_STAT_IOCTL - get resource usage statistics
+ *
+ * This ioctl is used to get resource usage statistics. User passes an array of
+ * resource statistics requests and the resources statistics type that is if it
+ * is statically allocated or available resources. Each request specifies the
+ * tile, module type and the resource type. This ioctl returns the number of
+ * resources of the specified statistics type per request.
+ */
+#define AIE_RSC_GET_STAT_IOCTL		_IOW(AIE_IOCTL_BASE, 0x1a, \
+					struct aie_rsc_user_stat_array)
+
+#endif
