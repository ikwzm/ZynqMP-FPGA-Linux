diff --git a/Documentation/devicetree/bindings/net/xilinx-tsn-ethernet.txt b/Documentation/devicetree/bindings/net/xilinx-tsn-ethernet.txt
new file mode 100644
index 000000000..e66b64bc1
--- /dev/null
+++ b/Documentation/devicetree/bindings/net/xilinx-tsn-ethernet.txt
@@ -0,0 +1,54 @@
+Xilinx TSN (time sensitive networking) TEMAC axi ethernet driver (xilinx_axienet)
+-----------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-ethernet-1.00.a".
+- reg			: Physical base address and size of the TSN registers map.
+- interrupts		: Property with a value describing the interrupt
+			  number.
+- interrupts-names	: Property denotes the interrupt names.
+- interrupt-parent	: Must be core interrupt controller.
+- phy-handle		: See ethernet.txt file [1].
+- local-mac-address	: See ethernet.txt file [1].
+- phy-mode		: see ethernet.txt file [1].
+
+Optional properties:
+- xlnx,tsn		: Denotes a ethernet with TSN capabilities.
+- xlnx,tsn-slave	: Denotes a TSN slave port.
+- xlnx,txcsum		: Tx checksum mode (Full, Partial and None).
+- xlnx,rxcsum		: Rx checksum mode (Full, Partial and None).
+- xlnx,phy-type		: Xilinx phy device type. See xilinx-phy.txt [2].
+- xlnx,eth-hasnobuf	: Used when 1G MAC is configured in non-processor mode.
+- xlnx,num-queue	: Number of queue supported in current design, range is
+			  2 to 5 and default value is 5.
+- xlnx,num-tc		: Number of traffic class supported in current design,
+			  range is 2,3 and default value is 3. It denotes
+			  the traffic classes based on VLAN-PCP value.
+- xlnx,qbv-addr		: Denotes mac scheduler physical base address.
+- xlnx,qbv-size		: Denotes mac scheduler address space size.
+
+[1] Documentation/devicetree/bindings/net/ethernet.txt
+[2] Documentation/devicetree/bindings/net/xilinx-phy.txt
+
+Example:
+
+	tsn_emac_0: tsn_mac@80040000 {
+		compatible = "xlnx,tsn-ethernet-1.00.a";
+		interrupt-parent = <&gic>;
+		interrupts = <0 104 4 0 106 4 0 91 4 0 110 4>;
+		interrupt-names = "interrupt_ptp_rx_1", "interrupt_ptp_tx_1", "mac_irq_1", "interrupt_ptp_timer";
+		local-mac-address = [ 00 0A 35 00 01 0e ];
+		phy-mode = "rgmii";
+		reg = <0x0 0x80040000 0x0 0x14000>;
+		tsn,endpoint = <&tsn_ep>;
+		xlnx,tsn;
+		xlnx,tsn-slave;
+		xlnx,phy-type = <0x3>;
+		xlnx,eth-hasnobuf;
+		xlnx,num-queue = <0x2>;
+		xlnx,num-tc = <0x3>;
+		xlnx,qbv-addr = <0x80054000>;
+		xlnx,qbv-size = <0x2000>;
+		xlnx,txsum = <0>;
+		xlnx,rxsum = <0>;
+	};
diff --git a/Documentation/devicetree/bindings/net/xilinx_axienet.txt b/Documentation/devicetree/bindings/net/xilinx_axienet.txt
index 7360617cd..ed1605570 100644
--- a/Documentation/devicetree/bindings/net/xilinx_axienet.txt
+++ b/Documentation/devicetree/bindings/net/xilinx_axienet.txt
@@ -1,34 +1,59 @@
 XILINX AXI ETHERNET Device Tree Bindings
 --------------------------------------------------------
 
-Also called  AXI 1G/2.5G Ethernet Subsystem, the xilinx axi ethernet IP core
-provides connectivity to an external ethernet PHY supporting different
-interfaces: MII, GMII, RGMII, SGMII, 1000BaseX. It also includes two
-segments of memory for buffering TX and RX, as well as the capability of
-offloading TX/RX checksum calculation off the processor.
+This driver supports following MAC configurations-
+a) AXI 1G/2.5G Ethernet Subsystem.
+b) 10G/25G High Speed Ethernet Subsystem.
+c) 10 Gigabit Ethernet Subsystem.
+d) USXGMII Ethernet Subsystem.
+e) MRMAC Ethernet Subsystem.
+
+AXI 1G/2.5G Ethernet Subsystem- also called  AXI 1G/2.5G Ethernet Subsystem,
+the xilinx axi ethernet IP core provides connectivity to an external ethernet
+PHY supporting different interfaces: MII, GMII, RGMII, SGMII, 1000BaseX.
+It also includes two segments of memory for buffering TX and RX, as well as
+the capability of offloading TX/RX checksum calculation off the processor.
 
 Management configuration is done through the AXI interface, while payload is
 sent and received through means of an AXI DMA controller. This driver
 includes the DMA driver code, so this driver is incompatible with AXI DMA
 driver.
 
-For more details about mdio please refer phy.txt file in the same directory.
+MRMAC is a hardened Ethernet IP on Versal supporting multiple rates from
+10G to 100G which can be used with a soft DMA controller.
+
+For details about MDIO please refer phy.txt [1].
 
 Required properties:
-- compatible	: Must be one of "xlnx,axi-ethernet-1.00.a",
-		  "xlnx,axi-ethernet-1.01.a", "xlnx,axi-ethernet-2.01.a"
+- compatible	: Must be one of "xlnx,axi-ethernet-1.00.a" or
+		  "xlnx,axi-ethernet-1.01.a" or "xlnx,axi-ethernet-2.01.a"
+		  for 1G MAC,
+		  "xlnx,ten-gig-eth-mac" for 10 Gigabit Ethernet Subsystem,
+		  "xlnx,xxv-ethernet-1.0" for 10G/25G MAC,
+		  "xlnx,axi-2_5-gig-ethernet-1.0" for 2.5G MAC,
+		  "xlnx,xxv-usxgmii-ethernet-1.0" for USXGMII and
+		  "xlnx,mrmac-ethernet-1.0" for MRMAC.
 - reg		: Address and length of the IO space, as well as the address
                   and length of the AXI DMA controller IO space, unless
                   axistream-connected is specified, in which case the reg
                   attribute of the node referenced by it is used.
 - interrupts	: Should be a list of 2 or 3 interrupts: TX DMA, RX DMA,
-		  and optionally Ethernet core. If axistream-connected is
-		  specified, the TX/RX DMA interrupts should be on that node
-		  instead, and only the Ethernet core interrupt is optionally
-		  specified here.
+		  and optionally Ethernet core.
 - phy-handle	: Should point to the external phy device.
 		  See ethernet.txt file in the same directory.
 - xlnx,rxmem	: Set to allocated memory buffer for Rx/Tx in the hardware
+Required properties (When AxiEthernet is configured with MCDMA):
+- xlnx,channel-ids	: Queue Identifier associated with the MCDMA Channel.
+- interrupt-names	: Should contain the interrupt names.
+
+Required properties when configured as MRMAC:
+- xlnx,mrmac-rate	: Can be 10000 or 25000 providing rate in Mbps.
+- xlnx,gtlane		: Indicate the GT reset and speed control lane for the
+			  the current MRMAC lane. Valid range is 0 to 3.
+- xlnx,gtpll		: Handle to AXI GPIO instance for GT PLL mask control.
+			  This is required to control the common PLL mask bits.
+- xlnx,gtctrl		: Handle to AXI GPIO instance for GT speed and reset
+			  control for each MRMAC lane.
 
 Optional properties:
 - phy-mode	: See ethernet.txt
@@ -38,39 +63,106 @@ Optional properties:
 		  1 to enable partial TX checksum offload,
 		  2 to enable full TX checksum offload
 - xlnx,rxcsum	: Same values as xlnx,txcsum but for RX checksum offload
-- clocks	: AXI bus clock for the device. Refer to common clock bindings.
-		  Used to calculate MDIO clock divisor. If not specified, it is
-		  auto-detected from the CPU clock (but only on platforms where
-		  this is possible). New device trees should specify this - the
-		  auto detection is only for backward compatibility.
-- axistream-connected: Reference to another node which contains the resources
-		       for the AXI DMA controller used by this device.
-		       If this is specified, the DMA-related resources from that
-		       device (DMA registers and DMA TX/RX interrupts) rather
-		       than this one will be used.
+- clocks		: Input clock specifier. Refer to common clock bindings.
+- clock-names		: Input clock names. Refer to IP PG for signal description.
+			  1G/2.5G: s_axi_lite_clk, axis_clk and ref_clk.
+			  10G/25G and USXGMII: s_axi_aclk, rx_core_clk and dclk.
+			  10 Gigabit: s_axi_aclk and dclk.
+			  AXI DMA and MCDMA: m_axi_sg_aclk, m_axi_mm2s_aclk and
+			  m_axi_s2mm_aclk.
  - mdio		: Child node for MDIO bus. Must be defined if PHY access is
 		  required through the core's MDIO interface (i.e. always,
 		  unless the PHY is accessed through a different bus).
+- dma-coherent		: Present if dma operations are coherent.
+- xlnx,eth-hasnobuf	: Used when 1G MAC is configured in non-processor mode.
+- xlnx,rxtsfifo		: Configures the axi fifo for receive timestamping.
+
+Optional properties for connected DMA node:
+- xlnx,addrwidth	: Specify the width of the DMA address space in bits.
+			  Value type is u8. Valid range is 32-64. Default is 32.
+- xlnx,include-dre	: Tells whether DMA h/w is configured with data
+			  realignment engine(DRE) or not.
+
+Optional properties (When USXGMII is in use):
+- xlnx,usxgmii-rate	: USXGMII PHY speed - can be 10, 100, 1000, 2500,
+			  5000 or 10000.
+
+Optional properties (When AxiEthernet is configured with MCDMA):
+- xlnx,num-queues	: Number of queues h/w configured for.
+
+Optional properties for MRMAC:
+- xlnx,phcindex		: Indicate the index of the physical hardware clock
+			  to be used as per PTP clock connected to the given
+			  MRMAC lane. Valid range is 0 to 3.
+
+NOTE: Time Sensitive Networking (TSN) related DT bindings are explained in [4].
+
+[1] Documentation/devicetree/bindings/net/phy.txt
+[2] Documentation/devicetree/bindings/net/ethernet.txt
+[3] Documentation/devicetree/bindings/net/xilinx-phy.txt
+[4] Documentation/devicetree/bindings/net/xilinx_tsn.txt
+
 
-Example:
-	axi_ethernet_eth: ethernet@40c00000 {
-		compatible = "xlnx,axi-ethernet-1.00.a";
-		device_type = "network";
-		interrupt-parent = <&microblaze_0_axi_intc>;
-		interrupts = <2 0 1>;
-		clocks = <&axi_clk>;
-		phy-mode = "mii";
-		reg = <0x40c00000 0x40000 0x50c00000 0x40000>;
-		xlnx,rxcsum = <0x2>;
-		xlnx,rxmem = <0x800>;
-		xlnx,txcsum = <0x2>;
-		phy-handle = <&phy0>;
-		axi_ethernetlite_0_mdio: mdio {
-			#address-cells = <1>;
-			#size-cells = <0>;
-			phy0: phy@0 {
-				device_type = "ethernet-phy";
-				reg = <1>;
+Example: AXI 1G/2.5G Ethernet Subsystem + AXIDMA
+
+	axi_eth_0_dma: dma@80040000 {
+			#dma-cells = <1>;
+			compatible = "xlnx,eth-dma";
+			xlnx,addrwidth = /bits/ 8 <32>;
+			<snip>
+	};
+
+	axi_eth_0: ethernet@80000000 {
+			axistream-connected = <&axi_eth_0_dma>;
+			compatible = "xlnx,axi-ethernet-1.00.a";
+			device_type = "network";
+			interrupt-names = "interrupt";
+			interrupt-parent = <&gic>;
+			interrupts = <0 91 4>;
+			phy-handle = <&phy2>;
+			phy-mode = "sgmii";
+			reg = <0x0 0x80000000 0x0 0x40000>;
+			xlnx,include-dre ;
+			xlnx,phy-type = <0x5>;
+			xlnx,rxcsum = <0x0>;
+			xlnx,rxmem = <0x1000>;
+			xlnx,txcsum = <0x0>;
+			axi_eth_0_mdio: mdio {
+				#address-cells = <1>;
+				#size-cells = <0>;
+				phy2: phy@2 {
+					device_type = "ethernet-phy";
+					reg = <2>;
+				};
 			};
-		};
+	};
+
+Example for MRMAC Ethernet subsystem with MCDMA:
+	axi_mcdma_0: axi_mcdma@a4050000 {
+			#dma-cells = <1>;
+			compatible = "xlnx,axi-mcdma-1.1";
+			xlnx,addrwidth = <0x20>;
+			xlnx,include-dre;
+			<snip>
+	};
+
+	gt_pll: gpio@a4000000 {
+		reg = <0x0 0xa4000000 0x0 0x10000>;
+		<snip>
+	}
+
+	gt_ctrl: gpio@a4010000 {
+		reg = <0x0 0xa4010000 0x0 0x40000>;
+		<snip>
+	};
+
+	mrmac_0: mrmac@80000000 {
+		axistream-connected = <&axi_mcdma_0>;
+		compatible = "xlnx,mrmac-ethernet-1.0";
+		reg = <0x0 0xa4090000 0x0 0x1000>;
+		xlnx,mrmac-rate = <10000>;
+		xlnx,gtpll = <&gt_pll>;
+		xlnx,gtctrl = <&gt_ctrl>;
+		xlnx,gtlane = <0x0>;
+		xlnx,rxmem = <0x8000>;
 	};
diff --git a/Documentation/devicetree/bindings/net/xilinx_emaclite.txt b/Documentation/devicetree/bindings/net/xilinx_emaclite.txt
new file mode 100644
index 000000000..989d29efe
--- /dev/null
+++ b/Documentation/devicetree/bindings/net/xilinx_emaclite.txt
@@ -0,0 +1,35 @@
+Xilinx Axi Ethernetlite controller Device Tree Bindings
+---------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,opb-ethernetlite-1.01.a" or
+			  "xlnx,opb-ethernetlite-1.01.b" or
+			  "xlnx,opb-ethernetlite-1.00.a" or
+			  "xlnx,xps-ethernetlite-2.00.a" or
+			  "xlnx,xps-ethernetlite-2.01.a" or
+			  "xlnx,xps-ethernetlite-3.00.a" or.
+- reg			: Physical base address and size of the Axi ethernetlite
+			  registers map.
+- interrupts		: Property with a value describing the interrupt
+			  number.
+- interrupt-parent	: Must be core interrupt controller.
+- phy-handle		: See ethernet.txt file in the same directory.
+
+Optional properties:
+- local-mac-address	: See ethernet.txt file in the same directory.
+			  If absent, random mac address is selected.
+- xlnx,tx-ping-pong	: If present, hardware supports tx ping pong buffer.
+- xlnx,rx-ping-pong	: If present, hardware supports rx ping pong buffer.
+
+Example:
+	axi_ethernetlite_1: ethernet@40e00000 {
+                        compatible = "xlnx,axi-ethernetlite-3.0", "xlnx,xps-ethernetlite-1.00.a";
+                        device_type = "network";
+                        interrupt-parent = <&axi_intc_1>;
+                        interrupts = <1 0>;
+                        local-mac-address = [00 0a 35 00 00 00];
+                        phy-handle = <&phy0>;
+                        reg = <0x40e00000 0x10000>;
+                        xlnx,rx-ping-pong;
+                        xlnx,tx-ping-pong;
+	}
diff --git a/Documentation/devicetree/bindings/net/xilinx_tsn.txt b/Documentation/devicetree/bindings/net/xilinx_tsn.txt
new file mode 100644
index 000000000..8ef9fa9f3
--- /dev/null
+++ b/Documentation/devicetree/bindings/net/xilinx_tsn.txt
@@ -0,0 +1,14 @@
+Xilinx TSN (time sensitive networking) IP driver (xilinx_tsn_ip)
+-----------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be one of "xlnx,tsn-endpoint-ethernet-mac-1.0",
+			  "xlnx,tsn-endpoint-ethernet-mac-2.0" for TSN.
+- reg			: Physical base address and size of the TSN registers map.
+
+Example:
+
+	tsn_endpoint_ip_0: tsn_endpoint_ip_0 {
+		compatible = "xlnx,tsn-endpoint-ethernet-mac-2.0";
+		reg = <0x0 0x80040000 0x0 0x40000>;
+	};
diff --git a/Documentation/devicetree/bindings/net/xilinx_tsn_ep.txt b/Documentation/devicetree/bindings/net/xilinx_tsn_ep.txt
new file mode 100644
index 000000000..f42e5417d
--- /dev/null
+++ b/Documentation/devicetree/bindings/net/xilinx_tsn_ep.txt
@@ -0,0 +1,35 @@
+Xilinx TSN (time sensitive networking) EndPoint Driver (xilinx_tsn_ep)
+-------------------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-ep"
+- reg			: Physical base address and size of the TSN Endpoint
+				registers map
+- interrupts		: Property with a value describing the interrupt
+- interrupts-names	: Property denotes the interrupt names.
+- interrupt-parent	: Must be core interrupt controller.
+- local-mac-address	: See ethernet.txt [1].
+
+Optional properties:
+- xlnx,num-tc		: Number of traffic class supported in current design,
+			  range is 2,3 and default value is 3. It denotes
+			  the traffic classes based on VLAN-PCP value.
+- xlnx,channel-ids 	: Queue Identifier associated with the MCDMA Channel, range
+			  is Tx: "1 to 2" and Rx: "2 to 5", default value is "1 to 5".
+- xlnx,eth-hasnobuf	: Used when 1G MAC is configured in non processor mode.
+
+[1] Documentation/devicetree/bindings/net/ethernet.txt
+
+Example:
+
+	tsn_ep: tsn_ep@80056000 {
+		compatible = "xlnx,tsn-ep";
+		reg = <0x0 0x80056000 0x0 0xA000>;
+		xlnx,num-tc = <0x3>;
+		interrupt-names = "tsn_ep_scheduler_irq";
+		interrupt-parent = <&gic>;
+		interrupts = <0 111 4>;
+		local-mac-address = [00 0A 35 00 01 10];
+		xlnx,channel-ids = "1","2","3","4","5";
+		xlnx,eth-hasnobuf ;
+	};
diff --git a/Documentation/devicetree/bindings/net/xilinx_tsn_switch.txt b/Documentation/devicetree/bindings/net/xilinx_tsn_switch.txt
new file mode 100644
index 000000000..898e5b7b5
--- /dev/null
+++ b/Documentation/devicetree/bindings/net/xilinx_tsn_switch.txt
@@ -0,0 +1,23 @@
+Xilinx TSN (time sensitive networking) Switch Driver (xilinx_tsn_switch)
+-----------------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-switch"
+- reg			: Physical base address and size of the TSN registers map.
+
+Optional properties:
+- xlnx,num-tc		: Number of traffic class supported in current design,
+			  range is 2,3 and default value is 3. It denotes
+			  the traffic classes based on VLAN-PCP value.
+- xlnx,has-hwaddr-learning	: Denotes hardware address learning support
+- xlnx,has-inband-mgmt-tag	: Denotes inband management support
+
+Example:
+
+	epswitch: tsn_switch@80078000 {
+		compatible = "xlnx,tsn-switch";
+		reg = <0x0 0x80078000 0x0 0x4000>;
+		xlnx,num-tc = <0x3>;
+		xlnx,has-hwaddr-learning ;
+		xlnx,has-inband-mgmt-tag ;
+	};
diff --git a/drivers/net/ethernet/xilinx/Kconfig b/drivers/net/ethernet/xilinx/Kconfig
index d0d0d4fe9..1ea4413e2 100644
--- a/drivers/net/ethernet/xilinx/Kconfig
+++ b/drivers/net/ethernet/xilinx/Kconfig
@@ -1,6 +1,6 @@
 # SPDX-License-Identifier: GPL-2.0-only
 #
-# Xilink device configuration
+# Xilinx device configuration
 #
 
 config NET_VENDOR_XILINX
@@ -18,18 +18,32 @@ if NET_VENDOR_XILINX
 
 config XILINX_EMACLITE
 	tristate "Xilinx 10/100 Ethernet Lite support"
-	depends on PPC32 || MICROBLAZE || ARCH_ZYNQ || MIPS
 	select PHYLIB
+	depends on HAS_IOMEM
 	help
 	  This driver supports the 10/100 Ethernet Lite from Xilinx.
 
 config XILINX_AXI_EMAC
 	tristate "Xilinx 10/100/1000 AXI Ethernet support"
-	select PHYLINK
+	select PHYLIB
 	help
 	  This driver supports the 10/100/1000 Ethernet from Xilinx for the
 	  AXI bus interface used in Xilinx Virtex FPGAs and Soc's.
 
+config XILINX_AXI_EMAC_HWTSTAMP
+	bool "Generate hardware packet timestamps"
+	depends on XILINX_AXI_EMAC
+	select PTP_1588_CLOCK
+	default n
+	help
+	  Generate hardware packet timestamps. This is to facilitate IEE 1588.
+config  AXIENET_HAS_MCDMA
+	bool "AXI Ethernet is configured with MCDMA"
+	depends on XILINX_AXI_EMAC
+	default n
+	help
+	  When hardware is generated with AXI Ethernet with MCDMA select this option.
+
 config XILINX_LL_TEMAC
 	tristate "Xilinx LL TEMAC (LocalLink Tri-mode Ethernet MAC) driver"
 	select PHYLIB
@@ -37,4 +51,54 @@ config XILINX_LL_TEMAC
 	  This driver supports the Xilinx 10/100/1000 LocalLink TEMAC
 	  core used in Xilinx Spartan and Virtex FPGAs
 
+config XILINX_TSN
+	bool "Enable Xilinx's TSN IP"
+	default n
+	help
+	  Enable Xilinx's TSN IP.
+
+config XILINX_TSN_PTP
+	bool "Generate hardware packet timestamps using Xilinx's TSN IP"
+	depends on XILINX_TSN
+	select PTP_1588_CLOCK
+	default y
+	help
+	  Generate hardare packet timestamps. This is to facilitate IEE 1588.
+
+config XILINX_TSN_QBV
+	bool "Support Qbv protocol in TSN"
+	depends on XILINX_TSN_PTP
+	select PTP_1588_CLOCK
+	default y
+	help
+	  Enables TSN Qbv protocol.
+
+config XILINX_TSN_SWITCH
+	bool "Support TSN switch"
+	depends on XILINX_TSN
+	default y
+	help
+	  Enable Xilinx's TSN Switch support.
+
+config XILINX_TSN_QCI
+	bool "Support Qci protocol in TSN"
+	depends on XILINX_TSN_SWITCH
+	default y
+	help
+	  Enable TSN QCI protocol.
+
+config XILINX_TSN_CB
+	bool "Support CB protocol in TSN"
+	depends on XILINX_TSN_SWITCH
+	default y
+	help
+	  Enable TSN CB protocol support.
+
+config XILINX_TSN_QBR
+       bool "Support QBR protocol in TSN"
+       depends on XILINX_TSN_SWITCH
+       default y
+       help
+         Enable TSN QBR protocol support.
+
 endif # NET_VENDOR_XILINX
diff --git a/drivers/net/ethernet/xilinx/Makefile b/drivers/net/ethernet/xilinx/Makefile
index 7d7dc1771..08cbee3cc 100644
--- a/drivers/net/ethernet/xilinx/Makefile
+++ b/drivers/net/ethernet/xilinx/Makefile
@@ -1,10 +1,18 @@
 # SPDX-License-Identifier: GPL-2.0
 #
-# Makefile for the Xilink network device drivers.
+# Makefile for the Xilinx network device drivers.
 #
 
 ll_temac-objs := ll_temac_main.o ll_temac_mdio.o
 obj-$(CONFIG_XILINX_LL_TEMAC) += ll_temac.o
 obj-$(CONFIG_XILINX_EMACLITE) += xilinx_emaclite.o
-xilinx_emac-objs := xilinx_axienet_main.o xilinx_axienet_mdio.o
+obj-$(CONFIG_XILINX_TSN) += xilinx_tsn_ep.o xilinx_tsn_ip.o
+obj-$(CONFIG_XILINX_TSN_PTP) += xilinx_tsn_ptp_xmit.o xilinx_tsn_ptp_clock.o
+obj-$(CONFIG_XILINX_TSN_QBV) += xilinx_tsn_shaper.o
+obj-$(CONFIG_XILINX_TSN_QCI) += xilinx_tsn_qci.o
+obj-$(CONFIG_XILINX_TSN_CB) += xilinx_tsn_cb.o
+obj-$(CONFIG_XILINX_TSN_SWITCH) += xilinx_tsn_switch.o
+xilinx_emac-objs := xilinx_axienet_main.o xilinx_axienet_mdio.o xilinx_axienet_dma.o
 obj-$(CONFIG_XILINX_AXI_EMAC) += xilinx_emac.o
+obj-$(CONFIG_XILINX_TSN_QBR) += xilinx_tsn_preemption.o
+obj-$(CONFIG_AXIENET_HAS_MCDMA) += xilinx_axienet_mcdma.o
diff --git a/drivers/net/ethernet/xilinx/ll_temac_main.c b/drivers/net/ethernet/xilinx/ll_temac_main.c
index 130f4b707..b105e1d35 100644
--- a/drivers/net/ethernet/xilinx/ll_temac_main.c
+++ b/drivers/net/ethernet/xilinx/ll_temac_main.c
@@ -942,9 +942,6 @@ temac_start_xmit(struct sk_buff *skb, struct net_device *ndev)
 	wmb();
 	lp->dma_out(lp, TX_TAILDESC_PTR, tail_p); /* DMA start */
 
-	if (temac_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1))
-		netif_stop_queue(ndev);
-
 	return NETDEV_TX_OK;
 }
 
@@ -1421,8 +1418,6 @@ static int temac_probe(struct platform_device *pdev)
 		lp->indirect_lock = devm_kmalloc(&pdev->dev,
 						 sizeof(*lp->indirect_lock),
 						 GFP_KERNEL);
-		if (!lp->indirect_lock)
-			return -ENOMEM;
 		spin_lock_init(lp->indirect_lock);
 	}
 
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet.h b/drivers/net/ethernet/xilinx/xilinx_axienet.h
index 7326ad4d5..2f4af39ce 100644
--- a/drivers/net/ethernet/xilinx/xilinx_axienet.h
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet.h
@@ -13,7 +13,9 @@
 #include <linux/spinlock.h>
 #include <linux/interrupt.h>
 #include <linux/if_vlan.h>
-#include <linux/phylink.h>
+#include <linux/net_tstamp.h>
+#include <linux/phy.h>
+#include <linux/of_platform.h>
 
 /* Packet size info */
 #define XAE_HDR_SIZE			14 /* Size of Ethernet header */
@@ -25,29 +27,39 @@
 #define XAE_MAX_VLAN_FRAME_SIZE  (XAE_MTU + VLAN_ETH_HLEN + XAE_TRL_SIZE)
 #define XAE_MAX_JUMBO_FRAME_SIZE (XAE_JUMBO_MTU + XAE_HDR_SIZE + XAE_TRL_SIZE)
 
+/* DMA address width min and max range */
+#define XAE_DMA_MASK_MIN	32
+#define XAE_DMA_MASK_MAX	64
+
+/* In AXI DMA Tx and Rx queue count is same */
+#define for_each_tx_dma_queue(lp, var) \
+	for ((var) = 0; (var) < (lp)->num_tx_queues; (var)++)
+
+#define for_each_rx_dma_queue(lp, var) \
+	for ((var) = 0; (var) < (lp)->num_rx_queues; (var)++)
 /* Configuration options */
 
 /* Accept all incoming packets. Default: disabled (cleared) */
-#define XAE_OPTION_PROMISC			(1 << 0)
+#define XAE_OPTION_PROMISC			BIT(0)
 
 /* Jumbo frame support for Tx & Rx. Default: disabled (cleared) */
-#define XAE_OPTION_JUMBO			(1 << 1)
+#define XAE_OPTION_JUMBO			BIT(1)
 
 /* VLAN Rx & Tx frame support. Default: disabled (cleared) */
-#define XAE_OPTION_VLAN				(1 << 2)
+#define XAE_OPTION_VLAN				BIT(2)
 
 /* Enable recognition of flow control frames on Rx. Default: enabled (set) */
-#define XAE_OPTION_FLOW_CONTROL			(1 << 4)
+#define XAE_OPTION_FLOW_CONTROL			BIT(4)
 
 /* Strip FCS and PAD from incoming frames. Note: PAD from VLAN frames is not
  * stripped. Default: disabled (set)
  */
-#define XAE_OPTION_FCS_STRIP			(1 << 5)
+#define XAE_OPTION_FCS_STRIP			BIT(5)
 
 /* Generate FCS field and add PAD automatically for outgoing frames.
  * Default: enabled (set)
  */
-#define XAE_OPTION_FCS_INSERT			(1 << 6)
+#define XAE_OPTION_FCS_INSERT			BIT(6)
 
 /* Enable Length/Type error checking for incoming frames. When this option is
  * set, the MAC will filter frames that have a mismatched type/length field
@@ -55,13 +67,13 @@
  * types of frames are encountered. When this option is cleared, the MAC will
  * allow these types of frames to be received. Default: enabled (set)
  */
-#define XAE_OPTION_LENTYPE_ERR			(1 << 7)
+#define XAE_OPTION_LENTYPE_ERR			BIT(7)
 
 /* Enable the transmitter. Default: enabled (set) */
-#define XAE_OPTION_TXEN				(1 << 11)
+#define XAE_OPTION_TXEN				BIT(11)
 
 /*  Enable the receiver. Default: enabled (set) */
-#define XAE_OPTION_RXEN				(1 << 12)
+#define XAE_OPTION_RXEN				BIT(12)
 
 /*  Default options set when device is initialized or reset */
 #define XAE_OPTION_DEFAULTS				   \
@@ -122,7 +134,7 @@
 /* Default TX/RX Threshold and waitbound values for SGDMA mode */
 #define XAXIDMA_DFT_TX_THRESHOLD	24
 #define XAXIDMA_DFT_TX_WAITBOUND	254
-#define XAXIDMA_DFT_RX_THRESHOLD	24
+#define XAXIDMA_DFT_RX_THRESHOLD	1
 #define XAXIDMA_DFT_RX_WAITBOUND	254
 
 #define XAXIDMA_BD_CTRL_TXSOF_MASK	0x08000000 /* First tx packet */
@@ -141,6 +153,24 @@
 
 #define XAXIDMA_BD_MINIMUM_ALIGNMENT	0x40
 
+/* AXI Tx Timestamp Stream FIFO Register Definitions */
+#define XAXIFIFO_TXTS_ISR	0x00000000 /* Interrupt Status Register */
+#define XAXIFIFO_TXTS_TDFV	0x0000000C /* Transmit Data FIFO Vacancy */
+#define XAXIFIFO_TXTS_TXFD	0x00000010 /* Tx Data Write Port */
+#define XAXIFIFO_TXTS_TLR	0x00000014 /* Transmit Length Register */
+#define XAXIFIFO_TXTS_RFO	0x0000001C /* Rx Fifo Occupancy */
+#define XAXIFIFO_TXTS_RDFR	0x00000018 /* Rx Fifo reset */
+#define XAXIFIFO_TXTS_RXFD	0x00000020 /* Rx Data Read Port */
+#define XAXIFIFO_TXTS_RLR	0x00000024 /* Receive Length Register */
+#define XAXIFIFO_TXTS_SRR	0x00000028 /* AXI4-Stream Reset */
+
+#define XAXIFIFO_TXTS_INT_RC_MASK	0x04000000
+#define XAXIFIFO_TXTS_RXFD_MASK		0x7FFFFFFF
+#define XAXIFIFO_TXTS_RESET_MASK	0x000000A5
+#define XAXIFIFO_TXTS_TAG_MASK		0xFFFF0000
+#define XAXIFIFO_TXTS_TAG_SHIFT		16
+#define XAXIFIFO_TXTS_TAG_MAX		0xFFFE
+
 /* Axi Ethernet registers definition */
 #define XAE_RAF_OFFSET		0x00000000 /* Reset and Address filter */
 #define XAE_TPF_OFFSET		0x00000004 /* Tx Pause Frame */
@@ -159,16 +189,21 @@
 #define XAE_RCW1_OFFSET		0x00000404 /* Rx Configuration Word 1 */
 #define XAE_TC_OFFSET		0x00000408 /* Tx Configuration */
 #define XAE_FCC_OFFSET		0x0000040C /* Flow Control Configuration */
-#define XAE_EMMC_OFFSET		0x00000410 /* EMAC mode configuration */
-#define XAE_PHYC_OFFSET		0x00000414 /* RGMII/SGMII configuration */
 #define XAE_ID_OFFSET		0x000004F8 /* Identification register */
-#define XAE_MDIO_MC_OFFSET	0x00000500 /* MII Management Config */
-#define XAE_MDIO_MCR_OFFSET	0x00000504 /* MII Management Control */
-#define XAE_MDIO_MWD_OFFSET	0x00000508 /* MII Management Write Data */
-#define XAE_MDIO_MRD_OFFSET	0x0000050C /* MII Management Read Data */
+#define XAE_EMMC_OFFSET		0x00000410 /* MAC speed configuration */
+#define XAE_RMFC_OFFSET		0x00000414 /* RX Max Frame Configuration */
+#define XAE_TSN_ABL_OFFSET	0x000004FC /* Ability Register */
+#define XAE_MDIO_MC_OFFSET	0x00000500 /* MDIO Setup */
+#define XAE_MDIO_MCR_OFFSET	0x00000504 /* MDIO Control */
+#define XAE_MDIO_MWD_OFFSET	0x00000508 /* MDIO Write Data */
+#define XAE_MDIO_MRD_OFFSET	0x0000050C /* MDIO Read Data */
+#define XAE_TEMAC_IS_OFFSET	0x00000600 /* TEMAC Interrupt Status */
+#define XAE_TEMAC_IP_OFFSET	0x00000610 /* TEMAC Interrupt Pending Status */
+#define XAE_TEMAC_IE_OFFSET	0x00000620 /* TEMAC Interrupt Enable Status */
+#define XAE_TEMAC_IC_OFFSET	0x00000630 /* TEMAC Interrupt Clear Status */
 #define XAE_UAW0_OFFSET		0x00000700 /* Unicast address word 0 */
 #define XAE_UAW1_OFFSET		0x00000704 /* Unicast address word 1 */
-#define XAE_FMI_OFFSET		0x00000708 /* Filter Mask Index */
+#define XAE_FMC_OFFSET		0x00000708 /* Frame Filter Control */
 #define XAE_AF0_OFFSET		0x00000710 /* Address Filter 0 */
 #define XAE_AF1_OFFSET		0x00000714 /* Address Filter 1 */
 
@@ -229,6 +264,7 @@
 #define XAE_TPID_3_MASK		0xFFFF0000 /* TPID 1 */
 
 /* Bit masks for Axi Ethernet RCW1 register */
+#define XAE_RCW1_INBAND1588_MASK 0x00400000 /* Inband 1588 Enable */
 #define XAE_RCW1_RST_MASK	0x80000000 /* Reset */
 #define XAE_RCW1_JUM_MASK	0x40000000 /* Jumbo frame enable */
 /* In-Band FCS enable (FCS not stripped) */
@@ -245,6 +281,7 @@
 #define XAE_RCW1_PAUSEADDR_MASK 0x0000FFFF
 
 /* Bit masks for Axi Ethernet TC register */
+#define XAE_TC_INBAND1588_MASK 0x00400000 /* Inband 1588 Enable */
 #define XAE_TC_RST_MASK		0x80000000 /* Reset */
 #define XAE_TC_JUM_MASK		0x40000000 /* Jumbo frame enable */
 /* In-Band FCS enable (FCS not generated) */
@@ -269,18 +306,7 @@
 #define XAE_EMMC_LINKSPD_10	0x00000000 /* Link Speed mask for 10 Mbit */
 #define XAE_EMMC_LINKSPD_100	0x40000000 /* Link Speed mask for 100 Mbit */
 #define XAE_EMMC_LINKSPD_1000	0x80000000 /* Link Speed mask for 1000 Mbit */
-
-/* Bit masks for Axi Ethernet PHYC register */
-#define XAE_PHYC_SGMIILINKSPEED_MASK	0xC0000000 /* SGMII link speed mask*/
-#define XAE_PHYC_RGMIILINKSPEED_MASK	0x0000000C /* RGMII link speed */
-#define XAE_PHYC_RGMIIHD_MASK		0x00000002 /* RGMII Half-duplex */
-#define XAE_PHYC_RGMIILINK_MASK		0x00000001 /* RGMII link status */
-#define XAE_PHYC_RGLINKSPD_10		0x00000000 /* RGMII link 10 Mbit */
-#define XAE_PHYC_RGLINKSPD_100		0x00000004 /* RGMII link 100 Mbit */
-#define XAE_PHYC_RGLINKSPD_1000		0x00000008 /* RGMII link 1000 Mbit */
-#define XAE_PHYC_SGLINKSPD_10		0x00000000 /* SGMII link 10 Mbit */
-#define XAE_PHYC_SGLINKSPD_100		0x40000000 /* SGMII link 100 Mbit */
-#define XAE_PHYC_SGLINKSPD_1000		0x80000000 /* SGMII link 1000 Mbit */
+#define XAE_EMMC_LINKSPD_2500	0x80000000 /* Link Speed mask for 2500 Mbit */
 
 /* Bit masks for Axi Ethernet MDIO interface MC register */
 #define XAE_MDIO_MC_MDIOEN_MASK		0x00000040 /* MII management enable */
@@ -298,38 +324,27 @@
 #define XAE_MDIO_MCR_INITIATE_MASK	0x00000800 /* Ready Mask */
 #define XAE_MDIO_MCR_READY_MASK		0x00000080 /* Ready Mask */
 
-/* Bit masks for Axi Ethernet MDIO interface MIS, MIP, MIE, MIC registers */
-#define XAE_MDIO_INT_MIIM_RDY_MASK	0x00000001 /* MIIM Interrupt */
-
 /* Bit masks for Axi Ethernet UAW1 register */
 /* Station address bits [47:32]; Station address
  * bits [31:0] are stored in register UAW0
  */
 #define XAE_UAW1_UNICASTADDR_MASK	0x0000FFFF
 
-/* Bit masks for Axi Ethernet FMI register */
-#define XAE_FMI_PM_MASK			0x80000000 /* Promis. mode enable */
-#define XAE_FMI_IND_MASK		0x00000003 /* Index Mask */
+/* Bit masks for Axi Ethernet FMC register */
+#define XAE_FMC_PM_MASK			0x80000000 /* Promis. mode enable */
+#define XAE_FMC_IND_MASK		0x00000003 /* Index Mask */
 
 #define XAE_MDIO_DIV_DFT		29 /* Default MDIO clock divisor */
 
-/* Defines for different options for C_PHY_TYPE parameter in Axi Ethernet IP */
-#define XAE_PHY_TYPE_MII		0
-#define XAE_PHY_TYPE_GMII		1
-#define XAE_PHY_TYPE_RGMII_1_3		2
-#define XAE_PHY_TYPE_RGMII_2_0		3
-#define XAE_PHY_TYPE_SGMII		4
-#define XAE_PHY_TYPE_1000BASE_X		5
-
- /* Total number of entries in the hardware multicast table. */
+/* Total number of entries in the hardware multicast table. */
 #define XAE_MULTICAST_CAM_TABLE_NUM	4
 
 /* Axi Ethernet Synthesis features */
-#define XAE_FEATURE_PARTIAL_RX_CSUM	(1 << 0)
-#define XAE_FEATURE_PARTIAL_TX_CSUM	(1 << 1)
-#define XAE_FEATURE_FULL_RX_CSUM	(1 << 2)
-#define XAE_FEATURE_FULL_TX_CSUM	(1 << 3)
-#define XAE_FEATURE_DMA_64BIT		(1 << 4)
+#define XAE_FEATURE_PARTIAL_RX_CSUM	BIT(0)
+#define XAE_FEATURE_PARTIAL_TX_CSUM	BIT(1)
+#define XAE_FEATURE_FULL_RX_CSUM	BIT(2)
+#define XAE_FEATURE_FULL_TX_CSUM	BIT(3)
+#define XAE_FEATURE_DMA_64BIT		BIT(4)
 
 #define XAE_NO_CSUM_OFFLOAD		0
 
@@ -339,12 +354,220 @@
 
 #define DELAY_OF_ONE_MILLISEC		1000
 
+#define XAXIENET_NAPI_WEIGHT		64
+
+/* Definition of 1588 PTP in Axi Ethernet IP */
+#define TX_TS_OP_NOOP           0x0
+#define TX_TS_OP_ONESTEP        0x1
+#define TX_TS_OP_TWOSTEP        0x2
+#define TX_TS_CSUM_UPDATE       0x1
+#define TX_TS_CSUM_UPDATE_MRMAC		0x4
+#define TX_TS_PDELAY_UPDATE_MRMAC	0x8
+#define TX_PTP_CSUM_OFFSET      0x28
+#define TX_PTP_TS_OFFSET        0x4C
+#define TX_PTP_CF_OFFSET        0x32
+
+/* XXV MAC Register Definitions */
+#define XXV_GT_RESET_OFFSET		0x00000000
+#define XXV_TC_OFFSET			0x0000000C
+#define XXV_RCW1_OFFSET			0x00000014
+#define XXV_JUM_OFFSET			0x00000018
+#define XXV_TICKREG_OFFSET		0x00000020
+#define XXV_STATRX_BLKLCK_OFFSET	0x0000040C
+#define XXV_USXGMII_AN_OFFSET		0x000000C8
+#define XXV_USXGMII_AN_STS_OFFSET	0x00000458
+
+/* XXV MAC Register Mask Definitions */
+#define XXV_GT_RESET_MASK	BIT(0)
+#define XXV_TC_TX_MASK		BIT(0)
+#define XXV_RCW1_RX_MASK	BIT(0)
+#define XXV_RCW1_FCS_MASK	BIT(1)
+#define XXV_TC_FCS_MASK		BIT(1)
+#define XXV_MIN_JUM_MASK	GENMASK(7, 0)
+#define XXV_MAX_JUM_MASK	GENMASK(10, 8)
+#define XXV_RX_BLKLCK_MASK	BIT(0)
+#define XXV_TICKREG_STATEN_MASK BIT(0)
+#define XXV_MAC_MIN_PKT_LEN	64
+
+/* USXGMII Register Mask Definitions  */
+#define USXGMII_AN_EN		BIT(5)
+#define USXGMII_AN_RESET	BIT(6)
+#define USXGMII_AN_RESTART	BIT(7)
+#define USXGMII_EN		BIT(16)
+#define USXGMII_RATE_MASK	0x0E000700
+#define USXGMII_RATE_1G		0x04000200
+#define USXGMII_RATE_2G5	0x08000400
+#define USXGMII_RATE_10M	0x0
+#define USXGMII_RATE_100M	0x02000100
+#define USXGMII_RATE_5G		0x0A000500
+#define USXGMII_RATE_10G	0x06000300
+#define USXGMII_FD		BIT(28)
+#define USXGMII_LINK_STS	BIT(31)
+
+/* USXGMII AN STS register mask definitions */
+#define USXGMII_AN_STS_COMP_MASK	BIT(16)
+
+/* MCDMA Register Definitions */
+#define XMCDMA_CR_OFFSET	0x00
+#define XMCDMA_SR_OFFSET	0x04
+#define XMCDMA_CHEN_OFFSET	0x08
+#define XMCDMA_CHSER_OFFSET	0x0C
+#define XMCDMA_ERR_OFFSET	0x10
+#define XMCDMA_PKTDROP_OFFSET	0x14
+#define XMCDMA_TXWEIGHT0_OFFSET 0x18
+#define XMCDMA_TXWEIGHT1_OFFSET 0x1C
+#define XMCDMA_RXINT_SER_OFFSET 0x20
+#define XMCDMA_TXINT_SER_OFFSET 0x28
+
+#define XMCDMA_CHOBS1_OFFSET	0x440
+#define XMCDMA_CHOBS2_OFFSET	0x444
+#define XMCDMA_CHOBS3_OFFSET	0x448
+#define XMCDMA_CHOBS4_OFFSET	0x44C
+#define XMCDMA_CHOBS5_OFFSET	0x450
+#define XMCDMA_CHOBS6_OFFSET	0x454
+
+#define XMCDMA_CHAN_RX_OFFSET  0x500
+
+/* Per Channel Registers */
+#define XMCDMA_CHAN_CR_OFFSET(chan_id)		(0x40 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_SR_OFFSET(chan_id)		(0x44 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_CURDESC_OFFSET(chan_id)	(0x48 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_TAILDESC_OFFSET(chan_id)	(0x50 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_PKTDROP_OFFSET(chan_id)	(0x58 + ((chan_id) - 1) * 0x40)
+
+#define XMCDMA_RX_OFFSET	0x500
+
+/* MCDMA Mask registers */
+#define XMCDMA_CR_RUNSTOP_MASK		BIT(0) /* Start/stop DMA channel */
+#define XMCDMA_CR_RESET_MASK		BIT(2) /* Reset DMA engine */
+
+#define XMCDMA_SR_HALTED_MASK		BIT(0)
+#define XMCDMA_SR_IDLE_MASK		BIT(1)
+
+#define XMCDMA_IRQ_ERRON_OTHERQ_MASK	BIT(3)
+#define XMCDMA_IRQ_PKTDROP_MASK		BIT(4)
+#define XMCDMA_IRQ_IOC_MASK		BIT(5)
+#define XMCDMA_IRQ_DELAY_MASK		BIT(6)
+#define XMCDMA_IRQ_ERR_MASK		BIT(7)
+#define XMCDMA_IRQ_ALL_MASK		GENMASK(7, 5)
+#define XMCDMA_PKTDROP_COALESCE_MASK	GENMASK(15, 8)
+#define XMCDMA_COALESCE_MASK		GENMASK(23, 16)
+#define XMCDMA_DELAY_MASK		GENMASK(31, 24)
+
+#define XMCDMA_CHEN_MASK		GENMASK(7, 0)
+#define XMCDMA_CHID_MASK		GENMASK(7, 0)
+
+#define XMCDMA_ERR_INTERNAL_MASK	BIT(0)
+#define XMCDMA_ERR_SLAVE_MASK		BIT(1)
+#define XMCDMA_ERR_DECODE_MASK		BIT(2)
+#define XMCDMA_ERR_SG_INT_MASK		BIT(4)
+#define XMCDMA_ERR_SG_SLV_MASK		BIT(5)
+#define XMCDMA_ERR_SG_DEC_MASK		BIT(6)
+
+#define XMCDMA_PKTDROP_CNT_MASK		GENMASK(31, 0)
+
+#define XMCDMA_BD_CTRL_TXSOF_MASK	0x80000000 /* First tx packet */
+#define XMCDMA_BD_CTRL_TXEOF_MASK	0x40000000 /* Last tx packet */
+#define XMCDMA_BD_CTRL_ALL_MASK		0xC0000000 /* All control bits */
+#define XMCDMA_BD_STS_ALL_MASK		0xF0000000 /* All status bits */
+
+#define XMCDMA_COALESCE_SHIFT		16
+#define XMCDMA_DELAY_SHIFT		24
+#define XMCDMA_DFT_TX_THRESHOLD		1
+
+#define XMCDMA_TXWEIGHT_CH_MASK(chan_id)	GENMASK(((chan_id) * 4 + 3), \
+							(chan_id) * 4)
+#define XMCDMA_TXWEIGHT_CH_SHIFT(chan_id)	((chan_id) * 4)
+
+/* PTP Packet length */
+#define XAE_TX_PTP_LEN		16
+#define XXV_TX_PTP_LEN		12
+
+/* Macros used when AXI DMA h/w is configured without DRE */
+#define XAE_TX_BUFFERS		64
+#define XAE_MAX_PKT_LEN		8192
+
+/* MRMAC Register Definitions */
+/* Configuration Registers */
+#define MRMAC_REV_OFFSET		0x00000000
+#define MRMAC_RESET_OFFSET		0x00000004
+#define MRMAC_MODE_OFFSET		0x00000008
+#define MRMAC_CONFIG_TX_OFFSET		0x0000000C
+#define MRMAC_CONFIG_RX_OFFSET		0x00000010
+#define MRMAC_TICK_OFFSET		0x0000002C
+#define MRMAC_CFG1588_OFFSET	0x00000040
+
+/* Status Registers */
+#define MRMAC_TX_STS_OFFSET		0x00000740
+#define MRMAC_RX_STS_OFFSET		0x00000744
+#define MRMAC_TX_RT_STS_OFFSET		0x00000748
+#define MRMAC_RX_RT_STS_OFFSET		0x0000074C
+#define MRMAC_STATRX_BLKLCK_OFFSET	0x00000754
+
+/* Register bit masks */
+#define MRMAC_RX_SERDES_RST_MASK	(BIT(3) | BIT(2) | BIT(1) | BIT(0))
+#define MRMAC_TX_SERDES_RST_MASK	BIT(4)
+#define MRMAC_RX_RST_MASK		BIT(5)
+#define MRMAC_TX_RST_MASK		BIT(6)
+#define MRMAC_RX_AXI_RST_MASK		BIT(8)
+#define MRMAC_TX_AXI_RST_MASK		BIT(9)
+#define MRMAC_STS_ALL_MASK		0xFFFFFFFF
+
+#define MRMAC_RX_EN_MASK		BIT(0)
+#define MRMAC_RX_DEL_FCS_MASK		BIT(1)
+
+#define MRMAC_TX_EN_MASK		BIT(0)
+#define MRMAC_TX_INS_FCS_MASK		BIT(1)
+
+#define MRMAC_RX_BLKLCK_MASK		BIT(0)
+
+#define MRMAC_CTL_DATA_RATE_MASK	GENMASK(2, 0)
+#define MRMAC_CTL_DATA_RATE_10G		0
+#define MRMAC_CTL_DATA_RATE_25G		1
+#define MRMAC_CTL_DATA_RATE_40G		2
+#define MRMAC_CTL_DATA_RATE_50G		3
+#define MRMAC_CTL_DATA_RATE_100G	4
+
+#define MRMAC_CTL_AXIS_CFG_MASK		GENMASK(11, 9)
+#define MRMAC_CTL_AXIS_CFG_SHIFT	9
+#define MRMAC_CTL_AXIS_CFG_10G_IND	1
+#define MRMAC_CTL_AXIS_CFG_25G_IND	1
+
+#define MRMAC_CTL_SERDES_WIDTH_MASK	GENMASK(6, 4)
+#define MRMAC_CTL_SERDES_WIDTH_SHIFT	4
+#define MRMAC_CTL_SERDES_WIDTH_10G	4
+#define MRMAC_CTL_SERDES_WIDTH_25G	6
+
+#define MRMAC_CTL_RATE_CFG_MASK		(MRMAC_CTL_DATA_RATE_MASK |	\
+					 MRMAC_CTL_AXIS_CFG_MASK |	\
+					 MRMAC_CTL_SERDES_WIDTH_MASK)
+
+#define MRMAC_CTL_PM_TICK_MASK		BIT(30)
+#define MRMAC_TICK_TRIGGER		BIT(0)
+#define MRMAC_ONE_STEP_EN		BIT(0)
+
+/* MRMAC GT wrapper registers */
+#define MRMAC_GT_PLL_OFFSET		0x0
+#define MRMAC_GT_PLL_STS_OFFSET		0x8
+#define MRMAC_GT_RATE_OFFSET		0x0
+#define MRMAC_GT_CTRL_OFFSET		0x8
+
+#define MRMAC_GT_PLL_RST_MASK		0x00030003
+#define MRMAC_GT_PLL_DONE_MASK		0xFF
+#define MRMAC_GT_RST_ALL_MASK		BIT(0)
+#define MRMAC_GT_RST_RX_MASK		BIT(1)
+#define MRMAC_GT_RST_TX_MASK		BIT(2)
+#define MRMAC_GT_10G_MASK		0x00000001
+#define MRMAC_GT_25G_MASK		0x00000002
+
+#define MRMAC_GT_LANE_OFFSET		BIT(16)
+#define MRMAC_MAX_GT_LANES		4
 /**
  * struct axidma_bd - Axi Dma buffer descriptor layout
  * @next:         MM2S/S2MM Next Descriptor Pointer
- * @next_msb:     MM2S/S2MM Next Descriptor Pointer (high 32 bits)
+ * @reserved1:    Reserved and not used for 32-bit
  * @phys:         MM2S/S2MM Buffer Address
- * @phys_msb:     MM2S/S2MM Buffer Address (high 32 bits)
+ * @reserved2:    Reserved and not used for 32-bit
  * @reserved3:    Reserved and not used
  * @reserved4:    Reserved and not used
  * @cntrl:        MM2S/S2MM Control value
@@ -354,12 +577,23 @@
  * @app2:         MM2S/S2MM User Application Field 2.
  * @app3:         MM2S/S2MM User Application Field 3.
  * @app4:         MM2S/S2MM User Application Field 4.
+ * @sw_id_offset: MM2S/S2MM Sw ID
+ * @ptp_tx_skb:   If timestamping is enabled used for timestamping skb
+ *		  Otherwise reserved.
+ * @ptp_tx_ts_tag: Tag value of 2 step timestamping if timestamping is enabled
+ *		   Otherwise reserved.
+ * @tx_skb:	  Transmit skb address
+ * @tx_desc_mapping: Tx Descriptor DMA mapping type.
  */
 struct axidma_bd {
-	u32 next;	/* Physical address of next buffer descriptor */
-	u32 next_msb;	/* high 32 bits for IP >= v7.1, reserved on older IP */
-	u32 phys;
-	u32 phys_msb;	/* for IP >= v7.1, reserved for older IP */
+	phys_addr_t next;	/* Physical address of next buffer descriptor */
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved1;
+#endif
+	phys_addr_t phys;
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved2;
+#endif
 	u32 reserved3;
 	u32 reserved4;
 	u32 cntrl;
@@ -368,37 +602,129 @@ struct axidma_bd {
 	u32 app1;	/* TX start << 16 | insert */
 	u32 app2;	/* TX csum seed */
 	u32 app3;
-	u32 app4;   /* Last field used by HW */
-	struct sk_buff *skb;
+	u32 app4;
+	phys_addr_t sw_id_offset; /* first unused field by h/w */
+	phys_addr_t ptp_tx_skb;
+	u32 ptp_tx_ts_tag;
+	phys_addr_t tx_skb;
+	u32 tx_desc_mapping;
 } __aligned(XAXIDMA_BD_MINIMUM_ALIGNMENT);
+/**
+ * struct aximcdma_bd - Axi MCDMA buffer descriptor layout
+ * @next:         MM2S/S2MM Next Descriptor Pointer
+ * @reserved1:    Reserved and not used for 32-bit
+ * @phys:         MM2S/S2MM Buffer Address
+ * @reserved2:    Reserved and not used for 32-bit
+ * @reserved3:    Reserved and not used
+ * @cntrl:        MM2S/S2MM Control value
+ * @status:       S2MM Status value
+ * @sband_stats:  S2MM Sideband Status value
+ *		  MM2S Status value
+ * @app0:         MM2S/S2MM User Application Field 0.
+ * @app1:         MM2S/S2MM User Application Field 1.
+ * @app2:         MM2S/S2MM User Application Field 2.
+ * @app3:         MM2S/S2MM User Application Field 3.
+ * @app4:         MM2S/S2MM User Application Field 4.
+ * @sw_id_offset: MM2S/S2MM Sw ID
+ * @ptp_tx_skb:   If timestamping is enabled used for timestamping skb
+ *		  Otherwise reserved.
+ * @ptp_tx_ts_tag: Tag value of 2 step timestamping if timestamping is enabled
+ *		   Otherwise reserved.
+ * @tx_skb:	  Transmit skb address
+ * @tx_desc_mapping: Tx Descriptor DMA mapping type.
+ */
+struct aximcdma_bd {
+	phys_addr_t next;	/* Physical address of next buffer descriptor */
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved1;
+#endif
+	phys_addr_t phys;
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved2;
+#endif
+	u32 reserved3;
+	u32 cntrl;
+	u32 status;
+	u32 sband_stats;
+	u32 app0;
+	u32 app1;	/* TX start << 16 | insert */
+	u32 app2;	/* TX csum seed */
+	u32 app3;
+	u32 app4;
+	phys_addr_t sw_id_offset; /* first unused field by h/w */
+	phys_addr_t ptp_tx_skb;
+	u32 ptp_tx_ts_tag;
+	phys_addr_t tx_skb;
+	u32 tx_desc_mapping;
+} __aligned(XAXIDMA_BD_MINIMUM_ALIGNMENT);
+
+#define DESC_DMA_MAP_SINGLE 0
+#define DESC_DMA_MAP_PAGE 1
+
+#if defined(CONFIG_XILINX_TSN)
+#define XAE_MAX_QUEUES		5
+#elif defined(CONFIG_AXIENET_HAS_MCDMA)
+#define XAE_MAX_QUEUES		16
+#else
+#define XAE_MAX_QUEUES		1
+#endif
+
+#ifdef CONFIG_XILINX_TSN
+/* TSN queues range is 2 to 5. For eg: for num_tc = 2 minimum queues = 2;
+ * for num_tc = 3 with sideband signalling maximum queues = 5
+ */
+#define XAE_MAX_TSN_TC		3
+#define XAE_TSN_MIN_QUEUES	2
+#define TSN_BRIDGEEP_EPONLY	BIT(29)
+#endif
+
+enum axienet_tsn_ioctl {
+	SIOCCHIOCTL = SIOCDEVPRIVATE,
+	SIOC_GET_SCHED,
+	SIOC_PREEMPTION_CFG,
+	SIOC_PREEMPTION_CTRL,
+	SIOC_PREEMPTION_STS,
+	SIOC_PREEMPTION_COUNTER,
+	SIOC_QBU_USER_OVERRIDE,
+	SIOC_QBU_STS,
+};
 
 /**
  * struct axienet_local - axienet private per device data
  * @ndev:	Pointer for net_device to which it will be attached.
  * @dev:	Pointer to device structure
  * @phy_node:	Pointer to device node structure
+ * @clk:	AXI bus clock
  * @mii_bus:	Pointer to MII bus structure
+ * @mii_clk_div: MII bus clock divider value
  * @regs_start: Resource start for axienet device addresses
  * @regs:	Base address for the axienet_local device address space
- * @dma_regs:	Base address for the axidma device address space
- * @dma_err_tasklet: Tasklet structure to process Axi DMA errors
- * @tx_irq:	Axidma TX IRQ number
- * @rx_irq:	Axidma RX IRQ number
+ * @mcdma_regs:	Base address for the aximcdma device address space
+ * @napi:	Napi Structure array for all dma queues
+ * @num_tx_queues: Total number of Tx DMA queues
+ * @num_rx_queues: Total number of Rx DMA queues
+ * @dq:		DMA queues data
  * @phy_mode:	Phy type to identify between MII/GMII/RGMII/SGMII/1000 Base-X
+ * @is_tsn:	Denotes a tsn port
+ * @num_tc:	Total number of TSN Traffic classes
+ * @timer_priv: PTP timer private data pointer
+ * @ptp_tx_irq: PTP tx irq
+ * @ptp_rx_irq: PTP rx irq
+ * @rtc_irq:	PTP RTC irq
+ * @qbv_irq:	QBV shed irq
+ * @ptp_ts_type: ptp time stamp type - 1 or 2 step mode
+ * @ptp_rx_hw_pointer: ptp rx hw pointer
+ * @ptp_rx_sw_pointer: ptp rx sw pointer
+ * @ptp_txq:	PTP tx queue header
+ * @tx_tstamp_work: PTP timestamping work queue
+ * @ptp_tx_lock: PTP tx lock
+ * @dma_err_tasklet: Tasklet structure to process Axi DMA errors
+ * @eth_irq:	Axi Ethernet IRQ number
  * @options:	AxiEthernet option word
  * @last_link:	Phy link state in which the PHY was negotiated earlier
  * @features:	Stores the extended features supported by the axienet hw
- * @tx_bd_v:	Virtual address of the TX buffer descriptor ring
- * @tx_bd_p:	Physical address(start address) of the TX buffer descr. ring
- * @rx_bd_v:	Virtual address of the RX buffer descriptor ring
- * @rx_bd_p:	Physical address(start address) of the RX buffer descr. ring
- * @tx_bd_ci:	Stores the index of the Tx buffer descriptor in the ring being
- *		accessed currently. Used while alloc. BDs before a TX starts
- * @tx_bd_tail:	Stores the index of the Tx buffer descriptor in the ring being
- *		accessed currently. Used while processing BDs after the TX
- *		completed.
- * @rx_bd_ci:	Stores the index of the Rx buffer descriptor in the ring being
- *		accessed currently.
+ * @tx_bd_num:	Number of TX buffer descriptors.
+ * @rx_bd_num:	Number of RX buffer descriptors.
  * @max_frm_size: Stores the maximum size of the frame that can be that
  *		  Txed/Rxed in the existing hardware. If jumbo option is
  *		  supported, the maximum frame size would be 9k. Else it is
@@ -408,6 +734,33 @@ struct axidma_bd {
  * @csum_offload_on_rx_path:	Stores the checksum selection on RX side.
  * @coalesce_count_rx:	Store the irq coalesce on RX side.
  * @coalesce_count_tx:	Store the irq coalesce on TX side.
+ * @phy_flags:	Phy interface flags.
+ * @eth_hasnobuf: Ethernet is configured in Non buf mode.
+ * @eth_hasptp: Ethernet is configured for ptp.
+ * @axienet_config: Ethernet config structure
+ * @tx_ts_regs:	  Base address for the axififo device address space.
+ * @rx_ts_regs:	  Base address for the rx axififo device address space.
+ * @tstamp_config: Hardware timestamp config structure.
+ * @tx_ptpheader: Stores the tx ptp header.
+ * @aclk: AXI4-Lite clock for ethernet and dma.
+ * @eth_sclk: AXI4-Stream interface clock.
+ * @eth_refclk: Stable clock used by signal delay primitives and transceivers.
+ * @eth_dclk: Dynamic Reconfiguration Port(DRP) clock.
+ * @dma_sg_clk: DMA Scatter Gather Clock.
+ * @dma_rx_clk: DMA S2MM Primary Clock.
+ * @dma_tx_clk: DMA MM2S Primary Clock.
+ * @qnum:     Axi Ethernet queue number to be operate on.
+ * @chan_num: MCDMA Channel number to be operate on.
+ * @chan_id:  MCMDA Channel id used in conjunction with weight parameter.
+ * @weight:   MCDMA Channel weight value to be configured for.
+ * @dma_mask: Specify the width of the DMA address space.
+ * @usxgmii_rate: USXGMII PHY speed.
+ * @mrmac_rate: MRMAC speed.
+ * @gt_pll: Common GT PLL mask control register space.
+ * @gt_ctrl: GT speed and reset control register space.
+ * @phc_index: Index to corresponding PTP clock used.
+ * @gt_lane: MRMAC GT lane index used.
+ * @ptp_os_cf: CF TS of PTP PDelay req for one step usage.
  */
 struct axienet_local {
 	struct net_device *ndev;
@@ -416,43 +769,57 @@ struct axienet_local {
 	/* Connection to PHY device */
 	struct device_node *phy_node;
 
-	struct phylink *phylink;
-	struct phylink_config phylink_config;
-
-	/* Reference to PCS/PMA PHY if used */
-	struct mdio_device *pcs_phy;
-
 	/* Clock for AXI bus */
 	struct clk *clk;
 
 	/* MDIO bus data */
 	struct mii_bus *mii_bus;	/* MII bus reference */
+	u8 mii_clk_div; /* MII bus clock divider value */
 
 	/* IO registers, dma functions and IRQs */
 	resource_size_t regs_start;
 	void __iomem *regs;
-	void __iomem *dma_regs;
+	void __iomem *mcdma_regs;
 
-	struct work_struct dma_err_task;
+	struct tasklet_struct dma_err_tasklet[XAE_MAX_QUEUES];
+	struct napi_struct napi[XAE_MAX_QUEUES];	/* NAPI Structure */
+
+	u16    num_tx_queues;	/* Number of TX DMA queues */
+	u16    num_rx_queues;	/* Number of RX DMA queues */
+	struct axienet_dma_q *dq[XAE_MAX_QUEUES];	/* DMA queue data*/
 
-	int tx_irq;
-	int rx_irq;
-	int eth_irq;
 	phy_interface_t phy_mode;
 
+	bool is_tsn;
+#ifdef CONFIG_XILINX_TSN
+	u16    num_tc;
+	struct net_device *master; /* master endpoint */
+	struct net_device *slaves[2]; /* two front panel ports */
+#ifdef CONFIG_XILINX_TSN_PTP
+	void *timer_priv;
+	int ptp_tx_irq;
+	int ptp_rx_irq;
+	int rtc_irq;
+	int qbv_irq;
+	int ptp_ts_type;
+	u8  ptp_rx_hw_pointer;
+	u8  ptp_rx_sw_pointer;
+	struct sk_buff_head ptp_txq;
+	struct work_struct tx_tstamp_work;
+#endif
+#ifdef CONFIG_XILINX_TSN_QBV
+	void __iomem *qbv_regs;
+#endif
+#endif
+	spinlock_t ptp_tx_lock;		/* PTP tx lock*/
+	int eth_irq;
+
 	u32 options;			/* Current options word */
+	u32 last_link;
 	u32 features;
 
-	/* Buffer descriptors */
-	struct axidma_bd *tx_bd_v;
-	dma_addr_t tx_bd_p;
-	u32 tx_bd_num;
-	struct axidma_bd *rx_bd_v;
-	dma_addr_t rx_bd_p;
+	u16 tx_bd_num;
 	u32 rx_bd_num;
-	u32 tx_bd_ci;
-	u32 tx_bd_tail;
-	u32 rx_bd_ci;
 
 	u32 max_frm_size;
 	u32 rxmem;
@@ -462,6 +829,150 @@ struct axienet_local {
 
 	u32 coalesce_count_rx;
 	u32 coalesce_count_tx;
+	u32 phy_flags;
+	bool eth_hasnobuf;
+	bool eth_hasptp;
+	const struct axienet_config *axienet_config;
+
+#if defined(CONFIG_XILINX_AXI_EMAC_HWTSTAMP) || defined(CONFIG_XILINX_TSN_PTP)
+	void __iomem *tx_ts_regs;
+	void __iomem *rx_ts_regs;
+	struct hwtstamp_config tstamp_config;
+	u8 *tx_ptpheader;
+#endif
+	struct clk *aclk;
+	struct clk *eth_sclk;
+	struct clk *eth_refclk;
+	struct clk *eth_dclk;
+	struct clk *dma_sg_clk;
+	struct clk *dma_rx_clk;
+	struct clk *dma_tx_clk;
+
+	/* MCDMA Fields */
+	int qnum[XAE_MAX_QUEUES];
+	int chan_num[XAE_MAX_QUEUES];
+	/* WRR Fields */
+	u16 chan_id;
+	u16 weight;
+
+	u8 dma_mask;
+	u32 usxgmii_rate;
+
+	u32 mrmac_rate;		/* MRMAC speed */
+	void __iomem *gt_pll;	/* Common GT PLL mask control register space */
+	void __iomem *gt_ctrl;	/* GT speed and reset control register space */
+	u32 phc_index;		/* Index to corresponding PTP clock used  */
+	u32 gt_lane;		/* MRMAC GT lane index used */
+	u64 ptp_os_cf;		/* CF TS of PTP PDelay req for one step usage */
+};
+
+/**
+ * struct axienet_dma_q - axienet private per dma queue data
+ * @lp:		Parent pointer
+ * @dma_regs:	Base address for the axidma device address space
+ * @tx_irq:	Axidma TX IRQ number
+ * @rx_irq:	Axidma RX IRQ number
+ * @tx_lock:	Spin lock for tx path
+ * @rx_lock:	Spin lock for tx path
+ * @tx_bd_v:	Virtual address of the TX buffer descriptor ring
+ * @tx_bd_p:	Physical address(start address) of the TX buffer descr. ring
+ * @rx_bd_v:	Virtual address of the RX buffer descriptor ring
+ * @rx_bd_p:	Physical address(start address) of the RX buffer descr. ring
+ * @tx_buf:	Virtual address of the Tx buffer pool used by the driver when
+ *		DMA h/w is configured without DRE.
+ * @tx_bufs:	Virutal address of the Tx buffer address.
+ * @tx_bufs_dma: Physical address of the Tx buffer address used by the driver
+ *		 when DMA h/w is configured without DRE.
+ * @eth_hasdre: Tells whether DMA h/w is configured with dre or not.
+ * @tx_bd_ci:	Stores the index of the Tx buffer descriptor in the ring being
+ *		accessed currently. Used while alloc. BDs before a TX starts
+ * @tx_bd_tail:	Stores the index of the Tx buffer descriptor in the ring being
+ *		accessed currently. Used while processing BDs after the TX
+ *		completed.
+ * @rx_bd_ci:	Stores the index of the Rx buffer descriptor in the ring being
+ *		accessed currently.
+ * @chan_id:    MCDMA channel to operate on.
+ * @rx_offset:	MCDMA S2MM channel starting offset.
+ * @txq_bd_v:	Virtual address of the MCDMA TX buffer descriptor ring
+ * @rxq_bd_v:	Virtual address of the MCDMA RX buffer descriptor ring
+ * @tx_packets: Number of transmit packets processed by the dma queue.
+ * @tx_bytes:   Number of transmit bytes processed by the dma queue.
+ * @rx_packets: Number of receive packets processed by the dma queue.
+ * @rx_bytes:	Number of receive bytes processed by the dma queue.
+ */
+struct axienet_dma_q {
+	struct axienet_local	*lp; /* parent */
+	void __iomem *dma_regs;
+
+	int tx_irq;
+	int rx_irq;
+
+	spinlock_t tx_lock;		/* tx lock */
+	spinlock_t rx_lock;		/* rx lock */
+
+	/* Buffer descriptors */
+	struct axidma_bd *tx_bd_v;
+	struct axidma_bd *rx_bd_v;
+	dma_addr_t rx_bd_p;
+	dma_addr_t tx_bd_p;
+
+	unsigned char *tx_buf[XAE_TX_BUFFERS];
+	unsigned char *tx_bufs;
+	dma_addr_t tx_bufs_dma;
+	bool eth_hasdre;
+
+	u32 tx_bd_ci;
+	u32 rx_bd_ci;
+	u32 tx_bd_tail;
+
+	/* MCDMA fields */
+#ifdef CONFIG_XILINX_TSN
+#define MCDMA_MGMT_CHAN		BIT(0)
+#define MCDMA_MGMT_CHAN_PORT0	BIT(1)
+#define MCDMA_MGMT_CHAN_PORT1	BIT(2)
+	u32 flags;
+#endif
+	u16 chan_id;
+	u32 rx_offset;
+	struct aximcdma_bd *txq_bd_v;
+	struct aximcdma_bd *rxq_bd_v;
+
+	unsigned long tx_packets;
+	unsigned long tx_bytes;
+	unsigned long rx_packets;
+	unsigned long rx_bytes;
+};
+
+#define AXIENET_ETHTOOLS_SSTATS_LEN 6
+#define AXIENET_TX_SSTATS_LEN(lp) ((lp)->num_tx_queues * 2)
+#define AXIENET_RX_SSTATS_LEN(lp) ((lp)->num_rx_queues * 2)
+
+/**
+ * enum axienet_ip_type - AXIENET IP/MAC type.
+ *
+ * @XAXIENET_1G:	 IP is 1G MAC
+ * @XAXIENET_2_5G:	 IP type is 2.5G MAC.
+ * @XAXIENET_LEGACY_10G: IP type is legacy 10G MAC.
+ * @XAXIENET_10G_25G:	 IP type is 10G/25G MAC(XXV MAC).
+ * @XAXIENET_MRMAC:	 IP type is hardened Multi Rate MAC (MRMAC).
+ *
+ */
+enum axienet_ip_type {
+	XAXIENET_1G = 0,
+	XAXIENET_2_5G,
+	XAXIENET_LEGACY_10G,
+	XAXIENET_10G_25G,
+	XAXIENET_MRMAC,
+};
+
+struct axienet_config {
+	enum axienet_ip_type mactype;
+	void (*setoptions)(struct net_device *ndev, u32 options);
+	int (*clk_init)(struct platform_device *pdev, struct clk **axi_aclk,
+			struct clk **axis_clk, struct clk **ref_clk,
+			struct clk **dclk);
+	u32 tx_ptplen;
+	u8 ts_header_len;
 };
 
 /**
@@ -476,6 +987,17 @@ struct axienet_option {
 	u32 m_or;
 };
 
+struct xxvenet_option {
+	u32 opt;
+	u32 reg;
+	u32 m_or;
+};
+
+extern void __iomem *mrmac_gt_pll;
+extern void __iomem *mrmac_gt_ctrl;
+extern int mrmac_pll_reg;
+extern int mrmac_pll_rst;
+
 /**
  * axienet_ior - Memory mapped Axi Ethernet register read
  * @lp:         Pointer to axienet local structure
@@ -510,10 +1032,242 @@ static inline void axienet_iow(struct axienet_local *lp, off_t offset,
 	iowrite32(value, lp->regs + offset);
 }
 
+/**
+ * axienet_get_mrmac_blocklock - Write to Clear MRMAC RX block lock status register
+ * and read the latest status
+ * @lp:         Pointer to axienet local structure
+ *
+ * Return: The contents of the Contents of MRMAC RX block lock status register
+ */
+
+static inline u32 axienet_get_mrmac_blocklock(struct axienet_local *lp)
+{
+	axienet_iow(lp, MRMAC_STATRX_BLKLCK_OFFSET, MRMAC_STS_ALL_MASK);
+	return axienet_ior(lp, MRMAC_STATRX_BLKLCK_OFFSET);
+}
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+/**
+ * axienet_txts_ior - Memory mapped AXI FIFO MM S register read
+ * @lp:         Pointer to axienet_local structure
+ * @reg:     Address offset from the base address of AXI FIFO MM S
+ *              core
+ *
+ * Return: the contents of the AXI FIFO MM S register
+ */
+
+static inline u32 axienet_txts_ior(struct axienet_local *lp, off_t reg)
+{
+	return ioread32(lp->tx_ts_regs + reg);
+}
+
+/**
+ * axienet_txts_iow - Memory mapper AXI FIFO MM S register write
+ * @lp:         Pointer to axienet_local structure
+ * @reg:     Address offset from the base address of AXI FIFO MM S
+ *              core.
+ * @value:      Value to be written into the AXI FIFO MM S register
+ */
+static inline void axienet_txts_iow(struct  axienet_local *lp, off_t reg,
+				    u32 value)
+{
+	iowrite32(value, (lp->tx_ts_regs + reg));
+}
+
+/**
+ * axienet_rxts_ior - Memory mapped AXI FIFO MM S register read
+ * @lp:         Pointer to axienet_local structure
+ * @reg:     Address offset from the base address of AXI FIFO MM S
+ *              core
+ *
+ * Return: the contents of the AXI FIFO MM S register
+ */
+
+static inline u32 axienet_rxts_ior(struct axienet_local *lp, off_t reg)
+{
+	return ioread32(lp->rx_ts_regs + reg);
+}
+
+/**
+ * axienet_rxts_iow - Memory mapper AXI FIFO MM S register write
+ * @lp:         Pointer to axienet_local structure
+ * @reg:     Address offset from the base address of AXI FIFO MM S
+ *              core.
+ * @value:      Value to be written into the AXI FIFO MM S register
+ */
+static inline void axienet_rxts_iow(struct  axienet_local *lp, off_t reg,
+				    u32 value)
+{
+	iowrite32(value, (lp->rx_ts_regs + reg));
+}
+#endif
+
+/**
+ * axienet_dma_in32 - Memory mapped Axi DMA register read
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ *
+ * Return: The contents of the Axi DMA register
+ *
+ * This function returns the contents of the corresponding Axi DMA register.
+ */
+static inline u32 axienet_dma_in32(struct axienet_dma_q *q, off_t reg)
+{
+	return ioread32(q->dma_regs + reg);
+}
+
+/**
+ * axienet_dma_out32 - Memory mapped Axi DMA register write.
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ * @value:	Value to be written into the Axi DMA register
+ *
+ * This function writes the desired value into the corresponding Axi DMA
+ * register.
+ */
+static inline void axienet_dma_out32(struct axienet_dma_q *q,
+				     off_t reg, u32 value)
+{
+	iowrite32(value, q->dma_regs + reg);
+}
+
+/**
+ * axienet_dma_bdout - Memory mapped Axi DMA register Buffer Descriptor write.
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ * @value:	Value to be written into the Axi DMA register
+ *
+ * This function writes the desired value into the corresponding Axi DMA
+ * register.
+ */
+static inline void axienet_dma_bdout(struct axienet_dma_q *q,
+				     off_t reg, dma_addr_t value)
+{
+#if defined(CONFIG_PHYS_ADDR_T_64BIT)
+	writeq(value, (q->dma_regs + reg));
+#else
+	writel(value, (q->dma_regs + reg));
+#endif
+}
+
+#ifdef CONFIG_XILINX_TSN_QBV
+/**
+ * axienet_qbv_ior - Memory mapped TSN QBV register read
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ *
+ * Return: The contents of the Axi Ethernet register
+ *
+ * This function returns the contents of the corresponding register.
+ */
+static inline u32 axienet_qbv_ior(struct axienet_local *lp, off_t offset)
+{
+	return ioread32(lp->qbv_regs + offset);
+}
+
+/**
+ * axienet_qbv_iow - Memory mapped TSN QBV register write
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ * @value:      Value to be written into the Axi Ethernet register
+ *
+ * This function writes the desired value into the corresponding Axi Ethernet
+ * register.
+ */
+static inline void axienet_qbv_iow(struct axienet_local *lp, off_t offset,
+				   u32 value)
+{
+	iowrite32(value, (lp->qbv_regs + offset));
+}
+#endif
+
 /* Function prototypes visible in xilinx_axienet_mdio.c for other files */
 int axienet_mdio_enable(struct axienet_local *lp);
 void axienet_mdio_disable(struct axienet_local *lp);
 int axienet_mdio_setup(struct axienet_local *lp);
 void axienet_mdio_teardown(struct axienet_local *lp);
+void axienet_adjust_link(struct net_device *ndev);
+#ifdef CONFIG_XILINX_TSN
+int axienet_tsn_open(struct net_device *ndev);
+int axienet_tsn_probe(struct platform_device *pdev,
+		      struct axienet_local *lp,
+		      struct net_device *ndev);
+int axienet_tsn_xmit(struct sk_buff *skb, struct net_device *ndev);
+#endif
+#ifdef CONFIG_XILINX_TSN_PTP
+void *axienet_ptp_timer_probe(void __iomem *base, struct platform_device *pdev);
+void axienet_tx_tstamp(struct work_struct *work);
+int axienet_ptp_timer_remove(void *priv);
+#endif
+#ifdef CONFIG_XILINX_TSN_QBV
+int axienet_qbv_init(struct net_device *ndev);
+void axienet_qbv_remove(struct net_device *ndev);
+int axienet_set_schedule(struct net_device *ndev, void __user *useraddr);
+int axienet_get_schedule(struct net_device *ndev, void __user *useraddr);
+#endif
+
+#ifdef CONFIG_XILINX_TSN_QBR
+int axienet_preemption(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_ctrl(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_sts(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_cnt(struct net_device *ndev, void __user *useraddr);
+#ifdef CONFIG_XILINX_TSN_QBV
+int axienet_qbu_user_override(struct net_device *ndev, void __user *useraddr);
+int axienet_qbu_sts(struct net_device *ndev, void __user *useraddr);
+#endif
+#endif
+
+int axienet_mdio_wait_until_ready(struct axienet_local *lp);
+void __maybe_unused axienet_bd_free(struct net_device *ndev,
+				    struct axienet_dma_q *q);
+int __maybe_unused axienet_dma_q_init(struct net_device *ndev,
+				      struct axienet_dma_q *q);
+void axienet_dma_err_handler(unsigned long data);
+irqreturn_t __maybe_unused axienet_tx_irq(int irq, void *_ndev);
+irqreturn_t __maybe_unused axienet_rx_irq(int irq, void *_ndev);
+void axienet_start_xmit_done(struct net_device *ndev, struct axienet_dma_q *q);
+void axienet_dma_bd_release(struct net_device *ndev);
+void __axienet_device_reset(struct axienet_dma_q *q);
+void axienet_set_mac_address(struct net_device *ndev, const void *address);
+void axienet_set_multicast_list(struct net_device *ndev);
+int xaxienet_rx_poll(struct napi_struct *napi, int quota);
+void axienet_setoptions(struct net_device *ndev, u32 options);
+int axienet_queue_xmit(struct sk_buff *skb, struct net_device *ndev,
+		       u16 map);
+
+#if defined(CONFIG_AXIENET_HAS_MCDMA)
+int __maybe_unused axienet_mcdma_rx_q_init(struct net_device *ndev,
+					   struct axienet_dma_q *q);
+int __maybe_unused axienet_mcdma_tx_q_init(struct net_device *ndev,
+					   struct axienet_dma_q *q);
+void __maybe_unused axienet_mcdma_tx_bd_free(struct net_device *ndev,
+					     struct axienet_dma_q *q);
+void __maybe_unused axienet_mcdma_rx_bd_free(struct net_device *ndev,
+					     struct axienet_dma_q *q);
+irqreturn_t __maybe_unused axienet_mcdma_tx_irq(int irq, void *_ndev);
+irqreturn_t __maybe_unused axienet_mcdma_rx_irq(int irq, void *_ndev);
+void __maybe_unused axienet_mcdma_err_handler(unsigned long data);
+void axienet_strings(struct net_device *ndev, u32 sset, u8 *data);
+int axienet_sset_count(struct net_device *ndev, int sset);
+void axienet_get_stats(struct net_device *ndev,
+		       struct ethtool_stats *stats,
+		       u64 *data);
+int axeinet_mcdma_create_sysfs(struct kobject *kobj);
+void axeinet_mcdma_remove_sysfs(struct kobject *kobj);
+int __maybe_unused axienet_mcdma_tx_probe(struct platform_device *pdev,
+					  struct device_node *np,
+					  struct axienet_local *lp);
+int __maybe_unused axienet_mcdma_rx_probe(struct platform_device *pdev,
+					  struct axienet_local *lp,
+					  struct net_device *ndev);
+#endif
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+void axienet_tx_hwtstamp(struct axienet_local *lp,
+			 struct aximcdma_bd *cur_p);
+#else
+void axienet_tx_hwtstamp(struct axienet_local *lp,
+			 struct axidma_bd *cur_p);
+#endif
 
 #endif /* XILINX_AXI_ENET_H */
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_dma.c b/drivers/net/ethernet/xilinx/xilinx_axienet_dma.c
new file mode 100644
index 000000000..0007e5782
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_dma.c
@@ -0,0 +1,486 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx AXI Ethernet (DMA programming)
+ *
+ * Copyright (c) 2008 Nissin Systems Co., Ltd.,  Yoshio Kashiwagi
+ * Copyright (c) 2005-2008 DLA Systems,  David H. Lynch Jr. <dhlii@dlasys.net>
+ * Copyright (c) 2008-2009 Secret Lab Technologies Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2010 - 2012 Xilinx, Inc.
+ * Copyright (C) 2018 Xilinx, Inc. All rights reserved.
+ *
+ * This file contains helper functions for AXI DMA TX and RX programming.
+ */
+
+#include "xilinx_axienet.h"
+
+/**
+ * axienet_bd_free - Release buffer descriptor rings for individual dma queue
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is helper function to axienet_dma_bd_release.
+ */
+
+void __maybe_unused axienet_bd_free(struct net_device *ndev,
+				    struct axienet_dma_q *q)
+{
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		dma_unmap_single(ndev->dev.parent, q->rx_bd_v[i].phys,
+				 lp->max_frm_size, DMA_FROM_DEVICE);
+		dev_kfree_skb((struct sk_buff *)
+			      (q->rx_bd_v[i].sw_id_offset));
+	}
+
+	if (q->rx_bd_v) {
+		dma_free_coherent(ndev->dev.parent,
+				  sizeof(*q->rx_bd_v) * lp->rx_bd_num,
+				  q->rx_bd_v,
+				  q->rx_bd_p);
+	}
+	if (q->tx_bd_v) {
+		dma_free_coherent(ndev->dev.parent,
+				  sizeof(*q->tx_bd_v) * lp->tx_bd_num,
+				  q->tx_bd_v,
+				  q->tx_bd_p);
+	}
+	if (q->tx_bufs) {
+		dma_free_coherent(ndev->dev.parent,
+				  XAE_MAX_PKT_LEN * lp->tx_bd_num,
+				  q->tx_bufs,
+				  q->tx_bufs_dma);
+	}
+}
+
+/**
+ * __dma_txq_init - Setup buffer descriptor rings for individual Axi DMA-Tx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_q_init
+ */
+static int __dma_txq_init(struct net_device *ndev, struct axienet_dma_q *q)
+{
+	int i;
+	u32 cr;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+
+	q->tx_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					sizeof(*q->tx_bd_v) * lp->tx_bd_num,
+					&q->tx_bd_p, GFP_KERNEL);
+	if (!q->tx_bd_v)
+		goto out;
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		q->tx_bd_v[i].next = q->tx_bd_p +
+				     sizeof(*q->tx_bd_v) *
+				     ((i + 1) % lp->tx_bd_num);
+	}
+
+	if (!q->eth_hasdre) {
+		q->tx_bufs = dma_alloc_coherent(ndev->dev.parent,
+						XAE_MAX_PKT_LEN * lp->tx_bd_num,
+						&q->tx_bufs_dma,
+						GFP_KERNEL);
+		if (!q->tx_bufs)
+			goto out;
+
+		for (i = 0; i < lp->tx_bd_num; i++)
+			q->tx_buf[i] = &q->tx_bufs[i * XAE_MAX_PKT_LEN];
+	}
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XAXIDMA_COALESCE_MASK)) |
+	      ((lp->coalesce_count_tx) << XAXIDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = (((cr & ~XAXIDMA_DELAY_MASK)) |
+	      (XAXIDMA_DFT_TX_WAITBOUND << XAXIDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XAXIDMA_IRQ_ALL_MASK;
+	/* Write to the Tx channel control register */
+	axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, cr);
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting.
+	 */
+	axienet_dma_bdout(q, XAXIDMA_TX_CDESC_OFFSET, q->tx_bd_p);
+	cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+	axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET,
+			  cr | XAXIDMA_CR_RUNSTOP_MASK);
+	return 0;
+out:
+	return -ENOMEM;
+}
+
+/**
+ * __dma_rxq_init - Setup buffer descriptor rings for individual Axi DMA-Rx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_q_init
+ */
+static int __dma_rxq_init(struct net_device *ndev,
+			  struct axienet_dma_q *q)
+{
+	int i;
+	u32 cr;
+	struct sk_buff *skb;
+	struct axienet_local *lp = netdev_priv(ndev);
+	/* Reset the indexes which are used for accessing the BDs */
+	q->rx_bd_ci = 0;
+
+	/* Allocate the Rx buffer descriptors. */
+	q->rx_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					sizeof(*q->rx_bd_v) * lp->rx_bd_num,
+					&q->rx_bd_p, GFP_KERNEL);
+	if (!q->rx_bd_v)
+		goto out;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		q->rx_bd_v[i].next = q->rx_bd_p +
+				     sizeof(*q->rx_bd_v) *
+				     ((i + 1) % lp->rx_bd_num);
+
+		skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+		if (!skb)
+			goto out;
+
+		/* Ensure that the skb is completely updated
+		 * prio to mapping the DMA
+		 */
+		wmb();
+
+		q->rx_bd_v[i].sw_id_offset = (phys_addr_t)skb;
+		q->rx_bd_v[i].phys = dma_map_single(ndev->dev.parent,
+						    skb->data,
+						    lp->max_frm_size,
+						    DMA_FROM_DEVICE);
+		q->rx_bd_v[i].cntrl = lp->max_frm_size;
+	}
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XAXIDMA_COALESCE_MASK) |
+	      ((lp->coalesce_count_rx) << XAXIDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = ((cr & ~XAXIDMA_DELAY_MASK) |
+	      (XAXIDMA_DFT_RX_WAITBOUND << XAXIDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XAXIDMA_IRQ_ALL_MASK;
+	/* Write to the Rx channel control register */
+	axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XAXIDMA_RX_CDESC_OFFSET, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+	axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET,
+			  cr | XAXIDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XAXIDMA_RX_TDESC_OFFSET, q->rx_bd_p +
+			  (sizeof(*q->rx_bd_v) * (lp->rx_bd_num - 1)));
+
+	return 0;
+out:
+	return -ENOMEM;
+}
+
+/**
+ * axienet_dma_q_init - Setup buffer descriptor rings for individual Axi DMA
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_dma_q_init(struct net_device *ndev,
+				      struct axienet_dma_q *q)
+{
+	if (__dma_txq_init(ndev, q))
+		goto out;
+
+	if (__dma_rxq_init(ndev, q))
+		goto out;
+
+	return 0;
+out:
+	axienet_dma_bd_release(ndev);
+	return -ENOMEM;
+}
+
+/**
+ * map_dma_q_irq - Map dma q based on interrupt number.
+ * @irq:	irq number
+ * @lp:		axienet local structure
+ *
+ * Return: DMA queue.
+ *
+ * This returns the DMA number on which interrupt has occurred.
+ */
+static int map_dma_q_irq(int irq, struct axienet_local *lp)
+{
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		if (irq == lp->dq[i]->tx_irq || irq == lp->dq[i]->rx_irq)
+			return i;
+	}
+	pr_err("Error mapping DMA irq\n");
+	return -ENODEV;
+}
+
+/**
+ * axienet_tx_irq - Tx Done Isr.
+ * @irq:	irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return: IRQ_HANDLED if device generated a TX interrupt, IRQ_NONE otherwise.
+ *
+ * This is the Axi DMA Tx done Isr. It invokes "axienet_start_xmit_done"
+ * to complete the BD processing.
+ */
+irqreturn_t __maybe_unused axienet_tx_irq(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i = map_dma_q_irq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (i < 0)
+		return IRQ_NONE;
+
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XAXIDMA_TX_SR_OFFSET);
+	if (status & (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK)) {
+		axienet_dma_out32(q, XAXIDMA_TX_SR_OFFSET, status);
+		axienet_start_xmit_done(lp->ndev, q);
+		goto out;
+	}
+
+	if (!(status & XAXIDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	if (status & XAXIDMA_IRQ_ERROR_MASK) {
+		dev_err(&ndev->dev, "DMA Tx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: %pa\n",
+			&q->tx_bd_v[q->tx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XAXIDMA_IRQ_ALL_MASK);
+		/* Write to the Tx channel control register */
+		axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, cr);
+
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XAXIDMA_IRQ_ALL_MASK);
+		/* Write to the Rx channel control register */
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XAXIDMA_TX_SR_OFFSET, status);
+	}
+out:
+	return IRQ_HANDLED;
+}
+
+/**
+ * axienet_rx_irq - Rx Isr.
+ * @irq:	irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return: IRQ_HANDLED if device generated a RX interrupt, IRQ_NONE otherwise.
+ *
+ * This is the Axi DMA Rx Isr. It invokes "axienet_recv" to complete the BD
+ * processing.
+ */
+irqreturn_t __maybe_unused axienet_rx_irq(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i = map_dma_q_irq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (i < 0)
+		return IRQ_NONE;
+
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XAXIDMA_RX_SR_OFFSET);
+	if (status & (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK)) {
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		cr &= ~(XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+		napi_schedule(&lp->napi[i]);
+	}
+
+	if (!(status & XAXIDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	if (status & XAXIDMA_IRQ_ERROR_MASK) {
+		dev_err(&ndev->dev, "DMA Rx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: %pa\n",
+			&q->rx_bd_v[q->rx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XAXIDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, cr);
+
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XAXIDMA_IRQ_ALL_MASK);
+			/* write to the Rx channel control register */
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XAXIDMA_RX_SR_OFFSET, status);
+	}
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * axienet_dma_err_handler - Tasklet handler for Axi DMA Error
+ * @data:	Data passed
+ *
+ * Resets the Axi DMA and Axi Ethernet devices, and reconfigures the
+ * Tx/Rx BDs.
+ */
+void __maybe_unused axienet_dma_err_handler(unsigned long data)
+{
+	u32 axienet_status;
+	u32 cr, i;
+	struct axienet_dma_q *q = (struct axienet_dma_q *)data;
+	struct axienet_local *lp = q->lp;
+	struct net_device *ndev = lp->ndev;
+	struct axidma_bd *cur_p;
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+
+	__axienet_device_reset(q);
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		cur_p = &q->tx_bd_v[i];
+		if (cur_p->phys)
+			dma_unmap_single(ndev->dev.parent, cur_p->phys,
+					 (cur_p->cntrl &
+					  XAXIDMA_BD_CTRL_LENGTH_MASK),
+					 DMA_TO_DEVICE);
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq((struct sk_buff *)cur_p->tx_skb);
+		cur_p->phys = 0;
+		cur_p->cntrl = 0;
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+		cur_p->sw_id_offset = 0;
+		cur_p->tx_skb = 0;
+	}
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		cur_p = &q->rx_bd_v[i];
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+	}
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+	q->rx_bd_ci = 0;
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XAXIDMA_COALESCE_MASK) |
+	      (XAXIDMA_DFT_RX_THRESHOLD << XAXIDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = ((cr & ~XAXIDMA_DELAY_MASK) |
+	      (XAXIDMA_DFT_RX_WAITBOUND << XAXIDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XAXIDMA_IRQ_ALL_MASK;
+	/* Finally write to the Rx channel control register */
+	axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XAXIDMA_COALESCE_MASK)) |
+	      (XAXIDMA_DFT_TX_THRESHOLD << XAXIDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = (((cr & ~XAXIDMA_DELAY_MASK)) |
+	      (XAXIDMA_DFT_TX_WAITBOUND << XAXIDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XAXIDMA_IRQ_ALL_MASK;
+	/* Finally write to the Tx channel control register */
+	axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XAXIDMA_RX_CDESC_OFFSET, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+	axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET,
+			  cr | XAXIDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XAXIDMA_RX_TDESC_OFFSET, q->rx_bd_p +
+			  (sizeof(*q->rx_bd_v) * (lp->rx_bd_num - 1)));
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting
+	 */
+	axienet_dma_bdout(q, XAXIDMA_TX_CDESC_OFFSET, q->tx_bd_p);
+	cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+	axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET,
+			  cr | XAXIDMA_CR_RUNSTOP_MASK);
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G) {
+		axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+		axienet_status &= ~XAE_RCW1_RX_MASK;
+		axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_1G && !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G)
+		axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	axienet_set_mac_address(ndev, NULL);
+	axienet_set_multicast_list(ndev);
+	lp->axienet_config->setoptions(ndev, lp->options);
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_main.c b/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
index 3d91baf2e..04b619490 100644
--- a/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_main.c
@@ -23,6 +23,7 @@
  */
 
 #include <linux/clk.h>
+#include <linux/circ_buf.h>
 #include <linux/delay.h>
 #include <linux/etherdevice.h>
 #include <linux/module.h>
@@ -37,13 +38,19 @@
 #include <linux/phy.h>
 #include <linux/mii.h>
 #include <linux/ethtool.h>
+#include <linux/iopoll.h>
+#include <linux/ptp_classify.h>
+#include <linux/net_tstamp.h>
+#include <linux/random.h>
+#include <net/sock.h>
+#include <linux/xilinx_phy.h>
+#include <linux/clk.h>
 
 #include "xilinx_axienet.h"
 
 /* Descriptors defines for Tx and Rx DMA */
-#define TX_BD_NUM_DEFAULT		128
-#define RX_BD_NUM_DEFAULT		1024
-#define TX_BD_NUM_MIN			(MAX_SKB_FRAGS + 1)
+#define TX_BD_NUM_DEFAULT		64
+#define RX_BD_NUM_DEFAULT		128
 #define TX_BD_NUM_MAX			4096
 #define RX_BD_NUM_MAX			4096
 
@@ -53,16 +60,41 @@
 #define DRIVER_VERSION		"1.00a"
 
 #define AXIENET_REGS_N		40
+#define AXIENET_TS_HEADER_LEN	8
+#define XXVENET_TS_HEADER_LEN	4
+#define MRMAC_TS_HEADER_LEN		16
+#define MRMAC_TS_HEADER_WORDS   (MRMAC_TS_HEADER_LEN / 4)
+#define NS_PER_SEC              1000000000ULL /* Nanoseconds per second */
+
+#define MRMAC_RESET_DELAY	1 /* Delay in msecs*/
+
+/* IEEE1588 Message Type field values  */
+#define PTP_TYPE_SYNC		0
+#define PTP_TYPE_PDELAY_REQ	2
+#define PTP_TYPE_PDELAY_RESP	3
+#define PTP_TYPE_OFFSET		42
+/* SW flags used to convey message type for command FIFO handling */
+#define MSG_TYPE_SHIFT			4
+#define MSG_TYPE_SYNC_FLAG		((PTP_TYPE_SYNC + 1) << MSG_TYPE_SHIFT)
+#define MSG_TYPE_PDELAY_RESP_FLAG	((PTP_TYPE_PDELAY_RESP + 1) << \
+									 MSG_TYPE_SHIFT)
+
+#ifdef CONFIG_XILINX_TSN_PTP
+int axienet_phc_index = -1;
+EXPORT_SYMBOL(axienet_phc_index);
+#endif
 
-/* Match table for of_platform binding */
-static const struct of_device_id axienet_of_match[] = {
-	{ .compatible = "xlnx,axi-ethernet-1.00.a", },
-	{ .compatible = "xlnx,axi-ethernet-1.01.a", },
-	{ .compatible = "xlnx,axi-ethernet-2.01.a", },
-	{},
-};
+void __iomem *mrmac_gt_pll;
+EXPORT_SYMBOL(mrmac_gt_pll);
 
-MODULE_DEVICE_TABLE(of, axienet_of_match);
+void __iomem *mrmac_gt_ctrl;
+EXPORT_SYMBOL(mrmac_gt_ctrl);
+
+int mrmac_pll_reg;
+EXPORT_SYMBOL(mrmac_pll_reg);
+
+int mrmac_pll_rst;
+EXPORT_SYMBOL(mrmac_pll_rst);
 
 /* Option table for setting up Axi Ethernet hardware options */
 static struct axienet_option axienet_options[] = {
@@ -105,8 +137,8 @@ static struct axienet_option axienet_options[] = {
 		.m_or = XAE_FCC_FCTX_MASK,
 	}, { /* Turn on promiscuous frame filtering */
 		.opt = XAE_OPTION_PROMISC,
-		.reg = XAE_FMI_OFFSET,
-		.m_or = XAE_FMI_PM_MASK,
+		.reg = XAE_FMC_OFFSET,
+		.m_or = XAE_FMC_PM_MASK,
 	}, { /* Enable transmitter */
 		.opt = XAE_OPTION_TXEN,
 		.reg = XAE_TC_OFFSET,
@@ -119,62 +151,62 @@ static struct axienet_option axienet_options[] = {
 	{}
 };
 
-/**
- * axienet_dma_in32 - Memory mapped Axi DMA register read
- * @lp:		Pointer to axienet local structure
- * @reg:	Address offset from the base address of the Axi DMA core
- *
- * Return: The contents of the Axi DMA register
- *
- * This function returns the contents of the corresponding Axi DMA register.
- */
-static inline u32 axienet_dma_in32(struct axienet_local *lp, off_t reg)
-{
-	return ioread32(lp->dma_regs + reg);
-}
-
-/**
- * axienet_dma_out32 - Memory mapped Axi DMA register write.
- * @lp:		Pointer to axienet local structure
- * @reg:	Address offset from the base address of the Axi DMA core
- * @value:	Value to be written into the Axi DMA register
- *
- * This function writes the desired value into the corresponding Axi DMA
- * register.
- */
-static inline void axienet_dma_out32(struct axienet_local *lp,
-				     off_t reg, u32 value)
-{
-	iowrite32(value, lp->dma_regs + reg);
-}
-
-static void axienet_dma_out_addr(struct axienet_local *lp, off_t reg,
-				 dma_addr_t addr)
-{
-	axienet_dma_out32(lp, reg, lower_32_bits(addr));
-
-	if (lp->features & XAE_FEATURE_DMA_64BIT)
-		axienet_dma_out32(lp, reg + 4, upper_32_bits(addr));
-}
-
-static void desc_set_phys_addr(struct axienet_local *lp, dma_addr_t addr,
-			       struct axidma_bd *desc)
-{
-	desc->phys = lower_32_bits(addr);
-	if (lp->features & XAE_FEATURE_DMA_64BIT)
-		desc->phys_msb = upper_32_bits(addr);
-}
+/* Option table for setting up Axi Ethernet hardware options */
+static struct xxvenet_option xxvenet_options[] = {
+	{ /* Turn on FCS stripping on receive packets */
+		.opt = XAE_OPTION_FCS_STRIP,
+		.reg = XXV_RCW1_OFFSET,
+		.m_or = XXV_RCW1_FCS_MASK,
+	}, { /* Turn on FCS insertion on transmit packets */
+		.opt = XAE_OPTION_FCS_INSERT,
+		.reg = XXV_TC_OFFSET,
+		.m_or = XXV_TC_FCS_MASK,
+	}, { /* Enable transmitter */
+		.opt = XAE_OPTION_TXEN,
+		.reg = XXV_TC_OFFSET,
+		.m_or = XXV_TC_TX_MASK,
+	}, { /* Enable receiver */
+		.opt = XAE_OPTION_RXEN,
+		.reg = XXV_RCW1_OFFSET,
+		.m_or = XXV_RCW1_RX_MASK,
+	},
+	{}
+};
 
-static dma_addr_t desc_get_phys_addr(struct axienet_local *lp,
-				     struct axidma_bd *desc)
-{
-	dma_addr_t ret = desc->phys;
+/* Option table for setting up MRMAC hardware options */
+static struct xxvenet_option mrmacenet_options[] = {
+	{ /* Turn on FCS stripping on receive packets */
+		.opt = XAE_OPTION_FCS_STRIP,
+		.reg = MRMAC_CONFIG_RX_OFFSET,
+		.m_or = MRMAC_RX_DEL_FCS_MASK,
+	}, { /* Turn on FCS insertion on transmit packets */
+		.opt = XAE_OPTION_FCS_INSERT,
+		.reg = MRMAC_CONFIG_TX_OFFSET,
+		.m_or = MRMAC_TX_INS_FCS_MASK,
+	}, { /* Enable transmitter */
+		.opt = XAE_OPTION_TXEN,
+		.reg = MRMAC_CONFIG_TX_OFFSET,
+		.m_or = MRMAC_TX_EN_MASK,
+	}, { /* Enable receiver */
+		.opt = XAE_OPTION_RXEN,
+		.reg = MRMAC_CONFIG_RX_OFFSET,
+		.m_or = MRMAC_RX_EN_MASK,
+	},
+	{}
+};
 
-	if (lp->features & XAE_FEATURE_DMA_64BIT)
-		ret |= ((dma_addr_t)desc->phys_msb << 16) << 16;
+struct axienet_ethtools_stat {
+	const char *name;
+};
 
-	return ret;
-}
+static struct axienet_ethtools_stat axienet_get_ethtools_strings_stats[] = {
+	{ "tx_packets" },
+	{ "rx_packets" },
+	{ "tx_bytes" },
+	{ "rx_bytes" },
+	{ "tx_errors" },
+	{ "rx_errors" },
+};
 
 /**
  * axienet_dma_bd_release - Release buffer descriptor rings
@@ -184,53 +216,30 @@ static dma_addr_t desc_get_phys_addr(struct axienet_local *lp,
  * axienet_dma_bd_init. axienet_dma_bd_release is called when Axi Ethernet
  * driver stop api is called.
  */
-static void axienet_dma_bd_release(struct net_device *ndev)
+void axienet_dma_bd_release(struct net_device *ndev)
 {
 	int i;
 	struct axienet_local *lp = netdev_priv(ndev);
 
-	/* If we end up here, tx_bd_v must have been DMA allocated. */
-	dma_free_coherent(ndev->dev.parent,
-			  sizeof(*lp->tx_bd_v) * lp->tx_bd_num,
-			  lp->tx_bd_v,
-			  lp->tx_bd_p);
-
-	if (!lp->rx_bd_v)
-		return;
-
-	for (i = 0; i < lp->rx_bd_num; i++) {
-		dma_addr_t phys;
-
-		/* A NULL skb means this descriptor has not been initialised
-		 * at all.
-		 */
-		if (!lp->rx_bd_v[i].skb)
-			break;
-
-		dev_kfree_skb(lp->rx_bd_v[i].skb);
-
-		/* For each descriptor, we programmed cntrl with the (non-zero)
-		 * descriptor size, after it had been successfully allocated.
-		 * So a non-zero value in there means we need to unmap it.
-		 */
-		if (lp->rx_bd_v[i].cntrl) {
-			phys = desc_get_phys_addr(lp, &lp->rx_bd_v[i]);
-			dma_unmap_single(ndev->dev.parent, phys,
-					 lp->max_frm_size, DMA_FROM_DEVICE);
-		}
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	for_each_tx_dma_queue(lp, i) {
+		axienet_mcdma_tx_bd_free(ndev, lp->dq[i]);
+	}
+#endif
+	for_each_rx_dma_queue(lp, i) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		axienet_mcdma_rx_bd_free(ndev, lp->dq[i]);
+#else
+		axienet_bd_free(ndev, lp->dq[i]);
+#endif
 	}
-
-	dma_free_coherent(ndev->dev.parent,
-			  sizeof(*lp->rx_bd_v) * lp->rx_bd_num,
-			  lp->rx_bd_v,
-			  lp->rx_bd_p);
 }
 
 /**
  * axienet_dma_bd_init - Setup buffer descriptor rings for Axi DMA
  * @ndev:	Pointer to the net_device structure
  *
- * Return: 0, on success -ENOMEM, on failure
+ * Return: 0, on success -ENOMEM, on failure -EINVAL, on default return
  *
  * This function is called to initialize the Rx and Tx DMA descriptor
  * rings. This initializes the descriptors with required default values
@@ -238,113 +247,29 @@ static void axienet_dma_bd_release(struct net_device *ndev)
  */
 static int axienet_dma_bd_init(struct net_device *ndev)
 {
-	u32 cr;
-	int i;
-	struct sk_buff *skb;
+	int i, ret = -EINVAL;
 	struct axienet_local *lp = netdev_priv(ndev);
 
-	/* Reset the indexes which are used for accessing the BDs */
-	lp->tx_bd_ci = 0;
-	lp->tx_bd_tail = 0;
-	lp->rx_bd_ci = 0;
-
-	/* Allocate the Tx and Rx buffer descriptors. */
-	lp->tx_bd_v = dma_alloc_coherent(ndev->dev.parent,
-					 sizeof(*lp->tx_bd_v) * lp->tx_bd_num,
-					 &lp->tx_bd_p, GFP_KERNEL);
-	if (!lp->tx_bd_v)
-		return -ENOMEM;
-
-	lp->rx_bd_v = dma_alloc_coherent(ndev->dev.parent,
-					 sizeof(*lp->rx_bd_v) * lp->rx_bd_num,
-					 &lp->rx_bd_p, GFP_KERNEL);
-	if (!lp->rx_bd_v)
-		goto out;
-
-	for (i = 0; i < lp->tx_bd_num; i++) {
-		dma_addr_t addr = lp->tx_bd_p +
-				  sizeof(*lp->tx_bd_v) *
-				  ((i + 1) % lp->tx_bd_num);
-
-		lp->tx_bd_v[i].next = lower_32_bits(addr);
-		if (lp->features & XAE_FEATURE_DMA_64BIT)
-			lp->tx_bd_v[i].next_msb = upper_32_bits(addr);
-	}
-
-	for (i = 0; i < lp->rx_bd_num; i++) {
-		dma_addr_t addr;
-
-		addr = lp->rx_bd_p + sizeof(*lp->rx_bd_v) *
-			((i + 1) % lp->rx_bd_num);
-		lp->rx_bd_v[i].next = lower_32_bits(addr);
-		if (lp->features & XAE_FEATURE_DMA_64BIT)
-			lp->rx_bd_v[i].next_msb = upper_32_bits(addr);
-
-		skb = netdev_alloc_skb_ip_align(ndev, lp->max_frm_size);
-		if (!skb)
-			goto out;
-
-		lp->rx_bd_v[i].skb = skb;
-		addr = dma_map_single(ndev->dev.parent, skb->data,
-				      lp->max_frm_size, DMA_FROM_DEVICE);
-		if (dma_mapping_error(ndev->dev.parent, addr)) {
-			netdev_err(ndev, "DMA mapping error\n");
-			goto out;
-		}
-		desc_set_phys_addr(lp, addr, &lp->rx_bd_v[i]);
-
-		lp->rx_bd_v[i].cntrl = lp->max_frm_size;
-	}
-
-	/* Start updating the Rx channel control register */
-	cr = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-	/* Update the interrupt coalesce count */
-	cr = ((cr & ~XAXIDMA_COALESCE_MASK) |
-	      ((lp->coalesce_count_rx) << XAXIDMA_COALESCE_SHIFT));
-	/* Update the delay timer count */
-	cr = ((cr & ~XAXIDMA_DELAY_MASK) |
-	      (XAXIDMA_DFT_RX_WAITBOUND << XAXIDMA_DELAY_SHIFT));
-	/* Enable coalesce, delay timer and error interrupts */
-	cr |= XAXIDMA_IRQ_ALL_MASK;
-	/* Write to the Rx channel control register */
-	axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, cr);
-
-	/* Start updating the Tx channel control register */
-	cr = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-	/* Update the interrupt coalesce count */
-	cr = (((cr & ~XAXIDMA_COALESCE_MASK)) |
-	      ((lp->coalesce_count_tx) << XAXIDMA_COALESCE_SHIFT));
-	/* Update the delay timer count */
-	cr = (((cr & ~XAXIDMA_DELAY_MASK)) |
-	      (XAXIDMA_DFT_TX_WAITBOUND << XAXIDMA_DELAY_SHIFT));
-	/* Enable coalesce, delay timer and error interrupts */
-	cr |= XAXIDMA_IRQ_ALL_MASK;
-	/* Write to the Tx channel control register */
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, cr);
-
-	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
-	 * halted state. This will make the Rx side ready for reception.
-	 */
-	axienet_dma_out_addr(lp, XAXIDMA_RX_CDESC_OFFSET, lp->rx_bd_p);
-	cr = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-	axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET,
-			  cr | XAXIDMA_CR_RUNSTOP_MASK);
-	axienet_dma_out_addr(lp, XAXIDMA_RX_TDESC_OFFSET, lp->rx_bd_p +
-			     (sizeof(*lp->rx_bd_v) * (lp->rx_bd_num - 1)));
-
-	/* Write to the RS (Run-stop) bit in the Tx channel control register.
-	 * Tx channel is now ready to run. But only after we write to the
-	 * tail pointer register that the Tx channel will start transmitting.
-	 */
-	axienet_dma_out_addr(lp, XAXIDMA_TX_CDESC_OFFSET, lp->tx_bd_p);
-	cr = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET,
-			  cr | XAXIDMA_CR_RUNSTOP_MASK);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	for_each_tx_dma_queue(lp, i) {
+		ret = axienet_mcdma_tx_q_init(ndev, lp->dq[i]);
+		if (ret != 0)
+			break;
+	}
+#endif
+	for_each_rx_dma_queue(lp, i) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		ret = axienet_mcdma_rx_q_init(ndev, lp->dq[i]);
+#else
+		ret = axienet_dma_q_init(ndev, lp->dq[i]);
+#endif
+		if (ret != 0) {
+			netdev_err(ndev, "%s: Failed to init DMA buf %d\n", __func__, ret);
+			break;
+		}
+	}
 
-	return 0;
-out:
-	axienet_dma_bd_release(ndev);
-	return -ENOMEM;
+	return ret;
 }
 
 /**
@@ -355,16 +280,20 @@ static int axienet_dma_bd_init(struct net_device *ndev)
  * This function is called to initialize the MAC address of the Axi Ethernet
  * core. It writes to the UAW0 and UAW1 registers of the core.
  */
-static void axienet_set_mac_address(struct net_device *ndev,
-				    const void *address)
+void axienet_set_mac_address(struct net_device *ndev,
+			     const void *address)
 {
 	struct axienet_local *lp = netdev_priv(ndev);
 
 	if (address)
-		memcpy(ndev->dev_addr, address, ETH_ALEN);
+		ether_addr_copy(ndev->dev_addr, address);
 	if (!is_valid_ether_addr(ndev->dev_addr))
 		eth_hw_addr_random(ndev);
 
+	if (lp->axienet_config->mactype != XAXIENET_1G &&
+	    lp->axienet_config->mactype != XAXIENET_2_5G)
+		return;
+
 	/* Set up unicast MAC address filter set its mac address */
 	axienet_iow(lp, XAE_UAW0_OFFSET,
 		    (ndev->dev_addr[0]) |
@@ -392,6 +321,7 @@ static void axienet_set_mac_address(struct net_device *ndev,
 static int netdev_set_mac_address(struct net_device *ndev, void *p)
 {
 	struct sockaddr *addr = p;
+
 	axienet_set_mac_address(ndev, addr->sa_data);
 	return 0;
 }
@@ -407,12 +337,15 @@ static int netdev_set_mac_address(struct net_device *ndev, void *p)
  * means whenever the multicast table entries need to be updated this
  * function gets called.
  */
-static void axienet_set_multicast_list(struct net_device *ndev)
+void axienet_set_multicast_list(struct net_device *ndev)
 {
 	int i;
 	u32 reg, af0reg, af1reg;
 	struct axienet_local *lp = netdev_priv(ndev);
 
+	if ((lp->axienet_config->mactype != XAXIENET_1G) || lp->eth_hasnobuf)
+		return;
+
 	if (ndev->flags & (IFF_ALLMULTI | IFF_PROMISC) ||
 	    netdev_mc_count(ndev) > XAE_MULTICAST_CAM_TABLE_NUM) {
 		/* We must make the kernel realize we had to move into
@@ -420,9 +353,9 @@ static void axienet_set_multicast_list(struct net_device *ndev)
 		 * the flag is already set. If not we set it.
 		 */
 		ndev->flags |= IFF_PROMISC;
-		reg = axienet_ior(lp, XAE_FMI_OFFSET);
-		reg |= XAE_FMI_PM_MASK;
-		axienet_iow(lp, XAE_FMI_OFFSET, reg);
+		reg = axienet_ior(lp, XAE_FMC_OFFSET);
+		reg |= XAE_FMC_PM_MASK;
+		axienet_iow(lp, XAE_FMC_OFFSET, reg);
 		dev_info(&ndev->dev, "Promiscuous mode enabled.\n");
 	} else if (!netdev_mc_empty(ndev)) {
 		struct netdev_hw_addr *ha;
@@ -440,25 +373,25 @@ static void axienet_set_multicast_list(struct net_device *ndev)
 			af1reg = (ha->addr[4]);
 			af1reg |= (ha->addr[5] << 8);
 
-			reg = axienet_ior(lp, XAE_FMI_OFFSET) & 0xFFFFFF00;
+			reg = axienet_ior(lp, XAE_FMC_OFFSET) & 0xFFFFFF00;
 			reg |= i;
 
-			axienet_iow(lp, XAE_FMI_OFFSET, reg);
+			axienet_iow(lp, XAE_FMC_OFFSET, reg);
 			axienet_iow(lp, XAE_AF0_OFFSET, af0reg);
 			axienet_iow(lp, XAE_AF1_OFFSET, af1reg);
 			i++;
 		}
 	} else {
-		reg = axienet_ior(lp, XAE_FMI_OFFSET);
-		reg &= ~XAE_FMI_PM_MASK;
+		reg = axienet_ior(lp, XAE_FMC_OFFSET);
+		reg &= ~XAE_FMC_PM_MASK;
 
-		axienet_iow(lp, XAE_FMI_OFFSET, reg);
+		axienet_iow(lp, XAE_FMC_OFFSET, reg);
 
 		for (i = 0; i < XAE_MULTICAST_CAM_TABLE_NUM; i++) {
-			reg = axienet_ior(lp, XAE_FMI_OFFSET) & 0xFFFFFF00;
+			reg = axienet_ior(lp, XAE_FMC_OFFSET) & 0xFFFFFF00;
 			reg |= i;
 
-			axienet_iow(lp, XAE_FMI_OFFSET, reg);
+			axienet_iow(lp, XAE_FMC_OFFSET, reg);
 			axienet_iow(lp, XAE_AF0_OFFSET, 0);
 			axienet_iow(lp, XAE_AF1_OFFSET, 0);
 		}
@@ -478,7 +411,7 @@ static void axienet_set_multicast_list(struct net_device *ndev)
  * these options in the Axi Ethernet hardware. This is done through
  * axienet_option structure .
  */
-static void axienet_setoptions(struct net_device *ndev, u32 options)
+void axienet_setoptions(struct net_device *ndev, u32 options)
 {
 	int reg;
 	struct axienet_local *lp = netdev_priv(ndev);
@@ -495,11 +428,115 @@ static void axienet_setoptions(struct net_device *ndev, u32 options)
 	lp->options |= options;
 }
 
-static int __axienet_device_reset(struct axienet_local *lp)
+static void xxvenet_setoptions(struct net_device *ndev, u32 options)
 {
-	u32 value;
-	int ret;
+	int reg;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct xxvenet_option *tp;
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+		tp = &mrmacenet_options[0];
+	else
+		tp = &xxvenet_options[0];
+
+	while (tp->opt) {
+		reg = ((axienet_ior(lp, tp->reg)) & ~(tp->m_or));
+		if (options & tp->opt)
+			reg |= tp->m_or;
+		axienet_iow(lp, tp->reg, reg);
+		tp++;
+	}
+
+	lp->options |= options;
+}
+
+static inline void axienet_mrmac_reset(struct axienet_local *lp)
+{
+	u32 val, reg;
+
+	val = axienet_ior(lp, MRMAC_RESET_OFFSET);
+	val |= (MRMAC_RX_SERDES_RST_MASK | MRMAC_TX_SERDES_RST_MASK |
+		MRMAC_RX_RST_MASK | MRMAC_TX_RST_MASK);
+	axienet_iow(lp, MRMAC_RESET_OFFSET, val);
+	mdelay(MRMAC_RESET_DELAY);
+
+	reg = axienet_ior(lp, MRMAC_MODE_OFFSET);
+	if (lp->mrmac_rate == SPEED_25000) {
+		reg &= ~MRMAC_CTL_RATE_CFG_MASK;
+		reg |= MRMAC_CTL_DATA_RATE_25G;
+		reg |= (MRMAC_CTL_AXIS_CFG_25G_IND << MRMAC_CTL_AXIS_CFG_SHIFT);
+		reg |= (MRMAC_CTL_SERDES_WIDTH_25G <<
+			MRMAC_CTL_SERDES_WIDTH_SHIFT);
+	} else {
+		reg &= ~MRMAC_CTL_RATE_CFG_MASK;
+		reg |= MRMAC_CTL_DATA_RATE_10G;
+		reg |= (MRMAC_CTL_AXIS_CFG_10G_IND << MRMAC_CTL_AXIS_CFG_SHIFT);
+		reg |= (MRMAC_CTL_SERDES_WIDTH_10G <<
+			MRMAC_CTL_SERDES_WIDTH_SHIFT);
+	}
+
+	/* For tick reg */
+	reg |= MRMAC_CTL_PM_TICK_MASK;
+	axienet_iow(lp, MRMAC_MODE_OFFSET, reg);
+
+	val = axienet_ior(lp, MRMAC_RESET_OFFSET);
+	val &= ~(MRMAC_RX_SERDES_RST_MASK | MRMAC_TX_SERDES_RST_MASK |
+		MRMAC_RX_RST_MASK | MRMAC_TX_RST_MASK);
+	axienet_iow(lp, MRMAC_RESET_OFFSET, val);
+}
+
+static inline int axienet_mrmac_gt_reset(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 err, val;
+	int i;
+
+	if (mrmac_pll_rst == 0) {
+		/* PLL reset for all lanes */
+
+		for (i = 0; i < MRMAC_MAX_GT_LANES; i++) {
+			iowrite32(MRMAC_GT_RST_ALL_MASK, (lp->gt_ctrl +
+				  (MRMAC_GT_LANE_OFFSET * i) +
+				  MRMAC_GT_CTRL_OFFSET));
+			mdelay(MRMAC_RESET_DELAY);
+			iowrite32(0, (lp->gt_ctrl + (MRMAC_GT_LANE_OFFSET * i) +
+				      MRMAC_GT_CTRL_OFFSET));
+		}
+
+		/* Wait for PLL lock with timeout */
+		err = readl_poll_timeout(lp->gt_pll + MRMAC_GT_PLL_STS_OFFSET,
+					 val, (val & MRMAC_GT_PLL_DONE_MASK),
+					 10, DELAY_OF_ONE_MILLISEC);
+		if (err) {
+			netdev_err(ndev, "MRMAC PLL lock not complete! Cross-check the MAC ref clock configuration\n");
+			return -ENODEV;
+		}
+		mrmac_pll_rst = 1;
+	}
+
+	if (lp->mrmac_rate == SPEED_25000)
+		iowrite32(MRMAC_GT_25G_MASK, (lp->gt_ctrl +
+			  MRMAC_GT_LANE_OFFSET * lp->gt_lane +
+			  MRMAC_GT_RATE_OFFSET));
+	else
+		iowrite32(MRMAC_GT_10G_MASK, (lp->gt_ctrl +
+			  MRMAC_GT_LANE_OFFSET * lp->gt_lane +
+			  MRMAC_GT_RATE_OFFSET));
+
+	iowrite32(MRMAC_GT_RST_RX_MASK | MRMAC_GT_RST_TX_MASK,
+		  (lp->gt_ctrl + MRMAC_GT_LANE_OFFSET * lp->gt_lane +
+		  MRMAC_GT_CTRL_OFFSET));
+	mdelay(MRMAC_RESET_DELAY);
+	iowrite32(0, (lp->gt_ctrl + MRMAC_GT_LANE_OFFSET * lp->gt_lane +
+		  MRMAC_GT_CTRL_OFFSET));
+	mdelay(MRMAC_RESET_DELAY);
+
+	return 0;
+}
 
+void __axienet_device_reset(struct axienet_dma_q *q)
+{
+	u32 timeout;
 	/* Reset Axi DMA. This would reset Axi Ethernet core as well. The reset
 	 * process of Axi DMA takes a while to complete as all pending
 	 * commands/transfers will be flushed or completed during this
@@ -507,91 +544,164 @@ static int __axienet_device_reset(struct axienet_local *lp)
 	 * Note that even though both TX and RX have their own reset register,
 	 * they both reset the entire DMA core, so only one needs to be used.
 	 */
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, XAXIDMA_CR_RESET_MASK);
-	ret = read_poll_timeout(axienet_dma_in32, value,
-				!(value & XAXIDMA_CR_RESET_MASK),
-				DELAY_OF_ONE_MILLISEC, 50000, false, lp,
-				XAXIDMA_TX_CR_OFFSET);
-	if (ret) {
-		dev_err(lp->dev, "%s: DMA reset timeout!\n", __func__);
-		return ret;
-	}
-
-	/* Wait for PhyRstCmplt bit to be set, indicating the PHY reset has finished */
-	ret = read_poll_timeout(axienet_ior, value,
-				value & XAE_INT_PHYRSTCMPLT_MASK,
-				DELAY_OF_ONE_MILLISEC, 50000, false, lp,
-				XAE_IS_OFFSET);
-	if (ret) {
-		dev_err(lp->dev, "%s: timeout waiting for PhyRstCmplt\n", __func__);
-		return ret;
+	axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, XAXIDMA_CR_RESET_MASK);
+	timeout = DELAY_OF_ONE_MILLISEC;
+	while (axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET) &
+				XAXIDMA_CR_RESET_MASK) {
+		udelay(1);
+		if (--timeout == 0) {
+			netdev_err(q->lp->ndev, "%s: DMA reset timeout!\n",
+				   __func__);
+			break;
+		}
 	}
-
-	return 0;
 }
 
 /**
  * axienet_device_reset - Reset and initialize the Axi Ethernet hardware.
  * @ndev:	Pointer to the net_device structure
  *
+ * Return: 0 on success, Negative value on errors
+ *
  * This function is called to reset and initialize the Axi Ethernet core. This
  * is typically called during initialization. It does a reset of the Axi DMA
  * Rx/Tx channels and initializes the Axi DMA BDs. Since Axi DMA reset lines
  * areconnected to Axi Ethernet reset lines, this in turn resets the Axi
  * Ethernet core. No separate hardware reset is done for the Axi Ethernet
  * core.
- * Returns 0 on success or a negative error number otherwise.
  */
 static int axienet_device_reset(struct net_device *ndev)
 {
 	u32 axienet_status;
 	struct axienet_local *lp = netdev_priv(ndev);
+	u32 err, val;
+	struct axienet_dma_q *q;
+	u32 i;
 	int ret;
 
-	ret = __axienet_device_reset(lp);
-	if (ret)
-		return ret;
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G) {
+		/* Reset the XXV MAC */
+		val = axienet_ior(lp, XXV_GT_RESET_OFFSET);
+		val |= XXV_GT_RESET_MASK;
+		axienet_iow(lp, XXV_GT_RESET_OFFSET, val);
+		/* Wait for 1ms for GT reset to complete as per spec */
+		mdelay(1);
+		val = axienet_ior(lp, XXV_GT_RESET_OFFSET);
+		val &= ~XXV_GT_RESET_MASK;
+		axienet_iow(lp, XXV_GT_RESET_OFFSET, val);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		/* Reset MRMAC */
+		axienet_mrmac_reset(lp);
+		ret = axienet_mrmac_gt_reset(ndev);
+		if (ret < 0)
+			return ret;
+	}
+
+	if (!lp->is_tsn) {
+		for_each_rx_dma_queue(lp, i) {
+			q = lp->dq[i];
+			__axienet_device_reset(q);
+#ifndef CONFIG_AXIENET_HAS_MCDMA
+			__axienet_device_reset(q);
+#endif
+		}
+	}
 
 	lp->max_frm_size = XAE_MAX_VLAN_FRAME_SIZE;
-	lp->options |= XAE_OPTION_VLAN;
-	lp->options &= (~XAE_OPTION_JUMBO);
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC) {
+		lp->options |= XAE_OPTION_VLAN;
+		lp->options &= (~XAE_OPTION_JUMBO);
+	}
 
-	if ((ndev->mtu > XAE_MTU) &&
-		(ndev->mtu <= XAE_JUMBO_MTU)) {
+	if ((ndev->mtu > XAE_MTU) && (ndev->mtu <= XAE_JUMBO_MTU)) {
 		lp->max_frm_size = ndev->mtu + VLAN_ETH_HLEN +
 					XAE_TRL_SIZE;
-
-		if (lp->max_frm_size <= lp->rxmem)
+		if (lp->max_frm_size <= lp->rxmem &&
+		    (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+		     lp->axienet_config->mactype != XAXIENET_MRMAC))
 			lp->options |= XAE_OPTION_JUMBO;
 	}
 
-	ret = axienet_dma_bd_init(ndev);
-	if (ret) {
-		netdev_err(ndev, "%s: descriptor allocation failed\n",
-			   __func__);
-		return ret;
+	if (!lp->is_tsn) {
+		ret = axienet_dma_bd_init(ndev);
+		if (ret < 0) {
+			netdev_err(ndev, "%s: descriptor allocation failed\n",
+				   __func__);
+			return ret;
+		}
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC) {
+		axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+		axienet_status &= ~XAE_RCW1_RX_MASK;
+		axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G) {
+		/* Check for block lock bit got set or not
+		 * This ensures that 10G ethernet IP
+		 * is functioning normally or not.
+		 */
+		err = readl_poll_timeout(lp->regs + XXV_STATRX_BLKLCK_OFFSET,
+					 val, (val & XXV_RX_BLKLCK_MASK),
+					 10, DELAY_OF_ONE_MILLISEC);
+		if (err) {
+			netdev_err(ndev, "XXV MAC block lock not complete! Cross-check the MAC ref clock configuration\n");
+		}
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+		if (!lp->is_tsn) {
+			axienet_rxts_iow(lp, XAXIFIFO_TXTS_RDFR,
+					 XAXIFIFO_TXTS_RESET_MASK);
+			axienet_rxts_iow(lp, XAXIFIFO_TXTS_SRR,
+					 XAXIFIFO_TXTS_RESET_MASK);
+			axienet_txts_iow(lp, XAXIFIFO_TXTS_RDFR,
+					 XAXIFIFO_TXTS_RESET_MASK);
+			axienet_txts_iow(lp, XAXIFIFO_TXTS_SRR,
+					 XAXIFIFO_TXTS_RESET_MASK);
+		}
+#endif
 	}
 
-	axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
-	axienet_status &= ~XAE_RCW1_RX_MASK;
-	axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+	}
+#endif
 
-	axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
-	if (axienet_status & XAE_INT_RXRJECT_MASK)
-		axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
-	axienet_iow(lp, XAE_IE_OFFSET, lp->eth_irq > 0 ?
-		    XAE_INT_RECV_ERROR_MASK : 0);
+	if ((lp->axienet_config->mactype == XAXIENET_1G) &&
+	    !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+		/* Enable receive erros */
+		axienet_iow(lp, XAE_IE_OFFSET, lp->eth_irq > 0 ?
+			    XAE_INT_RECV_ERROR_MASK : 0);
+	}
 
-	axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		lp->options |= XAE_OPTION_FCS_STRIP;
+		lp->options |= XAE_OPTION_FCS_INSERT;
+	} else {
+		axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+	}
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
 
-	/* Sync default options with HW but leave receiver and
-	 * transmitter disabled.
-	 */
-	axienet_setoptions(ndev, lp->options &
-			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
 	axienet_set_mac_address(ndev, NULL);
 	axienet_set_multicast_list(ndev);
-	axienet_setoptions(ndev, lp->options);
+	lp->axienet_config->setoptions(ndev, lp->options);
 
 	netif_trans_update(ndev);
 
@@ -599,97 +709,225 @@ static int axienet_device_reset(struct net_device *ndev)
 }
 
 /**
- * axienet_free_tx_chain - Clean up a series of linked TX descriptors.
+ * axienet_adjust_link - Adjust the PHY link speed/duplex.
  * @ndev:	Pointer to the net_device structure
- * @first_bd:	Index of first descriptor to clean up
- * @nr_bds:	Number of descriptors to clean up, can be -1 if unknown.
- * @sizep:	Pointer to a u32 filled with the total sum of all bytes
- * 		in all cleaned-up descriptors. Ignored if NULL.
  *
- * Would either be called after a successful transmit operation, or after
- * there was an error when setting up the chain.
- * Returns the number of descriptors handled.
+ * This function is called to change the speed and duplex setting after
+ * auto negotiation is done by the PHY. This is the function that gets
+ * registered with the PHY interface through the "of_phy_connect" call.
  */
-static int axienet_free_tx_chain(struct net_device *ndev, u32 first_bd,
-				 int nr_bds, u32 *sizep)
+void axienet_adjust_link(struct net_device *ndev)
 {
+	u32 emmc_reg;
+	u32 link_state;
+	u32 setspeed = 1;
 	struct axienet_local *lp = netdev_priv(ndev);
-	struct axidma_bd *cur_p;
-	int max_bds = nr_bds;
-	unsigned int status;
-	dma_addr_t phys;
-	int i;
-
-	if (max_bds == -1)
-		max_bds = lp->tx_bd_num;
+	struct phy_device *phy = ndev->phydev;
+
+	link_state = phy->speed | (phy->duplex << 1) | phy->link;
+	if (lp->last_link != link_state) {
+		if ((phy->speed == SPEED_10) || (phy->speed == SPEED_100)) {
+			if (lp->phy_mode == PHY_INTERFACE_MODE_1000BASEX)
+				setspeed = 0;
+		} else {
+			if ((phy->speed == SPEED_1000) &&
+			    (lp->phy_mode == PHY_INTERFACE_MODE_MII))
+				setspeed = 0;
+		}
 
-	for (i = 0; i < max_bds; i++) {
-		cur_p = &lp->tx_bd_v[(first_bd + i) % lp->tx_bd_num];
-		status = cur_p->status;
+		if (setspeed == 1) {
+			emmc_reg = axienet_ior(lp, XAE_EMMC_OFFSET);
+			emmc_reg &= ~XAE_EMMC_LINKSPEED_MASK;
 
-		/* If no number is given, clean up *all* descriptors that have
-		 * been completed by the MAC.
-		 */
-		if (nr_bds == -1 && !(status & XAXIDMA_BD_STS_COMPLETE_MASK))
-			break;
+			switch (phy->speed) {
+			case SPEED_2500:
+				emmc_reg |= XAE_EMMC_LINKSPD_2500;
+				break;
+			case SPEED_1000:
+				emmc_reg |= XAE_EMMC_LINKSPD_1000;
+				break;
+			case SPEED_100:
+				emmc_reg |= XAE_EMMC_LINKSPD_100;
+				break;
+			case SPEED_10:
+				emmc_reg |= XAE_EMMC_LINKSPD_10;
+				break;
+			default:
+				dev_err(&ndev->dev, "Speed other than 10, 100 ");
+				dev_err(&ndev->dev, "or 1Gbps is not supported\n");
+				break;
+			}
 
-		/* Ensure we see complete descriptor update */
-		dma_rmb();
-		phys = desc_get_phys_addr(lp, cur_p);
-		dma_unmap_single(ndev->dev.parent, phys,
-				 (cur_p->cntrl & XAXIDMA_BD_CTRL_LENGTH_MASK),
-				 DMA_TO_DEVICE);
+			axienet_iow(lp, XAE_EMMC_OFFSET, emmc_reg);
+			phy_print_status(phy);
+		} else {
+			netdev_err(ndev,
+				   "Error setting Axi Ethernet mac speed\n");
+		}
 
-		if (cur_p->skb && (status & XAXIDMA_BD_STS_COMPLETE_MASK))
-			dev_consume_skb_irq(cur_p->skb);
+		lp->last_link = link_state;
+	}
+}
 
-		cur_p->app0 = 0;
-		cur_p->app1 = 0;
-		cur_p->app2 = 0;
-		cur_p->app4 = 0;
-		cur_p->skb = NULL;
-		/* ensure our transmit path and device don't prematurely see status cleared */
-		wmb();
-		cur_p->cntrl = 0;
-		cur_p->status = 0;
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+/**
+ * axienet_tx_hwtstamp - Read tx timestamp from hw and update it to the skbuff
+ * @lp:		Pointer to axienet local structure
+ * @cur_p:	Pointer to the axi_dma/axi_mcdma current bd
+ *
+ * Return:	None.
+ */
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+void axienet_tx_hwtstamp(struct axienet_local *lp,
+			 struct aximcdma_bd *cur_p)
+#else
+void axienet_tx_hwtstamp(struct axienet_local *lp,
+			 struct axidma_bd *cur_p)
+#endif
+{
+	u32 sec = 0, nsec = 0, val;
+	u64 time64;
+	int err = 0;
+	u32 count, len = lp->axienet_config->tx_ptplen;
+	struct skb_shared_hwtstamps *shhwtstamps =
+		skb_hwtstamps((struct sk_buff *)cur_p->ptp_tx_skb);
+
+	val = axienet_txts_ior(lp, XAXIFIFO_TXTS_ISR);
+	if (unlikely(!(val & XAXIFIFO_TXTS_INT_RC_MASK)))
+		dev_info(lp->dev, "Did't get FIFO tx interrupt %d\n", val);
+
+	/* If FIFO is configured in cut through Mode we will get Rx complete
+	 * interrupt even one byte is there in the fifo wait for the full packet
+	 */
+	err = readl_poll_timeout_atomic(lp->tx_ts_regs + XAXIFIFO_TXTS_RLR, val,
+					((val & XAXIFIFO_TXTS_RXFD_MASK) >=
+					len), 0, 1000000);
+	if (err) {
+		netdev_err(lp->ndev, "%s: Didn't get the full timestamp packet",
+			   __func__);
+		goto skb_exit;
+	}
 
-		if (sizep)
-			*sizep += status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
+	nsec = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	sec  = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	val = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	val = ((val & XAXIFIFO_TXTS_TAG_MASK) >> XAXIFIFO_TXTS_TAG_SHIFT);
+	dev_dbg(lp->dev, "tx_stamp:[%04x] %04x %u %9u\n",
+		cur_p->ptp_tx_ts_tag, val, sec, nsec);
+
+	if (val != cur_p->ptp_tx_ts_tag) {
+		count = axienet_txts_ior(lp, XAXIFIFO_TXTS_RFO);
+		while (count) {
+			nsec = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+			sec  = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+			val = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+			val = ((val & XAXIFIFO_TXTS_TAG_MASK) >>
+				XAXIFIFO_TXTS_TAG_SHIFT);
+
+			dev_dbg(lp->dev, "tx_stamp:[%04x] %04x %u %9u\n",
+				cur_p->ptp_tx_ts_tag, val, sec, nsec);
+			if (val == cur_p->ptp_tx_ts_tag)
+				break;
+			count = axienet_txts_ior(lp, XAXIFIFO_TXTS_RFO);
+		}
+		if (val != cur_p->ptp_tx_ts_tag) {
+			dev_info(lp->dev, "Mismatching 2-step tag. Got %x",
+				 val);
+			dev_info(lp->dev, "Expected %x\n",
+				 cur_p->ptp_tx_ts_tag);
+		}
 	}
 
-	return i;
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		val = axienet_txts_ior(lp, XAXIFIFO_TXTS_RXFD);
+
+skb_exit:
+	time64 = sec * NS_PER_SEC + nsec;
+	memset(shhwtstamps, 0, sizeof(struct skb_shared_hwtstamps));
+	shhwtstamps->hwtstamp = ns_to_ktime(time64);
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		skb_pull((struct sk_buff *)cur_p->ptp_tx_skb,
+			 AXIENET_TS_HEADER_LEN);
+
+	skb_tstamp_tx((struct sk_buff *)cur_p->ptp_tx_skb, shhwtstamps);
+	dev_kfree_skb_any((struct sk_buff *)cur_p->ptp_tx_skb);
+	cur_p->ptp_tx_skb = 0;
+}
+
+static inline bool is_ptp_os_pdelay_req(struct sk_buff *skb,
+					struct axienet_local *lp)
+{
+	u8 *msg_type;
+
+	msg_type = (u8 *)skb->data + PTP_TYPE_OFFSET;
+	return (((*msg_type & 0xF) == PTP_TYPE_PDELAY_REQ) &&
+		(lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_P2P));
 }
 
 /**
- * axienet_check_tx_bd_space - Checks if a BD/group of BDs are currently busy
- * @lp:		Pointer to the axienet_local structure
- * @num_frag:	The number of BDs to check for
- *
- * Return: 0, on success
- *	    NETDEV_TX_BUSY, if any of the descriptors are not free
+ * axienet_rx_hwtstamp - Read rx timestamp from hw and update it to the skbuff
+ * @lp:		Pointer to axienet local structure
+ * @skb:	Pointer to the sk_buff structure
  *
- * This function is invoked before BDs are allocated and transmission starts.
- * This function returns 0 if a BD or group of BDs can be allocated for
- * transmission. If the BD or any of the BDs are not free the function
- * returns a busy status. This is invoked from axienet_start_xmit.
+ * Return:	None.
  */
-static inline int axienet_check_tx_bd_space(struct axienet_local *lp,
-					    int num_frag)
+static void axienet_rx_hwtstamp(struct axienet_local *lp,
+				struct sk_buff *skb)
 {
-	struct axidma_bd *cur_p;
+	u32 sec = 0, nsec = 0, val;
+	u64 time64;
+	int err = 0;
+	struct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);
+
+	val = axienet_rxts_ior(lp, XAXIFIFO_TXTS_ISR);
+	if (unlikely(!(val & XAXIFIFO_TXTS_INT_RC_MASK))) {
+		dev_info(lp->dev, "Did't get FIFO rx interrupt %d\n", val);
+		return;
+	}
 
-	/* Ensure we see all descriptor updates from device or TX IRQ path */
-	rmb();
-	cur_p = &lp->tx_bd_v[(lp->tx_bd_tail + num_frag) % lp->tx_bd_num];
-	if (cur_p->cntrl)
-		return NETDEV_TX_BUSY;
-	return 0;
+	val = axienet_rxts_ior(lp, XAXIFIFO_TXTS_RFO);
+	if (!val)
+		return;
+
+	/* If FIFO is configured in cut through Mode we will get Rx complete
+	 * interrupt even one byte is there in the fifo wait for the full packet
+	 */
+	err = readl_poll_timeout_atomic(lp->rx_ts_regs + XAXIFIFO_TXTS_RLR, val,
+					((val & XAXIFIFO_TXTS_RXFD_MASK) >= 12),
+					0, 1000000);
+	if (err) {
+		netdev_err(lp->ndev, "%s: Didn't get the full timestamp packet",
+			   __func__);
+		return;
+	}
+
+	nsec = axienet_rxts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	sec  = axienet_rxts_ior(lp, XAXIFIFO_TXTS_RXFD);
+	val = axienet_rxts_ior(lp, XAXIFIFO_TXTS_RXFD);
+
+	if (is_ptp_os_pdelay_req(skb, lp)) {
+		/* Need to save PDelay resp RX time for HW 1 step
+		 * timestamping on PDelay Response.
+		 */
+		lp->ptp_os_cf = mul_u32_u32(sec, NSEC_PER_SEC);
+		lp->ptp_os_cf += nsec;
+		lp->ptp_os_cf = (lp->ptp_os_cf << 16);
+	}
+
+	if (lp->tstamp_config.rx_filter == HWTSTAMP_FILTER_ALL) {
+		time64 = sec * NS_PER_SEC + nsec;
+		shhwtstamps->hwtstamp = ns_to_ktime(time64);
+	}
 }
+#endif
 
 /**
  * axienet_start_xmit_done - Invoked once a transmit is completed by the
  * Axi DMA Tx channel.
  * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
  *
  * This function is invoked from the Axi DMA Tx isr to notify the completion
  * of transmit operation. It clears fields in the corresponding Tx BDs and
@@ -697,69 +935,420 @@ static inline int axienet_check_tx_bd_space(struct axienet_local *lp,
  * buffer. It finally invokes "netif_wake_queue" to restart transmission if
  * required.
  */
-static void axienet_start_xmit_done(struct net_device *ndev)
+void axienet_start_xmit_done(struct net_device *ndev,
+			     struct axienet_dma_q *q)
 {
-	struct axienet_local *lp = netdev_priv(ndev);
-	u32 packets = 0;
 	u32 size = 0;
+	u32 packets = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
 
-	packets = axienet_free_tx_chain(ndev, lp->tx_bd_ci, -1, &size);
-
-	lp->tx_bd_ci += packets;
-	if (lp->tx_bd_ci >= lp->tx_bd_num)
-		lp->tx_bd_ci -= lp->tx_bd_num;
-
-	ndev->stats.tx_packets += packets;
-	ndev->stats.tx_bytes += size;
-
-	/* Matches barrier in axienet_start_xmit */
-	smp_mb();
-
-	if (!axienet_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1))
-		netif_wake_queue(ndev);
-}
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
+	struct axidma_bd *cur_p;
+#endif
+	unsigned int status = 0;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p = &q->txq_bd_v[q->tx_bd_ci];
+	status = cur_p->sband_stats;
+#else
+	cur_p = &q->tx_bd_v[q->tx_bd_ci];
+	status = cur_p->status;
+#endif
+	while (status & XAXIDMA_BD_STS_COMPLETE_MASK) {
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+		if (cur_p->ptp_tx_skb)
+			axienet_tx_hwtstamp(lp, cur_p);
+#endif
+		if (cur_p->tx_desc_mapping == DESC_DMA_MAP_PAGE)
+			dma_unmap_page(ndev->dev.parent, cur_p->phys,
+				       cur_p->cntrl &
+				       XAXIDMA_BD_CTRL_LENGTH_MASK,
+				       DMA_TO_DEVICE);
+		else
+			dma_unmap_single(ndev->dev.parent, cur_p->phys,
+					 cur_p->cntrl &
+					 XAXIDMA_BD_CTRL_LENGTH_MASK,
+					 DMA_TO_DEVICE);
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq((struct sk_buff *)cur_p->tx_skb);
+		/*cur_p->phys = 0;*/
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app4 = 0;
+		cur_p->status = 0;
+		cur_p->tx_skb = 0;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p->sband_stats = 0;
+#endif
+
+		size += status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
+		packets++;
+
+		if (++q->tx_bd_ci >= lp->tx_bd_num)
+			q->tx_bd_ci = 0;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p = &q->txq_bd_v[q->tx_bd_ci];
+		status = cur_p->sband_stats;
+#else
+		cur_p = &q->tx_bd_v[q->tx_bd_ci];
+		status = cur_p->status;
+#endif
+	}
+
+	ndev->stats.tx_packets += packets;
+	ndev->stats.tx_bytes += size;
+	q->tx_packets += packets;
+	q->tx_bytes += size;
+
+	/* Matches barrier in axienet_start_xmit */
+	smp_mb();
+
+	/* Fixme: With the existing multiqueue implementation
+	 * in the driver it is difficult to get the exact queue info.
+	 * We should wake only the particular queue
+	 * instead of waking all ndev queues.
+	 */
+	netif_tx_wake_all_queues(ndev);
+}
 
 /**
- * axienet_start_xmit - Starts the transmission.
- * @skb:	sk_buff pointer that contains data to be Txed.
- * @ndev:	Pointer to net_device structure.
+ * axienet_check_tx_bd_space - Checks if a BD/group of BDs are currently busy
+ * @q:		Pointer to DMA queue structure
+ * @num_frag:	The number of BDs to check for
  *
- * Return: NETDEV_TX_OK, on success
+ * Return: 0, on success
  *	    NETDEV_TX_BUSY, if any of the descriptors are not free
  *
- * This function is invoked from upper layers to initiate transmission. The
- * function uses the next available free BDs and populates their fields to
- * start the transmission. Additionally if checksum offloading is supported,
- * it populates AXI Stream Control fields with appropriate values.
+ * This function is invoked before BDs are allocated and transmission starts.
+ * This function returns 0 if a BD or group of BDs can be allocated for
+ * transmission. If the BD or any of the BDs are not free the function
+ * returns a busy status. This is invoked from axienet_start_xmit.
+ */
+static inline int axienet_check_tx_bd_space(struct axienet_dma_q *q,
+					    int num_frag)
+{
+	struct axienet_local *lp = q->lp;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+
+	if (CIRC_SPACE(q->tx_bd_tail, q->tx_bd_ci, lp->tx_bd_num) < (num_frag + 1))
+		return NETDEV_TX_BUSY;
+
+	cur_p = &q->txq_bd_v[(q->tx_bd_tail + num_frag) % lp->tx_bd_num];
+	if (cur_p->sband_stats & XMCDMA_BD_STS_ALL_MASK)
+		return NETDEV_TX_BUSY;
+#else
+	struct axidma_bd *cur_p;
+
+	if (CIRC_SPACE(q->tx_bd_tail, q->tx_bd_ci, lp->tx_bd_num) < (num_frag + 1))
+		return NETDEV_TX_BUSY;
+
+	cur_p = &q->tx_bd_v[(q->tx_bd_tail + num_frag) % lp->tx_bd_num];
+	if (cur_p->status & XAXIDMA_BD_STS_ALL_MASK)
+		return NETDEV_TX_BUSY;
+#endif
+	return 0;
+}
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+/**
+ * axienet_create_tsheader - Create timestamp header for tx
+ * @q:		Pointer to DMA queue structure
+ * @buf:	Pointer to the buf to copy timestamp header
+ * @msg_type:	PTP message type
+ *
+ * Return: 0, on success
+ *	    NETDEV_TX_BUSY, if timestamp FIFO has no vacancy
  */
-static netdev_tx_t
-axienet_start_xmit(struct sk_buff *skb, struct net_device *ndev)
+static int axienet_create_tsheader(u8 *buf, u8 msg_type,
+				   struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = q->lp;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
+	struct axidma_bd *cur_p;
+#endif
+	u64 val;
+	u32 tmp[MRMAC_TS_HEADER_WORDS];
+	unsigned long flags;
+	int i;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p = &q->txq_bd_v[q->tx_bd_tail];
+#else
+	cur_p = &q->tx_bd_v[q->tx_bd_tail];
+#endif
+
+	if ((msg_type & 0xF) == TX_TS_OP_NOOP) {
+		buf[0] = TX_TS_OP_NOOP;
+	} else if ((msg_type & 0xF) == TX_TS_OP_ONESTEP) {
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			/* For Sync Packet */
+			if ((msg_type & 0xF0) == MSG_TYPE_SYNC_FLAG) {
+				buf[0] = TX_TS_OP_ONESTEP | TX_TS_CSUM_UPDATE_MRMAC;
+				buf[2] = cur_p->ptp_tx_ts_tag & 0xFF;
+				buf[3] = (cur_p->ptp_tx_ts_tag >> 8) & 0xFF;
+				buf[4] = TX_PTP_CF_OFFSET;
+				buf[6] = TX_PTP_CSUM_OFFSET;
+			}
+			/* For PDelay Response packet */
+			if ((msg_type & 0xF0) == MSG_TYPE_PDELAY_RESP_FLAG) {
+				buf[0] = TX_TS_OP_ONESTEP | TX_TS_CSUM_UPDATE_MRMAC |
+					TX_TS_PDELAY_UPDATE_MRMAC;
+				buf[2] = cur_p->ptp_tx_ts_tag & 0xFF;
+				buf[3] = (cur_p->ptp_tx_ts_tag >> 8) & 0xFF;
+				buf[4] = TX_PTP_CF_OFFSET;
+				buf[6] = TX_PTP_CSUM_OFFSET;
+				/* Prev saved TS */
+				memcpy(&buf[8], &lp->ptp_os_cf, 8);
+			}
+		} else {
+			/* Legacy */
+			buf[0] = TX_TS_OP_ONESTEP;
+			buf[1] = TX_TS_CSUM_UPDATE;
+			buf[4] = TX_PTP_TS_OFFSET;
+			buf[6] = TX_PTP_CSUM_OFFSET;
+		}
+	} else {
+		buf[0] = TX_TS_OP_TWOSTEP;
+		buf[2] = cur_p->ptp_tx_ts_tag & 0xFF;
+		buf[3] = (cur_p->ptp_tx_ts_tag >> 8) & 0xFF;
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_1G ||
+	    lp->axienet_config->mactype == XAXIENET_2_5G) {
+		memcpy(&val, buf, AXIENET_TS_HEADER_LEN);
+		swab64s(&val);
+		memcpy(buf, &val, AXIENET_TS_HEADER_LEN);
+	} else if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+		   lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		memcpy(&tmp[0], buf, lp->axienet_config->ts_header_len);
+		/* Check for Transmit Data FIFO Vacancy */
+		spin_lock_irqsave(&lp->ptp_tx_lock, flags);
+		if (!axienet_txts_ior(lp, XAXIFIFO_TXTS_TDFV)) {
+			spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
+
+		for (i = 0; i < lp->axienet_config->ts_header_len / 4; i++)
+			axienet_txts_iow(lp, XAXIFIFO_TXTS_TXFD, tmp[i]);
+
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_TLR, lp->axienet_config->ts_header_len);
+		spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+	}
+
+	return 0;
+}
+#endif
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+static inline u8 ptp_os(struct sk_buff *skb, struct axienet_local *lp)
+{
+	u8 *msg_type;
+	int packet_flags = 0;
+
+	/* Identify and return packets requiring PTP one step TS */
+	msg_type = (u8 *)skb->data + PTP_TYPE_OFFSET;
+	if ((*msg_type & 0xF) == PTP_TYPE_SYNC)
+		packet_flags = MSG_TYPE_SYNC_FLAG;
+	else if (((*msg_type & 0xF) == PTP_TYPE_PDELAY_RESP) &&
+		 (lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_P2P))
+		packet_flags = MSG_TYPE_PDELAY_RESP_FLAG;
+
+	return packet_flags;
+}
+
+static int axienet_skb_tstsmp(struct sk_buff **__skb, struct axienet_dma_q *q,
+			      struct net_device *ndev)
+{
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
+	struct axidma_bd *cur_p;
+#endif
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct sk_buff *old_skb = *__skb;
+	struct sk_buff *skb = *__skb;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p = &q->txq_bd_v[q->tx_bd_tail];
+#else
+	cur_p = &q->tx_bd_v[q->tx_bd_tail];
+#endif
+
+	if ((((lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_SYNC) ||
+	      (lp->tstamp_config.tx_type == HWTSTAMP_TX_ON)) ||
+	       lp->eth_hasptp) && (lp->axienet_config->mactype !=
+	       XAXIENET_10G_25G) &&
+	       (lp->axienet_config->mactype != XAXIENET_MRMAC)) {
+		u8 *tmp;
+		struct sk_buff *new_skb;
+
+		if (skb_headroom(old_skb) < AXIENET_TS_HEADER_LEN) {
+			new_skb =
+			skb_realloc_headroom(old_skb,
+					     AXIENET_TS_HEADER_LEN);
+			if (!new_skb) {
+				dev_err(&ndev->dev, "failed to allocate new socket buffer\n");
+				dev_kfree_skb_any(old_skb);
+				return NETDEV_TX_BUSY;
+			}
+
+			/*  Transfer the ownership to the
+			 *  new socket buffer if required
+			 */
+			if (old_skb->sk)
+				skb_set_owner_w(new_skb, old_skb->sk);
+			dev_kfree_skb_any(old_skb);
+			*__skb = new_skb;
+			skb = new_skb;
+		}
+
+		tmp = skb_push(skb, AXIENET_TS_HEADER_LEN);
+		memset(tmp, 0, AXIENET_TS_HEADER_LEN);
+		cur_p->ptp_tx_ts_tag++;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) {
+			if (lp->tstamp_config.tx_type ==
+				HWTSTAMP_TX_ONESTEP_SYNC) {
+				axienet_create_tsheader(tmp,
+							TX_TS_OP_ONESTEP
+							, q);
+			} else {
+				axienet_create_tsheader(tmp,
+							TX_TS_OP_TWOSTEP
+							, q);
+				skb_shinfo(skb)->tx_flags
+						|= SKBTX_IN_PROGRESS;
+				cur_p->ptp_tx_skb =
+					(unsigned long)skb_get(skb);
+			}
+		}
+	} else if ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&
+		   (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+		   lp->axienet_config->mactype == XAXIENET_MRMAC)) {
+			cur_p->ptp_tx_ts_tag = prandom_u32_max(XAXIFIFO_TXTS_TAG_MAX) + 1;
+			dev_dbg(lp->dev, "tx_tag:[%04x]\n",
+				cur_p->ptp_tx_ts_tag);
+			if (lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_SYNC ||
+			    lp->tstamp_config.tx_type == HWTSTAMP_TX_ONESTEP_P2P) {
+				u8 packet_flags = ptp_os(skb, lp);
+
+				/* Pass one step flag with packet type (sync/pdelay resp)
+				 * to command FIFO helper only when one step TS is required.
+				 * Pass the default two step flag for other PTP events.
+				 */
+				if (!packet_flags)
+					packet_flags = TX_TS_OP_TWOSTEP;
+				else
+					packet_flags |= TX_TS_OP_ONESTEP;
+
+				if (axienet_create_tsheader(lp->tx_ptpheader,
+							    packet_flags,
+							    q))
+					return NETDEV_TX_BUSY;
+
+				/* skb TS passing is required for non one step TS packets */
+				if (packet_flags == TX_TS_OP_TWOSTEP) {
+					skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+					cur_p->ptp_tx_skb = (phys_addr_t)skb_get(skb);
+				}
+			} else {
+				if (axienet_create_tsheader(lp->tx_ptpheader,
+							    TX_TS_OP_TWOSTEP,
+							    q))
+					return NETDEV_TX_BUSY;
+				skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+				cur_p->ptp_tx_skb = (phys_addr_t)skb_get(skb);
+			}
+	} else if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+		   lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			dev_dbg(lp->dev, "tx_tag:NOOP\n");
+			if (axienet_create_tsheader(lp->tx_ptpheader,
+						    TX_TS_OP_NOOP, q))
+				return NETDEV_TX_BUSY;
+	}
+
+	return NETDEV_TX_OK;
+}
+#endif
+
+int axienet_queue_xmit(struct sk_buff *skb,
+		       struct net_device *ndev, u16 map)
 {
 	u32 ii;
 	u32 num_frag;
 	u32 csum_start_off;
 	u32 csum_index_off;
-	skb_frag_t *frag;
-	dma_addr_t tail_p, phys;
+	dma_addr_t tail_p;
 	struct axienet_local *lp = netdev_priv(ndev);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
 	struct axidma_bd *cur_p;
-	u32 orig_tail_ptr = lp->tx_bd_tail;
+#endif
+	unsigned long flags;
+	struct axienet_dma_q *q;
+
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		/* Need to manually pad the small frames in case of XXV MAC
+		 * because the pad field is not added by the IP. We must present
+		 * a packet that meets the minimum length to the IP core.
+		 * When the IP core is configured to calculate and add the FCS
+		 * to the packet the minimum packet length is 60 bytes.
+		 */
+		if (eth_skb_pad(skb)) {
+			ndev->stats.tx_dropped++;
+			ndev->stats.tx_errors++;
+			return NETDEV_TX_OK;
+		}
+	}
 
 	num_frag = skb_shinfo(skb)->nr_frags;
-	cur_p = &lp->tx_bd_v[lp->tx_bd_tail];
 
-	if (axienet_check_tx_bd_space(lp, num_frag + 1)) {
-		/* Should not happen as last start_xmit call should have
-		 * checked for sufficient space and queue should only be
-		 * woken when sufficient space is available.
-		 */
+	q = lp->dq[map];
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p = &q->txq_bd_v[q->tx_bd_tail];
+#else
+	cur_p = &q->tx_bd_v[q->tx_bd_tail];
+#endif
+
+	spin_lock_irqsave(&q->tx_lock, flags);
+	if (axienet_check_tx_bd_space(q, num_frag)) {
+		if (netif_queue_stopped(ndev)) {
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
+
 		netif_stop_queue(ndev);
-		if (net_ratelimit())
-			netdev_warn(ndev, "TX ring unexpectedly full\n");
+
+		/* Matches barrier in axienet_start_xmit_done */
+		smp_mb();
+
+		/* Space might have just been freed - check again */
+		if (axienet_check_tx_bd_space(q, num_frag)) {
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
+
+		netif_wake_queue(ndev);
+	}
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	if (axienet_skb_tstsmp(&skb, q, ndev)) {
+		spin_unlock_irqrestore(&q->tx_lock, flags);
 		return NETDEV_TX_BUSY;
 	}
+#endif
 
-	if (skb->ip_summed == CHECKSUM_PARTIAL) {
+	if (skb->ip_summed == CHECKSUM_PARTIAL && !lp->eth_hasnobuf &&
+	    (lp->axienet_config->mactype == XAXIENET_1G)) {
 		if (lp->features & XAE_FEATURE_FULL_TX_CSUM) {
 			/* Tx Full Checksum Offload Enabled */
 			cur_p->app0 |= 2;
@@ -770,78 +1359,121 @@ axienet_start_xmit(struct sk_buff *skb, struct net_device *ndev)
 			cur_p->app0 |= 1;
 			cur_p->app1 = (csum_start_off << 16) | csum_index_off;
 		}
-	} else if (skb->ip_summed == CHECKSUM_UNNECESSARY) {
+	} else if (skb->ip_summed == CHECKSUM_UNNECESSARY &&
+		   !lp->eth_hasnobuf &&
+		   (lp->axienet_config->mactype == XAXIENET_1G)) {
 		cur_p->app0 |= 2; /* Tx Full Checksum Offload Enabled */
 	}
 
-	phys = dma_map_single(ndev->dev.parent, skb->data,
-			      skb_headlen(skb), DMA_TO_DEVICE);
-	if (unlikely(dma_mapping_error(ndev->dev.parent, phys))) {
-		if (net_ratelimit())
-			netdev_err(ndev, "TX DMA mapping error\n");
-		ndev->stats.tx_dropped++;
-		return NETDEV_TX_OK;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p->cntrl = (skb_headlen(skb) | XMCDMA_BD_CTRL_TXSOF_MASK);
+#else
+	cur_p->cntrl = (skb_headlen(skb) | XAXIDMA_BD_CTRL_TXSOF_MASK);
+#endif
+
+	if (!q->eth_hasdre &&
+	    (((phys_addr_t)skb->data & 0x3) || num_frag > 0)) {
+		skb_copy_and_csum_dev(skb, q->tx_buf[q->tx_bd_tail]);
+
+		cur_p->phys = q->tx_bufs_dma +
+			      (q->tx_buf[q->tx_bd_tail] - q->tx_bufs);
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p->cntrl = skb_pagelen(skb) | XMCDMA_BD_CTRL_TXSOF_MASK;
+#else
+		cur_p->cntrl = skb_pagelen(skb) | XAXIDMA_BD_CTRL_TXSOF_MASK;
+#endif
+		goto out;
+	} else {
+		cur_p->phys = dma_map_single(ndev->dev.parent, skb->data,
+					     skb_headlen(skb), DMA_TO_DEVICE);
 	}
-	desc_set_phys_addr(lp, phys, cur_p);
-	cur_p->cntrl = skb_headlen(skb) | XAXIDMA_BD_CTRL_TXSOF_MASK;
+	cur_p->tx_desc_mapping = DESC_DMA_MAP_SINGLE;
 
 	for (ii = 0; ii < num_frag; ii++) {
-		if (++lp->tx_bd_tail >= lp->tx_bd_num)
-			lp->tx_bd_tail = 0;
-		cur_p = &lp->tx_bd_v[lp->tx_bd_tail];
-		frag = &skb_shinfo(skb)->frags[ii];
-		phys = dma_map_single(ndev->dev.parent,
-				      skb_frag_address(frag),
-				      skb_frag_size(frag),
-				      DMA_TO_DEVICE);
-		if (unlikely(dma_mapping_error(ndev->dev.parent, phys))) {
-			if (net_ratelimit())
-				netdev_err(ndev, "TX DMA mapping error\n");
-			ndev->stats.tx_dropped++;
-			axienet_free_tx_chain(ndev, orig_tail_ptr, ii + 1,
-					      NULL);
-			lp->tx_bd_tail = orig_tail_ptr;
+		u32 len;
+		skb_frag_t *frag;
 
-			return NETDEV_TX_OK;
-		}
-		desc_set_phys_addr(lp, phys, cur_p);
-		cur_p->cntrl = skb_frag_size(frag);
+		if (++q->tx_bd_tail >= lp->tx_bd_num)
+			q->tx_bd_tail = 0;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p = &q->txq_bd_v[q->tx_bd_tail];
+#else
+		cur_p = &q->tx_bd_v[q->tx_bd_tail];
+#endif
+		frag = &skb_shinfo(skb)->frags[ii];
+		len = skb_frag_size(frag);
+		cur_p->phys = skb_frag_dma_map(ndev->dev.parent, frag, 0, len,
+					       DMA_TO_DEVICE);
+		cur_p->cntrl = len;
+		cur_p->tx_desc_mapping = DESC_DMA_MAP_PAGE;
 	}
 
+out:
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p->cntrl |= XMCDMA_BD_CTRL_TXEOF_MASK;
+	tail_p = q->tx_bd_p + sizeof(*q->txq_bd_v) * q->tx_bd_tail;
+#else
 	cur_p->cntrl |= XAXIDMA_BD_CTRL_TXEOF_MASK;
-	cur_p->skb = skb;
+	tail_p = q->tx_bd_p + sizeof(*q->tx_bd_v) * q->tx_bd_tail;
+#endif
+	cur_p->tx_skb = (phys_addr_t)skb;
+	cur_p->tx_skb = (phys_addr_t)skb;
+
+	tail_p = q->tx_bd_p + sizeof(*q->tx_bd_v) * q->tx_bd_tail;
+	/* Ensure BD write before starting transfer */
+	wmb();
 
-	tail_p = lp->tx_bd_p + sizeof(*lp->tx_bd_v) * lp->tx_bd_tail;
 	/* Start the transfer */
-	axienet_dma_out_addr(lp, XAXIDMA_TX_TDESC_OFFSET, tail_p);
-	if (++lp->tx_bd_tail >= lp->tx_bd_num)
-		lp->tx_bd_tail = 0;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id),
+			  tail_p);
+#else
+	axienet_dma_bdout(q, XAXIDMA_TX_TDESC_OFFSET, tail_p);
+#endif
+	if (++q->tx_bd_tail >= lp->tx_bd_num)
+		q->tx_bd_tail = 0;
 
-	/* Stop queue if next transmit may not have space */
-	if (axienet_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1)) {
-		netif_stop_queue(ndev);
+	spin_unlock_irqrestore(&q->tx_lock, flags);
 
-		/* Matches barrier in axienet_start_xmit_done */
-		smp_mb();
+	return NETDEV_TX_OK;
+}
 
-		/* Space might have just been freed - check again */
-		if (!axienet_check_tx_bd_space(lp, MAX_SKB_FRAGS + 1))
-			netif_wake_queue(ndev);
-	}
+/**
+ * axienet_start_xmit - Starts the transmission.
+ * @skb:	sk_buff pointer that contains data to be Txed.
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: NETDEV_TX_OK, on success
+ *	    NETDEV_TX_BUSY, if any of the descriptors are not free
+ *
+ * This function is invoked from upper layers to initiate transmission. The
+ * function uses the next available free BDs and populates their fields to
+ * start the transmission. Additionally if checksum offloading is supported,
+ * it populates AXI Stream Control fields with appropriate values.
+ */
+static int axienet_start_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	u16 map = skb_get_queue_mapping(skb); /* Single dma queue default*/
 
-	return NETDEV_TX_OK;
+	return axienet_queue_xmit(skb, ndev, map);
 }
 
 /**
  * axienet_recv - Is called from Axi DMA Rx Isr to complete the received
  *		  BD processing.
  * @ndev:	Pointer to net_device structure.
+ * @budget:	NAPI budget
+ * @q:		Pointer to axienet DMA queue structure
  *
- * This function is invoked from the Axi DMA Rx isr to process the Rx BDs. It
- * does minimal processing and invokes "netif_rx" to complete further
- * processing.
+ * This function is invoked from the Axi DMA Rx isr(poll) to process the Rx BDs
+ * It does minimal processing and invokes "netif_receive_skb" to complete
+ * further processing.
+ * Return: Number of BD's processed.
  */
-static void axienet_recv(struct net_device *ndev)
+static int axienet_recv(struct net_device *ndev, int budget,
+			struct axienet_dma_q *q)
 {
 	u32 length;
 	u32 csumstatus;
@@ -850,191 +1482,246 @@ static void axienet_recv(struct net_device *ndev)
 	dma_addr_t tail_p = 0;
 	struct axienet_local *lp = netdev_priv(ndev);
 	struct sk_buff *skb, *new_skb;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	struct aximcdma_bd *cur_p;
+#else
 	struct axidma_bd *cur_p;
+#endif
+	unsigned int numbdfree = 0;
 
-	cur_p = &lp->rx_bd_v[lp->rx_bd_ci];
+	/* Get relevat BD status value */
+	rmb();
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+#else
+	cur_p = &q->rx_bd_v[q->rx_bd_ci];
+#endif
 
-	while ((cur_p->status & XAXIDMA_BD_STS_COMPLETE_MASK)) {
-		dma_addr_t phys;
+	while ((numbdfree < budget) &&
+	       (cur_p->status & XAXIDMA_BD_STS_COMPLETE_MASK)) {
+		new_skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+		if (!new_skb) {
+			dev_err(lp->dev, "No memory for new_skb\n");
+			break;
+		}
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		tail_p = q->rx_bd_p + sizeof(*q->rxq_bd_v) * q->rx_bd_ci;
+#else
+		tail_p = q->rx_bd_p + sizeof(*q->rx_bd_v) * q->rx_bd_ci;
+#endif
 
-		/* Ensure we see complete descriptor update */
-		dma_rmb();
+		dma_unmap_single(ndev->dev.parent, cur_p->phys,
+				 lp->max_frm_size,
+				 DMA_FROM_DEVICE);
 
-		skb = cur_p->skb;
-		cur_p->skb = NULL;
+		skb = (struct sk_buff *)(cur_p->sw_id_offset);
 
-		/* skb could be NULL if a previous pass already received the
-		 * packet for this slot in the ring, but failed to refill it
-		 * with a newly allocated buffer. In this case, don't try to
-		 * receive it again.
-		 */
-		if (likely(skb)) {
+		if (lp->eth_hasnobuf ||
+		    (lp->axienet_config->mactype != XAXIENET_1G))
+			length = cur_p->status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
+		else
 			length = cur_p->app4 & 0x0000FFFF;
 
-			phys = desc_get_phys_addr(lp, cur_p);
-			dma_unmap_single(ndev->dev.parent, phys, lp->max_frm_size,
-					 DMA_FROM_DEVICE);
-
-			skb_put(skb, length);
-			skb->protocol = eth_type_trans(skb, ndev);
-			/*skb_checksum_none_assert(skb);*/
-			skb->ip_summed = CHECKSUM_NONE;
-
-			/* if we're doing Rx csum offload, set it up */
-			if (lp->features & XAE_FEATURE_FULL_RX_CSUM) {
-				csumstatus = (cur_p->app2 &
-					      XAE_FULL_CSUM_STATUS_MASK) >> 3;
-				if (csumstatus == XAE_IP_TCP_CSUM_VALIDATED ||
-				    csumstatus == XAE_IP_UDP_CSUM_VALIDATED) {
-					skb->ip_summed = CHECKSUM_UNNECESSARY;
-				}
-			} else if ((lp->features & XAE_FEATURE_PARTIAL_RX_CSUM) != 0 &&
-				   skb->protocol == htons(ETH_P_IP) &&
-				   skb->len > 64) {
-				skb->csum = be32_to_cpu(cur_p->app3 & 0xFFFF);
-				skb->ip_summed = CHECKSUM_COMPLETE;
+		skb_put(skb, length);
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	if (!lp->is_tsn) {
+		if ((lp->tstamp_config.rx_filter == HWTSTAMP_FILTER_ALL ||
+			lp->eth_hasptp) &&
+			(lp->axienet_config->mactype != XAXIENET_10G_25G) &&
+			(lp->axienet_config->mactype != XAXIENET_MRMAC)) {
+			u32 sec, nsec;
+			u64 time64;
+			struct skb_shared_hwtstamps *shhwtstamps;
+
+			if (lp->axienet_config->mactype == XAXIENET_1G ||
+			    lp->axienet_config->mactype == XAXIENET_2_5G) {
+				/* The first 8 bytes will be the timestamp */
+				memcpy(&sec, &skb->data[0], 4);
+				memcpy(&nsec, &skb->data[4], 4);
+
+				sec = cpu_to_be32(sec);
+				nsec = cpu_to_be32(nsec);
+			} else {
+				/* The first 8 bytes will be the timestamp */
+				memcpy(&nsec, &skb->data[0], 4);
+				memcpy(&sec, &skb->data[4], 4);
 			}
 
-			netif_rx(skb);
-
-			size += length;
-			packets++;
+			/* Remove these 8 bytes from the buffer */
+			skb_pull(skb, 8);
+			time64 = sec * NS_PER_SEC + nsec;
+			shhwtstamps = skb_hwtstamps(skb);
+			shhwtstamps->hwtstamp = ns_to_ktime(time64);
+		} else if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+			   lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			axienet_rx_hwtstamp(lp, skb);
+		}
+	}
+#endif
+		skb->protocol = eth_type_trans(skb, ndev);
+		/*skb_checksum_none_assert(skb);*/
+		skb->ip_summed = CHECKSUM_NONE;
+
+		/* if we're doing Rx csum offload, set it up */
+		if (lp->features & XAE_FEATURE_FULL_RX_CSUM &&
+		    (lp->axienet_config->mactype == XAXIENET_1G) &&
+		    !lp->eth_hasnobuf) {
+			csumstatus = (cur_p->app2 &
+				      XAE_FULL_CSUM_STATUS_MASK) >> 3;
+			if ((csumstatus == XAE_IP_TCP_CSUM_VALIDATED) ||
+			    (csumstatus == XAE_IP_UDP_CSUM_VALIDATED)) {
+				skb->ip_summed = CHECKSUM_UNNECESSARY;
+			}
+		} else if ((lp->features & XAE_FEATURE_PARTIAL_RX_CSUM) != 0 &&
+			   skb->protocol == htons(ETH_P_IP) &&
+			   skb->len > 64 && !lp->eth_hasnobuf &&
+			   (lp->axienet_config->mactype == XAXIENET_1G)) {
+			skb->csum = be32_to_cpu(cur_p->app3 & 0xFFFF);
+			skb->ip_summed = CHECKSUM_COMPLETE;
+		}
+#ifdef CONFIG_XILINX_TSN
+		if (unlikely(q->flags & MCDMA_MGMT_CHAN)) {
+			struct net_device *ndev = NULL;
+
+			/* received packet on mgmt channel */
+			if (q->flags & MCDMA_MGMT_CHAN_PORT0)
+				ndev = lp->slaves[0];
+			else if (q->flags & MCDMA_MGMT_CHAN_PORT1)
+				ndev = lp->slaves[1];
+
+			/* send to one of the front panel port */
+			if (ndev && netif_running(ndev)) {
+				skb->dev = ndev;
+				netif_receive_skb(skb);
+			} else {
+				kfree(skb); /* dont send up the stack */
+			}
+		} else {
+			netif_receive_skb(skb); /* send on normal data path */
 		}
+#else
 
-		new_skb = netdev_alloc_skb_ip_align(ndev, lp->max_frm_size);
-		if (!new_skb)
-			break;
+		netif_receive_skb(skb);
+#endif
 
-		phys = dma_map_single(ndev->dev.parent, new_skb->data,
-				      lp->max_frm_size,
-				      DMA_FROM_DEVICE);
-		if (unlikely(dma_mapping_error(ndev->dev.parent, phys))) {
-			if (net_ratelimit())
-				netdev_err(ndev, "RX DMA mapping error\n");
-			dev_kfree_skb(new_skb);
-			break;
-		}
-		desc_set_phys_addr(lp, phys, cur_p);
+		size += length;
+		packets++;
 
+		/* Ensure that the skb is completely updated
+		 * prio to mapping the DMA
+		 */
+		wmb();
+
+		cur_p->phys = dma_map_single(ndev->dev.parent, new_skb->data,
+					   lp->max_frm_size,
+					   DMA_FROM_DEVICE);
 		cur_p->cntrl = lp->max_frm_size;
 		cur_p->status = 0;
-		cur_p->skb = new_skb;
+		cur_p->sw_id_offset = (phys_addr_t)new_skb;
 
-		/* Only update tail_p to mark this slot as usable after it has
-		 * been successfully refilled.
-		 */
-		tail_p = lp->rx_bd_p + sizeof(*lp->rx_bd_v) * lp->rx_bd_ci;
+		if (++q->rx_bd_ci >= lp->rx_bd_num)
+			q->rx_bd_ci = 0;
 
-		if (++lp->rx_bd_ci >= lp->rx_bd_num)
-			lp->rx_bd_ci = 0;
-		cur_p = &lp->rx_bd_v[lp->rx_bd_ci];
+		/* Get relevat BD status value */
+		rmb();
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+#else
+		cur_p = &q->rx_bd_v[q->rx_bd_ci];
+#endif
+		numbdfree++;
 	}
 
 	ndev->stats.rx_packets += packets;
 	ndev->stats.rx_bytes += size;
+	q->rx_packets += packets;
+	q->rx_bytes += size;
+
+	if (tail_p) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+				  q->rx_offset, tail_p);
+#else
+		axienet_dma_bdout(q, XAXIDMA_RX_TDESC_OFFSET, tail_p);
+#endif
+	}
 
-	if (tail_p)
-		axienet_dma_out_addr(lp, XAXIDMA_RX_TDESC_OFFSET, tail_p);
+	return numbdfree;
 }
 
 /**
- * axienet_tx_irq - Tx Done Isr.
- * @irq:	irq number
- * @_ndev:	net_device pointer
+ * xaxienet_rx_poll - Poll routine for rx packets (NAPI)
+ * @napi:	napi structure pointer
+ * @quota:	Max number of rx packets to be processed.
  *
- * Return: IRQ_HANDLED if device generated a TX interrupt, IRQ_NONE otherwise.
+ * This is the poll routine for rx part.
+ * It will process the packets maximux quota value.
  *
- * This is the Axi DMA Tx done Isr. It invokes "axienet_start_xmit_done"
- * to complete the BD processing.
+ * Return: number of packets received
  */
-static irqreturn_t axienet_tx_irq(int irq, void *_ndev)
+int xaxienet_rx_poll(struct napi_struct *napi, int quota)
 {
-	u32 cr;
-	unsigned int status;
-	struct net_device *ndev = _ndev;
+	struct net_device *ndev = napi->dev;
 	struct axienet_local *lp = netdev_priv(ndev);
-
-	status = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);
-	if (status & (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK)) {
-		axienet_dma_out32(lp, XAXIDMA_TX_SR_OFFSET, status);
-		axienet_start_xmit_done(lp->ndev);
-		goto out;
+	int work_done = 0;
+	unsigned int status, cr;
+
+	int map = napi - lp->napi;
+
+	struct axienet_dma_q *q = lp->dq[map];
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	spin_lock(&q->rx_lock);
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset);
+	while ((status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) &&
+	       (work_done < quota)) {
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+		if (status & XMCDMA_IRQ_ERR_MASK) {
+			dev_err(lp->dev, "Rx error 0x%x\n\r", status);
+			break;
+		}
+		work_done += axienet_recv(lp->ndev, quota - work_done, q);
+		status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+					  q->rx_offset);
 	}
-	if (!(status & XAXIDMA_IRQ_ALL_MASK))
-		return IRQ_NONE;
-	if (status & XAXIDMA_IRQ_ERROR_MASK) {
-		dev_err(&ndev->dev, "DMA Tx error 0x%x\n", status);
-		dev_err(&ndev->dev, "Current BD is at: 0x%x%08x\n",
-			(lp->tx_bd_v[lp->tx_bd_ci]).phys_msb,
-			(lp->tx_bd_v[lp->tx_bd_ci]).phys);
-
-		cr = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-		/* Disable coalesce, delay timer and error interrupts */
-		cr &= (~XAXIDMA_IRQ_ALL_MASK);
-		/* Write to the Tx channel control register */
-		axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, cr);
-
-		cr = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-		/* Disable coalesce, delay timer and error interrupts */
-		cr &= (~XAXIDMA_IRQ_ALL_MASK);
-		/* Write to the Rx channel control register */
-		axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, cr);
-
-		schedule_work(&lp->dma_err_task);
-		axienet_dma_out32(lp, XAXIDMA_TX_SR_OFFSET, status);
+	spin_unlock(&q->rx_lock);
+#else
+	spin_lock(&q->rx_lock);
+
+	status = axienet_dma_in32(q, XAXIDMA_RX_SR_OFFSET);
+	while ((status & (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK)) &&
+	       (work_done < quota)) {
+		axienet_dma_out32(q, XAXIDMA_RX_SR_OFFSET, status);
+		if (status & XAXIDMA_IRQ_ERROR_MASK) {
+			dev_err(lp->dev, "Rx error 0x%x\n\r", status);
+			break;
+		}
+		work_done += axienet_recv(lp->ndev, quota - work_done, q);
+		status = axienet_dma_in32(q, XAXIDMA_RX_SR_OFFSET);
 	}
-out:
-	return IRQ_HANDLED;
-}
-
-/**
- * axienet_rx_irq - Rx Isr.
- * @irq:	irq number
- * @_ndev:	net_device pointer
- *
- * Return: IRQ_HANDLED if device generated a RX interrupt, IRQ_NONE otherwise.
- *
- * This is the Axi DMA Rx Isr. It invokes "axienet_recv" to complete the BD
- * processing.
- */
-static irqreturn_t axienet_rx_irq(int irq, void *_ndev)
-{
-	u32 cr;
-	unsigned int status;
-	struct net_device *ndev = _ndev;
-	struct axienet_local *lp = netdev_priv(ndev);
+	spin_unlock(&q->rx_lock);
+#endif
 
-	status = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);
-	if (status & (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK)) {
-		axienet_dma_out32(lp, XAXIDMA_RX_SR_OFFSET, status);
-		axienet_recv(lp->ndev);
-		goto out;
-	}
-	if (!(status & XAXIDMA_IRQ_ALL_MASK))
-		return IRQ_NONE;
-	if (status & XAXIDMA_IRQ_ERROR_MASK) {
-		dev_err(&ndev->dev, "DMA Rx error 0x%x\n", status);
-		dev_err(&ndev->dev, "Current BD is at: 0x%x%08x\n",
-			(lp->rx_bd_v[lp->rx_bd_ci]).phys_msb,
-			(lp->rx_bd_v[lp->rx_bd_ci]).phys);
-
-		cr = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-		/* Disable coalesce, delay timer and error interrupts */
-		cr &= (~XAXIDMA_IRQ_ALL_MASK);
-		/* Finally write to the Tx channel control register */
-		axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, cr);
-
-		cr = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-		/* Disable coalesce, delay timer and error interrupts */
-		cr &= (~XAXIDMA_IRQ_ALL_MASK);
-		/* write to the Rx channel control register */
-		axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, cr);
-
-		schedule_work(&lp->dma_err_task);
-		axienet_dma_out32(lp, XAXIDMA_RX_SR_OFFSET, status);
+	if (work_done < quota) {
+		napi_complete(napi);
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		/* Enable the interrupts again */
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      XMCDMA_RX_OFFSET);
+		cr |= (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  XMCDMA_RX_OFFSET, cr);
+#else
+		/* Enable the interrupts again */
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		cr |= (XAXIDMA_IRQ_IOC_MASK | XAXIDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
+#endif
 	}
-out:
-	return IRQ_HANDLED;
+
+	return work_done;
 }
 
 /**
@@ -1066,8 +1753,6 @@ static irqreturn_t axienet_eth_irq(int irq, void *_ndev)
 	return IRQ_HANDLED;
 }
 
-static void axienet_dma_err_handler(struct work_struct *work);
-
 /**
  * axienet_open - Driver open routine.
  * @ndev:	Pointer to net_device structure
@@ -1075,73 +1760,220 @@ static void axienet_dma_err_handler(struct work_struct *work);
  * Return: 0, on success.
  *	    non-zero error value on failure
  *
- * This is the driver open routine. It calls phylink_start to start the
- * PHY device.
+ * This is the driver open routine. It calls phy_start to start the PHY device.
  * It also allocates interrupt service routines, enables the interrupt lines
  * and ISR handling. Axi Ethernet core is reset through Axi DMA core. Buffer
  * descriptors are initialized.
  */
 static int axienet_open(struct net_device *ndev)
 {
-	int ret;
+	int ret = 0, i = 0;
 	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	u32 reg, err;
+	struct phy_device *phydev = NULL;
 
 	dev_dbg(&ndev->dev, "axienet_open()\n");
 
-	/* Disable the MDIO interface till Axi Ethernet Reset is completed.
-	 * When we do an Axi Ethernet reset, it resets the complete core
-	 * including the MDIO. MDIO must be disabled before resetting
-	 * and re-enabled afterwards.
-	 * Hold MDIO bus lock to avoid MDIO accesses during the reset.
-	 */
-	mutex_lock(&lp->mii_bus->mdio_lock);
-	axienet_mdio_disable(lp);
-	ret = axienet_device_reset(ndev);
-	if (ret == 0)
-		ret = axienet_mdio_enable(lp);
-	mutex_unlock(&lp->mii_bus->mdio_lock);
-	if (ret < 0)
+	ret  = axienet_device_reset(ndev);
+	if (ret < 0) {
+		dev_err(lp->dev, "axienet_device_reset failed\n");
 		return ret;
+	}
 
-	ret = phylink_of_phy_connect(lp->phylink, lp->dev->of_node, 0);
-	if (ret) {
-		dev_err(lp->dev, "phylink_of_phy_connect() failed: %d\n", ret);
-		return ret;
+	if (lp->phy_node) {
+		phydev = of_phy_connect(lp->ndev, lp->phy_node,
+					axienet_adjust_link,
+					lp->phy_flags,
+					lp->phy_mode);
+
+		if (!phydev)
+			dev_err(lp->dev, "of_phy_connect() failed\n");
+		else
+			phy_start(phydev);
 	}
+	if (!lp->is_tsn) {
+		/* Enable tasklets for Axi DMA error handling */
+		for_each_rx_dma_queue(lp, i) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			tasklet_init(&lp->dma_err_tasklet[i],
+				     axienet_mcdma_err_handler,
+				     (unsigned long)lp->dq[i]);
+#else
+			tasklet_init(&lp->dma_err_tasklet[i],
+				     axienet_dma_err_handler,
+				     (unsigned long)lp->dq[i]);
+#endif
 
-	phylink_start(lp->phylink);
+			/* Enable NAPI scheduling before enabling Axi DMA Rx
+			 * IRQ, or you might run into a race condition; the RX
+			 * ISR disables IRQ processing before scheduling the
+			 * NAPI function to complete the processing. If NAPI
+			 * scheduling is (still) disabled at that time, no more
+			 * RX IRQs will be processed as only the NAPI function
+			 * re-enables them!
+			 */
+			napi_enable(&lp->napi[i]);
+		}
+		for_each_tx_dma_queue(lp, i) {
+			struct axienet_dma_q *q = lp->dq[i];
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			/* Enable interrupts for Axi MCDMA Tx */
+			ret = request_irq(q->tx_irq, axienet_mcdma_tx_irq,
+					  IRQF_SHARED, ndev->name, ndev);
+			if (ret)
+				goto err_tx_irq;
+#else
+			/* Enable interrupts for Axi DMA Tx */
+			ret = request_irq(q->tx_irq, axienet_tx_irq,
+					  0, ndev->name, ndev);
+			if (ret)
+				goto err_tx_irq;
+#endif
+		}
 
-	/* Enable worker thread for Axi DMA error handling */
-	INIT_WORK(&lp->dma_err_task, axienet_dma_err_handler);
+		for_each_rx_dma_queue(lp, i) {
+			struct axienet_dma_q *q = lp->dq[i];
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+			/* Enable interrupts for Axi MCDMA Rx */
+			ret = request_irq(q->rx_irq, axienet_mcdma_rx_irq,
+					  IRQF_SHARED, ndev->name, ndev);
+			if (ret)
+				goto err_rx_irq;
+#else
+			/* Enable interrupts for Axi DMA Rx */
+			ret = request_irq(q->rx_irq, axienet_rx_irq,
+					  0, ndev->name, ndev);
+			if (ret)
+				goto err_rx_irq;
+#endif
+		}
+	}
+
+	if (lp->phy_mode == PHY_INTERFACE_MODE_USXGMII) {
+		netdev_dbg(ndev, "RX reg: 0x%x\n",
+			   axienet_ior(lp, XXV_RCW1_OFFSET));
+		/* USXGMII setup at selected speed */
+		reg = axienet_ior(lp, XXV_USXGMII_AN_OFFSET);
+		reg &= ~USXGMII_RATE_MASK;
+		netdev_dbg(ndev, "usxgmii_rate %d\n", lp->usxgmii_rate);
+		switch (lp->usxgmii_rate) {
+		case SPEED_1000:
+			reg |= USXGMII_RATE_1G;
+			break;
+		case SPEED_2500:
+			reg |= USXGMII_RATE_2G5;
+			break;
+		case SPEED_10:
+			reg |= USXGMII_RATE_10M;
+			break;
+		case SPEED_100:
+			reg |= USXGMII_RATE_100M;
+			break;
+		case SPEED_5000:
+			reg |= USXGMII_RATE_5G;
+			break;
+		case SPEED_10000:
+			reg |= USXGMII_RATE_10G;
+			break;
+		default:
+			reg |= USXGMII_RATE_1G;
+		}
+		reg |= USXGMII_FD;
+		reg |= (USXGMII_EN | USXGMII_LINK_STS);
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET, reg);
+		reg |= USXGMII_AN_EN;
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET, reg);
+		/* AN Restart bit should be reset, set and then reset as per
+		 * spec with a 1 ms delay for a raising edge trigger
+		 */
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET,
+			    reg & ~USXGMII_AN_RESTART);
+		mdelay(1);
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET,
+			    reg | USXGMII_AN_RESTART);
+		mdelay(1);
+		axienet_iow(lp, XXV_USXGMII_AN_OFFSET,
+			    reg & ~USXGMII_AN_RESTART);
+
+		/* Check block lock bit to make sure RX path is ok with
+		 * USXGMII initialization.
+		 */
+		err = readl_poll_timeout(lp->regs + XXV_STATRX_BLKLCK_OFFSET,
+					 reg, (reg & XXV_RX_BLKLCK_MASK),
+					 100, DELAY_OF_ONE_MILLISEC);
+		if (err) {
+			netdev_err(ndev, "%s: USXGMII Block lock bit not set",
+				   __func__);
+			ret = -ENODEV;
+			goto err_eth_irq;
+		}
+
+		err = readl_poll_timeout(lp->regs + XXV_USXGMII_AN_STS_OFFSET,
+					 reg, (reg & USXGMII_AN_STS_COMP_MASK),
+					 1000000, DELAY_OF_ONE_MILLISEC);
+		if (err) {
+			netdev_err(ndev, "%s: USXGMII AN not complete",
+				   __func__);
+			ret = -ENODEV;
+			goto err_eth_irq;
+		}
+
+		netdev_info(ndev, "USXGMII setup at %d\n", lp->usxgmii_rate);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		u32 val;
+
+		/* Reset MRMAC */
+		axienet_mrmac_reset(lp);
+
+		mdelay(MRMAC_RESET_DELAY);
+		/* Check for block lock bit to be set. This ensures that
+		 * MRMAC ethernet IP is functioning normally.
+		 */
+		axienet_iow(lp, MRMAC_TX_STS_OFFSET, MRMAC_STS_ALL_MASK);
+		axienet_iow(lp, MRMAC_RX_STS_OFFSET, MRMAC_STS_ALL_MASK);
+		err = readx_poll_timeout(axienet_get_mrmac_blocklock, lp, val,
+					 (val & MRMAC_RX_BLKLCK_MASK), 10, DELAY_OF_ONE_MILLISEC);
+		if (err) {
+			netdev_err(ndev, "MRMAC block lock not complete! Cross-check the MAC ref clock configuration\n");
+			ret = -ENODEV;
+			goto err_eth_irq;
+		}
+		netdev_info(ndev, "MRMAC setup at %d\n", lp->mrmac_rate);
+		axienet_iow(lp, MRMAC_TICK_OFFSET, MRMAC_TICK_TRIGGER);
+	}
 
-	/* Enable interrupts for Axi DMA Tx */
-	ret = request_irq(lp->tx_irq, axienet_tx_irq, IRQF_SHARED,
-			  ndev->name, ndev);
-	if (ret)
-		goto err_tx_irq;
-	/* Enable interrupts for Axi DMA Rx */
-	ret = request_irq(lp->rx_irq, axienet_rx_irq, IRQF_SHARED,
-			  ndev->name, ndev);
-	if (ret)
-		goto err_rx_irq;
 	/* Enable interrupts for Axi Ethernet core (if defined) */
-	if (lp->eth_irq > 0) {
+	if (!lp->eth_hasnobuf && (lp->axienet_config->mactype == XAXIENET_1G)) {
 		ret = request_irq(lp->eth_irq, axienet_eth_irq, IRQF_SHARED,
 				  ndev->name, ndev);
 		if (ret)
 			goto err_eth_irq;
 	}
 
+	netif_tx_start_all_queues(ndev);
 	return 0;
 
 err_eth_irq:
-	free_irq(lp->rx_irq, ndev);
+	while (i--) {
+		q = lp->dq[i];
+		free_irq(q->rx_irq, ndev);
+	}
+	i = lp->num_tx_queues;
 err_rx_irq:
-	free_irq(lp->tx_irq, ndev);
+	while (i--) {
+		q = lp->dq[i];
+		free_irq(q->tx_irq, ndev);
+	}
 err_tx_irq:
-	phylink_stop(lp->phylink);
-	phylink_disconnect_phy(lp->phylink);
-	cancel_work_sync(&lp->dma_err_task);
+	for_each_rx_dma_queue(lp, i)
+		napi_disable(&lp->napi[i]);
+	if (phydev)
+		phy_disconnect(phydev);
+	for_each_rx_dma_queue(lp, i)
+		tasklet_kill(&lp->dma_err_tasklet[i]);
 	dev_err(lp->dev, "request_irq() failed\n");
 	return ret;
 }
@@ -1152,7 +1984,7 @@ static int axienet_open(struct net_device *ndev)
  *
  * Return: 0, on success.
  *
- * This is the driver stop routine. It calls phylink_disconnect to stop the PHY
+ * This is the driver stop routine. It calls phy_disconnect to stop the PHY
  * device. It also removes the interrupt handlers and disables the interrupts.
  * The Axi DMA Tx/Rx BDs are released.
  */
@@ -1160,54 +1992,67 @@ static int axienet_stop(struct net_device *ndev)
 {
 	u32 cr, sr;
 	int count;
+	u32 i;
 	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
 
 	dev_dbg(&ndev->dev, "axienet_close()\n");
 
-	phylink_stop(lp->phylink);
-	phylink_disconnect_phy(lp->phylink);
-
-	axienet_setoptions(ndev, lp->options &
+	lp->axienet_config->setoptions(ndev, lp->options &
 			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
 
-	cr = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-	cr &= ~(XAXIDMA_CR_RUNSTOP_MASK | XAXIDMA_IRQ_ALL_MASK);
-	axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, cr);
+	if (!lp->is_tsn) {
+		for_each_tx_dma_queue(lp, i) {
+			q = lp->dq[i];
+			cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+			cr &= ~(XAXIDMA_CR_RUNSTOP_MASK | XAXIDMA_IRQ_ALL_MASK);
+			axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET, cr);
 
-	cr = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-	cr &= ~(XAXIDMA_CR_RUNSTOP_MASK | XAXIDMA_IRQ_ALL_MASK);
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, cr);
+			cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+			cr &= ~(XAXIDMA_CR_RUNSTOP_MASK | XAXIDMA_IRQ_ALL_MASK);
+			axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, cr);
 
-	axienet_iow(lp, XAE_IE_OFFSET, 0);
+			axienet_iow(lp, XAE_IE_OFFSET, 0);
 
-	/* Give DMAs a chance to halt gracefully */
-	sr = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);
-	for (count = 0; !(sr & XAXIDMA_SR_HALT_MASK) && count < 5; ++count) {
-		msleep(20);
-		sr = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);
-	}
+			/* Give DMAs a chance to halt gracefully */
+			sr = axienet_dma_in32(q, XAXIDMA_RX_SR_OFFSET);
+			for (count = 0; !(sr & XAXIDMA_SR_HALT_MASK) && count < 5; ++count) {
+				msleep(20);
+				sr = axienet_dma_in32(q, XAXIDMA_RX_SR_OFFSET);
+			}
 
-	sr = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);
-	for (count = 0; !(sr & XAXIDMA_SR_HALT_MASK) && count < 5; ++count) {
-		msleep(20);
-		sr = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);
-	}
+			sr = axienet_dma_in32(q, XAXIDMA_TX_SR_OFFSET);
+			for (count = 0; !(sr & XAXIDMA_SR_HALT_MASK) && count < 5; ++count) {
+				msleep(20);
+				sr = axienet_dma_in32(q, XAXIDMA_TX_SR_OFFSET);
+			}
 
-	/* Do a reset to ensure DMA is really stopped */
-	mutex_lock(&lp->mii_bus->mdio_lock);
-	axienet_mdio_disable(lp);
-	__axienet_device_reset(lp);
-	axienet_mdio_enable(lp);
-	mutex_unlock(&lp->mii_bus->mdio_lock);
+			__axienet_device_reset(q);
+			free_irq(q->tx_irq, ndev);
+		}
 
-	cancel_work_sync(&lp->dma_err_task);
+		for_each_rx_dma_queue(lp, i) {
+			q = lp->dq[i];
+			netif_stop_queue(ndev);
+			napi_disable(&lp->napi[i]);
+			tasklet_kill(&lp->dma_err_tasklet[i]);
+			free_irq(q->rx_irq, ndev);
+		}
+#ifdef CONFIG_XILINX_TSN_PTP
+		if (lp->is_tsn) {
+			free_irq(lp->ptp_tx_irq, ndev);
+			free_irq(lp->ptp_rx_irq, ndev);
+		}
+#endif
+		if ((lp->axienet_config->mactype == XAXIENET_1G) && !lp->eth_hasnobuf)
+			free_irq(lp->eth_irq, ndev);
 
-	if (lp->eth_irq > 0)
-		free_irq(lp->eth_irq, ndev);
-	free_irq(lp->tx_irq, ndev);
-	free_irq(lp->rx_irq, ndev);
+		if (ndev->phydev)
+			phy_disconnect(ndev->phydev);
 
-	axienet_dma_bd_release(ndev);
+		if (!lp->is_tsn)
+			axienet_dma_bd_release(ndev);
+	}
 	return 0;
 }
 
@@ -1249,34 +2094,249 @@ static int axienet_change_mtu(struct net_device *ndev, int new_mtu)
 static void axienet_poll_controller(struct net_device *ndev)
 {
 	struct axienet_local *lp = netdev_priv(ndev);
-	disable_irq(lp->tx_irq);
-	disable_irq(lp->rx_irq);
-	axienet_rx_irq(lp->tx_irq, ndev);
-	axienet_tx_irq(lp->rx_irq, ndev);
-	enable_irq(lp->tx_irq);
-	enable_irq(lp->rx_irq);
+	int i;
+
+	for_each_tx_dma_queue(lp, i)
+		disable_irq(lp->dq[i]->tx_irq);
+	for_each_rx_dma_queue(lp, i)
+		disable_irq(lp->dq[i]->rx_irq);
+
+	for_each_rx_dma_queue(lp, i)
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		axienet_mcdma_rx_irq(lp->dq[i]->rx_irq, ndev);
+#else
+		axienet_rx_irq(lp->dq[i]->rx_irq, ndev);
+#endif
+	for_each_tx_dma_queue(lp, i)
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		axienet_mcdma_tx_irq(lp->dq[i]->tx_irq, ndev);
+#else
+		axienet_tx_irq(lp->dq[i]->tx_irq, ndev);
+#endif
+	for_each_tx_dma_queue(lp, i)
+		enable_irq(lp->dq[i]->tx_irq);
+	for_each_rx_dma_queue(lp, i)
+		enable_irq(lp->dq[i]->rx_irq);
+}
+#endif
+
+#if defined(CONFIG_XILINX_AXI_EMAC_HWTSTAMP) || defined(CONFIG_XILINX_TSN_PTP)
+/**
+ *  axienet_set_timestamp_mode - sets up the hardware for the requested mode
+ *  @lp: Pointer to axienet local structure
+ *  @config: the hwtstamp configuration requested
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_set_timestamp_mode(struct axienet_local *lp,
+				      struct hwtstamp_config *config)
+{
+	u32 regval;
+
+#ifdef CONFIG_XILINX_TSN_PTP
+	if (lp->is_tsn) {
+		/* reserved for future extensions */
+		if (config->flags)
+			return -EINVAL;
+
+		if (config->tx_type < HWTSTAMP_TX_OFF ||
+		    config->tx_type > HWTSTAMP_TX_ONESTEP_SYNC)
+			return -ERANGE;
+
+		lp->ptp_ts_type = config->tx_type;
+
+		/* On RX always timestamp everything */
+		switch (config->rx_filter) {
+		case HWTSTAMP_FILTER_NONE:
+			break;
+		default:
+			config->rx_filter = HWTSTAMP_FILTER_ALL;
+		}
+		return 0;
+	}
+#endif
+
+	/* reserved for future extensions */
+	if (config->flags)
+		return -EINVAL;
+
+	/* Read the current value in the MAC TX CTRL register */
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		regval = axienet_ior(lp, XAE_TC_OFFSET);
+
+	switch (config->tx_type) {
+	case HWTSTAMP_TX_OFF:
+		regval &= ~XAE_TC_INBAND1588_MASK;
+		break;
+	case HWTSTAMP_TX_ON:
+		config->tx_type = HWTSTAMP_TX_ON;
+		regval |= XAE_TC_INBAND1588_MASK;
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, 0x0);
+		break;
+	case HWTSTAMP_TX_ONESTEP_SYNC:
+		config->tx_type = HWTSTAMP_TX_ONESTEP_SYNC;
+		regval |= XAE_TC_INBAND1588_MASK;
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, MRMAC_ONE_STEP_EN);
+		break;
+	case HWTSTAMP_TX_ONESTEP_P2P:
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			config->tx_type = HWTSTAMP_TX_ONESTEP_P2P;
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, MRMAC_ONE_STEP_EN);
+		} else {
+			return -ERANGE;
+		}
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_TC_OFFSET, regval);
+
+	/* Read the current value in the MAC RX RCW1 register */
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		regval = axienet_ior(lp, XAE_RCW1_OFFSET);
+
+	/* On RX always timestamp everything */
+	switch (config->rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		regval &= ~XAE_RCW1_INBAND1588_MASK;
+		break;
+	default:
+		config->rx_filter = HWTSTAMP_FILTER_ALL;
+		regval |= XAE_RCW1_INBAND1588_MASK;
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_RCW1_OFFSET, regval);
+
+	return 0;
+}
+
+/**
+ * axienet_set_ts_config - user entry point for timestamp mode
+ * @lp: Pointer to axienet local structure
+ * @ifr: ioctl data
+ *
+ * Set hardware to the requested more. If unsupported return an error
+ * with no changes. Otherwise, store the mode for future reference
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_set_ts_config(struct axienet_local *lp, struct ifreq *ifr)
+{
+	struct hwtstamp_config config;
+	int err;
+
+	if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	err = axienet_set_timestamp_mode(lp, &config);
+	if (err)
+		return err;
+
+	/* save these settings for future reference */
+	memcpy(&lp->tstamp_config, &config, sizeof(lp->tstamp_config));
+
+	return copy_to_user(ifr->ifr_data, &config,
+			    sizeof(config)) ? -EFAULT : 0;
+}
+
+/**
+ * axienet_get_ts_config - return the current timestamp configuration
+ * to the user
+ * @lp: pointer to axienet local structure
+ * @ifr: ioctl data
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_get_ts_config(struct axienet_local *lp, struct ifreq *ifr)
+{
+	struct hwtstamp_config *config = &lp->tstamp_config;
+
+	return copy_to_user(ifr->ifr_data, config,
+			    sizeof(*config)) ? -EFAULT : 0;
 }
 #endif
 
+/* Ioctl MII Interface */
 static int axienet_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
 {
+#if defined(CONFIG_XILINX_AXI_EMAC_HWTSTAMP) || defined(CONFIG_XILINX_TSN_PTP)
 	struct axienet_local *lp = netdev_priv(dev);
+#endif
 
 	if (!netif_running(dev))
 		return -EINVAL;
 
-	return phylink_mii_ioctl(lp->phylink, rq, cmd);
+	switch (cmd) {
+	case SIOCGMIIPHY:
+	case SIOCGMIIREG:
+	case SIOCSMIIREG:
+		if (!dev->phydev)
+			return -EOPNOTSUPP;
+		return phy_mii_ioctl(dev->phydev, rq, cmd);
+#if defined(CONFIG_XILINX_AXI_EMAC_HWTSTAMP) || defined(CONFIG_XILINX_TSN_PTP)
+	case SIOCSHWTSTAMP:
+		return axienet_set_ts_config(lp, rq);
+	case SIOCGHWTSTAMP:
+		return axienet_get_ts_config(lp, rq);
+#endif
+#ifdef CONFIG_XILINX_TSN_QBV
+	case SIOCCHIOCTL:
+		if (lp->qbv_regs)
+			return axienet_set_schedule(dev, rq->ifr_data);
+		return -EINVAL;
+	case SIOC_GET_SCHED:
+		if (lp->qbv_regs)
+			return axienet_get_schedule(dev, rq->ifr_data);
+		return -EINVAL;
+#endif
+#ifdef CONFIG_XILINX_TSN_QBR
+	case SIOC_PREEMPTION_CFG:
+		return axienet_preemption(dev, rq->ifr_data);
+	case SIOC_PREEMPTION_CTRL:
+		return axienet_preemption_ctrl(dev, rq->ifr_data);
+	case SIOC_PREEMPTION_STS:
+		return axienet_preemption_sts(dev, rq->ifr_data);
+	case SIOC_PREEMPTION_COUNTER:
+		return axienet_preemption_cnt(dev, rq->ifr_data);
+#ifdef CONFIG_XILINX_TSN_QBV
+	case SIOC_QBU_USER_OVERRIDE:
+		return axienet_qbu_user_override(dev, rq->ifr_data);
+	case SIOC_QBU_STS:
+		return axienet_qbu_sts(dev, rq->ifr_data);
+#endif
+#endif
+
+	default:
+		return -EOPNOTSUPP;
+	}
 }
 
 static const struct net_device_ops axienet_netdev_ops = {
+#ifdef CONFIG_XILINX_TSN
+	.ndo_open = axienet_tsn_open,
+#else
 	.ndo_open = axienet_open,
+#endif
 	.ndo_stop = axienet_stop,
+#ifdef CONFIG_XILINX_TSN
+	.ndo_start_xmit = axienet_tsn_xmit,
+#else
 	.ndo_start_xmit = axienet_start_xmit,
+#endif
 	.ndo_change_mtu	= axienet_change_mtu,
 	.ndo_set_mac_address = netdev_set_mac_address,
 	.ndo_validate_addr = eth_validate_addr,
-	.ndo_do_ioctl = axienet_ioctl,
 	.ndo_set_rx_mode = axienet_set_multicast_list,
+	.ndo_do_ioctl = axienet_ioctl,
 #ifdef CONFIG_NET_POLL_CONTROLLER
 	.ndo_poll_controller = axienet_poll_controller,
 #endif
@@ -1325,7 +2385,7 @@ static int axienet_ethtools_get_regs_len(struct net_device *ndev)
 static void axienet_ethtools_get_regs(struct net_device *ndev,
 				      struct ethtool_regs *regs, void *ret)
 {
-	u32 *data = (u32 *) ret;
+	u32 *data = (u32 *)ret;
 	size_t len = sizeof(u32) * AXIENET_REGS_N;
 	struct axienet_local *lp = netdev_priv(ndev);
 
@@ -1351,24 +2411,29 @@ static void axienet_ethtools_get_regs(struct net_device *ndev,
 	data[15] = axienet_ior(lp, XAE_TC_OFFSET);
 	data[16] = axienet_ior(lp, XAE_FCC_OFFSET);
 	data[17] = axienet_ior(lp, XAE_EMMC_OFFSET);
-	data[18] = axienet_ior(lp, XAE_PHYC_OFFSET);
+	data[18] = axienet_ior(lp, XAE_RMFC_OFFSET);
 	data[19] = axienet_ior(lp, XAE_MDIO_MC_OFFSET);
 	data[20] = axienet_ior(lp, XAE_MDIO_MCR_OFFSET);
 	data[21] = axienet_ior(lp, XAE_MDIO_MWD_OFFSET);
 	data[22] = axienet_ior(lp, XAE_MDIO_MRD_OFFSET);
+	data[23] = axienet_ior(lp, XAE_TEMAC_IS_OFFSET);
+	data[24] = axienet_ior(lp, XAE_TEMAC_IP_OFFSET);
+	data[25] = axienet_ior(lp, XAE_TEMAC_IE_OFFSET);
+	data[26] = axienet_ior(lp, XAE_TEMAC_IC_OFFSET);
 	data[27] = axienet_ior(lp, XAE_UAW0_OFFSET);
 	data[28] = axienet_ior(lp, XAE_UAW1_OFFSET);
-	data[29] = axienet_ior(lp, XAE_FMI_OFFSET);
+	data[29] = axienet_ior(lp, XAE_FMC_OFFSET);
 	data[30] = axienet_ior(lp, XAE_AF0_OFFSET);
 	data[31] = axienet_ior(lp, XAE_AF1_OFFSET);
-	data[32] = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-	data[33] = axienet_dma_in32(lp, XAXIDMA_TX_SR_OFFSET);
-	data[34] = axienet_dma_in32(lp, XAXIDMA_TX_CDESC_OFFSET);
-	data[35] = axienet_dma_in32(lp, XAXIDMA_TX_TDESC_OFFSET);
-	data[36] = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-	data[37] = axienet_dma_in32(lp, XAXIDMA_RX_SR_OFFSET);
-	data[38] = axienet_dma_in32(lp, XAXIDMA_RX_CDESC_OFFSET);
-	data[39] = axienet_dma_in32(lp, XAXIDMA_RX_TDESC_OFFSET);
+	/* Support only single DMA queue */
+	data[32] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_CR_OFFSET);
+	data[33] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_SR_OFFSET);
+	data[34] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_CDESC_OFFSET);
+	data[35] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_TDESC_OFFSET);
+	data[36] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_CR_OFFSET);
+	data[37] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_SR_OFFSET);
+	data[38] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_CDESC_OFFSET);
+	data[39] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_TDESC_OFFSET);
 }
 
 static void axienet_ethtools_get_ringparam(struct net_device *ndev,
@@ -1394,8 +2459,7 @@ static int axienet_ethtools_set_ringparam(struct net_device *ndev,
 	if (ering->rx_pending > RX_BD_NUM_MAX ||
 	    ering->rx_mini_pending ||
 	    ering->rx_jumbo_pending ||
-	    ering->tx_pending < TX_BD_NUM_MIN ||
-	    ering->tx_pending > TX_BD_NUM_MAX)
+	    ering->rx_pending > TX_BD_NUM_MAX)
 		return -EINVAL;
 
 	if (netif_running(ndev))
@@ -1419,16 +2483,20 @@ static void
 axienet_ethtools_get_pauseparam(struct net_device *ndev,
 				struct ethtool_pauseparam *epauseparm)
 {
+	u32 regval;
 	struct axienet_local *lp = netdev_priv(ndev);
 
-	phylink_ethtool_get_pauseparam(lp->phylink, epauseparm);
+	epauseparm->autoneg  = 0;
+	regval = axienet_ior(lp, XAE_FCC_OFFSET);
+	epauseparm->tx_pause = regval & XAE_FCC_FCTX_MASK;
+	epauseparm->rx_pause = regval & XAE_FCC_FCRX_MASK;
 }
 
 /**
  * axienet_ethtools_set_pauseparam - Set device pause parameter(flow control)
  *				     settings.
  * @ndev:	Pointer to net_device structure
- * @epauseparm:Pointer to ethtool_pauseparam structure
+ * @epauseparm:	Pointer to ethtool_pauseparam structure
  *
  * This implements ethtool command for enabling flow control on Rx and Tx
  * paths. Issue "ethtool -A ethX tx on|off" under linux prompt to execute this
@@ -1440,9 +2508,27 @@ static int
 axienet_ethtools_set_pauseparam(struct net_device *ndev,
 				struct ethtool_pauseparam *epauseparm)
 {
+	u32 regval = 0;
 	struct axienet_local *lp = netdev_priv(ndev);
 
-	return phylink_ethtool_set_pauseparam(lp->phylink, epauseparm);
+	if (netif_running(ndev)) {
+		netdev_err(ndev,
+			   "Please stop netif before applying configuration\n");
+		return -EFAULT;
+	}
+
+	regval = axienet_ior(lp, XAE_FCC_OFFSET);
+	if (epauseparm->tx_pause)
+		regval |= XAE_FCC_FCTX_MASK;
+	else
+		regval &= ~XAE_FCC_FCTX_MASK;
+	if (epauseparm->rx_pause)
+		regval |= XAE_FCC_FCRX_MASK;
+	else
+		regval &= ~XAE_FCC_FCRX_MASK;
+	axienet_iow(lp, XAE_FCC_OFFSET, regval);
+
+	return 0;
 }
 
 /**
@@ -1461,12 +2547,24 @@ static int axienet_ethtools_get_coalesce(struct net_device *ndev,
 {
 	u32 regval = 0;
 	struct axienet_local *lp = netdev_priv(ndev);
-	regval = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-	ecoalesce->rx_max_coalesced_frames = (regval & XAXIDMA_COALESCE_MASK)
-					     >> XAXIDMA_COALESCE_SHIFT;
-	regval = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-	ecoalesce->tx_max_coalesced_frames = (regval & XAXIDMA_COALESCE_MASK)
-					     >> XAXIDMA_COALESCE_SHIFT;
+	struct axienet_dma_q *q;
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+
+		regval = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		ecoalesce->rx_max_coalesced_frames +=
+						(regval & XAXIDMA_COALESCE_MASK)
+						     >> XAXIDMA_COALESCE_SHIFT;
+	}
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		regval = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		ecoalesce->tx_max_coalesced_frames +=
+						(regval & XAXIDMA_COALESCE_MASK)
+						     >> XAXIDMA_COALESCE_SHIFT;
+	}
 	return 0;
 }
 
@@ -1486,345 +2584,612 @@ static int axienet_ethtools_set_coalesce(struct net_device *ndev,
 {
 	struct axienet_local *lp = netdev_priv(ndev);
 
-	if (netif_running(ndev)) {
-		netdev_err(ndev,
-			   "Please stop netif before applying configuration\n");
-		return -EFAULT;
+	if (netif_running(ndev)) {
+		netdev_err(ndev,
+			   "Please stop netif before applying configuration\n");
+		return -EFAULT;
+	}
+
+	if ((ecoalesce->rx_coalesce_usecs) ||
+	    (ecoalesce->rx_coalesce_usecs_irq) ||
+	    (ecoalesce->rx_max_coalesced_frames_irq) ||
+	    (ecoalesce->tx_coalesce_usecs) ||
+	    (ecoalesce->tx_coalesce_usecs_irq) ||
+	    (ecoalesce->tx_max_coalesced_frames_irq) ||
+	    (ecoalesce->stats_block_coalesce_usecs) ||
+	    (ecoalesce->use_adaptive_rx_coalesce) ||
+	    (ecoalesce->use_adaptive_tx_coalesce) ||
+	    (ecoalesce->pkt_rate_low) ||
+	    (ecoalesce->rx_coalesce_usecs_low) ||
+	    (ecoalesce->rx_max_coalesced_frames_low) ||
+	    (ecoalesce->tx_coalesce_usecs_low) ||
+	    (ecoalesce->tx_max_coalesced_frames_low) ||
+	    (ecoalesce->pkt_rate_high) ||
+	    (ecoalesce->rx_coalesce_usecs_high) ||
+	    (ecoalesce->rx_max_coalesced_frames_high) ||
+	    (ecoalesce->tx_coalesce_usecs_high) ||
+	    (ecoalesce->tx_max_coalesced_frames_high) ||
+	    (ecoalesce->rate_sample_interval))
+		return -EOPNOTSUPP;
+	if (ecoalesce->rx_max_coalesced_frames)
+		lp->coalesce_count_rx = ecoalesce->rx_max_coalesced_frames;
+	if (ecoalesce->tx_max_coalesced_frames)
+		lp->coalesce_count_tx = ecoalesce->tx_max_coalesced_frames;
+
+	return 0;
+}
+
+#if defined(CONFIG_XILINX_AXI_EMAC_HWTSTAMP) || defined(CONFIG_XILINX_TSN_PTP)
+/**
+ * axienet_ethtools_get_ts_info - Get h/w timestamping capabilities.
+ * @ndev:	Pointer to net_device structure
+ * @info:	Pointer to ethtool_ts_info structure
+ *
+ * Return: 0, on success, Non-zero error value on failure.
+ */
+static int axienet_ethtools_get_ts_info(struct net_device *ndev,
+					struct ethtool_ts_info *info)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	info->so_timestamping = SOF_TIMESTAMPING_TX_HARDWARE |
+				SOF_TIMESTAMPING_RX_HARDWARE |
+				SOF_TIMESTAMPING_RAW_HARDWARE;
+	info->tx_types = (1 << HWTSTAMP_TX_OFF) | (1 << HWTSTAMP_TX_ON) |
+			(1 << HWTSTAMP_TX_ONESTEP_SYNC) |
+			(1 << HWTSTAMP_TX_ONESTEP_P2P);
+	info->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |
+			   (1 << HWTSTAMP_FILTER_ALL);
+	info->phc_index = lp->phc_index;
+
+#ifdef CONFIG_XILINX_TSN_PTP
+	info->phc_index = axienet_phc_index;
+#endif
+	return 0;
+}
+#endif
+
+/**
+ * axienet_ethtools_sset_count - Get number of strings that
+ *				 get_strings will write.
+ * @ndev:	Pointer to net_device structure
+ * @sset:	Get the set strings
+ *
+ * Return: number of strings, on success, Non-zero error value on
+ *	   failure.
+ */
+int axienet_ethtools_sset_count(struct net_device *ndev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		return axienet_sset_count(ndev, sset);
+#else
+		return AXIENET_ETHTOOLS_SSTATS_LEN;
+#endif
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+/**
+ * axienet_ethtools_get_stats - Get the extended statistics
+ *				about the device.
+ * @ndev:	Pointer to net_device structure
+ * @stats:	Pointer to ethtool_stats structure
+ * @data:	To store the statistics values
+ *
+ * Return: None.
+ */
+void axienet_ethtools_get_stats(struct net_device *ndev,
+				struct ethtool_stats *stats,
+				u64 *data)
+{
+	unsigned int i = 0;
+
+	data[i++] = ndev->stats.tx_packets;
+	data[i++] = ndev->stats.rx_packets;
+	data[i++] = ndev->stats.tx_bytes;
+	data[i++] = ndev->stats.rx_bytes;
+	data[i++] = ndev->stats.tx_errors;
+	data[i++] = ndev->stats.rx_missed_errors + ndev->stats.rx_frame_errors;
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	axienet_get_stats(ndev, stats, data);
+#endif
+}
+
+/**
+ * axienet_ethtools_strings - Set of strings that describe
+ *			 the requested objects.
+ * @ndev:	Pointer to net_device structure
+ * @sset:	Get the set strings
+ * @data:	Data of Transmit and Receive statistics
+ *
+ * Return: None.
+ */
+void axienet_ethtools_strings(struct net_device *ndev, u32 sset, u8 *data)
+{
+	int i;
+
+	for (i = 0; i < AXIENET_ETHTOOLS_SSTATS_LEN; i++) {
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_ethtools_strings_stats[i].name,
+			       ETH_GSTRING_LEN);
+	}
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	axienet_strings(ndev, sset, data);
+#endif
+}
+
+static const struct ethtool_ops axienet_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES,
+	.get_drvinfo    = axienet_ethtools_get_drvinfo,
+	.get_regs_len   = axienet_ethtools_get_regs_len,
+	.get_regs       = axienet_ethtools_get_regs,
+	.get_link       = ethtool_op_get_link,
+	.get_ringparam	= axienet_ethtools_get_ringparam,
+	.set_ringparam  = axienet_ethtools_set_ringparam,
+	.get_pauseparam = axienet_ethtools_get_pauseparam,
+	.set_pauseparam = axienet_ethtools_set_pauseparam,
+	.get_coalesce   = axienet_ethtools_get_coalesce,
+	.set_coalesce   = axienet_ethtools_set_coalesce,
+	.get_sset_count	= axienet_ethtools_sset_count,
+	.get_ethtool_stats = axienet_ethtools_get_stats,
+	.get_strings = axienet_ethtools_strings,
+#if defined(CONFIG_XILINX_AXI_EMAC_HWTSTAMP) || defined(CONFIG_XILINX_TSN_PTP)
+	.get_ts_info    = axienet_ethtools_get_ts_info,
+#endif
+	.get_link_ksettings = phy_ethtool_get_link_ksettings,
+	.set_link_ksettings = phy_ethtool_set_link_ksettings,
+};
+
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+static int __maybe_unused axienet_mcdma_probe(struct platform_device *pdev,
+					      struct axienet_local *lp,
+					      struct net_device *ndev)
+{
+	int i, ret = 0;
+	struct axienet_dma_q *q;
+	struct device_node *np;
+	struct resource dmares;
+	const char *str;
+
+	ret = of_property_count_strings(pdev->dev.of_node, "xlnx,channel-ids");
+	if (ret < 0)
+		return -EINVAL;
+
+	for_each_rx_dma_queue(lp, i) {
+		q = kzalloc(sizeof(*q), GFP_KERNEL);
+
+		/* parent */
+		q->lp = lp;
+		lp->dq[i] = q;
+		ret = of_property_read_string_index(pdev->dev.of_node,
+						    "xlnx,channel-ids", i,
+						    &str);
+		ret = kstrtou16(str, 16, &q->chan_id);
+		lp->qnum[i] = i;
+		lp->chan_num[i] = q->chan_id;
+	}
+
+	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected",
+			      0);
+	if (IS_ERR(np)) {
+		dev_err(&pdev->dev, "could not find DMA node\n");
+		return ret;
+	}
+
+	ret = of_address_to_resource(np, 0, &dmares);
+	if (ret) {
+		dev_err(&pdev->dev, "unable to get DMA resource\n");
+		return ret;
+	}
+
+	ret = of_property_read_u8(np, "xlnx,addrwidth", (u8 *)&lp->dma_mask);
+	if (ret < 0 || lp->dma_mask < XAE_DMA_MASK_MIN ||
+	    lp->dma_mask > XAE_DMA_MASK_MAX) {
+		dev_info(&pdev->dev, "missing/invalid xlnx,addrwidth property, using default\n");
+		lp->dma_mask = XAE_DMA_MASK_MIN;
+	}
+
+	lp->mcdma_regs = devm_ioremap_resource(&pdev->dev, &dmares);
+	if (IS_ERR(lp->mcdma_regs)) {
+		dev_err(&pdev->dev, "iormeap failed for the dma\n");
+		ret = PTR_ERR(lp->mcdma_regs);
+		return ret;
+	}
+
+	axienet_mcdma_tx_probe(pdev, np, lp);
+	axienet_mcdma_rx_probe(pdev, lp, ndev);
+
+	return 0;
+}
+#endif
+
+static int __maybe_unused axienet_dma_probe(struct platform_device *pdev,
+					    struct net_device *ndev)
+{
+	int i, ret;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	struct device_node *np = NULL;
+	struct resource dmares;
+
+	for_each_rx_dma_queue(lp, i) {
+		q = devm_kzalloc(&pdev->dev, sizeof(*q), GFP_KERNEL);
+		if (!q)
+			return -ENOMEM;
+
+		/* parent */
+		q->lp = lp;
+
+		lp->dq[i] = q;
+	}
+
+	/* Find the DMA node, map the DMA registers, and decode the DMA IRQs */
+	/* TODO handle error ret */
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+
+		np = of_parse_phandle(pdev->dev.of_node, "axistream-connected",
+				      i);
+		if (np) {
+			ret = of_address_to_resource(np, 0, &dmares);
+			if (ret >= 0) {
+				q->dma_regs = devm_ioremap_resource(&pdev->dev,
+								    &dmares);
+			} else {
+				dev_err(&pdev->dev, "unable to get DMA resource for %pOF\n",
+					np);
+				return -ENODEV;
+			}
+			q->eth_hasdre = of_property_read_bool(np,
+							      "xlnx,include-dre");
+			ret = of_property_read_u8(np, "xlnx,addrwidth",
+						  (u8 *)&lp->dma_mask);
+			if (ret <  0 || lp->dma_mask < XAE_DMA_MASK_MIN ||
+			    lp->dma_mask > XAE_DMA_MASK_MAX) {
+				dev_info(&pdev->dev, "missing/invalid xlnx,addrwidth property, using default\n");
+				lp->dma_mask = XAE_DMA_MASK_MIN;
+			}
+
+		} else {
+			dev_err(&pdev->dev, "missing axistream-connected property\n");
+			return -EINVAL;
+		}
+		lp->dq[i]->tx_irq = irq_of_parse_and_map(np, 0);
+		lp->dq[i]->rx_irq = irq_of_parse_and_map(np, 1);
+
+	}
+
+	of_node_put(np);
+
+	for_each_rx_dma_queue(lp, i) {
+		struct axienet_dma_q *q = lp->dq[i];
+
+		spin_lock_init(&q->tx_lock);
+		spin_lock_init(&q->rx_lock);
 	}
 
-	if (ecoalesce->rx_max_coalesced_frames)
-		lp->coalesce_count_rx = ecoalesce->rx_max_coalesced_frames;
-	if (ecoalesce->tx_max_coalesced_frames)
-		lp->coalesce_count_tx = ecoalesce->tx_max_coalesced_frames;
+	for_each_rx_dma_queue(lp, i) {
+		netif_napi_add(ndev, &lp->napi[i], xaxienet_rx_poll,
+			       XAXIENET_NAPI_WEIGHT);
+	}
 
 	return 0;
 }
 
-static int
-axienet_ethtools_get_link_ksettings(struct net_device *ndev,
-				    struct ethtool_link_ksettings *cmd)
+static int axienet_clk_init(struct platform_device *pdev,
+			    struct clk **axi_aclk, struct clk **axis_clk,
+			    struct clk **ref_clk, struct clk **tmpclk)
 {
-	struct axienet_local *lp = netdev_priv(ndev);
+	int err;
 
-	return phylink_ethtool_ksettings_get(lp->phylink, cmd);
-}
+	*tmpclk = NULL;
 
-static int
-axienet_ethtools_set_link_ksettings(struct net_device *ndev,
-				    const struct ethtool_link_ksettings *cmd)
-{
-	struct axienet_local *lp = netdev_priv(ndev);
+	/* The "ethernet_clk" is deprecated and will be removed sometime in
+	 * the future. For proper clock usage check axiethernet binding
+	 * documentation.
+	 */
+	*axi_aclk = devm_clk_get(&pdev->dev, "ethernet_clk");
+	if (IS_ERR(*axi_aclk)) {
+		if (PTR_ERR(*axi_aclk) != -ENOENT) {
+			err = PTR_ERR(*axi_aclk);
+			return err;
+		}
 
-	return phylink_ethtool_ksettings_set(lp->phylink, cmd);
-}
+		*axi_aclk = devm_clk_get(&pdev->dev, "s_axi_lite_clk");
+		if (IS_ERR(*axi_aclk)) {
+			if (PTR_ERR(*axi_aclk) != -ENOENT) {
+				err = PTR_ERR(*axi_aclk);
+				return err;
+			}
+			*axi_aclk = NULL;
+		}
 
-static const struct ethtool_ops axienet_ethtool_ops = {
-	.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES,
-	.get_drvinfo    = axienet_ethtools_get_drvinfo,
-	.get_regs_len   = axienet_ethtools_get_regs_len,
-	.get_regs       = axienet_ethtools_get_regs,
-	.get_link       = ethtool_op_get_link,
-	.get_ringparam	= axienet_ethtools_get_ringparam,
-	.set_ringparam	= axienet_ethtools_set_ringparam,
-	.get_pauseparam = axienet_ethtools_get_pauseparam,
-	.set_pauseparam = axienet_ethtools_set_pauseparam,
-	.get_coalesce   = axienet_ethtools_get_coalesce,
-	.set_coalesce   = axienet_ethtools_set_coalesce,
-	.get_link_ksettings = axienet_ethtools_get_link_ksettings,
-	.set_link_ksettings = axienet_ethtools_set_link_ksettings,
-};
+	} else {
+		dev_warn(&pdev->dev, "ethernet_clk is deprecated and will be removed sometime in the future\n");
+	}
 
-static void axienet_validate(struct phylink_config *config,
-			     unsigned long *supported,
-			     struct phylink_link_state *state)
-{
-	struct net_device *ndev = to_net_dev(config->dev);
-	struct axienet_local *lp = netdev_priv(ndev);
-	__ETHTOOL_DECLARE_LINK_MODE_MASK(mask) = { 0, };
-
-	/* Only support the mode we are configured for */
-	if (state->interface != PHY_INTERFACE_MODE_NA &&
-	    state->interface != lp->phy_mode) {
-		netdev_warn(ndev, "Cannot use PHY mode %s, supported: %s\n",
-			    phy_modes(state->interface),
-			    phy_modes(lp->phy_mode));
-		bitmap_zero(supported, __ETHTOOL_LINK_MODE_MASK_NBITS);
-		return;
+	*axis_clk = devm_clk_get(&pdev->dev, "axis_clk");
+	if (IS_ERR(*axis_clk)) {
+		if (PTR_ERR(*axis_clk) != -ENOENT) {
+			err = PTR_ERR(*axis_clk);
+			return err;
+		}
+		*axis_clk = NULL;
 	}
 
-	phylink_set(mask, Autoneg);
-	phylink_set_port_modes(mask);
-
-	phylink_set(mask, Asym_Pause);
-	phylink_set(mask, Pause);
-
-	switch (state->interface) {
-	case PHY_INTERFACE_MODE_NA:
-	case PHY_INTERFACE_MODE_1000BASEX:
-	case PHY_INTERFACE_MODE_SGMII:
-	case PHY_INTERFACE_MODE_GMII:
-	case PHY_INTERFACE_MODE_RGMII:
-	case PHY_INTERFACE_MODE_RGMII_ID:
-	case PHY_INTERFACE_MODE_RGMII_RXID:
-	case PHY_INTERFACE_MODE_RGMII_TXID:
-		phylink_set(mask, 1000baseX_Full);
-		phylink_set(mask, 1000baseT_Full);
-		if (state->interface == PHY_INTERFACE_MODE_1000BASEX)
-			break;
-		fallthrough;
-	case PHY_INTERFACE_MODE_MII:
-		phylink_set(mask, 100baseT_Full);
-		phylink_set(mask, 10baseT_Full);
-	default:
-		break;
+	*ref_clk = devm_clk_get(&pdev->dev, "ref_clk");
+	if (IS_ERR(*ref_clk)) {
+		if (PTR_ERR(*ref_clk) != -ENOENT) {
+			err = PTR_ERR(*ref_clk);
+			return err;
+		}
+		*ref_clk = NULL;
 	}
 
-	bitmap_and(supported, supported, mask,
-		   __ETHTOOL_LINK_MODE_MASK_NBITS);
-	bitmap_and(state->advertising, state->advertising, mask,
-		   __ETHTOOL_LINK_MODE_MASK_NBITS);
-}
+	err = clk_prepare_enable(*axi_aclk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axi_aclk/ethernet_clk (%d)\n", err);
+		return err;
+	}
 
-static void axienet_mac_pcs_get_state(struct phylink_config *config,
-				      struct phylink_link_state *state)
-{
-	struct net_device *ndev = to_net_dev(config->dev);
-	struct axienet_local *lp = netdev_priv(ndev);
+	err = clk_prepare_enable(*axis_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axis_clk (%d)\n", err);
+		goto err_disable_axi_aclk;
+	}
 
-	switch (state->interface) {
-	case PHY_INTERFACE_MODE_SGMII:
-	case PHY_INTERFACE_MODE_1000BASEX:
-		phylink_mii_c22_pcs_get_state(lp->pcs_phy, state);
-		break;
-	default:
-		break;
+	err = clk_prepare_enable(*ref_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable ref_clk (%d)\n", err);
+		goto err_disable_axis_clk;
 	}
-}
 
-static void axienet_mac_an_restart(struct phylink_config *config)
-{
-	struct net_device *ndev = to_net_dev(config->dev);
-	struct axienet_local *lp = netdev_priv(ndev);
+	return 0;
 
-	phylink_mii_c22_pcs_an_restart(lp->pcs_phy);
+err_disable_axis_clk:
+	clk_disable_unprepare(*axis_clk);
+err_disable_axi_aclk:
+	clk_disable_unprepare(*axi_aclk);
+
+	return err;
 }
 
-static void axienet_mac_config(struct phylink_config *config, unsigned int mode,
-			       const struct phylink_link_state *state)
+static int axienet_dma_clk_init(struct platform_device *pdev)
 {
-	struct net_device *ndev = to_net_dev(config->dev);
+	int err;
+	struct net_device *ndev = platform_get_drvdata(pdev);
 	struct axienet_local *lp = netdev_priv(ndev);
-	int ret;
 
-	switch (state->interface) {
-	case PHY_INTERFACE_MODE_SGMII:
-	case PHY_INTERFACE_MODE_1000BASEX:
-		ret = phylink_mii_c22_pcs_config(lp->pcs_phy, mode,
-						 state->interface,
-						 state->advertising);
-		if (ret < 0)
-			netdev_warn(ndev, "Failed to configure PCS: %d\n",
-				    ret);
-		break;
+	/* The "dma_clk" is deprecated and will be removed sometime in
+	 * the future. For proper clock usage check axiethernet binding
+	 * documentation.
+	 */
+	lp->dma_tx_clk = devm_clk_get(&pdev->dev, "dma_clk");
+	if (IS_ERR(lp->dma_tx_clk)) {
+		if (PTR_ERR(lp->dma_tx_clk) != -ENOENT) {
+			err = PTR_ERR(lp->dma_tx_clk);
+			return err;
+		}
 
-	default:
-		break;
+		lp->dma_tx_clk = devm_clk_get(&pdev->dev, "m_axi_mm2s_aclk");
+		if (IS_ERR(lp->dma_tx_clk)) {
+			if (PTR_ERR(lp->dma_tx_clk) != -ENOENT) {
+				err = PTR_ERR(lp->dma_tx_clk);
+				return err;
+			}
+			lp->dma_tx_clk = NULL;
+		}
+	} else {
+		dev_warn(&pdev->dev, "dma_clk is deprecated and will be removed sometime in the future\n");
 	}
-}
 
-static void axienet_mac_link_down(struct phylink_config *config,
-				  unsigned int mode,
-				  phy_interface_t interface)
-{
-	/* nothing meaningful to do */
-}
+	lp->dma_rx_clk = devm_clk_get(&pdev->dev, "m_axi_s2mm_aclk");
+	if (IS_ERR(lp->dma_rx_clk)) {
+		if (PTR_ERR(lp->dma_rx_clk) != -ENOENT) {
+			err = PTR_ERR(lp->dma_rx_clk);
+			return err;
+		}
+		lp->dma_rx_clk = NULL;
+	}
 
-static void axienet_mac_link_up(struct phylink_config *config,
-				struct phy_device *phy,
-				unsigned int mode, phy_interface_t interface,
-				int speed, int duplex,
-				bool tx_pause, bool rx_pause)
-{
-	struct net_device *ndev = to_net_dev(config->dev);
-	struct axienet_local *lp = netdev_priv(ndev);
-	u32 emmc_reg, fcc_reg;
+	lp->dma_sg_clk = devm_clk_get(&pdev->dev, "m_axi_sg_aclk");
+	if (IS_ERR(lp->dma_sg_clk)) {
+		if (PTR_ERR(lp->dma_sg_clk) != -ENOENT) {
+			err = PTR_ERR(lp->dma_sg_clk);
+			return err;
+		}
+		lp->dma_sg_clk = NULL;
+	}
 
-	emmc_reg = axienet_ior(lp, XAE_EMMC_OFFSET);
-	emmc_reg &= ~XAE_EMMC_LINKSPEED_MASK;
+	err = clk_prepare_enable(lp->dma_tx_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable tx_clk/dma_clk (%d)\n", err);
+		return err;
+	}
 
-	switch (speed) {
-	case SPEED_1000:
-		emmc_reg |= XAE_EMMC_LINKSPD_1000;
-		break;
-	case SPEED_100:
-		emmc_reg |= XAE_EMMC_LINKSPD_100;
-		break;
-	case SPEED_10:
-		emmc_reg |= XAE_EMMC_LINKSPD_10;
-		break;
-	default:
-		dev_err(&ndev->dev,
-			"Speed other than 10, 100 or 1Gbps is not supported\n");
-		break;
+	err = clk_prepare_enable(lp->dma_rx_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable rx_clk (%d)\n", err);
+		goto err_disable_txclk;
+	}
+
+	err = clk_prepare_enable(lp->dma_sg_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable sg_clk (%d)\n", err);
+		goto err_disable_rxclk;
 	}
 
-	axienet_iow(lp, XAE_EMMC_OFFSET, emmc_reg);
+	return 0;
 
-	fcc_reg = axienet_ior(lp, XAE_FCC_OFFSET);
-	if (tx_pause)
-		fcc_reg |= XAE_FCC_FCTX_MASK;
-	else
-		fcc_reg &= ~XAE_FCC_FCTX_MASK;
-	if (rx_pause)
-		fcc_reg |= XAE_FCC_FCRX_MASK;
-	else
-		fcc_reg &= ~XAE_FCC_FCRX_MASK;
-	axienet_iow(lp, XAE_FCC_OFFSET, fcc_reg);
+err_disable_rxclk:
+	clk_disable_unprepare(lp->dma_rx_clk);
+err_disable_txclk:
+	clk_disable_unprepare(lp->dma_tx_clk);
+
+	return err;
 }
 
-static const struct phylink_mac_ops axienet_phylink_ops = {
-	.validate = axienet_validate,
-	.mac_pcs_get_state = axienet_mac_pcs_get_state,
-	.mac_an_restart = axienet_mac_an_restart,
-	.mac_config = axienet_mac_config,
-	.mac_link_down = axienet_mac_link_down,
-	.mac_link_up = axienet_mac_link_up,
-};
+static void axienet_clk_disable(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct axienet_local *lp = netdev_priv(ndev);
 
-/**
- * axienet_dma_err_handler - Work queue task for Axi DMA Error
- * @work:	pointer to work_struct
- *
- * Resets the Axi DMA and Axi Ethernet devices, and reconfigures the
- * Tx/Rx BDs.
- */
-static void axienet_dma_err_handler(struct work_struct *work)
+	clk_disable_unprepare(lp->dma_sg_clk);
+	clk_disable_unprepare(lp->dma_tx_clk);
+	clk_disable_unprepare(lp->dma_rx_clk);
+	clk_disable_unprepare(lp->eth_sclk);
+	clk_disable_unprepare(lp->eth_refclk);
+	clk_disable_unprepare(lp->eth_dclk);
+	clk_disable_unprepare(lp->aclk);
+}
+
+static int xxvenet_clk_init(struct platform_device *pdev,
+			    struct clk **axi_aclk, struct clk **axis_clk,
+			    struct clk **tmpclk, struct clk **dclk)
 {
-	u32 axienet_status;
-	u32 cr, i;
-	struct axienet_local *lp = container_of(work, struct axienet_local,
-						dma_err_task);
-	struct net_device *ndev = lp->ndev;
-	struct axidma_bd *cur_p;
+	int err;
 
-	axienet_setoptions(ndev, lp->options &
-			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
-	/* Disable the MDIO interface till Axi Ethernet Reset is completed.
-	 * When we do an Axi Ethernet reset, it resets the complete core
-	 * including the MDIO. MDIO must be disabled before resetting
-	 * and re-enabled afterwards.
-	 * Hold MDIO bus lock to avoid MDIO accesses during the reset.
+	*tmpclk = NULL;
+
+	/* The "ethernet_clk" is deprecated and will be removed sometime in
+	 * the future. For proper clock usage check axiethernet binding
+	 * documentation.
 	 */
-	mutex_lock(&lp->mii_bus->mdio_lock);
-	axienet_mdio_disable(lp);
-	__axienet_device_reset(lp);
-	axienet_mdio_enable(lp);
-	mutex_unlock(&lp->mii_bus->mdio_lock);
-
-	for (i = 0; i < lp->tx_bd_num; i++) {
-		cur_p = &lp->tx_bd_v[i];
-		if (cur_p->cntrl) {
-			dma_addr_t addr = desc_get_phys_addr(lp, cur_p);
-
-			dma_unmap_single(ndev->dev.parent, addr,
-					 (cur_p->cntrl &
-					  XAXIDMA_BD_CTRL_LENGTH_MASK),
-					 DMA_TO_DEVICE);
+	*axi_aclk = devm_clk_get(&pdev->dev, "ethernet_clk");
+	if (IS_ERR(*axi_aclk)) {
+		if (PTR_ERR(*axi_aclk) != -ENOENT) {
+			err = PTR_ERR(*axi_aclk);
+			return err;
 		}
-		if (cur_p->skb)
-			dev_kfree_skb_irq(cur_p->skb);
-		cur_p->phys = 0;
-		cur_p->phys_msb = 0;
-		cur_p->cntrl = 0;
-		cur_p->status = 0;
-		cur_p->app0 = 0;
-		cur_p->app1 = 0;
-		cur_p->app2 = 0;
-		cur_p->app3 = 0;
-		cur_p->app4 = 0;
-		cur_p->skb = NULL;
+
+		*axi_aclk = devm_clk_get(&pdev->dev, "s_axi_aclk");
+		if (IS_ERR(*axi_aclk)) {
+			if (PTR_ERR(*axi_aclk) != -ENOENT) {
+				err = PTR_ERR(*axi_aclk);
+				return err;
+			}
+			*axi_aclk = NULL;
+		}
+
+	} else {
+		dev_warn(&pdev->dev, "ethernet_clk is deprecated and will be removed sometime in the future\n");
 	}
 
-	for (i = 0; i < lp->rx_bd_num; i++) {
-		cur_p = &lp->rx_bd_v[i];
-		cur_p->status = 0;
-		cur_p->app0 = 0;
-		cur_p->app1 = 0;
-		cur_p->app2 = 0;
-		cur_p->app3 = 0;
-		cur_p->app4 = 0;
+	*axis_clk = devm_clk_get(&pdev->dev, "rx_core_clk");
+	if (IS_ERR(*axis_clk)) {
+		if (PTR_ERR(*axis_clk) != -ENOENT) {
+			err = PTR_ERR(*axis_clk);
+			return err;
+		}
+		*axis_clk = NULL;
 	}
 
-	lp->tx_bd_ci = 0;
-	lp->tx_bd_tail = 0;
-	lp->rx_bd_ci = 0;
-
-	/* Start updating the Rx channel control register */
-	cr = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-	/* Update the interrupt coalesce count */
-	cr = ((cr & ~XAXIDMA_COALESCE_MASK) |
-	      (XAXIDMA_DFT_RX_THRESHOLD << XAXIDMA_COALESCE_SHIFT));
-	/* Update the delay timer count */
-	cr = ((cr & ~XAXIDMA_DELAY_MASK) |
-	      (XAXIDMA_DFT_RX_WAITBOUND << XAXIDMA_DELAY_SHIFT));
-	/* Enable coalesce, delay timer and error interrupts */
-	cr |= XAXIDMA_IRQ_ALL_MASK;
-	/* Finally write to the Rx channel control register */
-	axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET, cr);
-
-	/* Start updating the Tx channel control register */
-	cr = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-	/* Update the interrupt coalesce count */
-	cr = (((cr & ~XAXIDMA_COALESCE_MASK)) |
-	      (XAXIDMA_DFT_TX_THRESHOLD << XAXIDMA_COALESCE_SHIFT));
-	/* Update the delay timer count */
-	cr = (((cr & ~XAXIDMA_DELAY_MASK)) |
-	      (XAXIDMA_DFT_TX_WAITBOUND << XAXIDMA_DELAY_SHIFT));
-	/* Enable coalesce, delay timer and error interrupts */
-	cr |= XAXIDMA_IRQ_ALL_MASK;
-	/* Finally write to the Tx channel control register */
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET, cr);
-
-	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
-	 * halted state. This will make the Rx side ready for reception.
-	 */
-	axienet_dma_out_addr(lp, XAXIDMA_RX_CDESC_OFFSET, lp->rx_bd_p);
-	cr = axienet_dma_in32(lp, XAXIDMA_RX_CR_OFFSET);
-	axienet_dma_out32(lp, XAXIDMA_RX_CR_OFFSET,
-			  cr | XAXIDMA_CR_RUNSTOP_MASK);
-	axienet_dma_out_addr(lp, XAXIDMA_RX_TDESC_OFFSET, lp->rx_bd_p +
-			     (sizeof(*lp->rx_bd_v) * (lp->rx_bd_num - 1)));
-
-	/* Write to the RS (Run-stop) bit in the Tx channel control register.
-	 * Tx channel is now ready to run. But only after we write to the
-	 * tail pointer register that the Tx channel will start transmitting
-	 */
-	axienet_dma_out_addr(lp, XAXIDMA_TX_CDESC_OFFSET, lp->tx_bd_p);
-	cr = axienet_dma_in32(lp, XAXIDMA_TX_CR_OFFSET);
-	axienet_dma_out32(lp, XAXIDMA_TX_CR_OFFSET,
-			  cr | XAXIDMA_CR_RUNSTOP_MASK);
-
-	axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
-	axienet_status &= ~XAE_RCW1_RX_MASK;
-	axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
-
-	axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
-	if (axienet_status & XAE_INT_RXRJECT_MASK)
-		axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
-	axienet_iow(lp, XAE_IE_OFFSET, lp->eth_irq > 0 ?
-		    XAE_INT_RECV_ERROR_MASK : 0);
-	axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
-
-	/* Sync default options with HW but leave receiver and
-	 * transmitter disabled.
-	 */
-	axienet_setoptions(ndev, lp->options &
-			   ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
-	axienet_set_mac_address(ndev, NULL);
-	axienet_set_multicast_list(ndev);
-	axienet_setoptions(ndev, lp->options);
+	*dclk = devm_clk_get(&pdev->dev, "dclk");
+	if (IS_ERR(*dclk)) {
+		if (PTR_ERR(*dclk) != -ENOENT) {
+			err = PTR_ERR(*dclk);
+			return err;
+		}
+		*dclk = NULL;
+	}
+
+	err = clk_prepare_enable(*axi_aclk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axi_clk/ethernet_clk (%d)\n", err);
+		return err;
+	}
+
+	err = clk_prepare_enable(*axis_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axis_clk (%d)\n", err);
+		goto err_disable_axi_aclk;
+	}
+
+	err = clk_prepare_enable(*dclk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable dclk (%d)\n", err);
+		goto err_disable_axis_clk;
+	}
+
+	return 0;
+
+err_disable_axis_clk:
+	clk_disable_unprepare(*axis_clk);
+err_disable_axi_aclk:
+	clk_disable_unprepare(*axi_aclk);
+
+	return err;
 }
 
+static const struct axienet_config axienet_1g_config = {
+	.mactype = XAXIENET_1G,
+	.setoptions = axienet_setoptions,
+	.clk_init = axienet_clk_init,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+static const struct axienet_config axienet_2_5g_config = {
+	.mactype = XAXIENET_2_5G,
+	.setoptions = axienet_setoptions,
+	.clk_init = axienet_clk_init,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+static const struct axienet_config axienet_10g_config = {
+	.mactype = XAXIENET_LEGACY_10G,
+	.setoptions = axienet_setoptions,
+	.clk_init = xxvenet_clk_init,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+static const struct axienet_config axienet_10g25g_config = {
+	.mactype = XAXIENET_10G_25G,
+	.setoptions = xxvenet_setoptions,
+	.clk_init = xxvenet_clk_init,
+	.tx_ptplen = XXV_TX_PTP_LEN,
+	.ts_header_len = XXVENET_TS_HEADER_LEN,
+};
+
+static const struct axienet_config axienet_usxgmii_config = {
+	.mactype = XAXIENET_10G_25G,
+	.setoptions = xxvenet_setoptions,
+	.clk_init = xxvenet_clk_init,
+	.tx_ptplen = 0,
+};
+
+static const struct axienet_config axienet_mrmac_config = {
+	.mactype = XAXIENET_MRMAC,
+	.setoptions = xxvenet_setoptions,
+	.clk_init = xxvenet_clk_init,
+	.tx_ptplen = XXV_TX_PTP_LEN,
+	.ts_header_len = MRMAC_TS_HEADER_LEN,
+};
+
+/* Match table for of_platform binding */
+static const struct of_device_id axienet_of_match[] = {
+	{ .compatible = "xlnx,axi-ethernet-1.00.a", .data = &axienet_1g_config},
+	{ .compatible = "xlnx,axi-ethernet-1.01.a", .data = &axienet_1g_config},
+	{ .compatible = "xlnx,axi-ethernet-2.01.a", .data = &axienet_1g_config},
+	{ .compatible = "xlnx,axi-2_5-gig-ethernet-1.0",
+						.data = &axienet_2_5g_config},
+	{ .compatible = "xlnx,ten-gig-eth-mac", .data = &axienet_10g_config},
+	{ .compatible = "xlnx,xxv-ethernet-1.0",
+						.data = &axienet_10g25g_config},
+	{ .compatible = "xlnx,tsn-ethernet-1.00.a", .data = &axienet_1g_config},
+	{ .compatible = "xlnx,xxv-usxgmii-ethernet-1.0",
+					.data = &axienet_usxgmii_config},
+	{ .compatible = "xlnx,mrmac-ethernet-1.0",
+					.data = &axienet_mrmac_config},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, axienet_of_match);
+
 /**
  * axienet_probe - Axi Ethernet probe function.
  * @pdev:	Pointer to platform device structure.
@@ -1839,20 +3204,52 @@ static void axienet_dma_err_handler(struct work_struct *work)
  */
 static int axienet_probe(struct platform_device *pdev)
 {
-	int ret;
+	int (*axienet_clk_init)(struct platform_device *pdev,
+				struct clk **axi_aclk, struct clk **axis_clk,
+				struct clk **ref_clk, struct clk **tmpclk) =
+					axienet_clk_init;
+	int ret = 0;
 	struct device_node *np;
 	struct axienet_local *lp;
 	struct net_device *ndev;
 	const void *mac_addr;
 	struct resource *ethres;
-	int addr_width = 32;
 	u32 value;
+	u16 num_queues = XAE_MAX_QUEUES;
+	bool is_tsn = false;
+
+	is_tsn = of_property_read_bool(pdev->dev.of_node, "xlnx,tsn");
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-queues",
+				   &num_queues);
+	if (ret) {
+		if (!is_tsn) {
+#ifndef CONFIG_AXIENET_HAS_MCDMA
+			num_queues = 1;
+#endif
+		}
+	}
+#ifdef CONFIG_XILINX_TSN
+	if (is_tsn && (num_queues < XAE_TSN_MIN_QUEUES ||
+		       num_queues > XAE_MAX_QUEUES))
+		num_queues = XAE_MAX_QUEUES;
+#endif
 
-	ndev = alloc_etherdev(sizeof(*lp));
+	ndev = alloc_etherdev_mq(sizeof(*lp), num_queues);
 	if (!ndev)
 		return -ENOMEM;
 
 	platform_set_drvdata(pdev, ndev);
+#ifdef CONFIG_XILINX_TSN
+	bool slave = false;
+	if (is_tsn) {
+		slave = of_property_read_bool(pdev->dev.of_node,
+					      "xlnx,tsn-slave");
+		if (slave)
+			snprintf(ndev->name, sizeof(ndev->name), "eth2");
+		else
+			snprintf(ndev->name, sizeof(ndev->name), "eth1");
+	}
+#endif
 
 	SET_NETDEV_DEV(ndev, &pdev->dev);
 	ndev->flags &= ~IFF_MULTICAST;  /* clear multicast */
@@ -1868,49 +3265,59 @@ static int axienet_probe(struct platform_device *pdev)
 	lp->ndev = ndev;
 	lp->dev = &pdev->dev;
 	lp->options = XAE_OPTION_DEFAULTS;
+	lp->num_tx_queues = num_queues;
+	lp->num_rx_queues = num_queues;
+	lp->is_tsn = is_tsn;
 	lp->rx_bd_num = RX_BD_NUM_DEFAULT;
 	lp->tx_bd_num = TX_BD_NUM_DEFAULT;
 
-	lp->clk = devm_clk_get_optional(&pdev->dev, NULL);
-	if (IS_ERR(lp->clk)) {
-		ret = PTR_ERR(lp->clk);
-		goto free_netdev;
-	}
-	ret = clk_prepare_enable(lp->clk);
-	if (ret) {
-		dev_err(&pdev->dev, "Unable to enable clock: %d\n", ret);
-		goto free_netdev;
-	}
+#ifdef CONFIG_XILINX_TSN
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc",
+				   &lp->num_tc);
+	if (ret || (lp->num_tc != 2 && lp->num_tc != 3))
+		lp->num_tc = XAE_MAX_TSN_TC;
+#endif
 
 	/* Map device registers */
 	ethres = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	lp->regs = devm_ioremap_resource(&pdev->dev, ethres);
 	if (IS_ERR(lp->regs)) {
-		dev_err(&pdev->dev, "could not map Axi Ethernet regs.\n");
 		ret = PTR_ERR(lp->regs);
-		goto cleanup_clk;
+		goto free_netdev;
 	}
 	lp->regs_start = ethres->start;
 
 	/* Setup checksum offload, but default to off if not specified */
 	lp->features = 0;
 
+	if (pdev->dev.of_node) {
+		const struct of_device_id *match;
+
+		match = of_match_node(axienet_of_match, pdev->dev.of_node);
+		if (match && match->data) {
+			lp->axienet_config = match->data;
+			axienet_clk_init = lp->axienet_config->clk_init;
+		}
+	}
+
 	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,txcsum", &value);
 	if (!ret) {
+		dev_info(&pdev->dev, "TX_CSUM %d\n", value);
+
 		switch (value) {
 		case 1:
 			lp->csum_offload_on_tx_path =
 				XAE_FEATURE_PARTIAL_TX_CSUM;
 			lp->features |= XAE_FEATURE_PARTIAL_TX_CSUM;
 			/* Can checksum TCP/UDP over IPv4. */
-			ndev->features |= NETIF_F_IP_CSUM;
+			ndev->features |= NETIF_F_IP_CSUM | NETIF_F_SG;
 			break;
 		case 2:
 			lp->csum_offload_on_tx_path =
 				XAE_FEATURE_FULL_TX_CSUM;
 			lp->features |= XAE_FEATURE_FULL_TX_CSUM;
 			/* Can checksum TCP/UDP over IPv4. */
-			ndev->features |= NETIF_F_IP_CSUM;
+			ndev->features |= NETIF_F_IP_CSUM | NETIF_F_SG;
 			break;
 		default:
 			lp->csum_offload_on_tx_path = XAE_NO_CSUM_OFFLOAD;
@@ -1918,6 +3325,8 @@ static int axienet_probe(struct platform_device *pdev)
 	}
 	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,rxcsum", &value);
 	if (!ret) {
+		dev_info(&pdev->dev, "RX_CSUM %d\n", value);
+
 		switch (value) {
 		case 1:
 			lp->csum_offload_on_rx_path =
@@ -1941,104 +3350,213 @@ static int axienet_probe(struct platform_device *pdev)
 	 */
 	of_property_read_u32(pdev->dev.of_node, "xlnx,rxmem", &lp->rxmem);
 
-	/* Start with the proprietary, and broken phy_type */
-	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,phy-type", &value);
-	if (!ret) {
-		netdev_warn(ndev, "Please upgrade your device tree binary blob to use phy-mode");
-		switch (value) {
-		case XAE_PHY_TYPE_MII:
-			lp->phy_mode = PHY_INTERFACE_MODE_MII;
-			break;
-		case XAE_PHY_TYPE_GMII:
-			lp->phy_mode = PHY_INTERFACE_MODE_GMII;
-			break;
-		case XAE_PHY_TYPE_RGMII_2_0:
-			lp->phy_mode = PHY_INTERFACE_MODE_RGMII_ID;
-			break;
-		case XAE_PHY_TYPE_SGMII:
-			lp->phy_mode = PHY_INTERFACE_MODE_SGMII;
-			break;
-		case XAE_PHY_TYPE_1000BASE_X:
-			lp->phy_mode = PHY_INTERFACE_MODE_1000BASEX;
-			break;
-		default:
-			ret = -EINVAL;
-			goto cleanup_clk;
+	/* The phy_mode is optional but when it is not specified it should not
+	 *  be a value that alters the driver behavior so set it to an invalid
+	 *  value as the default.
+	 */
+	lp->phy_mode = PHY_INTERFACE_MODE_NA;
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,phy-type", &lp->phy_mode);
+	if (!ret)
+		netdev_warn(ndev, "xlnx,phy-type is deprecated, Please upgrade your device tree to use phy-mode");
+
+	/* Set default USXGMII rate */
+	lp->usxgmii_rate = SPEED_1000;
+	of_property_read_u32(pdev->dev.of_node, "xlnx,usxgmii-rate",
+			     &lp->usxgmii_rate);
+
+	/* Set default MRMAC rate */
+	lp->mrmac_rate = SPEED_10000;
+	of_property_read_u32(pdev->dev.of_node, "xlnx,mrmac-rate",
+			     &lp->mrmac_rate);
+
+	lp->eth_hasnobuf = of_property_read_bool(pdev->dev.of_node,
+						 "xlnx,eth-hasnobuf");
+	lp->eth_hasptp = of_property_read_bool(pdev->dev.of_node,
+					       "xlnx,eth-hasptp");
+
+	if ((lp->axienet_config->mactype == XAXIENET_1G) && !lp->eth_hasnobuf)
+		lp->eth_irq = platform_get_irq(pdev, 0);
+
+	if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		struct resource gtpll, gtctrl;
+
+		if (mrmac_pll_reg) {
+			lp->gt_pll = mrmac_gt_pll;
+			lp->gt_ctrl = mrmac_gt_ctrl;
+		} else {
+			np = of_parse_phandle(pdev->dev.of_node,
+					      "xlnx,gtpll", 0);
+			if (IS_ERR(np)) {
+				dev_err(&pdev->dev,
+					"couldn't find GT PLL\n");
+				ret = PTR_ERR(np);
+				goto free_netdev;
+			}
+
+			ret = of_address_to_resource(np, 0, &gtpll);
+			if (ret) {
+				dev_err(&pdev->dev,
+					"unable to get GT PLL resource\n");
+				goto free_netdev;
+			}
+
+			lp->gt_pll = devm_ioremap_resource(&pdev->dev,
+							   &gtpll);
+			if (IS_ERR(lp->gt_pll)) {
+				dev_err(&pdev->dev,
+					"couldn't map GT PLL regs\n");
+				ret = PTR_ERR(lp->gt_pll);
+				goto free_netdev;
+			}
+
+			np = of_parse_phandle(pdev->dev.of_node,
+					      "xlnx,gtctrl", 0);
+			if (IS_ERR(np)) {
+				dev_err(&pdev->dev,
+					"couldn't find GT control\n");
+				ret = PTR_ERR(np);
+				goto free_netdev;
+			}
+
+			ret = of_address_to_resource(np, 0, &gtctrl);
+			if (ret) {
+				dev_err(&pdev->dev,
+					"unable to get GT control resource\n");
+				goto free_netdev;
+			}
+
+			lp->gt_ctrl = devm_ioremap_resource(&pdev->dev,
+							    &gtctrl);
+			if (IS_ERR(lp->gt_ctrl)) {
+				dev_err(&pdev->dev,
+					"couldn't map GT control regs\n");
+				ret = PTR_ERR(lp->gt_ctrl);
+				goto free_netdev;
+			}
+
+			mrmac_gt_pll = lp->gt_pll;
+			mrmac_gt_ctrl = lp->gt_ctrl;
+			mrmac_pll_reg = 1;
 		}
-	} else {
-		ret = of_get_phy_mode(pdev->dev.of_node, &lp->phy_mode);
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+		ret = of_property_read_u32(pdev->dev.of_node, "xlnx,phcindex",
+					   &lp->phc_index);
 		if (ret)
-			goto cleanup_clk;
+			dev_warn(&pdev->dev, "No phc index defaulting to 0\n");
+#endif
+		ret = of_property_read_u32(pdev->dev.of_node, "xlnx,gtlane",
+					   &lp->gt_lane);
+		if (ret) {
+			dev_err(&pdev->dev, "MRMAC GT lane information missing\n");
+			goto free_netdev;
+		}
+		dev_info(&pdev->dev, "GT lane: %d\n", lp->gt_lane);
 	}
 
-	/* Find the DMA node, map the DMA registers, and decode the DMA IRQs */
-	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected", 0);
-	if (np) {
-		struct resource dmares;
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	if (!lp->is_tsn) {
+		struct resource txtsres, rxtsres;
+
+		/* Find AXI Stream FIFO */
+		np = of_parse_phandle(pdev->dev.of_node, "axififo-connected",
+				      0);
+		if (IS_ERR(np)) {
+			dev_err(&pdev->dev, "could not find TX Timestamp FIFO\n");
+			ret = PTR_ERR(np);
+			goto free_netdev;
+		}
 
-		ret = of_address_to_resource(np, 0, &dmares);
+		ret = of_address_to_resource(np, 0, &txtsres);
 		if (ret) {
 			dev_err(&pdev->dev,
-				"unable to get DMA resource\n");
-			of_node_put(np);
-			goto cleanup_clk;
-		}
-		lp->dma_regs = devm_ioremap_resource(&pdev->dev,
-						     &dmares);
-		lp->rx_irq = irq_of_parse_and_map(np, 1);
-		lp->tx_irq = irq_of_parse_and_map(np, 0);
+				"unable to get Tx Timestamp resource\n");
+			goto free_netdev;
+		}
+
+		lp->tx_ts_regs = devm_ioremap_resource(&pdev->dev, &txtsres);
+		if (IS_ERR(lp->tx_ts_regs)) {
+			dev_err(&pdev->dev, "could not map Tx Timestamp regs\n");
+			ret = PTR_ERR(lp->tx_ts_regs);
+			goto free_netdev;
+		}
+
+		if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+		    lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			np = of_parse_phandle(pdev->dev.of_node,
+					      "xlnx,rxtsfifo", 0);
+			if (IS_ERR(np)) {
+				dev_err(&pdev->dev,
+					"couldn't find rx-timestamp FIFO\n");
+				ret = PTR_ERR(np);
+				goto free_netdev;
+			}
+
+			ret = of_address_to_resource(np, 0, &rxtsres);
+			if (ret) {
+				dev_err(&pdev->dev,
+					"unable to get rx-timestamp resource\n");
+				goto free_netdev;
+			}
+
+			lp->rx_ts_regs = devm_ioremap_resource(&pdev->dev,
+								&rxtsres);
+			if (IS_ERR(lp->rx_ts_regs)) {
+				dev_err(&pdev->dev,
+					"couldn't map rx-timestamp regs\n");
+				ret = PTR_ERR(lp->rx_ts_regs);
+				goto free_netdev;
+			}
+
+			lp->tx_ptpheader = devm_kzalloc(&pdev->dev,
+							lp->axienet_config->ts_header_len,
+							GFP_KERNEL);
+			spin_lock_init(&lp->ptp_tx_lock);
+		}
+
 		of_node_put(np);
-		lp->eth_irq = platform_get_irq_optional(pdev, 0);
-	} else {
-		/* Check for these resources directly on the Ethernet node. */
-		struct resource *res = platform_get_resource(pdev,
-							     IORESOURCE_MEM, 1);
-		lp->dma_regs = devm_ioremap_resource(&pdev->dev, res);
-		lp->rx_irq = platform_get_irq(pdev, 1);
-		lp->tx_irq = platform_get_irq(pdev, 0);
-		lp->eth_irq = platform_get_irq_optional(pdev, 2);
-	}
-	if (IS_ERR(lp->dma_regs)) {
-		dev_err(&pdev->dev, "could not map DMA regs\n");
-		ret = PTR_ERR(lp->dma_regs);
-		goto cleanup_clk;
-	}
-	if ((lp->rx_irq <= 0) || (lp->tx_irq <= 0)) {
-		dev_err(&pdev->dev, "could not determine irqs\n");
-		ret = -ENOMEM;
-		goto cleanup_clk;
-	}
-
-	/* Autodetect the need for 64-bit DMA pointers.
-	 * When the IP is configured for a bus width bigger than 32 bits,
-	 * writing the MSB registers is mandatory, even if they are all 0.
-	 * We can detect this case by writing all 1's to one such register
-	 * and see if that sticks: when the IP is configured for 32 bits
-	 * only, those registers are RES0.
-	 * Those MSB registers were introduced in IP v7.1, which we check first.
-	 */
-	if ((axienet_ior(lp, XAE_ID_OFFSET) >> 24) >= 0x9) {
-		void __iomem *desc = lp->dma_regs + XAXIDMA_TX_CDESC_OFFSET + 4;
-
-		iowrite32(0x0, desc);
-		if (ioread32(desc) == 0) {	/* sanity check */
-			iowrite32(0xffffffff, desc);
-			if (ioread32(desc) > 0) {
-				lp->features |= XAE_FEATURE_DMA_64BIT;
-				addr_width = 64;
-				dev_info(&pdev->dev,
-					 "autodetected 64-bit DMA range\n");
+	}
+#endif
+
+#ifdef CONFIG_XILINX_TSN
+	if (lp->is_tsn)
+		ret = axienet_tsn_probe(pdev, lp, ndev);
+#endif
+	if (!lp->is_tsn) {
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+		ret = axienet_mcdma_probe(pdev, lp, ndev);
+#else
+		ret = axienet_dma_probe(pdev, ndev);
+#endif
+		if (ret) {
+			pr_err("Getting DMA resource failed\n");
+			goto free_netdev;
+		}
+
+		if (dma_set_mask_and_coherent(lp->dev, DMA_BIT_MASK(lp->dma_mask)) != 0) {
+			dev_warn(&pdev->dev, "default to %d-bit dma mask\n", XAE_DMA_MASK_MIN);
+			if (dma_set_mask_and_coherent(lp->dev, DMA_BIT_MASK(XAE_DMA_MASK_MIN)) != 0) {
+				dev_err(&pdev->dev, "dma_set_mask_and_coherent failed, aborting\n");
+				goto free_netdev;
 			}
-			iowrite32(0x0, desc);
+		}
+
+		ret = axienet_dma_clk_init(pdev);
+		if (ret) {
+			if (ret != -EPROBE_DEFER)
+				dev_err(&pdev->dev, "DMA clock init failed %d\n", ret);
+			goto free_netdev;
 		}
 	}
 
-	ret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(addr_width));
+	ret = axienet_clk_init(pdev, &lp->aclk, &lp->eth_sclk,
+			       &lp->eth_refclk, &lp->eth_dclk);
 	if (ret) {
-		dev_err(&pdev->dev, "No suitable DMA available\n");
-		goto cleanup_clk;
+		if (ret != -EPROBE_DEFER)
+			dev_err(&pdev->dev, "Ethernet clock init failed %d\n", ret);
+		goto err_disable_clk;
 	}
 
+	lp->eth_irq = platform_get_irq(pdev, 0);
 	/* Check for Ethernet core IRQ (optional) */
 	if (lp->eth_irq <= 0)
 		dev_info(&pdev->dev, "Ethernet core IRQ not defined\n");
@@ -2055,65 +3573,54 @@ static int axienet_probe(struct platform_device *pdev)
 	lp->coalesce_count_rx = XAXIDMA_DFT_RX_THRESHOLD;
 	lp->coalesce_count_tx = XAXIDMA_DFT_TX_THRESHOLD;
 
-	/* Reset core now that clocks are enabled, prior to accessing MDIO */
-	ret = __axienet_device_reset(lp);
-	if (ret)
-		goto cleanup_clk;
-
-	ret = axienet_mdio_setup(lp);
-	if (ret)
-		dev_warn(&pdev->dev,
-			 "error registering MDIO bus: %d\n", ret);
-
-	if (lp->phy_mode == PHY_INTERFACE_MODE_SGMII ||
-	    lp->phy_mode == PHY_INTERFACE_MODE_1000BASEX) {
-		lp->phy_node = of_parse_phandle(pdev->dev.of_node, "phy-handle", 0);
-		if (!lp->phy_node) {
-			dev_err(&pdev->dev, "phy-handle required for 1000BaseX/SGMII\n");
-			ret = -EINVAL;
-			goto cleanup_mdio;
-		}
-		lp->pcs_phy = of_mdio_find_device(lp->phy_node);
-		if (!lp->pcs_phy) {
-			ret = -EPROBE_DEFER;
-			goto cleanup_mdio;
+	ret = of_get_phy_mode(pdev->dev.of_node, &lp->phy_mode);
+	if (ret < 0)
+		dev_warn(&pdev->dev, "couldn't find phy i/f\n");
+	if (lp->phy_mode == PHY_INTERFACE_MODE_1000BASEX)
+		lp->phy_flags = XAE_PHY_TYPE_1000BASE_X;
+
+	lp->phy_node = of_parse_phandle(pdev->dev.of_node, "phy-handle", 0);
+	if (lp->phy_node) {
+		lp->clk = devm_clk_get(&pdev->dev, NULL);
+		if (IS_ERR(lp->clk)) {
+			dev_warn(&pdev->dev, "Failed to get clock: %ld\n",
+				 PTR_ERR(lp->clk));
+			lp->clk = NULL;
+		} else {
+			ret = clk_prepare_enable(lp->clk);
+			if (ret) {
+				dev_err(&pdev->dev, "Unable to enable clock: %d\n",
+					ret);
+				goto free_netdev;
+			}
 		}
-		lp->phylink_config.pcs_poll = true;
-	}
 
-	lp->phylink_config.dev = &ndev->dev;
-	lp->phylink_config.type = PHYLINK_NETDEV;
+		ret = axienet_mdio_setup(lp);
+		if (ret)
+			dev_warn(&pdev->dev,
+				 "error registering MDIO bus: %d\n", ret);
+	}
 
-	lp->phylink = phylink_create(&lp->phylink_config, pdev->dev.fwnode,
-				     lp->phy_mode,
-				     &axienet_phylink_ops);
-	if (IS_ERR(lp->phylink)) {
-		ret = PTR_ERR(lp->phylink);
-		dev_err(&pdev->dev, "phylink_create error (%i)\n", ret);
-		goto cleanup_mdio;
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	/* Create sysfs file entries for the device */
+	ret = axeinet_mcdma_create_sysfs(&lp->dev->kobj);
+	if (ret < 0) {
+		dev_err(lp->dev, "unable to create sysfs entries\n");
+		return ret;
 	}
+#endif
 
 	ret = register_netdev(lp->ndev);
 	if (ret) {
 		dev_err(lp->dev, "register_netdev() error (%i)\n", ret);
-		goto cleanup_phylink;
+		axienet_mdio_teardown(lp);
+		goto err_disable_clk;
 	}
 
 	return 0;
 
-cleanup_phylink:
-	phylink_destroy(lp->phylink);
-
-cleanup_mdio:
-	if (lp->pcs_phy)
-		put_device(&lp->pcs_phy->dev);
-	if (lp->mii_bus)
-		axienet_mdio_teardown(lp);
-	of_node_put(lp->phy_node);
-
-cleanup_clk:
-	clk_disable_unprepare(lp->clk);
-
+err_disable_clk:
+	axienet_clk_disable(pdev);
 free_netdev:
 	free_netdev(ndev);
 
@@ -2124,19 +3631,30 @@ static int axienet_remove(struct platform_device *pdev)
 {
 	struct net_device *ndev = platform_get_drvdata(pdev);
 	struct axienet_local *lp = netdev_priv(ndev);
+	int i;
 
+	if (!lp->is_tsn) {
+		for_each_rx_dma_queue(lp, i)
+			netif_napi_del(&lp->napi[i]);
+	}
+#ifdef CONFIG_XILINX_TSN_PTP
+		axienet_ptp_timer_remove(lp->timer_priv);
+#ifdef CONFIG_XILINX_TSN_QBV
+		axienet_qbv_remove(ndev);
+#endif
+#endif
 	unregister_netdev(ndev);
+	axienet_clk_disable(pdev);
 
-	if (lp->phylink)
-		phylink_destroy(lp->phylink);
-
-	if (lp->pcs_phy)
-		put_device(&lp->pcs_phy->dev);
-
-	axienet_mdio_teardown(lp);
+	if (lp->mii_bus)
+		axienet_mdio_teardown(lp);
 
-	clk_disable_unprepare(lp->clk);
+	if (lp->clk)
+		clk_disable_unprepare(lp->clk);
 
+#ifdef CONFIG_AXIENET_HAS_MCDMA
+	axeinet_mcdma_remove_sysfs(&lp->dev->kobj);
+#endif
 	of_node_put(lp->phy_node);
 	lp->phy_node = NULL;
 
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_mcdma.c b/drivers/net/ethernet/xilinx/xilinx_axienet_mcdma.c
new file mode 100644
index 000000000..1d4e65084
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_mcdma.c
@@ -0,0 +1,1086 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx AXI Ethernet (MCDMA programming)
+ *
+ * Copyright (c) 2008 Nissin Systems Co., Ltd.,  Yoshio Kashiwagi
+ * Copyright (c) 2005-2008 DLA Systems,  David H. Lynch Jr. <dhlii@dlasys.net>
+ * Copyright (c) 2008-2009 Secret Lab Technologies Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2010 - 2012 Xilinx, Inc.
+ * Copyright (C) 2018 Xilinx, Inc. All rights reserved.
+ *
+ * This file contains helper functions for AXI MCDMA TX and RX programming.
+ */
+
+#include <linux/module.h>
+#include <linux/of_mdio.h>
+#include <linux/of_platform.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+
+#include "xilinx_axienet.h"
+
+struct axienet_stat {
+	const char *name;
+};
+
+#ifdef CONFIG_XILINX_TSN
+/* TODO
+ * The channel numbers for managemnet frames in 5 channel mcdma on EP+Switch
+ * system. These are not exposed via hdf/dtsi, so need to hardcode here
+ */
+#define TSN_MAX_RX_Q_EPSWITCH 5
+#define TSN_MGMT_CHAN0 2
+#define TSN_MGMT_CHAN1 3
+#endif
+
+static struct axienet_stat axienet_get_tx_strings_stats[] = {
+	{ "txq0_packets" },
+	{ "txq0_bytes"   },
+	{ "txq1_packets" },
+	{ "txq1_bytes"   },
+	{ "txq2_packets" },
+	{ "txq2_bytes"   },
+	{ "txq3_packets" },
+	{ "txq3_bytes"   },
+	{ "txq4_packets" },
+	{ "txq4_bytes"   },
+	{ "txq5_packets" },
+	{ "txq5_bytes"   },
+	{ "txq6_packets" },
+	{ "txq6_bytes"   },
+	{ "txq7_packets" },
+	{ "txq7_bytes"   },
+	{ "txq8_packets" },
+	{ "txq8_bytes"   },
+	{ "txq9_packets" },
+	{ "txq9_bytes"   },
+	{ "txq10_packets" },
+	{ "txq10_bytes"   },
+	{ "txq11_packets" },
+	{ "txq11_bytes"   },
+	{ "txq12_packets" },
+	{ "txq12_bytes"   },
+	{ "txq13_packets" },
+	{ "txq13_bytes"   },
+	{ "txq14_packets" },
+	{ "txq14_bytes"   },
+	{ "txq15_packets" },
+	{ "txq15_bytes"   },
+};
+
+static struct axienet_stat axienet_get_rx_strings_stats[] = {
+	{ "rxq0_packets" },
+	{ "rxq0_bytes"   },
+	{ "rxq1_packets" },
+	{ "rxq1_bytes"   },
+	{ "rxq2_packets" },
+	{ "rxq2_bytes"   },
+	{ "rxq3_packets" },
+	{ "rxq3_bytes"   },
+	{ "rxq4_packets" },
+	{ "rxq4_bytes"   },
+	{ "rxq5_packets" },
+	{ "rxq5_bytes"   },
+	{ "rxq6_packets" },
+	{ "rxq6_bytes"   },
+	{ "rxq7_packets" },
+	{ "rxq7_bytes"   },
+	{ "rxq8_packets" },
+	{ "rxq8_bytes"   },
+	{ "rxq9_packets" },
+	{ "rxq9_bytes"   },
+	{ "rxq10_packets" },
+	{ "rxq10_bytes"   },
+	{ "rxq11_packets" },
+	{ "rxq11_bytes"   },
+	{ "rxq12_packets" },
+	{ "rxq12_bytes"   },
+	{ "rxq13_packets" },
+	{ "rxq13_bytes"   },
+	{ "rxq14_packets" },
+	{ "rxq14_bytes"   },
+	{ "rxq15_packets" },
+	{ "rxq15_bytes"   },
+};
+
+/**
+ * axienet_mcdma_tx_bd_free - Release MCDMA Tx buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_mcdma_tx_q_init.
+ */
+void __maybe_unused axienet_mcdma_tx_bd_free(struct net_device *ndev,
+					     struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (q->txq_bd_v) {
+		dma_free_coherent(ndev->dev.parent,
+				  sizeof(*q->txq_bd_v) * lp->tx_bd_num,
+				  q->txq_bd_v,
+				  q->tx_bd_p);
+	}
+	if (q->tx_bufs) {
+		dma_free_coherent(ndev->dev.parent,
+				  XAE_MAX_PKT_LEN * lp->tx_bd_num,
+				  q->tx_bufs,
+				  q->tx_bufs_dma);
+	}
+}
+
+/**
+ * axienet_mcdma_rx_bd_free - Release MCDMA Rx buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_mcdma_rx_q_init.
+ */
+void __maybe_unused axienet_mcdma_rx_bd_free(struct net_device *ndev,
+					     struct axienet_dma_q *q)
+{
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (!q->rxq_bd_v)
+		return;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		if (q->rxq_bd_v[i].phys)
+			dma_unmap_single(ndev->dev.parent, q->rxq_bd_v[i].phys,
+					 lp->max_frm_size, DMA_FROM_DEVICE);
+		dev_kfree_skb((struct sk_buff *)
+			      (q->rxq_bd_v[i].sw_id_offset));
+	}
+
+	dma_free_coherent(ndev->dev.parent,
+			  sizeof(*q->rxq_bd_v) * lp->rx_bd_num,
+			  q->rxq_bd_v,
+			  q->rx_bd_p);
+	q->rxq_bd_v = NULL;
+}
+
+/**
+ * axienet_mcdma_tx_q_init - Setup buffer descriptor rings for individual Axi
+ * MCDMA-Tx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_mcdma_tx_q_init(struct net_device *ndev,
+					   struct axienet_dma_q *q)
+{
+	u32 cr, chan_en;
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+
+	q->txq_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					 sizeof(*q->txq_bd_v) * lp->tx_bd_num,
+					 &q->tx_bd_p, GFP_KERNEL);
+	if (!q->txq_bd_v)
+		goto out;
+
+	if (!q->eth_hasdre) {
+		q->tx_bufs = dma_alloc_coherent(ndev->dev.parent,
+						XAE_MAX_PKT_LEN * lp->tx_bd_num,
+						&q->tx_bufs_dma,
+						GFP_KERNEL);
+		if (!q->tx_bufs)
+			goto out;
+
+		for (i = 0; i < lp->tx_bd_num; i++)
+			q->tx_buf[i] = &q->tx_bufs[i * XAE_MAX_PKT_LEN];
+	}
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		q->txq_bd_v[i].next = q->tx_bd_p +
+				      sizeof(*q->txq_bd_v) *
+				      ((i + 1) % lp->tx_bd_num);
+	}
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XMCDMA_COALESCE_MASK)) |
+	      ((lp->coalesce_count_tx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = (((cr & ~XMCDMA_DELAY_MASK)) |
+	      (XAXIDMA_DFT_TX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Tx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id),
+			  q->tx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET, chan_en);
+
+	return 0;
+out:
+	for_each_tx_dma_queue(lp, i) {
+		axienet_mcdma_tx_bd_free(ndev, lp->dq[i]);
+	}
+	return -ENOMEM;
+}
+
+/**
+ * axienet_mcdma_rx_q_init - Setup buffer descriptor rings for individual Axi
+ * MCDMA-Rx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_mcdma_rx_q_init(struct net_device *ndev,
+					   struct axienet_dma_q *q)
+{
+	u32 cr, chan_en;
+	int i;
+	struct sk_buff *skb;
+	struct axienet_local *lp = netdev_priv(ndev);
+	dma_addr_t mapping;
+
+	q->rx_bd_ci = 0;
+	q->rx_offset = XMCDMA_CHAN_RX_OFFSET;
+
+	q->rxq_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					 sizeof(*q->rxq_bd_v) * lp->rx_bd_num,
+					 &q->rx_bd_p, GFP_KERNEL);
+	if (!q->rxq_bd_v)
+		goto out;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		q->rxq_bd_v[i].next = q->rx_bd_p +
+				      sizeof(*q->rxq_bd_v) *
+				      ((i + 1) % lp->rx_bd_num);
+
+		skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+		if (!skb)
+			goto out;
+
+		/* Ensure that the skb is completely updated
+		 * prio to mapping the DMA
+		 */
+		wmb();
+
+		q->rxq_bd_v[i].sw_id_offset = (phys_addr_t)skb;
+		mapping = dma_map_single(ndev->dev.parent,
+					 skb->data,
+					 lp->max_frm_size,
+					 DMA_FROM_DEVICE);
+		if (unlikely(dma_mapping_error(ndev->dev.parent, mapping))) {
+			dev_err(&ndev->dev, "mcdma map error\n");
+			goto out;
+		}
+
+		q->rxq_bd_v[i].phys = mapping;
+		q->rxq_bd_v[i].cntrl = lp->max_frm_size;
+	}
+
+#ifdef CONFIG_XILINX_TSN
+	/* check if this is a mgmt channel */
+	if (lp->num_rx_queues == TSN_MAX_RX_Q_EPSWITCH) {
+		if (q->chan_id == TSN_MGMT_CHAN0)
+			q->flags |= (MCDMA_MGMT_CHAN | MCDMA_MGMT_CHAN_PORT0);
+		else if (q->chan_id == TSN_MGMT_CHAN1)
+			q->flags |= (MCDMA_MGMT_CHAN | MCDMA_MGMT_CHAN_PORT1);
+	}
+#endif
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			      q->rx_offset);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XMCDMA_COALESCE_MASK) |
+	      ((lp->coalesce_count_rx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = ((cr & ~XMCDMA_DELAY_MASK) |
+	      (XAXIDMA_DFT_RX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Rx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			  q->rx_offset, cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET +  q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET +  q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) + q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p + (sizeof(*q->rxq_bd_v) *
+			    (lp->rx_bd_num - 1)));
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET + q->rx_offset);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET + q->rx_offset, chan_en);
+
+	return 0;
+
+out:
+	for_each_rx_dma_queue(lp, i) {
+		axienet_mcdma_rx_bd_free(ndev, lp->dq[i]);
+	}
+	return -ENOMEM;
+}
+
+static inline int get_mcdma_tx_q(struct axienet_local *lp, u32 chan_id)
+{
+	int i;
+
+	for_each_tx_dma_queue(lp, i) {
+		if (chan_id == lp->chan_num[i])
+			return lp->qnum[i];
+	}
+
+	return -ENODEV;
+}
+
+static inline int get_mcdma_rx_q(struct axienet_local *lp, u32 chan_id)
+{
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		if (chan_id == lp->chan_num[i])
+			return lp->qnum[i];
+	}
+
+	return -ENODEV;
+}
+
+static inline int map_dma_q_txirq(int irq, struct axienet_local *lp)
+{
+	int i, chan_sermask;
+	u16 chan_id = 1;
+	struct axienet_dma_q *q = lp->dq[0];
+
+	chan_sermask = axienet_dma_in32(q, XMCDMA_TXINT_SER_OFFSET);
+
+	for (i = 1, chan_id = 1; i != 0 && i <= chan_sermask;
+	     i <<= 1, chan_id++) {
+		if (chan_sermask & i)
+			return chan_id;
+	}
+
+	return -ENODEV;
+}
+
+irqreturn_t __maybe_unused axienet_mcdma_tx_irq(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i, j = map_dma_q_txirq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (j < 0)
+		return IRQ_NONE;
+
+	i = get_mcdma_tx_q(lp, j);
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id));
+	if (status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) {
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id), status);
+		axienet_start_xmit_done(lp->ndev, q);
+		goto out;
+	}
+	if (!(status & XMCDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+	if (status & XMCDMA_IRQ_ERR_MASK) {
+		dev_err(&ndev->dev, "DMA Tx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: %pa\n",
+			&q->txq_bd_v[q->tx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* write to the Rx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+	}
+out:
+	return IRQ_HANDLED;
+}
+
+static inline int map_dma_q_rxirq(int irq, struct axienet_local *lp)
+{
+	int i, chan_sermask;
+	u16 chan_id = 1;
+	struct axienet_dma_q *q = lp->dq[0];
+
+	chan_sermask = axienet_dma_in32(q, XMCDMA_RXINT_SER_OFFSET +
+					q->rx_offset);
+
+	for (i = 1, chan_id = 1; i != 0 && i <= chan_sermask;
+		i <<= 1, chan_id++) {
+		if (chan_sermask & i)
+			return chan_id;
+	}
+
+	return -ENODEV;
+}
+
+irqreturn_t __maybe_unused axienet_mcdma_rx_irq(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i, j = map_dma_q_rxirq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (j < 0)
+		return IRQ_NONE;
+
+	i = get_mcdma_rx_q(lp, j);
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset);
+	if (status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) {
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		cr &= ~(XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+		napi_schedule(&lp->napi[i]);
+	}
+
+	if (!(status & XMCDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	if (status & XMCDMA_IRQ_ERR_MASK) {
+		dev_err(&ndev->dev, "DMA Rx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: %pa\n",
+			&q->rxq_bd_v[q->rx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* write to the Rx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+	}
+
+	return IRQ_HANDLED;
+}
+
+void axienet_strings(struct net_device *ndev, u32 sset, u8 *data)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	int i = AXIENET_ETHTOOLS_SSTATS_LEN, j, k = 0;
+
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_tx_queues)
+			break;
+		q = lp->dq[j];
+		if (i % 2 == 0)
+			k = (q->chan_id - 1) * 2;
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_tx_strings_stats[k].name,
+			       ETH_GSTRING_LEN);
+		++i;
+		k++;
+		if (i % 2 == 0)
+			++j;
+	}
+	k = 0;
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_rx_queues)
+			break;
+		q = lp->dq[j];
+		if (i % 2 == 0)
+			k = (q->chan_id - 1) * 2;
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_rx_strings_stats[k].name,
+			       ETH_GSTRING_LEN);
+		++i;
+		k++;
+		if (i % 2 == 0)
+			++j;
+	}
+}
+
+int axienet_sset_count(struct net_device *ndev, int sset)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	switch (sset) {
+	case ETH_SS_STATS:
+		return (AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+void axienet_get_stats(struct net_device *ndev,
+		       struct ethtool_stats *stats,
+		       u64 *data)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	unsigned int i = AXIENET_ETHTOOLS_SSTATS_LEN, j;
+
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_tx_queues)
+			break;
+
+		q = lp->dq[j];
+		data[i++] = q->tx_packets;
+		data[i++] = q->tx_bytes;
+		++j;
+	}
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_rx_queues)
+			break;
+
+		q = lp->dq[j];
+		data[i++] = q->rx_packets;
+		data[i++] = q->rx_bytes;
+		++j;
+	}
+}
+
+/**
+ * axienet_mcdma_err_handler - Tasklet handler for Axi MCDMA Error
+ * @data:	Data passed
+ *
+ * Resets the Axi MCDMA and Axi Ethernet devices, and reconfigures the
+ * Tx/Rx BDs.
+ */
+void __maybe_unused axienet_mcdma_err_handler(unsigned long data)
+{
+	u32 axienet_status;
+	u32 cr, i, chan_en;
+	struct axienet_dma_q *q = (struct axienet_dma_q *)data;
+	struct axienet_local *lp = q->lp;
+	struct net_device *ndev = lp->ndev;
+	struct aximcdma_bd *cur_p;
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	__axienet_device_reset(q);
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		cur_p = &q->txq_bd_v[i];
+		if (cur_p->phys)
+			dma_unmap_single(ndev->dev.parent, cur_p->phys,
+					 (cur_p->cntrl &
+					  XAXIDMA_BD_CTRL_LENGTH_MASK),
+					 DMA_TO_DEVICE);
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq((struct sk_buff *)cur_p->tx_skb);
+		cur_p->phys = 0;
+		cur_p->cntrl = 0;
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+		cur_p->sw_id_offset = 0;
+		cur_p->tx_skb = 0;
+	}
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		cur_p = &q->rxq_bd_v[i];
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+	}
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+	q->rx_bd_ci = 0;
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			      q->rx_offset);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XMCDMA_COALESCE_MASK) |
+	      ((lp->coalesce_count_rx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = ((cr & ~XMCDMA_DELAY_MASK) |
+	      (XAXIDMA_DFT_RX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Rx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			  q->rx_offset, cr);
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XMCDMA_COALESCE_MASK)) |
+	      ((lp->coalesce_count_tx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = (((cr & ~XMCDMA_DELAY_MASK)) |
+	      (XAXIDMA_DFT_TX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Tx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET +  q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET +  q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) + q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p + (sizeof(*q->rxq_bd_v) *
+			    (lp->rx_bd_num - 1)));
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET + q->rx_offset);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET + q->rx_offset, chan_en);
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id),
+			  q->tx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET, chan_en);
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC) {
+		axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+		axienet_status &= ~XAE_RCW1_RX_MASK;
+		axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_1G && !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+
+#ifdef CONFIG_XILINX_AXI_EMAC_HWTSTAMP
+	if (lp->axienet_config->mactype == XAXIENET_10G_25G ||
+	    lp->axienet_config->mactype == XAXIENET_MRMAC) {
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_rxts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_RDFR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+		axienet_txts_iow(lp, XAXIFIFO_TXTS_SRR,
+				 XAXIFIFO_TXTS_RESET_MASK);
+	}
+#endif
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	axienet_set_mac_address(ndev, NULL);
+	axienet_set_multicast_list(ndev);
+	lp->axienet_config->setoptions(ndev, lp->options);
+}
+
+int __maybe_unused axienet_mcdma_tx_probe(struct platform_device *pdev,
+					  struct device_node *np,
+					  struct axienet_local *lp)
+{
+	int i;
+	char dma_name[24];
+	int ret = 0;
+
+#ifdef CONFIG_XILINX_TSN
+	u32 num = XAE_TSN_MIN_QUEUES;
+	/* get number of associated queues */
+	ret = of_property_read_u32(np, "xlnx,num-mm2s-channels", &num);
+	if (ret)
+		num = XAE_TSN_MIN_QUEUES;
+	lp->num_tx_queues = num;
+#endif
+
+	for_each_tx_dma_queue(lp, i) {
+		struct axienet_dma_q *q;
+
+		q = lp->dq[i];
+
+		q->dma_regs = lp->mcdma_regs;
+		snprintf(dma_name, sizeof(dma_name), "mm2s_ch%d_introut",
+			 q->chan_id);
+		q->tx_irq = platform_get_irq_byname(pdev, dma_name);
+#ifdef CONFIG_XILINX_TSN
+		q->eth_hasdre = of_property_read_bool(np,
+						      "xlnx,include-mm2s-dre");
+#else
+		q->eth_hasdre = of_property_read_bool(np,
+						      "xlnx,include-dre");
+#endif
+		spin_lock_init(&q->tx_lock);
+	}
+	of_node_put(np);
+
+	return 0;
+}
+
+int __maybe_unused axienet_mcdma_rx_probe(struct platform_device *pdev,
+					  struct axienet_local *lp,
+					  struct net_device *ndev)
+{
+	int i;
+	char dma_name[24];
+
+	for_each_rx_dma_queue(lp, i) {
+		struct axienet_dma_q *q;
+
+		q = lp->dq[i];
+
+		q->dma_regs = lp->mcdma_regs;
+		snprintf(dma_name, sizeof(dma_name), "s2mm_ch%d_introut",
+			 q->chan_id);
+		q->rx_irq = platform_get_irq_byname(pdev, dma_name);
+
+		spin_lock_init(&q->rx_lock);
+
+		netif_napi_add(ndev, &lp->napi[i], xaxienet_rx_poll,
+			       XAXIENET_NAPI_WEIGHT);
+	}
+
+	return 0;
+}
+
+static ssize_t rxch_obs1_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS1_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 1 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs2_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS2_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 2 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs3_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS3_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 3 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs4_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS4_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 4 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs5_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS5_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 5 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs6_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS6_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 6 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs1_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS1_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 1 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t txch_obs2_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS2_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 2 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs3_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS3_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 3 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs4_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS4_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 4 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs5_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS5_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 5 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs6_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS6_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 6 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t chan_weight_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	return sprintf(buf, "chan_id is %d and weight is %d\n",
+		       lp->chan_id, lp->weight);
+}
+
+static ssize_t chan_weight_store(struct device *dev,
+				 struct device_attribute *attr,
+				 const char *buf, size_t count)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	int ret;
+	u16 flags, chan_id;
+	u32 val;
+
+	ret = kstrtou16(buf, 16, &flags);
+	if (ret)
+		return ret;
+
+	lp->chan_id = (flags & 0xF0) >> 4;
+	lp->weight = flags & 0x0F;
+
+	if (lp->chan_id < 8)
+		val = axienet_dma_in32(q, XMCDMA_TXWEIGHT0_OFFSET);
+	else
+		val = axienet_dma_in32(q, XMCDMA_TXWEIGHT1_OFFSET);
+
+	if (lp->chan_id > 7)
+		chan_id = lp->chan_id - 8;
+	else
+		chan_id = lp->chan_id;
+
+	val &= ~XMCDMA_TXWEIGHT_CH_MASK(chan_id);
+	val |= lp->weight << XMCDMA_TXWEIGHT_CH_SHIFT(chan_id);
+
+	if (lp->chan_id < 8)
+		axienet_dma_out32(q, XMCDMA_TXWEIGHT0_OFFSET, val);
+	else
+		axienet_dma_out32(q, XMCDMA_TXWEIGHT1_OFFSET, val);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(chan_weight);
+static DEVICE_ATTR_RO(rxch_obs1);
+static DEVICE_ATTR_RO(rxch_obs2);
+static DEVICE_ATTR_RO(rxch_obs3);
+static DEVICE_ATTR_RO(rxch_obs4);
+static DEVICE_ATTR_RO(rxch_obs5);
+static DEVICE_ATTR_RO(rxch_obs6);
+static DEVICE_ATTR_RO(txch_obs1);
+static DEVICE_ATTR_RO(txch_obs2);
+static DEVICE_ATTR_RO(txch_obs3);
+static DEVICE_ATTR_RO(txch_obs4);
+static DEVICE_ATTR_RO(txch_obs5);
+static DEVICE_ATTR_RO(txch_obs6);
+static const struct attribute *mcdma_attrs[] = {
+	&dev_attr_chan_weight.attr,
+	&dev_attr_rxch_obs1.attr,
+	&dev_attr_rxch_obs2.attr,
+	&dev_attr_rxch_obs3.attr,
+	&dev_attr_rxch_obs4.attr,
+	&dev_attr_rxch_obs5.attr,
+	&dev_attr_rxch_obs6.attr,
+	&dev_attr_txch_obs1.attr,
+	&dev_attr_txch_obs2.attr,
+	&dev_attr_txch_obs3.attr,
+	&dev_attr_txch_obs4.attr,
+	&dev_attr_txch_obs5.attr,
+	&dev_attr_txch_obs6.attr,
+	NULL,
+};
+
+static const struct attribute_group mcdma_attributes = {
+	.attrs = (struct attribute **)mcdma_attrs,
+};
+
+int axeinet_mcdma_create_sysfs(struct kobject *kobj)
+{
+	return sysfs_create_group(kobj, &mcdma_attributes);
+}
+
+void axeinet_mcdma_remove_sysfs(struct kobject *kobj)
+{
+	sysfs_remove_group(kobj, &mcdma_attributes);
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_axienet_mdio.c b/drivers/net/ethernet/xilinx/xilinx_axienet_mdio.c
index 435ed308d..c8587c171 100644
--- a/drivers/net/ethernet/xilinx/xilinx_axienet_mdio.c
+++ b/drivers/net/ethernet/xilinx/xilinx_axienet_mdio.c
@@ -21,7 +21,7 @@
 #define DEFAULT_HOST_CLOCK	150000000 /* 150 MHz */
 
 /* Wait till MDIO interface is ready to accept a new transaction.*/
-static int axienet_mdio_wait_until_ready(struct axienet_local *lp)
+int axienet_mdio_wait_until_ready(struct axienet_local *lp)
 {
 	u32 val;
 
@@ -30,6 +30,23 @@ static int axienet_mdio_wait_until_ready(struct axienet_local *lp)
 				  1, 20000);
 }
 
+/* Enable the MDIO MDC. Called prior to a read/write operation */
+static void axienet_mdio_mdc_enable(struct axienet_local *lp)
+{
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET,
+		    ((u32)lp->mii_clk_div | XAE_MDIO_MC_MDIOEN_MASK));
+}
+
+/* Disable the MDIO MDC. Called after a read/write operation*/
+static void axienet_mdio_mdc_disable(struct axienet_local *lp)
+{
+	u32 mc_reg;
+
+	mc_reg = axienet_ior(lp, XAE_MDIO_MC_OFFSET);
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET,
+		    (mc_reg & ~XAE_MDIO_MC_MDIOEN_MASK));
+}
+
 /**
  * axienet_mdio_read - MDIO interface read function
  * @bus:	Pointer to mii bus structure
@@ -48,9 +65,13 @@ static int axienet_mdio_read(struct mii_bus *bus, int phy_id, int reg)
 	int ret;
 	struct axienet_local *lp = bus->priv;
 
+	axienet_mdio_mdc_enable(lp);
+
 	ret = axienet_mdio_wait_until_ready(lp);
-	if (ret < 0)
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
 		return ret;
+	}
 
 	axienet_iow(lp, XAE_MDIO_MCR_OFFSET,
 		    (((phy_id << XAE_MDIO_MCR_PHYAD_SHIFT) &
@@ -61,14 +82,17 @@ static int axienet_mdio_read(struct mii_bus *bus, int phy_id, int reg)
 		     XAE_MDIO_MCR_OP_READ_MASK));
 
 	ret = axienet_mdio_wait_until_ready(lp);
-	if (ret < 0)
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
 		return ret;
+	}
 
 	rc = axienet_ior(lp, XAE_MDIO_MRD_OFFSET) & 0x0000FFFF;
 
 	dev_dbg(lp->dev, "axienet_mdio_read(phy_id=%i, reg=%x) == %x\n",
 		phy_id, reg, rc);
 
+	axienet_mdio_mdc_disable(lp);
 	return rc;
 }
 
@@ -94,9 +118,13 @@ static int axienet_mdio_write(struct mii_bus *bus, int phy_id, int reg,
 	dev_dbg(lp->dev, "axienet_mdio_write(phy_id=%i, reg=%x, val=%x)\n",
 		phy_id, reg, val);
 
+	axienet_mdio_mdc_enable(lp);
+
 	ret = axienet_mdio_wait_until_ready(lp);
-	if (ret < 0)
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
 		return ret;
+	}
 
 	axienet_iow(lp, XAE_MDIO_MWD_OFFSET, (u32) val);
 	axienet_iow(lp, XAE_MDIO_MCR_OFFSET,
@@ -108,8 +136,11 @@ static int axienet_mdio_write(struct mii_bus *bus, int phy_id, int reg,
 		     XAE_MDIO_MCR_OP_WRITE_MASK));
 
 	ret = axienet_mdio_wait_until_ready(lp);
-	if (ret < 0)
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
 		return ret;
+	}
+	axienet_mdio_mdc_disable(lp);
 	return 0;
 }
 
@@ -124,7 +155,9 @@ static int axienet_mdio_write(struct mii_bus *bus, int phy_id, int reg,
  **/
 int axienet_mdio_enable(struct axienet_local *lp)
 {
-	u32 clk_div, host_clock;
+	u32 host_clock;
+
+	lp->mii_clk_div = 0;
 
 	if (lp->clk) {
 		host_clock = clk_get_rate(lp->clk);
@@ -176,19 +209,19 @@ int axienet_mdio_enable(struct axienet_local *lp)
 	 * "clock-frequency" from the CPU
 	 */
 
-	clk_div = (host_clock / (MAX_MDIO_FREQ * 2)) - 1;
+	lp->mii_clk_div = (host_clock / (MAX_MDIO_FREQ * 2)) - 1;
 	/* If there is any remainder from the division of
 	 * fHOST / (MAX_MDIO_FREQ * 2), then we need to add
 	 * 1 to the clock divisor or we will surely be above 2.5 MHz
 	 */
 	if (host_clock % (MAX_MDIO_FREQ * 2))
-		clk_div++;
+		lp->mii_clk_div++;
 
 	netdev_dbg(lp->ndev,
 		   "Setting MDIO clock divisor to %u/%u Hz host clock.\n",
-		   clk_div, host_clock);
+		   lp->mii_clk_div, host_clock);
 
-	axienet_iow(lp, XAE_MDIO_MC_OFFSET, clk_div | XAE_MDIO_MC_MDIOEN_MASK);
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET, lp->mii_clk_div | XAE_MDIO_MC_MDIOEN_MASK);
 
 	return axienet_mdio_wait_until_ready(lp);
 }
@@ -211,8 +244,8 @@ void axienet_mdio_disable(struct axienet_local *lp)
  * Return:	0 on success, -ETIMEDOUT on a timeout, -ENOMEM when
  *		mdiobus_alloc (to allocate memory for mii bus structure) fails.
  *
- * Sets up the MDIO interface by initializing the MDIO clock and enabling the
- * MDIO interface in hardware. Register the MDIO interface.
+ * Sets up the MDIO interface by initializing the MDIO clock.
+ * Register the MDIO interface.
  **/
 int axienet_mdio_setup(struct axienet_local *lp)
 {
@@ -246,6 +279,7 @@ int axienet_mdio_setup(struct axienet_local *lp)
 		lp->mii_bus = NULL;
 		return ret;
 	}
+	axienet_mdio_mdc_disable(lp);
 	return 0;
 }
 
diff --git a/drivers/net/ethernet/xilinx/xilinx_emaclite.c b/drivers/net/ethernet/xilinx/xilinx_emaclite.c
index f6ea4a0ad..2e2f3114f 100644
--- a/drivers/net/ethernet/xilinx/xilinx_emaclite.c
+++ b/drivers/net/ethernet/xilinx/xilinx_emaclite.c
@@ -97,7 +97,7 @@
 #define ALIGNMENT		4
 
 /* BUFFER_ALIGN(adr) calculates the number of bytes to the next alignment. */
-#define BUFFER_ALIGN(adr) ((ALIGNMENT - ((u32)adr)) % ALIGNMENT)
+#define BUFFER_ALIGN(adr) ((ALIGNMENT - ((uintptr_t)adr)) % ALIGNMENT)
 
 #ifdef __BIG_ENDIAN
 #define xemaclite_readl		ioread32be
@@ -338,7 +338,7 @@ static int xemaclite_send_data(struct net_local *drvdata, u8 *data,
 		 * if it is configured in HW
 		 */
 
-		addr = (void __iomem __force *)((u32 __force)addr ^
+		addr = (void __iomem __force *)((uintptr_t __force)addr ^
 						 XEL_BUFFER_OFFSET);
 		reg_data = xemaclite_readl(addr + XEL_TSR_OFFSET);
 
@@ -399,8 +399,9 @@ static u16 xemaclite_recv_data(struct net_local *drvdata, u8 *data, int maxlen)
 		 * will correct on subsequent calls
 		 */
 		if (drvdata->rx_ping_pong != 0)
-			addr = (void __iomem __force *)((u32 __force)addr ^
-							 XEL_BUFFER_OFFSET);
+			addr = (void __iomem __force *)
+				((uintptr_t __force)addr ^
+				 XEL_BUFFER_OFFSET);
 		else
 			return 0;	/* No data was available */
 
@@ -518,6 +519,7 @@ static int xemaclite_set_mac_address(struct net_device *dev, void *address)
 /**
  * xemaclite_tx_timeout - Callback for Tx Timeout
  * @dev:	Pointer to the network device
+ * @txqueue:	Unused
  *
  * This function is called when Tx time out occurs for Emaclite device.
  */
@@ -1185,8 +1187,8 @@ static int xemaclite_of_probe(struct platform_device *ofdev)
 	}
 
 	dev_info(dev,
-		 "Xilinx EmacLite at 0x%08X mapped to 0x%p, irq=%d\n",
-		 (unsigned int __force)ndev->mem_start, lp->base_addr, ndev->irq);
+		 "Xilinx EmacLite at 0x%08lX mapped to 0x%p, irq=%d\n",
+		 (unsigned long __force)ndev->mem_start, lp->base_addr, ndev->irq);
 	return 0;
 
 put_node:
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_cb.c b/drivers/net/ethernet/xilinx/xilinx_tsn_cb.c
new file mode 100644
index 000000000..4902a536c
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_cb.c
@@ -0,0 +1,177 @@
+/*
+ * Xilinx FPGA Xilinx TSN QCI Controller module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include "xilinx_tsn_switch.h"
+
+#define IN_PORTID_MASK				0x3
+#define IN_PORTID_SHIFT				24
+#define MAX_SEQID_MASK				0x0000FFFF
+
+#define SEQ_REC_HIST_LEN_MASK			0x000000FF
+#define SEQ_REC_HIST_LEN_SHIFT			16
+#define SPLIT_STREAM_INPORTID_SHIFT		12
+#define SPLIT_STREAM_INPORTID_MASK		0x3
+#define SPLIT_STREAM_VLANID_MASK		0x00000FFF
+
+#define GATE_ID_SHIFT				24
+#define MEMBER_ID_SHIFT				8
+#define SEQ_RESET_SHIFT				7
+#define REC_TIMEOUT_SHIFT			6
+#define GATE_STATE_SHIFT			5
+#define FRER_VALID_SHIFT			4
+#define WR_OP_TYPE_SHIFT			2
+#define OP_TYPE_SHIFT				1
+#define WR_OP_TYPE_MASK				0x3
+#define FRER_EN_CONTROL_MASK			0x1
+
+/**
+ * frer_control - Configure thr control for frer
+ * @data:	Value to be programmed
+ */
+void frer_control(struct frer_ctrl data)
+{
+	u32 mask = 0;
+
+	mask = data.gate_id << GATE_ID_SHIFT;
+	mask |= data.memb_id << MEMBER_ID_SHIFT;
+	mask |= data.seq_reset << SEQ_RESET_SHIFT;
+	mask |= data.gate_state << GATE_STATE_SHIFT;
+	mask |= data.rcvry_tmout << REC_TIMEOUT_SHIFT;
+	mask |= data.frer_valid << FRER_VALID_SHIFT;
+	mask |= (data.wr_op_type & WR_OP_TYPE_MASK) << WR_OP_TYPE_SHIFT;
+	mask |= data.op_type << OP_TYPE_SHIFT;
+	mask |= FRER_EN_CONTROL_MASK;
+
+	axienet_iow(&lp, FRER_CONTROL_OFFSET, mask);
+
+	/* wait for write to complete */
+	while ((axienet_ior(&lp, FRER_CONTROL_OFFSET) & FRER_EN_CONTROL_MASK))
+		;
+}
+
+/**
+ * get_ingress_filter_config -  Get Ingress Filter Configuration
+ * @data:	Value returned
+ */
+void get_ingress_filter_config(struct in_fltr *data)
+{
+	u32 reg_val = 0;
+
+	reg_val = axienet_ior(&lp, INGRESS_FILTER_OFFSET);
+
+	data->max_seq_id = reg_val & MAX_SEQID_MASK;
+	data->in_port_id = (reg_val >> IN_PORTID_SHIFT) & IN_PORTID_MASK;
+}
+
+/**
+ * config_stream_filter -  Configure Ingress Filter Configuration
+ * @data:	Value to be programmed
+ */
+void config_ingress_filter(struct in_fltr data)
+{
+	u32 mask = 0;
+
+	mask = ((data.in_port_id & IN_PORTID_MASK) << IN_PORTID_SHIFT) |
+					(data.max_seq_id & MAX_SEQID_MASK);
+	axienet_iow(&lp, INGRESS_FILTER_OFFSET, mask);
+}
+
+/**
+ * get_member_reg -  Read frer member Configuration registers value
+ * @data:	Value returned
+ */
+void get_member_reg(struct frer_memb_config *data)
+{
+	u32 conf_r1 = 0;
+
+	conf_r1 = axienet_ior(&lp, FRER_CONFIG_REG1);
+	data->rem_ticks = axienet_ior(&lp, FRER_CONFIG_REG2);
+
+	data->seq_rec_hist_len = (conf_r1 >> SEQ_REC_HIST_LEN_SHIFT)
+						& SEQ_REC_HIST_LEN_MASK;
+	data->split_strm_egport_id = (conf_r1 >> SPLIT_STREAM_INPORTID_SHIFT)
+						& SPLIT_STREAM_INPORTID_MASK;
+	data->split_strm_vlan_id = conf_r1 & SPLIT_STREAM_VLANID_MASK;
+}
+
+/**
+ * program_member_reg -  configure frer member Configuration registers
+ * @data:	Value to be programmed
+ */
+void program_member_reg(struct frer_memb_config data)
+{
+	u32 conf_r1 = 0;
+
+	conf_r1 = (data.seq_rec_hist_len & SEQ_REC_HIST_LEN_MASK)
+						<< SEQ_REC_HIST_LEN_SHIFT;
+	conf_r1 = conf_r1 | ((data.split_strm_egport_id
+					& SPLIT_STREAM_INPORTID_MASK)
+					<< SPLIT_STREAM_INPORTID_SHIFT);
+	conf_r1 = conf_r1 | (data.split_strm_vlan_id
+					& SPLIT_STREAM_VLANID_MASK);
+
+	axienet_iow(&lp, FRER_CONFIG_REG1, conf_r1);
+	axienet_iow(&lp, FRER_CONFIG_REG2, data.rem_ticks);
+}
+
+/**
+ * get_frer_static_counter -  get frer static counters value
+ * @data:	return value, containing counter value
+ */
+void get_frer_static_counter(struct frer_static_counter *data)
+{
+	int offset = (data->num) * 8;
+
+	data->frer_fr_count.lsb = axienet_ior(&lp, TOTAL_FRER_FRAMES_OFFSET +
+									offset);
+	data->frer_fr_count.msb = axienet_ior(&lp, TOTAL_FRER_FRAMES_OFFSET +
+								offset + 0x4);
+
+	data->disc_frames_in_portid.lsb = axienet_ior(&lp,
+						      FRER_DISCARD_INGS_FLTR_OFFSET + offset);
+	data->disc_frames_in_portid.msb = axienet_ior(&lp,
+						      FRER_DISCARD_INGS_FLTR_OFFSET + offset + 0x4);
+
+	data->pass_frames_ind_recv.lsb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_INDV_OFFSET + offset);
+	data->pass_frames_ind_recv.msb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_INDV_OFFSET + offset + 0x4);
+
+	data->disc_frames_ind_recv.lsb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_INDV_OFFSET + offset);
+	data->disc_frames_ind_recv.msb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_INDV_OFFSET + offset + 0x4);
+
+	data->pass_frames_seq_recv.lsb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_SEQ_OFFSET + offset);
+	data->pass_frames_seq_recv.msb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_SEQ_OFFSET + offset + 0x4);
+
+	data->disc_frames_seq_recv.lsb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_SEQ_OFFSET + offset);
+	data->disc_frames_seq_recv.msb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_SEQ_OFFSET + offset + 0x4);
+
+	data->rogue_frames_seq_recv.lsb = axienet_ior(&lp,
+						      FRER_ROGUE_FRAMES_SEQ_OFFSET + offset);
+	data->rogue_frames_seq_recv.msb = axienet_ior(&lp,
+						      FRER_ROGUE_FRAMES_SEQ_OFFSET + offset + 0x4);
+
+	data->seq_recv_rst.lsb = axienet_ior(&lp,
+					     SEQ_RECV_RESETS_OFFSET + offset);
+	data->seq_recv_rst.msb = axienet_ior(&lp,
+					     SEQ_RECV_RESETS_OFFSET + offset + 0x4);
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_ep.c b/drivers/net/ethernet/xilinx/xilinx_tsn_ep.c
new file mode 100644
index 000000000..22f28a41d
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_ep.c
@@ -0,0 +1,413 @@
+/*
+ * Xilinx FPGA Xilinx TSN End point driver.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/etherdevice.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+#include <linux/skbuff.h>
+
+#include "xilinx_axienet.h"
+
+#define TX_BD_NUM_DEFAULT	64
+#define RX_BD_NUM_DEFAULT	1024
+
+/**
+ * tsn_ep_open - TSN EP driver open routine.
+ * @ndev:	Pointer to net_device structure
+ *
+ * Return: 0, on success.
+ *	    non-zero error value on failure
+ *
+ * This is the driver open routine. It also allocates interrupt service
+ * routines, enables the interrupt lines and ISR handling. Axi Ethernet
+ * core is reset through Axi DMA core. Buffer descriptors are initialized.
+ */
+static int tsn_ep_open(struct net_device *ndev)
+{
+	int ret, i = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		/*MCDMA TX RESET*/
+		__axienet_device_reset(q);
+	}
+
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+
+		ret = axienet_mcdma_rx_q_init(ndev, q);
+		/* Enable interrupts for Axi MCDMA Rx
+		 */
+		ret = request_irq(q->rx_irq, axienet_mcdma_rx_irq,
+				  IRQF_SHARED, ndev->name, ndev);
+		if (ret)
+			goto err_dma_rx_irq;
+
+		tasklet_init(&lp->dma_err_tasklet[i],
+			     axienet_mcdma_err_handler,
+			     (unsigned long)lp->dq[i]);
+		napi_enable(&lp->napi[i]);
+	}
+
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+
+		ret = axienet_mcdma_tx_q_init(ndev, q);
+		/* Enable interrupts for Axi MCDMA Tx */
+		ret = request_irq(q->tx_irq, axienet_mcdma_tx_irq,
+				  IRQF_SHARED, ndev->name, ndev);
+		if (ret)
+			goto err_dma_tx_irq;
+	}
+
+	netif_tx_start_all_queues(ndev);
+	return 0;
+
+err_dma_tx_irq:
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		free_irq(q->rx_irq, ndev);
+	}
+err_dma_rx_irq:
+	return ret;
+}
+
+/**
+ * tsn_ep_stop - TSN EP driver stop routine.
+ * @ndev:	Pointer to net_device structure
+ *
+ * Return: 0, on success.
+ *
+ * This is the driver stop routine. It also removes the interrupt handlers
+ * and disables the interrupts. The Axi DMA Tx/Rx BDs are released.
+ */
+static int tsn_ep_stop(struct net_device *ndev)
+{
+	u32 cr;
+	u32 i;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET,
+				  cr & (~XAXIDMA_CR_RUNSTOP_MASK));
+		if (netif_running(ndev))
+			netif_stop_queue(ndev);
+		free_irq(q->tx_irq, ndev);
+	}
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET,
+				  cr & (~XAXIDMA_CR_RUNSTOP_MASK));
+		if (netif_running(ndev))
+			netif_stop_queue(ndev);
+		napi_disable(&lp->napi[i]);
+		tasklet_kill(&lp->dma_err_tasklet[i]);
+
+		free_irq(q->rx_irq, ndev);
+	}
+
+	return 0;
+}
+
+/**
+ * tsn_ep_ioctl - TSN endpoint ioctl interface.
+ * @dev: Pointer to the net_device structure
+ * @rq: Socket ioctl interface request structure
+ * @cmd: Ioctl case
+ *
+ * Return: 0 on success, Non-zero error value on failure.
+ *
+ * This is the ioctl interface for TSN end point. Currently this
+ * supports only gate programming.
+ */
+static int tsn_ep_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	switch (cmd) {
+#ifdef CONFIG_XILINX_TSN_QBV
+	case SIOCCHIOCTL:
+		return axienet_set_schedule(dev, rq->ifr_data);
+	case SIOC_GET_SCHED:
+		return axienet_get_schedule(dev, rq->ifr_data);
+#endif
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+/**
+ * tsn_ep_xmit - TSN endpoint xmit routine.
+ * @skb: Packet data
+ * @ndev: Pointer to the net_device structure
+ *
+ * Return: Always returns NETDEV_TX_OK.
+ *
+ * This is dummy xmit function for endpoint as all the data path is assumed to
+ * be connected by TEMAC1 as per linux view
+ */
+static int tsn_ep_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct ethhdr *hdr = (struct ethhdr *)skb->data;
+	u16 ether_type = ntohs(hdr->h_proto);
+	u16 vlan_tci;
+	u8 pcp = 0;
+	u16 queue = 0; /*BE*/
+
+	if (unlikely(ether_type == ETH_P_8021Q)) {
+		struct vlan_ethhdr *vhdr = (struct vlan_ethhdr *)skb->data;
+
+		/* ether_type = ntohs(vhdr->h_vlan_encapsulated_proto); */
+
+		vlan_tci = ntohs(vhdr->h_vlan_TCI);
+
+		pcp = (vlan_tci & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
+	}
+	/* for non-ST mcdma select tx queue */
+	if (lp->num_tc == 3 && (pcp == 2 || pcp == 3))
+		queue = 1; /* RES */
+
+	return axienet_queue_xmit(skb, ndev, queue);
+}
+
+static const struct net_device_ops ep_netdev_ops = {
+	.ndo_open = tsn_ep_open,
+	.ndo_stop = tsn_ep_stop,
+	.ndo_do_ioctl = tsn_ep_ioctl,
+	.ndo_start_xmit = tsn_ep_xmit,
+};
+
+static const struct of_device_id tsn_ep_of_match[] = {
+	{ .compatible = "xlnx,tsn-ep"},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsn_ep_of_match);
+
+/* separate function is needed to probe tsn mcdma
+ * as there is asymmetry between rx channels and tx channels
+ * having unique probe for both tsn and axienet with mcdma is not possible
+ */
+static int __maybe_unused tsn_mcdma_probe(struct platform_device *pdev,
+					  struct axienet_local *lp,
+					  struct net_device *ndev)
+{
+	int i, ret = 0;
+	struct axienet_dma_q *q;
+	struct device_node *np;
+	struct resource dmares;
+	const char *str;
+	u32 num;
+
+	ret = of_property_count_strings(pdev->dev.of_node, "xlnx,channel-ids");
+	if (ret < 0)
+		return -EINVAL;
+
+	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected-rx",
+			      0);
+	/* get number of associated queues */
+	ret = of_property_read_u32(np, "xlnx,num-s2mm-channels", &num);
+	if (ret < 0)
+		return -EINVAL;
+
+	lp->num_rx_queues = num;
+	pr_info("%s: num_rx_queues: %d\n", __func__, lp->num_rx_queues);
+
+	for_each_rx_dma_queue(lp, i) {
+		q = kzalloc(sizeof(*q), GFP_KERNEL);
+
+		/* parent */
+		q->lp = lp;
+		lp->dq[i] = q;
+		ret = of_property_read_string_index(pdev->dev.of_node,
+						    "xlnx,channel-ids", i,
+						    &str);
+		ret = kstrtou16(str, 16, &q->chan_id);
+		lp->qnum[i] = i;
+		lp->chan_num[i] = q->chan_id;
+	}
+
+	if (IS_ERR(np)) {
+		dev_err(&pdev->dev, "could not find DMA node\n");
+		return ret;
+	}
+
+	ret = of_address_to_resource(np, 0, &dmares);
+	if (ret) {
+		dev_err(&pdev->dev, "unable to get DMA resource\n");
+		return ret;
+	}
+
+	lp->mcdma_regs = devm_ioremap_resource(&pdev->dev, &dmares);
+	if (IS_ERR(lp->mcdma_regs)) {
+		dev_err(&pdev->dev, "iormeap failed for the dma\n");
+		ret = PTR_ERR(lp->mcdma_regs);
+		return ret;
+	}
+
+	axienet_mcdma_rx_probe(pdev, lp, ndev);
+	axienet_mcdma_tx_probe(pdev, np, lp);
+
+	return 0;
+}
+
+static const struct axienet_config tsn_endpoint_cfg = {
+	.mactype = XAXIENET_1G,
+	.setoptions = NULL,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+/**
+ * tsn_ep_probe - TSN ep pointer probe function.
+ * @pdev:	Pointer to platform device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe routine for TSN endpoint driver.
+ */
+static int tsn_ep_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct axienet_local *lp;
+	struct net_device *ndev;
+	struct resource *ethres;
+	const void *mac_addr;
+	u16 num_tc = 0;
+	char irq_name[32];
+
+	ndev = alloc_netdev(sizeof(*lp), "ep", NET_NAME_UNKNOWN, ether_setup);
+	if (!ndev)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ndev);
+
+	SET_NETDEV_DEV(ndev, &pdev->dev);
+	ndev->flags &= ~IFF_MULTICAST;  /* clear multicast */
+	ndev->features = NETIF_F_SG;
+	ndev->netdev_ops = &ep_netdev_ops;
+
+	/* MTU range: 64 - 9000 */
+	ndev->min_mtu = 64;
+	ndev->max_mtu = XAE_JUMBO_MTU;
+
+	lp = netdev_priv(ndev);
+	lp->ndev = ndev;
+	lp->dev = &pdev->dev;
+	lp->options = XAE_OPTION_DEFAULTS;
+	lp->tx_bd_num = TX_BD_NUM_DEFAULT;
+	lp->rx_bd_num = RX_BD_NUM_DEFAULT;
+
+	/* TODO
+	 * there are two temacs or two slaves to ep
+	 * get this infor from design?
+	 */
+	lp->slaves[0] = NULL;
+	lp->slaves[1] = NULL;
+
+	lp->axienet_config = &tsn_endpoint_cfg;
+
+	lp->max_frm_size = XAE_MAX_VLAN_FRAME_SIZE;
+
+	/* Setup checksum offload, but default to off if not specified */
+	lp->features = 0;
+
+	lp->eth_hasnobuf = of_property_read_bool(pdev->dev.of_node,
+						 "xlnx,eth-hasnobuf");
+
+	/* Retrieve the MAC address */
+	mac_addr = of_get_mac_address(pdev->dev.of_node);
+	if (!mac_addr) {
+		dev_err(&pdev->dev, "could not find MAC address\n");
+		goto free_netdev;
+	}
+	if (mac_addr)
+		ether_addr_copy(ndev->dev_addr, mac_addr);
+	if (!is_valid_ether_addr(ndev->dev_addr))
+		eth_hw_addr_random(ndev);
+
+	ret = tsn_mcdma_probe(pdev, lp, ndev);
+	if (ret) {
+		dev_err(&pdev->dev, "Getting MCDMA resource failed\n");
+		goto free_netdev;
+	}
+
+	ret = of_property_read_u16(
+		pdev->dev.of_node, "xlnx,num-tc", &num_tc);
+	if (ret || (num_tc != 2 && num_tc != 3))
+		lp->num_tc = XAE_MAX_TSN_TC;
+	else
+		lp->num_tc = num_tc;
+	/* Map device registers */
+	ethres = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	lp->regs = devm_ioremap_resource(&pdev->dev, ethres);
+	if (IS_ERR(lp->regs)) {
+		ret = PTR_ERR(lp->regs);
+		goto free_netdev;
+	}
+	lp->qbv_regs = lp->regs;
+
+	sprintf(irq_name, "tsn_ep_scheduler_irq");
+	lp->qbv_irq = platform_get_irq_byname(pdev, irq_name);
+	axienet_qbv_init(ndev);
+
+	ret = register_netdev(lp->ndev);
+	if (ret)
+		dev_err(lp->dev, "register_netdev() error (%i)\n", ret);
+
+	return ret;
+
+free_netdev:
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static int tsn_ep_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+
+	axienet_qbv_remove(ndev);
+	unregister_netdev(ndev);
+
+	free_netdev(ndev);
+
+	return 0;
+}
+
+static struct platform_driver tsn_ep_driver = {
+	.probe = tsn_ep_probe,
+	.remove = tsn_ep_remove,
+	.driver = {
+		 .name = "tsn_ep_axienet",
+		 .of_match_table = tsn_ep_of_match,
+	},
+};
+
+module_platform_driver(tsn_ep_driver);
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_ip.c b/drivers/net/ethernet/xilinx/xilinx_tsn_ip.c
new file mode 100644
index 000000000..0710dd677
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_ip.c
@@ -0,0 +1,373 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx FPGA Xilinx TSN IP driver.
+ *
+ * Copyright (C) 2018 Xilinx, Inc. All rights reserved.
+ *
+ * Author: Priyadarshini Babu <priyadar@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>
+#include <linux/phy.h>
+#include <linux/mii.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/of_address.h>
+#include <linux/xilinx_phy.h>
+
+#include "xilinx_axienet.h"
+
+#ifdef CONFIG_XILINX_TSN_PTP
+#include "xilinx_tsn_ptp.h"
+#include "xilinx_tsn_timer.h"
+#endif
+
+#define TSN_TX_BE_QUEUE  0
+#define TSN_TX_RES_QUEUE 1
+#define TSN_TX_ST_QUEUE  2
+
+#define XAE_TEMAC1 0
+#define XAE_TEMAC2 1
+static const struct of_device_id tsn_ip_of_match[] = {
+	{ .compatible = "xlnx,tsn-endpoint-ethernet-mac-1.0"},
+	{ .compatible = "xlnx,tsn-endpoint-ethernet-mac-2.0"},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsn_ip_of_match);
+
+/**
+ * tsn_ip_probe - TSN ip pointer probe function.
+ * @pdev:	Pointer to platform device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe routine for TSN driver.
+ */
+static int tsn_ip_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+
+	pr_info("TSN endpoint ethernet mac Probe\n");
+
+	ret = of_platform_populate(pdev->dev.of_node, NULL, NULL, &pdev->dev);
+	if (ret)
+		pr_err("TSN endpoint probe error (%i)\n", ret);
+
+	return ret;
+}
+
+static int tsn_ip_remove(struct platform_device *pdev)
+{
+	return 0;
+}
+
+/**
+ * axienet_tsn_xmit - Starts the TSN transmission.
+ * @skb:	sk_buff pointer that contains data to be Txed.
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: NETDEV_TX_OK, on success
+ *	    Non-zero error value on failure.
+ *
+ * This function is invoked from upper layers to initiate transmission. The
+ * function uses the next available free BDs and populates their fields to
+ * start the transmission. Use axienet_ptp_xmit() for PTP 1588 packets and
+ * use master EP xmit for other packets transmission.
+ */
+int axienet_tsn_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct ethhdr *hdr = (struct ethhdr *)skb->data;
+	u16 ether_type = ntohs(hdr->h_proto);
+	struct net_device *master = lp->master;
+
+#ifdef CONFIG_XILINX_TSN_PTP
+	/* check if skb is a PTP frame ? */
+	if (unlikely(ether_type == ETH_P_1588))
+		return axienet_ptp_xmit(skb, ndev);
+#endif
+	/* use EP to xmit non-PTP frames */
+	skb->dev = master;
+	dev_queue_xmit(skb);
+
+	return NETDEV_TX_OK;
+}
+
+/**
+ * axienet_tsn_probe - TSN mac probe function.
+ * @pdev:	Pointer to platform device structure.
+ * @lp:		Pointer to axienet local structure
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe for TSN mac nodes.
+ */
+int axienet_tsn_probe(struct platform_device *pdev,
+		      struct axienet_local *lp,
+		      struct net_device *ndev)
+{
+	int ret = 0;
+	char irq_name[32];
+	bool slave = false;
+	u8     temac_no;
+	u32 qbv_addr, qbv_size;
+	u32 abl_reg;
+	struct device_node *ep_node;
+	struct axienet_local *ep_lp;
+
+	slave = of_property_read_bool(pdev->dev.of_node,
+				      "xlnx,tsn-slave");
+	if (slave)
+		temac_no = XAE_TEMAC2;
+	else
+		temac_no = XAE_TEMAC1;
+
+	sprintf(irq_name, "interrupt_ptp_rx_%d", temac_no + 1);
+	lp->ptp_rx_irq = platform_get_irq_byname(pdev, irq_name);
+
+	pr_info("ptp RX irq: %d %s\n", lp->ptp_rx_irq, irq_name);
+	sprintf(irq_name, "interrupt_ptp_tx_%d", temac_no + 1);
+	lp->ptp_tx_irq = platform_get_irq_byname(pdev, irq_name);
+	pr_info("ptp TX irq: %d %s\n", lp->ptp_tx_irq, irq_name);
+
+	sprintf(irq_name, "tsn_switch_scheduler_irq_%d", temac_no + 1);
+	lp->qbv_irq = platform_get_irq_byname(pdev, irq_name);
+
+	/*Ignoring if the qbv_irq is not exist*/
+	if (lp->qbv_irq > 0)
+		pr_info("qbv_irq: %d %s\n", lp->qbv_irq, irq_name);
+
+	spin_lock_init(&lp->ptp_tx_lock);
+
+	if (temac_no == XAE_TEMAC1)
+		axienet_ptp_timer_probe((lp->regs + XAE_RTC_OFFSET), pdev);
+
+	/* enable VLAN */
+	lp->options |= XAE_OPTION_VLAN;
+	axienet_setoptions(lp->ndev, lp->options);
+
+	/* get the ep device */
+	ep_node = of_parse_phandle(pdev->dev.of_node, "tsn,endpoint", 0);
+
+	lp->master = of_find_net_device_by_node(ep_node);
+#ifdef CONFIG_XILINX_TSN_QBV
+	lp->qbv_regs = 0;
+	abl_reg = axienet_ior(lp, XAE_TSN_ABL_OFFSET);
+	if (!(abl_reg & TSN_BRIDGEEP_EPONLY)) {
+		if (of_property_read_u32(pdev->dev.of_node,
+					 "xlnx,qbv-addr", &qbv_addr) == 0) {
+			if ((of_property_read_u32(pdev->dev.of_node,
+						  "xlnx,qbv-size", &qbv_size) ==
+			     0) && qbv_size) {
+				lp->qbv_regs = devm_ioremap(&pdev->dev,
+							    qbv_addr, qbv_size);
+				if (IS_ERR(lp->qbv_regs)) {
+					dev_err(&pdev->dev,
+						"ioremap failed for the qbv\n");
+					ret = PTR_ERR(lp->qbv_regs);
+					return ret;
+				}
+				ret = axienet_qbv_init(ndev);
+			}
+		}
+	}
+#endif
+	/* EP+Switch */
+	/* store the slaves to master(ep) */
+	ep_lp = netdev_priv(lp->master);
+	ep_lp->slaves[temac_no] = ndev;
+
+	return 0;
+}
+
+/**
+ * axienet_device_reset - Reset and initialize the Axi Ethernet hardware.
+ * @ndev:	Pointer to the net_device structure
+ *
+ * This function is called to reset and initialize the Axi Ethernet core. This
+ * is typically called during initialization. It does a reset of the Axi DMA
+ * Rx/Tx channels and initializes the Axi DMA BDs. Since Axi DMA reset lines
+ * areconnected to Axi Ethernet reset lines, this in turn resets the Axi
+ * Ethernet core. No separate hardware reset is done for the Axi Ethernet
+ * core.
+ */
+static void axienet_device_reset(struct net_device *ndev)
+{
+	u32 axienet_status;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	lp->max_frm_size = XAE_MAX_VLAN_FRAME_SIZE;
+
+	lp->options |= XAE_OPTION_VLAN;
+	lp->options &= (~XAE_OPTION_JUMBO);
+
+	if (ndev->mtu > XAE_MTU && ndev->mtu <= XAE_JUMBO_MTU) {
+		lp->max_frm_size = ndev->mtu + VLAN_ETH_HLEN +
+					XAE_TRL_SIZE;
+		if (lp->max_frm_size <= lp->rxmem)
+			lp->options |= XAE_OPTION_JUMBO;
+	}
+
+	axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+	axienet_status &= ~XAE_RCW1_RX_MASK;
+	axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+
+	if (lp->axienet_config->mactype == XAXIENET_1G &&
+	    !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+
+		/* Enable Receive errors */
+		axienet_iow(lp, XAE_IE_OFFSET, XAE_INT_RECV_ERROR_MASK);
+	}
+
+	axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+
+	axienet_set_mac_address(ndev, NULL);
+	axienet_set_multicast_list(ndev);
+	lp->axienet_config->setoptions(ndev, lp->options);
+
+	netif_trans_update(ndev);
+}
+
+/**
+ * axienet_mii_init - MII init routine
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This routine initializes MII.
+ */
+static int axienet_mii_init(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int ret, mdio_mcreg;
+
+	mdio_mcreg = axienet_ior(lp, XAE_MDIO_MC_OFFSET);
+	ret = axienet_mdio_wait_until_ready(lp);
+	if (ret < 0)
+		return ret;
+
+	/* Disable the MDIO interface till Axi Ethernet Reset is completed.
+	 * When we do an Axi Ethernet reset, it resets the complete core
+	 * Including the MDIO. If MDIO is not disabled when the reset process is
+	 * Started, MDIO will be broken afterwards.
+	 */
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET,
+		    (mdio_mcreg & (~XAE_MDIO_MC_MDIOEN_MASK)));
+	axienet_device_reset(ndev);
+	/* Enable the MDIO */
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET, mdio_mcreg);
+	ret = axienet_mdio_wait_until_ready(lp);
+	if (ret < 0)
+		return ret;
+
+	return 0;
+}
+
+/**
+ * axienet_tsn_open - TSN driver open routine.
+ * @ndev:	Pointer to net_device structure
+ *
+ * Return: 0, on success.
+ *	    non-zero error value on failure
+ *
+ * This is the driver open routine. It calls phy_start to start the PHY device.
+ * It also allocates interrupt service routines, enables the interrupt lines
+ * and ISR handling. Axi Ethernet core is reset through Axi DMA core.
+ */
+int axienet_tsn_open(struct net_device *ndev)
+{
+	int ret;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct phy_device *phydev = NULL;
+
+	ret = axienet_mii_init(ndev);
+	if (ret < 0)
+		return ret;
+
+	if (lp->phy_node) {
+		if (lp->phy_mode == XAE_PHY_TYPE_GMII) {
+			phydev = of_phy_connect(lp->ndev, lp->phy_node,
+						axienet_adjust_link, 0,
+						PHY_INTERFACE_MODE_GMII);
+		} else if (lp->phy_mode == XAE_PHY_TYPE_RGMII_2_0) {
+			phydev = of_phy_connect(lp->ndev, lp->phy_node,
+						axienet_adjust_link, 0,
+						PHY_INTERFACE_MODE_RGMII_ID);
+		} else if ((lp->axienet_config->mactype == XAXIENET_1G) ||
+			     (lp->axienet_config->mactype == XAXIENET_2_5G)) {
+			phydev = of_phy_connect(lp->ndev, lp->phy_node,
+						axienet_adjust_link,
+						lp->phy_flags,
+						lp->phy_mode);
+		}
+
+		if (!phydev)
+			dev_err(lp->dev, "of_phy_connect() failed\n");
+		else
+			phy_start(phydev);
+	}
+
+	INIT_WORK(&lp->tx_tstamp_work, axienet_tx_tstamp);
+	skb_queue_head_init(&lp->ptp_txq);
+
+	lp->ptp_rx_hw_pointer = 0;
+	lp->ptp_rx_sw_pointer = 0xff;
+
+	axienet_iow(lp, PTP_RX_CONTROL_OFFSET, PTP_RX_PACKET_CLEAR);
+
+	ret = request_irq(lp->ptp_rx_irq, axienet_ptp_rx_irq,
+			  0, "ptp_rx", ndev);
+	if (ret)
+		goto err_ptp_rx_irq;
+
+	ret = request_irq(lp->ptp_tx_irq, axienet_ptp_tx_irq,
+			  0, "ptp_tx", ndev);
+	if (ret)
+		goto err_ptp_tx_irq;
+
+	netif_tx_start_all_queues(ndev);
+
+	return 0;
+
+err_ptp_tx_irq:
+	free_irq(lp->ptp_rx_irq, ndev);
+err_ptp_rx_irq:
+	return ret;
+}
+
+static struct platform_driver tsn_ip_driver = {
+	.probe = tsn_ip_probe,
+	.remove = tsn_ip_remove,
+	.driver = {
+		 .name = "tsn_ip_axienet",
+		 .of_match_table = tsn_ip_of_match,
+	},
+};
+
+module_platform_driver(tsn_ip_driver);
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_preemption.c b/drivers/net/ethernet/xilinx/xilinx_tsn_preemption.c
new file mode 100644
index 000000000..f48c2e0cb
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_preemption.c
@@ -0,0 +1,223 @@
+/*
+ * Xilinx FPGA Xilinx TSN QBU/QBR - Frame Preemption module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Priyadarshini Babu <priyadar@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_axienet.h"
+#include "xilinx_tsn_preemption.h"
+
+/**
+ * axienet_preemption -  Configure Frame Preemption
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: Value to be programmed
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u8 preemp;
+
+	if (copy_from_user(&preemp, useraddr, sizeof(preemp)))
+		return -EFAULT;
+
+	axienet_iow(lp, PREEMPTION_ENABLE_REG, preemp & PREEMPTION_ENABLE);
+	return 0;
+}
+
+/**
+ * axienet_preemption_ctrl -  Configure Frame Preemption Control register
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: Value to be programmed
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_ctrl(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct preempt_ctrl_sts data;
+	u32 value;
+
+	if (copy_from_user(&data, useraddr, sizeof(struct preempt_ctrl_sts)))
+		return -EFAULT;
+	value = axienet_ior(lp, PREEMPTION_CTRL_STS_REG);
+
+	value &= ~(VERIFY_TIMER_VALUE_MASK << VERIFY_TIMER_VALUE_SHIFT);
+	value |= (data.verify_timer_value << VERIFY_TIMER_VALUE_SHIFT);
+	value &= ~(ADDITIONAL_FRAG_SIZE_MASK << ADDITIONAL_FRAG_SIZE_SHIFT);
+	value |= (data.additional_frag_size << ADDITIONAL_FRAG_SIZE_SHIFT);
+	value &= ~(DISABLE_PREEMPTION_VERIFY);
+	value |= (data.disable_preemp_verify);
+
+	axienet_iow(lp, PREEMPTION_CTRL_STS_REG, value);
+	return 0;
+}
+
+/**
+ * axienet_preemption_sts -  Get Frame Preemption Status
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: return value, containing Frame Preemption status
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_sts(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct preempt_ctrl_sts status;
+	u32 value;
+
+	value = axienet_ior(lp, PREEMPTION_CTRL_STS_REG);
+
+	status.tx_preemp_sts = (value & TX_PREEMPTION_STS) ? 1 : 0;
+	status.mac_tx_verify_sts = (value >> MAC_MERGE_TX_VERIFY_STS_SHIFT) &
+					MAC_MERGE_TX_VERIFY_STS_MASK;
+	status.verify_timer_value = (value >> VERIFY_TIMER_VALUE_SHIFT) &
+					VERIFY_TIMER_VALUE_MASK;
+	status.additional_frag_size = (value >> ADDITIONAL_FRAG_SIZE_SHIFT) &
+					ADDITIONAL_FRAG_SIZE_MASK;
+	status.disable_preemp_verify = value & DISABLE_PREEMPTION_VERIFY;
+
+	if (copy_to_user(useraddr, &status, sizeof(struct preempt_ctrl_sts)))
+		return -EFAULT;
+	return 0;
+}
+
+/**
+ * statistic_cnts -  Read statistics counter registers
+ * @ndev: Pointer to the net_device structure
+ * @ptr: Buffer addr to fill the counter values
+ * @count: read #count number of registers
+ * @addr_off: Register address to be read
+ */
+static void statistic_cnts(struct net_device *ndev, void *ptr,
+			   unsigned int count, unsigned int addr_off)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int *buf = (int *)ptr;
+	int i = 0;
+
+	for (i = 0; i < count; i++) {
+		buf[i] = axienet_ior(lp, addr_off);
+		addr_off += 4;
+	}
+}
+
+/**
+ * axienet_preemption_cnt -  Get Frame Preemption Statistics counter
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: return value, containing counters value
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_cnt(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct emac_pmac_stats stats;
+
+	statistic_cnts(ndev, &stats.emac,
+		       sizeof(struct statistics_counters) / 4,
+		       RX_BYTES_EMAC_REG);
+
+	stats.preemp_en = axienet_ior(lp, PREEMPTION_ENABLE_REG);
+	if (stats.preemp_en) {
+		statistic_cnts(ndev, &stats.pmac.sts,
+			       sizeof(struct statistics_counters) / 4,
+			       RX_BYTES_PMAC_REG);
+		statistic_cnts(ndev, &stats.pmac.merge,
+			       sizeof(struct mac_merge_counters) / 4,
+			       TX_HOLD_REG);
+	}
+
+	if (copy_to_user(useraddr, &stats, sizeof(struct emac_pmac_stats)))
+		return -EFAULT;
+	return 0;
+}
+
+/**
+ * axienet_qbu_user_override -  Configure QBU user override register
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: Value to be programmed
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_qbu_user_override(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct qbu_user data;
+	u32 value;
+
+	if (copy_from_user(&data, useraddr, sizeof(struct qbu_user)))
+		return -EFAULT;
+
+	value = axienet_ior(lp, QBU_USER_OVERRIDE_REG);
+
+	if (data.set & QBU_WINDOW) {
+		if (data.user.hold_rel_window) {
+			value |= USER_HOLD_REL_ENABLE_VALUE;
+			value |= HOLD_REL_WINDOW_OVERRIDE;
+		} else {
+			value &= ~(USER_HOLD_REL_ENABLE_VALUE);
+			value &= ~(HOLD_REL_WINDOW_OVERRIDE);
+		}
+	}
+	if (data.set & QBU_GUARD_BAND) {
+		if (data.user.guard_band)
+			value |= GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE;
+		else
+			value &= ~(GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE);
+	}
+	if (data.set & QBU_HOLD_TIME) {
+		if (data.user.hold_time_override) {
+			value |= HOLD_TIME_OVERRIDE;
+			value &= ~(USER_HOLD_TIME_MASK << USER_HOLD_TIME_SHIFT);
+			value |= data.user.user_hold_time <<
+					USER_HOLD_TIME_SHIFT;
+		} else {
+			value &= ~(HOLD_TIME_OVERRIDE);
+			value &= ~(USER_HOLD_TIME_MASK << USER_HOLD_TIME_SHIFT);
+		}
+	}
+	if (data.set & QBU_REL_TIME) {
+		if (data.user.rel_time_override) {
+			value |= REL_TIME_OVERRIDE;
+			value &= ~(USER_REL_TIME_MASK << USER_REL_TIME_SHIFT);
+			value |= data.user.user_rel_time << USER_REL_TIME_SHIFT;
+		} else {
+			value &= ~(REL_TIME_OVERRIDE);
+			value &= ~(USER_REL_TIME_MASK << USER_REL_TIME_SHIFT);
+		}
+	}
+
+	axienet_iow(lp, QBU_USER_OVERRIDE_REG, value);
+	return 0;
+}
+
+/**
+ * axienet_qbu_sts -  Get QBU Core status
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: return value, containing QBU core status value
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_qbu_sts(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct qbu_core_status status;
+	u32 value = 0;
+
+	value = axienet_ior(lp, QBU_CORE_STS_REG);
+	status.hold_time = (value >> HOLD_TIME_STS_SHIFT) & HOLD_TIME_STS_MASK;
+	status.rel_time = (value >> REL_TIME_STS_SHIFT) & REL_TIME_STS_MASK;
+	status.hold_rel_en = (value & HOLD_REL_ENABLE_STS) ? 1 : 0;
+	status.pmac_hold_req = value & PMAC_HOLD_REQ_STS;
+
+	if (copy_to_user(useraddr, &status, sizeof(struct qbu_core_status)))
+		return -EFAULT;
+	return 0;
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_preemption.h b/drivers/net/ethernet/xilinx/xilinx_tsn_preemption.h
new file mode 100644
index 000000000..d86555136
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_preemption.h
@@ -0,0 +1,159 @@
+/**
+ * Xilinx TSN QBU/QBR - Frame Preemption header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Priyadarshini Babu <priyadar@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef XILINX_TSN_PREEMPTION_H
+#define XILINX_TSN_PREEMPTION_H
+
+#define PREEMPTION_ENABLE_REG			0x00000440
+#define PREEMPTION_CTRL_STS_REG			0x00000444
+#define QBU_USER_OVERRIDE_REG			0x00000448
+#define QBU_CORE_STS_REG			0x0000044c
+#define TX_HOLD_REG				0x00000910
+#define RX_BYTES_EMAC_REG			0x00000200
+#define RX_BYTES_PMAC_REG			0x00000800
+
+#define PREEMPTION_ENABLE			BIT(0)
+
+#define TX_PREEMPTION_STS			BIT(31)
+#define MAC_MERGE_TX_VERIFY_STS_MASK		0x7
+#define MAC_MERGE_TX_VERIFY_STS_SHIFT		24
+#define VERIFY_TIMER_VALUE_MASK			0x7F
+#define VERIFY_TIMER_VALUE_SHIFT		8
+#define ADDITIONAL_FRAG_SIZE_MASK		0x3
+#define ADDITIONAL_FRAG_SIZE_SHIFT		4
+#define DISABLE_PREEMPTION_VERIFY		BIT(0)
+
+#define USER_HOLD_REL_ENABLE_VALUE		BIT(31)
+#define USER_HOLD_TIME_MASK			0x1FF
+#define USER_HOLD_TIME_SHIFT			16
+#define USER_REL_TIME_MASK			0x3F
+#define USER_REL_TIME_SHIFT			8
+#define GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE	BIT(3)
+#define HOLD_REL_WINDOW_OVERRIDE		BIT(2)
+#define HOLD_TIME_OVERRIDE			BIT(1)
+#define REL_TIME_OVERRIDE			BIT(0)
+
+#define HOLD_REL_ENABLE_STS			BIT(31)
+#define HOLD_TIME_STS_MASK			0x1FF
+#define HOLD_TIME_STS_SHIFT			16
+#define REL_TIME_STS_MASK			0x3F
+#define REL_TIME_STS_SHIFT			8
+#define PMAC_HOLD_REQ_STS			BIT(0)
+
+struct preempt_ctrl_sts {
+	u8 tx_preemp_sts:1;
+	u8 mac_tx_verify_sts:3;
+	u8 verify_timer_value:7;
+	u8 additional_frag_size:2;
+	u8 disable_preemp_verify:1;
+} __packed;
+
+struct qbu_user_override {
+	u8 enable_value:1;
+	u16 user_hold_time:9;
+	u8 user_rel_time:6;
+	u8 guard_band:1;
+	u8 hold_rel_window:1;
+	u8 hold_time_override:1;
+	u8 rel_time_override:1;
+} __packed;
+
+struct qbu_user {
+	struct qbu_user_override user;
+	u8 set;
+};
+
+#define QBU_WINDOW BIT(0)
+#define QBU_GUARD_BAND BIT(1)
+#define QBU_HOLD_TIME BIT(2)
+#define QBU_REL_TIME BIT(3)
+
+struct qbu_core_status {
+	u16 hold_time;
+	u8 rel_time;
+	u8 hold_rel_en:1;
+	u8 pmac_hold_req:1;
+} __packed;
+
+struct cnt_64 {
+	unsigned int msb;
+	unsigned int lsb;
+};
+
+union static_cntr {
+	u64 cnt;
+	struct cnt_64 word;
+};
+
+struct mac_merge_counters {
+	union static_cntr tx_hold_cnt;
+	union static_cntr tx_frag_cnt;
+	union static_cntr rx_assembly_ok_cnt;
+	union static_cntr rx_assembly_err_cnt;
+	union static_cntr rx_smd_err_cnt;
+	union static_cntr rx_frag_cnt;
+};
+
+struct statistics_counters {
+	union static_cntr rx_bytes_cnt;
+	union static_cntr tx_bytes_cnt;
+	union static_cntr undersize_frames_cnt;
+	union static_cntr frag_frames_cnt;
+	union static_cntr rx_64_bytes_frames_cnt;
+	union static_cntr rx_65_127_bytes_frames_cnt;
+	union static_cntr rx_128_255_bytes_frames_cnt;
+	union static_cntr rx_256_511_bytes_frames_cnt;
+	union static_cntr rx_512_1023_bytes_frames_cnt;
+	union static_cntr rx_1024_max_frames_cnt;
+	union static_cntr rx_oversize_frames_cnt;
+	union static_cntr tx_64_bytes_frames_cnt;
+	union static_cntr tx_65_127_bytes_frames_cnt;
+	union static_cntr tx_128_255_bytes_frames_cnt;
+	union static_cntr tx_256_511_bytes_frames_cnt;
+	union static_cntr tx_512_1023_bytes_frames_cnt;
+	union static_cntr tx_1024_max_frames_cnt;
+	union static_cntr tx_oversize_frames_cnt;
+	union static_cntr rx_good_frames_cnt;
+	union static_cntr rx_fcs_err_cnt;
+	union static_cntr rx_good_broadcast_frames_cnt;
+	union static_cntr rx_good_multicast_frames_cnt;
+	union static_cntr rx_good_control_frames_cnt;
+	union static_cntr rx_out_of_range_err_cnt;
+	union static_cntr rx_good_vlan_frames_cnt;
+	union static_cntr rx_good_pause_frames_cnt;
+	union static_cntr rx_bad_opcode_frames_cnt;
+	union static_cntr tx_good_frames_cnt;
+	union static_cntr tx_good_broadcast_frames_cnt;
+	union static_cntr tx_good_multicast_frames_cnt;
+	union static_cntr tx_underrun_err_cnt;
+	union static_cntr tx_good_control_frames_cnt;
+	union static_cntr tx_good_vlan_frames_cnt;
+	union static_cntr tx_good_pause_frames_cnt;
+};
+
+struct pmac_counters {
+	struct statistics_counters sts;
+	struct mac_merge_counters merge;
+};
+
+struct emac_pmac_stats {
+	u8 preemp_en;
+	struct statistics_counters emac;
+	struct pmac_counters pmac;
+};
+
+#endif /* XILINX_TSN_PREEMPTION_H */
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_ptp.h b/drivers/net/ethernet/xilinx/xilinx_tsn_ptp.h
new file mode 100644
index 000000000..d81b0acf1
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_ptp.h
@@ -0,0 +1,88 @@
+/*
+ * Xilinx TSN PTP header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _TSN_PTP_H_
+#define _TSN_PTP_H_
+
+#define PTP_HW_TSTAMP_SIZE  8   /* 64 bit timestamp */
+#define PTP_RX_HWBUF_SIZE   256
+#define PTP_RX_FRAME_SIZE   252
+#define PTP_HW_TSTAMP_OFFSET (PTP_RX_HWBUF_SIZE - PTP_HW_TSTAMP_SIZE)
+
+#define PTP_MSG_TYPE_MASK				BIT(3)
+#define PTP_TYPE_SYNC                                   0x0
+#define PTP_TYPE_FOLLOW_UP                              0x8
+#define PTP_TYPE_PDELAYREQ                              0x2
+#define PTP_TYPE_PDELAYRESP                             0x3
+#define PTP_TYPE_PDELAYRESP_FOLLOW_UP                   0xA
+#define PTP_TYPE_ANNOUNCE                               0xB
+#define PTP_TYPE_SIGNALING                              0xC
+
+#define PTP_TX_CONTROL_OFFSET		0x00012000 /**< Tx PTP Control Reg */
+#define PTP_RX_CONTROL_OFFSET		0x00012004 /**< Rx PTP Control Reg */
+#define RX_FILTER_CONTROL		0x00012008 /**< Rx Filter Ctrl Reg */
+
+#define PTP_RX_BASE_OFFSET		0x00010000
+#define PTP_RX_CONTROL_OFFSET		0x00012004 /**< Rx PTP Control Reg */
+#define PTP_RX_PACKET_FIELD_MASK	0x00000F00
+#define PTP_RX_PACKET_CLEAR		0x00000001
+
+#define PTP_TX_BUFFER_OFFSET(index)	   (0x00011000 + (index) * 0x100)
+
+#define PTP_TX_CMD_FIELD_LEN			8
+#define PTP_TX_CMD_1STEP_SHIFT			BIT(16)
+#define PTP_TX_BUFFER_CMD2_FIELD		0x4
+
+#define PTP_TX_SYNC_OFFSET                 0x00011000
+#define PTP_TX_FOLLOW_UP_OFFSET            0x00011100
+#define PTP_TX_PDELAYREQ_OFFSET            0x00011200
+#define PTP_TX_PDELAYRESP_OFFSET           0x00011300
+#define PTP_TX_PDELAYRESP_FOLLOW_UP_OFFSET 0x00011400
+#define PTP_TX_ANNOUNCE_OFFSET             0x00011500
+#define PTP_TX_SIGNALING_OFFSET		   0x00011600
+#define PTP_TX_GENERIC_OFFSET		   0x00011700
+#define PTP_TX_SEND_SYNC_FRAME_MASK                     0x00000001
+#define PTP_TX_SEND_FOLLOWUP_FRAME_MASK                 0x00000002
+#define PTP_TX_SEND_PDELAYREQ_FRAME_MASK                0x00000004
+#define PTP_TX_SEND_PDELAYRESP_FRAME_MASK               0x00000008
+#define PTP_TX_SEND_PDELAYRESPFOLLOWUP_FRAME_MASK       0x00000010
+#define PTP_TX_SEND_ANNOUNCE_FRAME_MASK                 0x00000020
+#define PTP_TX_SEND_FRAME6_BIT_MASK                     0x00000040
+#define PTP_TX_SEND_FRAME7_BIT_MASK                     0x00000080
+#define PTP_TX_FRAME_WAITING_MASK			0x0000ff00
+#define PTP_TX_FRAME_WAITING_SHIFT			8
+#define PTP_TX_WAIT_SYNC_FRAME_MASK                     0x00000100
+#define PTP_TX_WAIT_FOLLOWUP_FRAME_MASK                 0x00000200
+#define PTP_TX_WAIT_PDELAYREQ_FRAME_MASK                0x00000400
+#define PTP_TX_WAIT_PDELAYRESP_FRAME_MASK               0x00000800
+#define PTP_TX_WAIT_PDELAYRESPFOLLOWUP_FRAME_MASK       0x00001000
+#define PTP_TX_WAIT_ANNOUNCE_FRAME_MASK                 0x00002000
+#define PTP_TX_WAIT_FRAME6_BIT_MASK                     0x00004000
+#define PTP_TX_WAIT_FRAME7_BIT_MASK                     0x00008000
+#define PTP_TX_WAIT_ALL_FRAMES_MASK                     0x0000FF00
+#define PTP_TX_PACKET_FIELD_MASK                        0x00070000
+#define PTP_TX_PACKET_FIELD_SHIFT                       16
+/* 1-step Correction Field offset 802.1 ASrev */
+#define PTP_CRCT_FIELD_OFFSET				22
+/* 1-step Time Of Day offset 1588-2008 */
+#define PTP_TOD_FIELD_OFFSET				48
+
+int axienet_ptp_xmit(struct sk_buff *skb, struct net_device *ndev);
+irqreturn_t axienet_ptp_rx_irq(int irq, void *_ndev);
+irqreturn_t axienet_ptp_tx_irq(int irq, void *_ndev);
+
+#endif
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_ptp_clock.c b/drivers/net/ethernet/xilinx/xilinx_tsn_ptp_clock.c
new file mode 100644
index 000000000..05c906019
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_ptp_clock.c
@@ -0,0 +1,325 @@
+/*
+ * Xilinx FPGA Xilinx TSN PTP protocol clock Controller module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/device.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/ptp_clock_kernel.h>
+#include <linux/platform_device.h>
+#include <linux/of_irq.h>
+#include "xilinx_tsn_timer.h"
+
+struct xlnx_ptp_timer {
+	struct                 device *dev;
+	void __iomem          *baseaddr;
+	struct ptp_clock      *ptp_clock;
+	struct ptp_clock_info  ptp_clock_info;
+	spinlock_t             reg_lock; /* ptp timer lock */
+	int                    irq;
+	int                    pps_enable;
+	int                    countpulse;
+};
+
+static void xlnx_tod_read(struct xlnx_ptp_timer *timer, struct timespec64 *ts)
+{
+	u32 sec, nsec;
+
+	nsec = in_be32(timer->baseaddr + XTIMER1588_CURRENT_RTC_NS);
+	sec = in_be32(timer->baseaddr + XTIMER1588_CURRENT_RTC_SEC_L);
+
+	ts->tv_sec = sec;
+	ts->tv_nsec = nsec;
+}
+
+static void xlnx_rtc_offset_write(struct xlnx_ptp_timer *timer,
+				  const struct timespec64 *ts)
+{
+	pr_debug("%s: sec: %ld nsec: %ld\n", __func__, ts->tv_sec, ts->tv_nsec);
+
+	out_be32((timer->baseaddr + XTIMER1588_RTC_OFFSET_SEC_H), 0);
+	out_be32((timer->baseaddr + XTIMER1588_RTC_OFFSET_SEC_L),
+		 (ts->tv_sec));
+	out_be32((timer->baseaddr + XTIMER1588_RTC_OFFSET_NS), ts->tv_nsec);
+}
+
+static void xlnx_rtc_offset_read(struct xlnx_ptp_timer *timer,
+				 struct timespec64 *ts)
+{
+	ts->tv_sec = in_be32(timer->baseaddr + XTIMER1588_RTC_OFFSET_SEC_L);
+	ts->tv_nsec = in_be32(timer->baseaddr + XTIMER1588_RTC_OFFSET_NS);
+}
+
+/* PTP clock operations
+ */
+static int xlnx_ptp_adjfreq(struct ptp_clock_info *ptp, s32 ppb)
+{
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+
+	int neg_adj = 0;
+	u64 freq;
+	u32 diff, incval;
+
+	/* This number should be replaced by a call to get the frequency
+	 * from the device-tree. Currently assumes 125MHz
+	 */
+	incval = 0x800000;
+	/* for 156.25 MHZ Ref clk the value is  incval = 0x800000; */
+
+	if (ppb < 0) {
+		neg_adj = 1;
+		ppb = -ppb;
+	}
+
+	freq = incval;
+	freq *= ppb;
+	diff = div_u64(freq, 1000000000ULL);
+
+	pr_debug("%s: adj: %d ppb: %d\n", __func__, diff, ppb);
+
+	incval = neg_adj ? (incval - diff) : (incval + diff);
+	out_be32((timer->baseaddr + XTIMER1588_RTC_INCREMENT), incval);
+	return 0;
+}
+
+static int xlnx_ptp_adjtime(struct ptp_clock_info *ptp, s64 delta)
+{
+	unsigned long flags;
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+	struct timespec64 now, then = ns_to_timespec64(delta);
+
+	spin_lock_irqsave(&timer->reg_lock, flags);
+
+	xlnx_rtc_offset_read(timer, &now);
+
+	now = timespec64_add(now, then);
+
+	xlnx_rtc_offset_write(timer, (const struct timespec64 *)&now);
+	spin_unlock_irqrestore(&timer->reg_lock, flags);
+
+	return 0;
+}
+
+static int xlnx_ptp_gettime(struct ptp_clock_info *ptp, struct timespec64 *ts)
+{
+	unsigned long flags;
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+	spin_lock_irqsave(&timer->reg_lock, flags);
+
+	xlnx_tod_read(timer, ts);
+
+	spin_unlock_irqrestore(&timer->reg_lock, flags);
+	return 0;
+}
+
+/**
+ * xlnx_ptp_settime - Set the current time on the hardware clock
+ * @ptp: ptp clock structure
+ * @ts: timespec64 containing the new time for the cycle counter
+ *
+ * Return: 0 in all cases.
+ *
+ * The seconds register is written first, then the nanosecond
+ * The hardware loads the entire new value when a nanosecond register
+ * is written
+ **/
+static int xlnx_ptp_settime(struct ptp_clock_info *ptp,
+			    const struct timespec64 *ts)
+{
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+	struct timespec64 delta, tod;
+	struct timespec64 offset;
+	unsigned long flags;
+
+	spin_lock_irqsave(&timer->reg_lock, flags);
+
+	/* First zero the offset */
+	offset.tv_sec = 0;
+	offset.tv_nsec = 0;
+	xlnx_rtc_offset_write(timer, &offset);
+
+	/* Get the current timer value */
+	xlnx_tod_read(timer, &tod);
+
+	/* Subtract the current reported time from our desired time */
+	delta = timespec64_sub(*ts, tod);
+
+	/* Don't write a negative offset */
+	if (delta.tv_sec <= 0) {
+		delta.tv_sec = 0;
+		if (delta.tv_nsec < 0)
+			delta.tv_nsec = 0;
+	}
+
+	xlnx_rtc_offset_write(timer, &delta);
+	spin_unlock_irqrestore(&timer->reg_lock, flags);
+	return 0;
+}
+
+static int xlnx_ptp_enable(struct ptp_clock_info *ptp,
+			   struct ptp_clock_request *rq, int on)
+{
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+
+	switch (rq->type) {
+	case PTP_CLK_REQ_PPS:
+		timer->pps_enable = 1;
+		return 0;
+	default:
+		break;
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static struct ptp_clock_info xlnx_ptp_clock_info = {
+	.owner    = THIS_MODULE,
+	.name     = "Xilinx Timer",
+	.max_adj  = 999999999,
+	.n_ext_ts	= 0,
+	.pps      = 1,
+	.adjfreq  = xlnx_ptp_adjfreq,
+	.adjtime  = xlnx_ptp_adjtime,
+	.gettime64  = xlnx_ptp_gettime,
+	.settime64 = xlnx_ptp_settime,
+	.enable   = xlnx_ptp_enable,
+};
+
+/* module operations */
+
+/**
+ * xlnx_ptp_timer_isr - Interrupt Service Routine
+ * @irq:               IRQ number
+ * @priv:              pointer to the timer structure
+ *
+ * Returns: IRQ_HANDLED for all cases
+ *
+ * Handles the timer interrupt. The timer interrupt fires 128 times per
+ * secound. When our count reaches 128 emit a PTP_CLOCK_PPS event
+ */
+static irqreturn_t xlnx_ptp_timer_isr(int irq, void *priv)
+{
+	struct xlnx_ptp_timer *timer = (struct xlnx_ptp_timer *)priv;
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_PPS;
+	++timer->countpulse;
+	if (timer->countpulse >= PULSESIN1PPS) {
+		timer->countpulse = 0;
+		if ((timer->ptp_clock) && (timer->pps_enable))
+			ptp_clock_event(timer->ptp_clock, &event);
+	}
+	out_be32((timer->baseaddr + XTIMER1588_INTERRUPT),
+		 (1 << XTIMER1588_INT_SHIFT));
+
+	return IRQ_HANDLED;
+}
+
+int axienet_ptp_timer_remove(void *priv)
+{
+	struct xlnx_ptp_timer *timer = (struct xlnx_ptp_timer *)priv;
+
+	free_irq(timer->irq, (void *)timer);
+
+	axienet_phc_index = -1;
+	if (timer->ptp_clock) {
+		ptp_clock_unregister(timer->ptp_clock);
+		timer->ptp_clock = NULL;
+	}
+	kfree(timer);
+	return 0;
+}
+
+int axienet_get_phc_index(void *priv)
+{
+	struct xlnx_ptp_timer *timer = (struct xlnx_ptp_timer *)priv;
+
+	if (timer->ptp_clock)
+		return ptp_clock_index(timer->ptp_clock);
+	else
+		return -1;
+}
+
+void *axienet_ptp_timer_probe(void __iomem *base, struct platform_device *pdev)
+{
+	struct xlnx_ptp_timer *timer;
+	struct timespec64 ts;
+	int err = 0;
+
+	timer = kzalloc(sizeof(*timer), GFP_KERNEL);
+	if (!timer)
+		return NULL;
+
+	timer->baseaddr = base;
+
+	timer->irq = platform_get_irq_byname(pdev, "interrupt_ptp_timer");
+
+	if (timer->irq < 0) {
+		timer->irq = platform_get_irq_byname(pdev, "rtc_irq");
+		if (timer->irq > 0) {
+			pr_err("ptp timer interrupt name 'rtc_irq' is"
+				"deprecated\n");
+		} else {
+			pr_err("ptp timer interrupt not found\n");
+			kfree(timer);
+			return NULL;
+		}
+	}
+	spin_lock_init(&timer->reg_lock);
+
+	timer->ptp_clock_info = xlnx_ptp_clock_info;
+
+	timer->ptp_clock = ptp_clock_register(&timer->ptp_clock_info,
+					      &pdev->dev);
+
+	if (IS_ERR(timer->ptp_clock)) {
+		err = PTR_ERR(timer->ptp_clock);
+		pr_debug("Failed to register ptp clock\n");
+		goto out;
+	}
+
+	axienet_phc_index = ptp_clock_index(timer->ptp_clock);
+
+	ts = ktime_to_timespec64(ktime_get_real());
+
+	xlnx_ptp_settime(&timer->ptp_clock_info, &ts);
+
+	/* Enable interrupts */
+	err = request_irq(timer->irq,
+			  xlnx_ptp_timer_isr,
+			  0,
+			  "ptp_rtc",
+			  (void *)timer);
+	if (err)
+		goto err_irq;
+
+	return timer;
+
+err_irq:
+	ptp_clock_unregister(timer->ptp_clock);
+out:
+	timer->ptp_clock = NULL;
+	return NULL;
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_ptp_xmit.c b/drivers/net/ethernet/xilinx/xilinx_tsn_ptp_xmit.c
new file mode 100644
index 000000000..831b4b7b5
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_ptp_xmit.c
@@ -0,0 +1,369 @@
+/*
+ * Xilinx FPGA Xilinx TSN PTP transfer protocol module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_axienet.h"
+#include "xilinx_tsn_ptp.h"
+#include "xilinx_tsn_timer.h"
+#include <linux/ptp_classify.h>
+
+#define PTP_ONE_SECOND            1000000000    /**< Value in ns */
+
+#define msg_type_string(type) \
+	((type) == PTP_TYPE_SYNC) ? "SYNC" : \
+	((type) == PTP_TYPE_FOLLOW_UP)		  ? "FOLLOW_UP" : \
+	((type) == PTP_TYPE_PDELAYREQ)		  ? "PDELAY_REQ" : \
+	((type) == PTP_TYPE_PDELAYRESP)		  ? "PDELAY_RESP" : \
+	((type) == PTP_TYPE_PDELAYRESP_FOLLOW_UP) ? "PDELAY_RESP_FOLLOW_UP" : \
+	((type) == PTP_TYPE_ANNOUNCE)		  ? "ANNOUNCE" : \
+	"UNKNOWN"
+
+/**
+ * memcpy_fromio_32 - copy ptp buffer from HW
+ * @lp:		Pointer to axienet local structure
+ * @offset:	Offset in the PTP buffer
+ * @data:	Destination buffer
+ * @len:	Len to copy
+ *
+ * This functions copies the data from PTP buffer to destination data buffer
+ */
+static void memcpy_fromio_32(struct axienet_local *lp,
+			     unsigned long offset, u8 *data, size_t len)
+{
+	while (len >= 4) {
+		*(u32 *)data = axienet_ior(lp, offset);
+		len -= 4;
+		offset += 4;
+		data += 4;
+	}
+
+	if (len > 0) {
+		u32 leftover = axienet_ior(lp, offset);
+		u8 *src = (u8 *)&leftover;
+
+		while (len) {
+			*data++ = *src++;
+			len--;
+		}
+	}
+}
+
+/**
+ * memcpy_toio_32 - copy ptp buffer from HW
+ * @lp:		Pointer to axienet local structure
+ * @offset:	Offset in the PTP buffer
+ * @data:	Source data
+ * @len:	Len to copy
+ *
+ * This functions copies the source data to desination ptp buffer
+ */
+static void memcpy_toio_32(struct axienet_local *lp,
+			   unsigned long offset, u8 *data, size_t len)
+{
+	while (len >= 4) {
+		axienet_iow(lp, offset, *(u32 *)data);
+		len -= 4;
+		offset += 4;
+		data += 4;
+	}
+
+	if (len > 0) {
+		u32 leftover = 0;
+		u8 *dest = (u8 *)&leftover;
+
+		while (len) {
+			*dest++ = *data++;
+			len--;
+		}
+		axienet_iow(lp, offset, leftover);
+	}
+}
+
+static int is_sync(struct sk_buff *skb)
+{
+	u8 *msg_type;
+
+	msg_type = (u8 *)skb->data + ETH_HLEN;
+
+	return (*msg_type & 0xf) == PTP_TYPE_SYNC;
+}
+
+/**
+ * axienet_ptp_xmit - xmit skb using PTP HW
+ * @skb:	sk_buff pointer that contains data to be Txed.
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: NETDEV_TX_OK, on success
+ *	    NETDEV_TX_BUSY, if any of the descriptors are not free
+ *
+ * This function is called to transmit a PTP skb. The function uses
+ * the free PTP TX buffer entry and sends the frame
+ */
+int axienet_ptp_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	u8 msg_type;
+	struct axienet_local *lp = netdev_priv(ndev);
+	unsigned long flags;
+	u8 tx_frame_waiting;
+	u8 free_index;
+	u32 cmd1_field = 0;
+	u32 cmd2_field = 0;
+
+	msg_type  = *(u8 *)(skb->data + ETH_HLEN);
+
+	pr_debug("  -->XMIT: protocol: %x message: %s frame_len: %d\n",
+		 skb->protocol,
+		 msg_type_string(msg_type & 0xf), skb->len);
+
+	tx_frame_waiting =  (axienet_ior(lp, PTP_TX_CONTROL_OFFSET) &
+				PTP_TX_FRAME_WAITING_MASK) >>
+				PTP_TX_FRAME_WAITING_SHIFT;
+
+	/* we reached last frame */
+	if (tx_frame_waiting & (1 << 7)) {
+		if (!netif_queue_stopped(ndev))
+			netif_stop_queue(ndev);
+		pr_debug("tx_frame_waiting: %d\n", tx_frame_waiting);
+		return NETDEV_TX_BUSY;
+	}
+
+	/* go to next available slot */
+	free_index  = fls(tx_frame_waiting);
+
+	/* write the len */
+	if (lp->ptp_ts_type == HWTSTAMP_TX_ONESTEP_SYNC &&
+	    is_sync(skb)) {
+		/* enable 1STEP SYNC */
+		cmd1_field |= PTP_TX_CMD_1STEP_SHIFT;
+		cmd2_field |= PTP_TOD_FIELD_OFFSET;
+	}
+
+	cmd1_field |= skb->len;
+
+	axienet_iow(lp, PTP_TX_BUFFER_OFFSET(free_index), cmd1_field);
+	axienet_iow(lp, PTP_TX_BUFFER_OFFSET(free_index) +
+			PTP_TX_BUFFER_CMD2_FIELD, cmd2_field);
+	memcpy_toio_32(lp,
+		       (PTP_TX_BUFFER_OFFSET(free_index) +
+			PTP_TX_CMD_FIELD_LEN),
+		       skb->data, skb->len);
+
+	/* send the frame */
+	axienet_iow(lp, PTP_TX_CONTROL_OFFSET, (1 << free_index));
+
+	if (lp->ptp_ts_type != HWTSTAMP_TX_ONESTEP_SYNC ||
+	    (!is_sync(skb))) {
+		spin_lock_irqsave(&lp->ptp_tx_lock, flags);
+		skb->cb[0] = free_index;
+		skb_queue_tail(&lp->ptp_txq, skb);
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)
+			skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+
+		skb_tx_timestamp(skb);
+		spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+	}
+	return NETDEV_TX_OK;
+}
+
+/**
+ * axienet_set_timestamp - timestamp skb with HW timestamp
+ * @lp:		Pointer to axienet local structure
+ * @hwtstamps:  Pointer to skb timestamp structure
+ * @offset:	offset of the timestamp in the PTP buffer
+ *
+ * Return:	None.
+ *
+ */
+static void axienet_set_timestamp(struct axienet_local *lp,
+				  struct skb_shared_hwtstamps *hwtstamps,
+				  unsigned int offset)
+{
+	u32 captured_ns;
+	u32 captured_sec;
+
+	captured_ns = axienet_ior(lp, offset + 4);
+	captured_sec = axienet_ior(lp, offset);
+
+	/* Upper 32 bits contain s, lower 32 bits contain ns. */
+	hwtstamps->hwtstamp = ktime_set(captured_sec,
+						captured_ns);
+}
+
+/**
+ * axienet_ptp_recv - receive ptp buffer in skb from HW
+ * @ndev:	Pointer to net_device structure.
+ *
+ * This function is called from the ptp rx isr. It allocates skb, and
+ * copies the ptp rx buffer data to it and calls netif_rx for further
+ * processing.
+ *
+ */
+static void axienet_ptp_recv(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	unsigned long ptp_frame_base_addr = 0;
+	struct sk_buff *skb;
+	u16 msg_len;
+	u8 msg_type;
+	u32 bytes = 0;
+	u32 packets = 0;
+
+	pr_debug("%s:\n ", __func__);
+
+	while (((lp->ptp_rx_hw_pointer & 0xf) !=
+		 (lp->ptp_rx_sw_pointer & 0xf))) {
+		skb = netdev_alloc_skb(ndev, PTP_RX_FRAME_SIZE);
+
+		lp->ptp_rx_sw_pointer += 1;
+
+		ptp_frame_base_addr = PTP_RX_BASE_OFFSET +
+				   ((lp->ptp_rx_sw_pointer & 0xf) *
+							PTP_RX_HWBUF_SIZE);
+
+		memset(skb->data, 0x0, PTP_RX_FRAME_SIZE);
+
+		memcpy_fromio_32(lp, ptp_frame_base_addr, skb->data,
+				 PTP_RX_FRAME_SIZE);
+
+		msg_type  = *(u8 *)(skb->data + ETH_HLEN) & 0xf;
+		msg_len  = *(u16 *)(skb->data + ETH_HLEN + 2);
+
+		skb_put(skb, ntohs(msg_len) + ETH_HLEN);
+
+		bytes += skb->len;
+		packets++;
+
+		skb->protocol = eth_type_trans(skb, ndev);
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+		pr_debug("  -->RECV: protocol: %x message: %s frame_len: %d\n",
+			 skb->protocol, msg_type_string(msg_type & 0xf),
+			 skb->len);
+		/* timestamp only event messages */
+		if (!(msg_type & PTP_MSG_TYPE_MASK)) {
+			axienet_set_timestamp(lp, skb_hwtstamps(skb),
+					      (ptp_frame_base_addr +
+					      PTP_HW_TSTAMP_OFFSET));
+		}
+
+		netif_rx(skb);
+	}
+	ndev->stats.rx_packets += packets;
+	ndev->stats.rx_bytes += bytes;
+}
+
+/**
+ * axienet_ptp_rx_irq - PTP RX ISR handler
+ * @irq:		irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return:	IRQ_HANDLED for all cases.
+ */
+irqreturn_t axienet_ptp_rx_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	pr_debug("%s: received\n ", __func__);
+	lp->ptp_rx_hw_pointer = (axienet_ior(lp, PTP_RX_CONTROL_OFFSET)
+					& PTP_RX_PACKET_FIELD_MASK)  >> 8;
+
+	axienet_ptp_recv(ndev);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * axienet_tx_tstamp - timestamp skb on trasmit path
+ * @work:	Pointer to work_struct structure
+ *
+ * This adds TX timestamp to skb
+ */
+void axienet_tx_tstamp(struct work_struct *work)
+{
+	struct axienet_local *lp = container_of(work, struct axienet_local,
+			tx_tstamp_work);
+	struct net_device *ndev = lp->ndev;
+	struct skb_shared_hwtstamps hwtstamps;
+	struct sk_buff *skb;
+	unsigned long ts_reg_offset;
+	unsigned long flags;
+	u8 tx_packet;
+	u8 index;
+	u32 bytes = 0;
+	u32 packets = 0;
+
+	memset(&hwtstamps, 0, sizeof(struct skb_shared_hwtstamps));
+
+	spin_lock_irqsave(&lp->ptp_tx_lock, flags);
+
+	tx_packet =  (axienet_ior(lp, PTP_TX_CONTROL_OFFSET) &
+				PTP_TX_PACKET_FIELD_MASK) >>
+				PTP_TX_PACKET_FIELD_SHIFT;
+
+	while ((skb = __skb_dequeue(&lp->ptp_txq)) != NULL) {
+		index = skb->cb[0];
+
+		/* dequeued packet yet to be xmited? */
+		if (index > tx_packet) {
+			/* enqueue it back and break */
+			skb_queue_tail(&lp->ptp_txq, skb);
+			break;
+		}
+		/* time stamp reg offset */
+		ts_reg_offset = PTP_TX_BUFFER_OFFSET(index) +
+					PTP_HW_TSTAMP_OFFSET;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS) {
+			axienet_set_timestamp(lp, &hwtstamps, ts_reg_offset);
+			skb_tstamp_tx(skb, &hwtstamps);
+		}
+
+		bytes += skb->len;
+		packets++;
+		dev_kfree_skb_any(skb);
+	}
+	ndev->stats.tx_packets += packets;
+	ndev->stats.tx_bytes += bytes;
+
+	spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+}
+
+/**
+ * axienet_ptp_tx_irq - PTP TX irq handler
+ * @irq:		irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return:	IRQ_HANDLED for all cases.
+ *
+ */
+irqreturn_t axienet_ptp_tx_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	pr_debug("%s: got tx interrupt\n", __func__);
+
+	/* read ctrl register to clear the interrupt */
+	axienet_ior(lp, PTP_TX_CONTROL_OFFSET);
+
+	schedule_work(&lp->tx_tstamp_work);
+
+	netif_wake_queue(ndev);
+
+	return IRQ_HANDLED;
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_qci.c b/drivers/net/ethernet/xilinx/xilinx_tsn_qci.c
new file mode 100644
index 000000000..20efa6c4d
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_qci.c
@@ -0,0 +1,151 @@
+/*
+ * Xilinx FPGA Xilinx TSN QCI Controller module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_tsn_switch.h"
+
+#define SMC_MODE_SHIFT				28
+#define	SMC_CBR_MASK				0x00FFFFFF
+#define	SMC_EBR_MASK				0x00FFFFFF
+#define IN_PORTID_MASK				0x3
+#define IN_PORT_SHIFT				14
+#define MAX_FR_SIZE_MASK			0x00000FFF
+
+#define GATE_ID_SHIFT				24
+#define METER_ID_SHIFT				8
+#define EN_METER_SHIFT				6
+#define ALLOW_STREM_SHIFT			5
+#define EN_PSFP_SHIFT				4
+#define WR_OP_TYPE_MASK				0x3
+#define WR_OP_TYPE_SHIFT			2
+#define OP_TYPE_SHIFT				1
+#define PSFP_EN_CONTROL_MASK			0x1
+
+/**
+ * psfp_control - Configure thr control for PSFP
+ * @data:	Value to be programmed
+ */
+void psfp_control(struct psfp_config data)
+{
+	u32 mask;
+	u32 timeout = 20000;
+
+	mask = data.gate_id << GATE_ID_SHIFT;
+	mask |= data.meter_id << METER_ID_SHIFT;
+	mask |= data.en_meter << EN_METER_SHIFT;
+	mask |= data.allow_stream << ALLOW_STREM_SHIFT;
+	mask |= data.en_psfp << EN_PSFP_SHIFT;
+	mask |= (data.wr_op_type & WR_OP_TYPE_MASK) << WR_OP_TYPE_SHIFT;
+	mask |= data.op_type << OP_TYPE_SHIFT;
+	mask |= PSFP_EN_CONTROL_MASK;
+
+	axienet_iow(&lp, PSFP_CONTROL_OFFSET, mask);
+
+	/* wait for write to complete */
+	while ((axienet_ior(&lp, PSFP_CONTROL_OFFSET) &
+		PSFP_EN_CONTROL_MASK) && timeout)
+		timeout--;
+
+	if (!timeout)
+		pr_warn("PSFP control write took longer time!!");
+}
+
+/**
+ * get_stream_filter_config -  Get Stream Filter Configuration
+ * @data:	Value returned
+ */
+void get_stream_filter_config(struct stream_filter *data)
+{
+	u32 reg_val;
+
+	reg_val = axienet_ior(&lp, STREAM_FILTER_CONFIG_OFFSET);
+
+	data->max_fr_size = reg_val & MAX_FR_SIZE_MASK;
+	data->in_pid = (reg_val >> IN_PORT_SHIFT) & IN_PORTID_MASK;
+}
+
+/**
+ * config_stream_filter -  Configure Stream Filter Configuration
+ * @data:	Value to be programmed
+ */
+void config_stream_filter(struct stream_filter data)
+{
+	u32 mask;
+
+	mask = ((data.in_pid & IN_PORTID_MASK) << IN_PORT_SHIFT) |
+					(data.max_fr_size & MAX_FR_SIZE_MASK);
+	axienet_iow(&lp, STREAM_FILTER_CONFIG_OFFSET, mask);
+}
+
+/**
+ * get_meter_reg -  Read Stream Meter Configuration registers value
+ * @data:	Value returned
+ */
+void get_meter_reg(struct meter_config *data)
+{
+	u32 conf_r4;
+
+	data->cir = axienet_ior(&lp, STREAM_METER_CIR_OFFSET);
+	data->eir = axienet_ior(&lp, STREAM_METER_EIR_OFFSET);
+	data->cbr = axienet_ior(&lp, STREAM_METER_CBR_OFFSET) & SMC_CBR_MASK;
+	conf_r4 = axienet_ior(&lp, STREAM_METER_EBR_OFFSET);
+
+	data->ebr = conf_r4 & SMC_EBR_MASK;
+	data->mode = (conf_r4 & 0xF0000000) >> SMC_MODE_SHIFT;
+}
+
+/**
+ * program_meter_reg -  configure Stream Meter Configuration registers
+ * @data:	Value to be programmed
+ */
+void program_meter_reg(struct meter_config data)
+{
+	u32 conf_r4;
+
+	axienet_iow(&lp, STREAM_METER_CIR_OFFSET, data.cir);
+	axienet_iow(&lp, STREAM_METER_EIR_OFFSET, data.eir);
+	axienet_iow(&lp, STREAM_METER_CBR_OFFSET, data.cbr & SMC_CBR_MASK);
+
+	conf_r4 = (data.ebr & SMC_EBR_MASK) | (data.mode << SMC_MODE_SHIFT);
+	axienet_iow(&lp, STREAM_METER_EBR_OFFSET, conf_r4);
+}
+
+/**
+ * get_psfp_static_counter -  get memory static counters value
+ * @data  :	return value, containing counter value
+ */
+void get_psfp_static_counter(struct psfp_static_counter *data)
+{
+	int offset = (data->num) * 8;
+
+	data->psfp_fr_count.lsb = axienet_ior(&lp, TOTAL_PSFP_FRAMES_OFFSET +
+									offset);
+	data->psfp_fr_count.msb = axienet_ior(&lp, TOTAL_PSFP_FRAMES_OFFSET  +
+								offset + 0x4);
+
+	data->err_filter_ins_port.lsb = axienet_ior(&lp,
+					FLTR_INGS_PORT_ERR_OFFSET + offset);
+	data->err_filter_ins_port.msb = axienet_ior(&lp,
+				FLTR_INGS_PORT_ERR_OFFSET + offset + 0x4);
+
+	data->err_filtr_sdu.lsb = axienet_ior(&lp, FLTR_STDU_ERR_OFFSET +
+									offset);
+	data->err_filtr_sdu.msb = axienet_ior(&lp, FLTR_STDU_ERR_OFFSET +
+								offset + 0x4);
+
+	data->err_meter.lsb = axienet_ior(&lp, METER_ERR_OFFSET + offset);
+	data->err_meter.msb = axienet_ior(&lp, METER_ERR_OFFSET + offset + 0x4);
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_shaper.c b/drivers/net/ethernet/xilinx/xilinx_tsn_shaper.c
new file mode 100644
index 000000000..b2abbf84c
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_shaper.c
@@ -0,0 +1,228 @@
+/*
+ * Xilinx FPGA Xilinx TSN QBV sheduler module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_axienet.h"
+#include "xilinx_tsn_shaper.h"
+
+static inline int axienet_map_gs_to_hw(struct axienet_local *lp, u32 gs)
+{
+	u8 be_queue = 0;
+	u8 re_queue = 1;
+	u8 st_queue = 2;
+	unsigned int acl_bit_map = 0;
+
+	if (lp->num_tc == 2)
+		st_queue = 1;
+
+	if (gs & GS_BE_OPEN)
+		acl_bit_map |= (1 << be_queue);
+	if (gs & GS_ST_OPEN)
+		acl_bit_map |= (1 << st_queue);
+	if (lp->num_tc == 3 && (gs & GS_RE_OPEN))
+		acl_bit_map |= (1 << re_queue);
+
+	return acl_bit_map;
+}
+
+static int __axienet_set_schedule(struct net_device *ndev, struct qbv_info *qbv)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u16 i;
+	unsigned int acl_bit_map = 0;
+	u32 u_config_change = 0;
+
+	if (qbv->cycle_time == 0) {
+		/* clear the gate enable bit */
+		u_config_change &= ~CC_ADMIN_GATE_ENABLE_BIT;
+		/* open all the gates */
+		u_config_change |= CC_ADMIN_GATE_STATE_SHIFT;
+
+		axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+
+		return 0;
+	}
+
+	if (axienet_qbv_ior(lp, PORT_STATUS) & 1) {
+		if (qbv->force) {
+			u_config_change &= ~CC_ADMIN_GATE_ENABLE_BIT;
+			axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+		} else {
+			return -EALREADY;
+		}
+	}
+	/* write admin time */
+	axienet_qbv_iow(lp, ADMIN_CYCLE_TIME_DENOMINATOR,
+			qbv->cycle_time & CYCLE_TIME_DENOMINATOR_MASK);
+
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_NS, qbv->ptp_time_ns);
+
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_SEC,
+			qbv->ptp_time_sec & 0xFFFFFFFF);
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_SECS,
+			(qbv->ptp_time_sec >> 32) & BASE_TIME_SECS_MASK);
+
+	u_config_change = axienet_qbv_ior(lp, CONFIG_CHANGE);
+
+	u_config_change &= ~(CC_ADMIN_CTRL_LIST_LENGTH_MASK <<
+				CC_ADMIN_CTRL_LIST_LENGTH_SHIFT);
+	u_config_change |= (qbv->list_length & CC_ADMIN_CTRL_LIST_LENGTH_MASK)
+					<< CC_ADMIN_CTRL_LIST_LENGTH_SHIFT;
+
+	/* program each list */
+	for (i = 0; i < qbv->list_length; i++) {
+		acl_bit_map = axienet_map_gs_to_hw(lp, qbv->acl_gate_state[i]);
+		axienet_qbv_iow(lp,  ADMIN_CTRL_LIST(i),
+				(acl_bit_map & (ACL_GATE_STATE_MASK)) <<
+				ACL_GATE_STATE_SHIFT);
+
+	    /* set the time for each entry */
+	    axienet_qbv_iow(lp, ADMIN_CTRL_LIST_TIME(i),
+			    qbv->acl_gate_time[i] &
+			    CTRL_LIST_TIME_INTERVAL_MASK);
+	}
+
+	/* clear interrupt status */
+	axienet_qbv_iow(lp, INT_STATUS, 0);
+
+	/* kick in new config change */
+	u_config_change |= CC_ADMIN_CONFIG_CHANGE_BIT;
+
+	/* enable gate */
+	u_config_change |= CC_ADMIN_GATE_ENABLE_BIT;
+
+	/* start */
+	axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+
+	return 0;
+}
+
+int axienet_set_schedule(struct net_device *ndev, void __user *useraddr)
+{
+	struct qbv_info *config;
+	int ret;
+
+	config = kmalloc(sizeof(*config), GFP_KERNEL);
+	if (!config)
+		return -ENOMEM;
+
+	if (copy_from_user(config, useraddr, sizeof(struct qbv_info))) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	pr_debug("setting new schedule\n");
+
+	ret = __axienet_set_schedule(ndev, config);
+out:
+	kfree(config);
+	return ret;
+}
+
+static int __axienet_get_schedule(struct net_device *ndev, struct qbv_info *qbv)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u16 i = 0;
+	u32 u_value = 0;
+
+	if (!(axienet_qbv_ior(lp, CONFIG_CHANGE) &
+			CC_ADMIN_GATE_ENABLE_BIT)) {
+		qbv->cycle_time = 0;
+		return 0;
+	}
+
+	u_value = axienet_qbv_ior(lp, GATE_STATE);
+	qbv->list_length = (u_value >> CC_ADMIN_CTRL_LIST_LENGTH_SHIFT) &
+				CC_ADMIN_CTRL_LIST_LENGTH_MASK;
+
+	u_value = axienet_qbv_ior(lp, OPER_CYCLE_TIME_DENOMINATOR);
+	qbv->cycle_time = u_value & CYCLE_TIME_DENOMINATOR_MASK;
+
+	u_value = axienet_qbv_ior(lp, OPER_BASE_TIME_NS);
+	qbv->ptp_time_ns = u_value & OPER_BASE_TIME_NS_MASK;
+
+	qbv->ptp_time_sec = axienet_qbv_ior(lp, OPER_BASE_TIME_SEC);
+	u_value = axienet_qbv_ior(lp, OPER_BASE_TIME_SECS);
+	qbv->ptp_time_sec |= (u64)(u_value & BASE_TIME_SECS_MASK) << 32;
+
+	for (i = 0; i < qbv->list_length; i++) {
+		u_value = axienet_qbv_ior(lp, OPER_CTRL_LIST(i));
+		qbv->acl_gate_state[i] = (u_value >> ACL_GATE_STATE_SHIFT) &
+					ACL_GATE_STATE_MASK;
+		/**
+		 * In 2Q system, the actual ST Gate state value is 2,
+		 * for user the ST Gate state value is always 4.
+		 */
+		if (lp->num_tc == 2 && qbv->acl_gate_state[i] == 2)
+			qbv->acl_gate_state[i] = 4;
+
+		u_value = axienet_qbv_ior(lp, OPER_CTRL_LIST_TIME(i));
+		qbv->acl_gate_time[i] = u_value & CTRL_LIST_TIME_INTERVAL_MASK;
+	}
+	return 0;
+}
+
+int axienet_get_schedule(struct net_device *ndev, void __user *useraddr)
+{
+	struct qbv_info *qbv;
+	int ret = 0;
+
+	qbv = kmalloc(sizeof(*qbv), GFP_KERNEL);
+	if (!qbv)
+		return -ENOMEM;
+
+	if (copy_from_user(qbv, useraddr, sizeof(struct qbv_info))) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	__axienet_get_schedule(ndev, qbv);
+
+	if (copy_to_user(useraddr, qbv, sizeof(struct qbv_info)))
+		ret = -EFAULT;
+out:
+	kfree(qbv);
+	return ret;
+}
+
+static irqreturn_t axienet_qbv_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	/* clear status */
+	axienet_qbv_iow(lp, INT_CLEAR, 0);
+
+	return IRQ_HANDLED;
+}
+
+int axienet_qbv_init(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int rc = 0;
+
+	if (lp->qbv_irq > 0)
+		rc = request_irq(lp->qbv_irq, axienet_qbv_irq, 0, ndev->name,
+				 ndev);
+	return rc;
+}
+
+void axienet_qbv_remove(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	free_irq(lp->qbv_irq, ndev);
+}
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_shaper.h b/drivers/net/ethernet/xilinx/xilinx_tsn_shaper.h
new file mode 100644
index 000000000..8149958b4
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_shaper.h
@@ -0,0 +1,136 @@
+/*
+ * Xilinx TSN QBV scheduler header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef XILINX_TSN_SHAPER_H
+#define XILINX_TSN_SHAPER_H
+
+/* 0x0		CONFIG_CHANGE
+ * 0x8		GATE_STATE
+ * 0x10		ADMIN_CTRL_LIST_LENGTH
+ * 0x18		ADMIN_CYCLE_TIME_DENOMINATOR
+ * 0x20         ADMIN_BASE_TIME_NS
+ * 0x24		ADMIN_BASE_TIME_SEC
+ * 0x28		ADMIN_BASE_TIME_SECS
+ * 0x30		INT_STAT
+ * 0x34		INT_EN
+ * 0x38		INT_CLR
+ * 0x3c		STATUS
+ * 0x40		CONFIG_CHANGE_TIME_NS
+ * 0x44		CONFIG_CHANGE_TIME_SEC
+ * 0x48		CONFIG_CHANGE_TIME_SECS
+ * 0x50		OPER_CTRL_LIST_LENGTH
+ * 0x58		OPER_CYCLE_TIME_DENOMINATOR
+ * 0x60		OPER_BASE_TIME_NS
+ * 0x64		OPER_BASE_TIME_SEC
+ * 0x68		OPER_BASE_TIME_SECS
+ * 0x6c		BE_XMIT_OVRRUN_CNT
+ * 0x74		RES_XMIT_OVRRUN_CNT
+ * 0x7c		ST_XMIT_OVRRUN_CNT
+ */
+
+#define CTRL_LIST_BASE			0x1000
+
+/* control list entries
+ * admin control list 0 : 31
+ * "Time interval between two gate entries" must be greater than
+ * "time required to transmit biggest supported frame" on that queue when
+ * the gate for the queue is going from open to close state.
+ */
+#define ADMIN_CTRL_LIST(n)		(CTRL_LIST_BASE + ((n) * 8))
+#define ACL_GATE_STATE_SHIFT		8
+#define ACL_GATE_STATE_MASK		0x7
+#define ADMIN_CTRL_LIST_TIME(n)		(ADMIN_CTRL_LIST(n) + 4)
+
+#define OPER_CTRL_LIST(n)		(CTRL_LIST_BASE + 0x800 + ((n) * 8))
+#define OPER_CTRL_LIST_TIME(n)		(OPER_CTRL_LIST(n) + 4)
+#define CTRL_LIST_TIME_INTERVAL_MASK	0xFFFFF
+
+#define CONFIG_CHANGE			0x0
+#define CC_ADMIN_GATE_STATE_SHIFT	0x7
+#define CC_ADMIN_GATE_STATE_MASK	(7)
+#define CC_ADMIN_CTRL_LIST_LENGTH_SHIFT	(8)
+#define CC_ADMIN_CTRL_LIST_LENGTH_MASK	(0x1FF)
+/* This request bit is set when all the related Admin* filelds are populated.
+ * This bit is set by S/W and clear by core when core start with new schedule.
+ * Once set it can only be cleared by core or hard/soft reset.
+ */
+#define CC_ADMIN_CONFIG_CHANGE_BIT	BIT(30)
+#define CC_ADMIN_GATE_ENABLE_BIT	BIT(31)
+
+#define GATE_STATE			0x8
+#define GS_OPER_GATE_STATE_SHIFT	(0)
+#define GS_OPER_GATE_STATE_MASK		(0x7)
+#define GS_OPER_CTRL_LIST_LENGTH_SHIFT	(8)
+#define GS_OPER_CTRL_LIST_LENGTH_MASK	(0x3F)
+#define GS_SUP_MAX_LIST_LENGTH_SHIFT	(16)
+#define GS_SUP_MAX_LIST_LENGTH_MASK	(0x3F)
+#define GS_TICK_GRANULARITY_SHIFT	(24)
+#define GS_TICK_GRANULARITY_MASK	(0x3F)
+
+#define ADMIN_CYCLE_TIME_DENOMINATOR	0x18
+#define ADMIN_BASE_TIME_NS		0x20
+#define ADMIN_BASE_TIME_SEC		0x24
+#define ADMIN_BASE_TIME_SECS		0x28
+
+#define INT_STATUS			0x30
+#define INT_ENABLE			0x34
+#define INT_CLEAR			0x38
+#define PORT_STATUS			0x3c
+
+/* Config Change time is valid after Config Pending bit is set. */
+#define CONFIG_CHANGE_TIME_NS		0x40
+#define CONFIG_CHANGE_TIME_SEC		0x44
+#define CONFIG_CHANGE_TIME_SECS		0x48
+
+#define OPER_CONTROL_LIST_LENGTH	0x50
+#define OPER_CYCLE_TIME_DENOMINATOR	0x58
+#define CYCLE_TIME_DENOMINATOR_MASK	(0x3FFFFFFF)
+
+#define OPER_BASE_TIME_NS		0x60
+#define OPER_BASE_TIME_NS_MASK		(0x3FFFFFFF)
+#define OPER_BASE_TIME_SEC		0x64
+#define OPER_BASE_TIME_SECS		0x68
+#define BASE_TIME_SECS_MASK		(0xFFFF)
+
+#define BE_XMIT_OVERRUN_COUNT		0x6c
+#define RES_XMIT_OVERRUN_COUNT		0x74
+#define ST_XMIT_OVERRUN_COUNT		0x7c
+
+/* internally hw deals with queues only,
+ * in 3q system ST acl bitmap would be would 1 << 2
+ * in 2q system ST acl bitmap would be 1 << 1
+ * But this is confusing to users.
+ * so use the following fixed gate state and internally
+ * map them to hw
+ */
+#define GS_BE_OPEN			BIT(0)
+#define GS_RE_OPEN			BIT(1)
+#define GS_ST_OPEN			BIT(2)
+#define QBV_MAX_ENTRIES			256
+
+struct qbv_info {
+	u8 port;
+	u8 force;
+	u32 cycle_time;
+	u64 ptp_time_sec;
+	u32 ptp_time_ns;
+	u32 list_length;
+	u32 acl_gate_state[QBV_MAX_ENTRIES];
+	u32 acl_gate_time[QBV_MAX_ENTRIES];
+};
+
+#endif /* XILINX_TSN_SHAPER_H */
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_switch.c b/drivers/net/ethernet/xilinx/xilinx_tsn_switch.c
new file mode 100644
index 000000000..cccaaa76c
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_switch.c
@@ -0,0 +1,807 @@
+/*
+ * Xilinx FPGA Xilinx TSN Switch Controller driver.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_tsn_switch.h"
+#include <linux/of_platform.h>
+#include <linux/module.h>
+#include <linux/miscdevice.h>
+
+static struct miscdevice switch_dev;
+struct axienet_local lp;
+
+#define ADD					1
+#define DELETE					0
+
+#define PMAP_EGRESS_QUEUE_MASK			0x7
+#define PMAP_EGRESS_QUEUE0_SELECT		0x0
+#define PMAP_EGRESS_QUEUE1_SELECT		0x1
+#define PMAP_EGRESS_QUEUE2_SELECT		0x2
+#define PMAP_PRIORITY0_SHIFT			0
+#define PMAP_PRIORITY1_SHIFT			4
+#define PMAP_PRIORITY2_SHIFT			8
+#define PMAP_PRIORITY3_SHIFT			12
+#define PMAP_PRIORITY4_SHIFT			16
+#define PMAP_PRIORITY5_SHIFT			20
+#define PMAP_PRIORITY6_SHIFT			24
+#define PMAP_PRIORITY7_SHIFT			28
+#define SDL_EN_CAM_IPV_SHIFT			28
+#define SDL_CAM_IPV_SHIFT			29
+
+#define SDL_CAM_WR_ENABLE			BIT(0)
+#define SDL_CAM_ADD_ENTRY			0x1
+#define SDL_CAM_DELETE_ENTRY			0x3
+#define SDL_CAM_VLAN_SHIFT			16
+#define SDL_CAM_VLAN_MASK			0xFFF
+#define SDL_CAM_IPV_MASK			0x7
+#define SDL_CAM_PORT_LIST_SHIFT			8
+#define SDL_GATEID_SHIFT			16
+#define SDL_CAM_FWD_TO_EP			BIT(0)
+#define SDL_CAM_FWD_TO_PORT_1			BIT(1)
+#define SDL_CAM_FWD_TO_PORT_2			BIT(2)
+#define SDL_CAM_EP_ACTION_LIST_SHIFT		0
+#define SDL_CAM_MAC_ACTION_LIST_SHIFT		4
+#define SDL_CAM_DEST_MAC_XLATION		BIT(0)
+#define SDL_CAM_VLAN_ID_XLATION			BIT(1)
+#define SDL_CAM_UNTAG_FRAME			BIT(2)
+
+/* Match table for of_platform binding */
+static const struct of_device_id tsnswitch_of_match[] = {
+	{ .compatible = "xlnx,tsn-switch", },
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsnswitch_of_match);
+
+static int switch_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int switch_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+/* set_frame_filter_option Frame Filtering Type Field Options */
+static void set_frame_filter_opt(u16 type1, u16 type2)
+{
+	int type = axienet_ior(&lp, XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET);
+
+	if (type1)
+		type = (type & 0x0000FFFF) | (type1 << 16);
+	if (type2)
+		type = (type & 0xFFFF0000) | type2;
+	axienet_iow(&lp, XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET, type);
+}
+
+/* MAC Port-1 Management Queueing Options */
+static void set_mac1_mngmntq(u32 config)
+{
+	axienet_iow(&lp, XAS_MAC1_MNG_Q_OPTION_OFFSET, config);
+}
+
+/* MAC Port-2 Management Queueing Options */
+static void set_mac2_mngmntq(u32 config)
+{
+	axienet_iow(&lp, XAS_MAC2_MNG_Q_OPTION_OFFSET, config);
+}
+
+/**
+ * set_switch_regs -  read the various status of switch
+ * @data:	Pointer which will be writen to switch
+ */
+static void set_switch_regs(struct switch_data *data)
+{
+	int tmp;
+	u8 mac_addr[6];
+
+	axienet_iow(&lp, XAS_CONTROL_OFFSET, data->switch_ctrl);
+	axienet_iow(&lp, XAS_PMAP_OFFSET, data->switch_prt);
+	mac_addr[0] = data->sw_mac_addr[0];
+	mac_addr[1] = data->sw_mac_addr[1];
+	mac_addr[2] = data->sw_mac_addr[2];
+	mac_addr[3] = data->sw_mac_addr[3];
+	mac_addr[4] = data->sw_mac_addr[4];
+	mac_addr[5] = data->sw_mac_addr[5];
+	axienet_iow(&lp, XAS_MAC_LSB_OFFSET,
+		    (mac_addr[0] << 24) | (mac_addr[1] << 16) |
+		    (mac_addr[2] << 8)  | (mac_addr[3]));
+	axienet_iow(&lp, XAS_MAC_MSB_OFFSET, (mac_addr[4] << 8) | mac_addr[5]);
+
+	/* Threshold */
+	tmp = (data->thld_ep_mac[0].t1 << 16) | data->thld_ep_mac[0].t2;
+	axienet_iow(&lp, XAS_EP2MAC_ST_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_ep_mac[1].t1 << 16) | data->thld_ep_mac[1].t2;
+	axienet_iow(&lp, XAS_EP2MAC_RE_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_ep_mac[2].t1 << 16) | data->thld_ep_mac[2].t2;
+	axienet_iow(&lp, XAS_EP2MAC_BE_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_mac_mac[0].t1 << 16) | data->thld_mac_mac[0].t2;
+	axienet_iow(&lp, XAS_MAC2MAC_ST_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_mac_mac[1].t1 << 16) | data->thld_mac_mac[1].t2;
+	axienet_iow(&lp, XAS_MAC2MAC_RE_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_mac_mac[2].t1 << 16) | data->thld_mac_mac[2].t2;
+	axienet_iow(&lp, XAS_MAC2MAC_BE_FIFOT_OFFSET, tmp);
+
+	/* Port VLAN ID */
+	axienet_iow(&lp, XAS_EP_PORT_VLAN_OFFSET, data->ep_vlan);
+	axienet_iow(&lp, XAS_MAC_PORT_VLAN_OFFSET, data->mac_vlan);
+
+	/* max frame size */
+	axienet_iow(&lp, XAS_ST_MAX_FRAME_SIZE_OFFSET, data->max_frame_sc_que);
+	axienet_iow(&lp, XAS_RE_MAX_FRAME_SIZE_OFFSET, data->max_frame_res_que);
+	axienet_iow(&lp, XAS_BE_MAX_FRAME_SIZE_OFFSET, data->max_frame_be_que);
+}
+
+/**
+ * get_switch_regs -  read the various status of switch
+ * @data:	Pointer which will return the switch status
+ */
+static void get_switch_regs(struct switch_data *data)
+{
+	int tmp;
+
+	data->switch_status = axienet_ior(&lp, XAS_STATUS_OFFSET);
+	data->switch_ctrl = axienet_ior(&lp, XAS_CONTROL_OFFSET);
+	data->switch_prt = axienet_ior(&lp, XAS_PMAP_OFFSET);
+	tmp = axienet_ior(&lp, XAS_MAC_LSB_OFFSET);
+	data->sw_mac_addr[0] = (tmp & 0xFF000000) >> 24;
+	data->sw_mac_addr[1] = (tmp & 0xFF0000) >> 16;
+	data->sw_mac_addr[2] = (tmp & 0xFF00) >> 8;
+	data->sw_mac_addr[3] = (tmp & 0xFF);
+	tmp = axienet_ior(&lp, XAS_MAC_MSB_OFFSET);
+	data->sw_mac_addr[4] = (tmp & 0xFF00) >> 8;
+	data->sw_mac_addr[5] = (tmp & 0xFF);
+
+	/* Threshold */
+	tmp = axienet_ior(&lp, XAS_EP2MAC_ST_FIFOT_OFFSET);
+	data->thld_ep_mac[0].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_ep_mac[0].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_EP2MAC_RE_FIFOT_OFFSET);
+	data->thld_ep_mac[1].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_ep_mac[1].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_EP2MAC_BE_FIFOT_OFFSET);
+	data->thld_ep_mac[2].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_ep_mac[2].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_MAC2MAC_ST_FIFOT_OFFSET);
+	data->thld_mac_mac[0].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_mac_mac[0].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_MAC2MAC_RE_FIFOT_OFFSET);
+	data->thld_mac_mac[1].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_mac_mac[1].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_MAC2MAC_BE_FIFOT_OFFSET);
+	data->thld_mac_mac[2].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_mac_mac[2].t2 = tmp & (0xFFFF);
+
+	/* Port VLAN ID */
+	data->ep_vlan = axienet_ior(&lp, XAS_EP_PORT_VLAN_OFFSET);
+	data->mac_vlan = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+
+	/* max frame size */
+	data->max_frame_sc_que = (axienet_ior(&lp,
+				XAS_ST_MAX_FRAME_SIZE_OFFSET) & 0xFFFF);
+	data->max_frame_res_que = (axienet_ior(&lp,
+				XAS_RE_MAX_FRAME_SIZE_OFFSET) & 0xFFFF);
+	data->max_frame_be_que = (axienet_ior(&lp,
+				XAS_BE_MAX_FRAME_SIZE_OFFSET) & 0xFFFF);
+
+	/* frame filter type options*/
+	tmp = axienet_ior(&lp, XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET);
+	data->typefield.type2 = (tmp & 0xFFFF0000) >> 16;
+	data->typefield.type2 = tmp & 0x0000FFFF;
+
+	/* MAC Port 1 Management Q option*/
+	data->mac1_config = axienet_ior(&lp, XAS_MAC1_MNG_Q_OPTION_OFFSET);
+	/* MAC Port 2 Management Q option*/
+	data->mac2_config = axienet_ior(&lp, XAS_MAC2_MNG_Q_OPTION_OFFSET);
+
+	/* Port VLAN Membership control*/
+	data->port_vlan_mem_ctrl = axienet_ior(&lp, XAS_VLAN_MEMB_CTRL_REG);
+	/* Port VLAN Membership read data*/
+	data->port_vlan_mem_data = axienet_ior(&lp, XAS_VLAN_MEMB_DATA_REG);
+}
+
+/**
+ * get_memory_static_counter -  get memory static counters value
+ * @data:	Value to be programmed
+ */
+static void get_memory_static_counter(struct switch_data *data)
+{
+	data->mem_arr_cnt.cam_lookup.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_CAM_LOOKUP);
+	data->mem_arr_cnt.cam_lookup.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_CAM_LOOKUP + 0x4);
+
+	data->mem_arr_cnt.multicast_fr.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_MULTCAST);
+	data->mem_arr_cnt.multicast_fr.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_MULTCAST + 0x4);
+
+	data->mem_arr_cnt.err_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_MAC1);
+	data->mem_arr_cnt.err_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_MAC1 + 0x4);
+
+	data->mem_arr_cnt.err_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_MAC2);
+	data->mem_arr_cnt.err_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_MAC2 + 0x4);
+
+	data->mem_arr_cnt.sc_mac1_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_MAC1_EP);
+	data->mem_arr_cnt.sc_mac1_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_MAC1_EP + 0x4);
+	data->mem_arr_cnt.res_mac1_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_MAC1_EP);
+	data->mem_arr_cnt.res_mac1_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_MAC1_EP + 0x4);
+	data->mem_arr_cnt.be_mac1_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_MAC1_EP);
+	data->mem_arr_cnt.be_mac1_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_MAC1_EP + 0x4);
+	data->mem_arr_cnt.err_sc_mac1_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_MAC1_EP);
+	data->mem_arr_cnt.err_sc_mac1_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_MAC1_EP + 0x4);
+	data->mem_arr_cnt.err_res_mac1_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_MAC1_EP);
+	data->mem_arr_cnt.err_res_mac1_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_MAC1_EP + 0x4);
+	data->mem_arr_cnt.err_be_mac1_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_MAC1_EP);
+	data->mem_arr_cnt.err_be_mac1_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_MAC1_EP + 0x4);
+
+	data->mem_arr_cnt.sc_mac2_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_MAC2_EP);
+	data->mem_arr_cnt.sc_mac2_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_MAC2_EP + 0x4);
+	data->mem_arr_cnt.res_mac2_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_MAC2_EP);
+	data->mem_arr_cnt.res_mac2_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_MAC2_EP + 0x4);
+	data->mem_arr_cnt.be_mac2_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_MAC2_EP);
+	data->mem_arr_cnt.be_mac2_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_MAC2_EP + 0x4);
+	data->mem_arr_cnt.err_sc_mac2_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_MAC2_EP);
+	data->mem_arr_cnt.err_sc_mac2_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_MAC2_EP + 0x4);
+	data->mem_arr_cnt.err_res_mac2_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_MAC2_EP);
+	data->mem_arr_cnt.err_res_mac2_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_MAC2_EP + 0x4);
+	data->mem_arr_cnt.err_be_mac2_ep.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_MAC2_EP);
+	data->mem_arr_cnt.err_be_mac2_ep.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_MAC2_EP + 0x4);
+
+	data->mem_arr_cnt.sc_ep_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_EP_MAC1);
+	data->mem_arr_cnt.sc_ep_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.res_ep_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_EP_MAC1);
+	data->mem_arr_cnt.res_ep_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.be_ep_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_EP_MAC1);
+	data->mem_arr_cnt.be_ep_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.err_sc_ep_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_EP_MAC1);
+	data->mem_arr_cnt.err_sc_ep_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.err_res_ep_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_EP_MAC1);
+	data->mem_arr_cnt.err_res_ep_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.err_be_ep_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_EP_MAC1);
+	data->mem_arr_cnt.err_be_ep_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_EP_MAC1 + 0x4);
+
+	data->mem_arr_cnt.sc_mac2_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_MAC2_MAC1);
+	data->mem_arr_cnt.sc_mac2_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.res_mac2_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_MAC2_MAC1);
+	data->mem_arr_cnt.res_mac2_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.be_mac2_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_MAC2_MAC1);
+	data->mem_arr_cnt.be_mac2_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.err_sc_mac2_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_MAC2_MAC1);
+	data->mem_arr_cnt.err_sc_mac2_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.err_res_mac2_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_MAC2_MAC1);
+	data->mem_arr_cnt.err_res_mac2_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.err_be_mac2_mac1.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_MAC2_MAC1);
+	data->mem_arr_cnt.err_be_mac2_mac1.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_MAC2_MAC1 + 0x4);
+
+	data->mem_arr_cnt.sc_ep_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_EP_MAC2);
+	data->mem_arr_cnt.sc_ep_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.res_ep_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_EP_MAC2);
+	data->mem_arr_cnt.res_ep_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.be_ep_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_EP_MAC2);
+	data->mem_arr_cnt.be_ep_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.err_sc_ep_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_EP_MAC2);
+	data->mem_arr_cnt.err_sc_ep_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.err_res_ep_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_EP_MAC2);
+	data->mem_arr_cnt.err_res_ep_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.err_be_ep_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_EP_MAC2);
+	data->mem_arr_cnt.err_be_ep_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_EP_MAC2 + 0x4);
+
+	data->mem_arr_cnt.sc_mac1_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_MAC1_MAC2);
+	data->mem_arr_cnt.sc_mac1_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_SC_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.res_mac1_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_MAC1_MAC2);
+	data->mem_arr_cnt.res_mac1_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_RES_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.be_mac1_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_MAC1_MAC2);
+	data->mem_arr_cnt.be_mac1_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_BE_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.err_sc_mac1_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_MAC1_MAC2);
+	data->mem_arr_cnt.err_sc_mac1_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_SC_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.err_res_mac1_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_MAC1_MAC2);
+	data->mem_arr_cnt.err_res_mac1_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_RES_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.err_be_mac1_mac2.lsb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_MAC1_MAC2);
+	data->mem_arr_cnt.err_be_mac1_mac2.msb = axienet_ior(&lp,
+					XAS_MEM_STCNTR_ERR_BE_MAC1_MAC2 + 0x4);
+}
+
+static void add_delete_cam_entry(struct cam_struct data, u8 add)
+{
+	u32 port_action = 0;
+	u32 tv2 = 0;
+	u32 timeout = 20000;
+
+	/* wait for cam init done */
+	while (!(axienet_ior(&lp, XAS_SDL_CAM_STATUS_OFFSET) &
+		SDL_CAM_WR_ENABLE) && timeout)
+		timeout--;
+
+	if (!timeout)
+		pr_warn("CAM init took longer time!!");
+	/* mac and vlan */
+	axienet_iow(&lp, XAS_SDL_CAM_KEY1_OFFSET,
+		    (data.dest_addr[0] << 24) | (data.dest_addr[1] << 16) |
+		    (data.dest_addr[2] << 8)  | (data.dest_addr[3]));
+	axienet_iow(&lp, XAS_SDL_CAM_KEY2_OFFSET,
+		    ((data.dest_addr[4] << 8) | data.dest_addr[5]) |
+		    ((data.vlanid & SDL_CAM_VLAN_MASK) << SDL_CAM_VLAN_SHIFT));
+
+	/* TV 1 and TV 2 */
+	axienet_iow(&lp, XAS_SDL_CAM_TV1_OFFSET,
+		    (data.src_addr[0] << 24) | (data.src_addr[1] << 16) |
+		    (data.src_addr[2] << 8)  | (data.src_addr[3]));
+
+	tv2 = ((data.src_addr[4] << 8) | data.src_addr[5]) |
+	       ((data.tv_vlanid & SDL_CAM_VLAN_MASK) << SDL_CAM_VLAN_SHIFT);
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI)
+	tv2 = tv2 | ((data.ipv & SDL_CAM_IPV_MASK) << SDL_CAM_IPV_SHIFT)
+				| (data.en_ipv << SDL_EN_CAM_IPV_SHIFT);
+#endif
+	axienet_iow(&lp, XAS_SDL_CAM_TV2_OFFSET, tv2);
+
+	if (data.tv_en)
+		port_action = ((SDL_CAM_DEST_MAC_XLATION |
+		SDL_CAM_VLAN_ID_XLATION) << SDL_CAM_MAC_ACTION_LIST_SHIFT);
+
+	port_action = port_action | (data.fwd_port << SDL_CAM_PORT_LIST_SHIFT);
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI) || IS_ENABLED(CONFIG_XILINX_TSN_CB)
+	port_action = port_action | (data.gate_id << SDL_GATEID_SHIFT);
+#endif
+
+	/* port action */
+	axienet_iow(&lp, XAS_SDL_CAM_PORT_ACT_OFFSET, port_action);
+
+	if (add)
+		axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, SDL_CAM_ADD_ENTRY);
+	else
+		axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, SDL_CAM_DELETE_ENTRY);
+
+	timeout = 20000;
+	/* wait for write to complete */
+	while ((axienet_ior(&lp, XAS_SDL_CAM_CTRL_OFFSET) &
+		SDL_CAM_WR_ENABLE) && timeout)
+		timeout--;
+
+	if (!timeout)
+		pr_warn("CAM write took longer time!!");
+}
+
+static void port_vlan_mem_ctrl(u32 port_vlan_mem)
+{
+		axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, port_vlan_mem);
+}
+
+static long switch_ioctl(struct file *file, unsigned int cmd,
+			 unsigned long arg)
+{
+	long retval = 0;
+	struct switch_data data;
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI)
+	struct qci qci_data;
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_CB)
+	struct cb cb_data;
+#endif
+	switch (cmd) {
+	case GET_STATUS_SWITCH:
+		/* Switch configurations */
+		get_switch_regs(&data);
+
+		/* Memory static counter*/
+		get_memory_static_counter(&data);
+		if (copy_to_user((char __user *)arg, &data, sizeof(data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+			}
+		break;
+
+	case SET_STATUS_SWITCH:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_switch_regs(&data);
+		break;
+
+	case ADD_CAM_ENTRY:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		add_delete_cam_entry(data.cam_data, ADD);
+		break;
+
+	case DELETE_CAM_ENTRY:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		add_delete_cam_entry(data.cam_data, DELETE);
+		break;
+
+	case PORT_VLAN_MEM_CTRL:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		port_vlan_mem_ctrl(data.port_vlan_mem_ctrl);
+		break;
+
+	case SET_FRAME_TYPE_FIELD:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_frame_filter_opt(data.typefield.type1,
+				     data.typefield.type2);
+		break;
+
+	case SET_MAC1_MNGMNT_Q_CONFIG:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_mac1_mngmntq(data.mac1_config);
+		break;
+
+	case SET_MAC2_MNGMNT_Q_CONFIG:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_mac2_mngmntq(data.mac2_config);
+		break;
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI)
+	case CONFIG_METER_MEM:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		program_meter_reg(qci_data.meter_config_data);
+		break;
+
+	case CONFIG_GATE_MEM:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		config_stream_filter(qci_data.stream_config_data);
+		break;
+
+	case PSFP_CONTROL:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			retval = -EINVAL;
+			pr_err("Copy from user failed\n");
+			goto end;
+		}
+		psfp_control(qci_data.psfp_config_data);
+		break;
+
+	case GET_STATIC_PSFP_COUNTER:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		get_psfp_static_counter(&qci_data.psfp_counter_data);
+		if (copy_to_user((char __user *)arg, &qci_data,
+				 sizeof(qci_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+	case GET_METER_REG:
+		get_meter_reg(&qci_data.meter_config_data);
+		if (copy_to_user((char __user *)arg, &qci_data,
+				 sizeof(qci_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+	case GET_STREAM_FLTR_CONFIG:
+		get_stream_filter_config(&qci_data.stream_config_data);
+		if (copy_to_user((char __user *)arg, &qci_data,
+				 sizeof(qci_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_CB)
+	case CONFIG_MEMBER_MEM:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		program_member_reg(cb_data.frer_memb_config_data);
+		break;
+
+	case CONFIG_INGRESS_FLTR:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		config_ingress_filter(cb_data.in_fltr_data);
+		break;
+
+	case FRER_CONTROL:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		frer_control(cb_data.frer_ctrl_data);
+		break;
+
+	case GET_STATIC_FRER_COUNTER:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		get_frer_static_counter(&cb_data.frer_counter_data);
+		if (copy_to_user((char __user *)arg, &cb_data,
+				 sizeof(cb_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case GET_MEMBER_REG:
+		get_member_reg(&cb_data.frer_memb_config_data);
+		if (copy_to_user((char __user *)arg, &cb_data,
+				 sizeof(cb_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case GET_INGRESS_FLTR:
+		get_ingress_filter_config(&cb_data.in_fltr_data);
+		if (copy_to_user((char __user *)arg, &cb_data,
+				 sizeof(cb_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+#endif
+	}
+end:
+	return retval;
+}
+
+static const struct file_operations switch_fops = {
+	.owner		=	THIS_MODULE,
+	.unlocked_ioctl	=	switch_ioctl,
+	.open		=	switch_open,
+	.release	=       switch_release,
+};
+
+static int tsn_switch_init(void)
+{
+	int ret;
+
+	switch_dev.minor = MISC_DYNAMIC_MINOR;
+	switch_dev.name = "switch";
+	switch_dev.fops = &switch_fops;
+	ret = misc_register(&switch_dev);
+	if (ret < 0) {
+		pr_err("Switch driver registration failed!\n");
+		return ret;
+	}
+
+	pr_debug("Xilinx TSN Switch driver initialized!\n");
+	return 0;
+}
+
+static int tsn_switch_cam_init(u16 num_q)
+{
+	u32 pmap;
+	u32 timeout = 20000;
+
+	/* wait for switch init done */
+	while (!(axienet_ior(&lp, XAS_STATUS_OFFSET) &
+		SDL_CAM_WR_ENABLE) && timeout)
+		timeout--;
+
+	if (!timeout)
+		pr_warn("Switch init took longer time!!");
+
+	if (num_q == 3) {
+	/* map pcp = 2,3 to queue1
+	 *     pcp = 4 to queue2
+	 */
+	pmap = ((PMAP_EGRESS_QUEUE1_SELECT << PMAP_PRIORITY2_SHIFT) |
+		(PMAP_EGRESS_QUEUE1_SELECT << PMAP_PRIORITY3_SHIFT) |
+		(PMAP_EGRESS_QUEUE2_SELECT << PMAP_PRIORITY4_SHIFT));
+	} else if (num_q == 2) {
+		/*     pcp = 4 to queue1 */
+		pmap = (PMAP_EGRESS_QUEUE1_SELECT << PMAP_PRIORITY4_SHIFT);
+	}
+
+	axienet_iow(&lp, XAS_PMAP_OFFSET, pmap);
+
+	timeout = 20000;
+	/* wait for cam init done */
+	while (!(axienet_ior(&lp, XAS_SDL_CAM_STATUS_OFFSET) &
+		SDL_CAM_WR_ENABLE) && timeout)
+		timeout--;
+
+	if (!timeout)
+		pr_warn("CAM init took longer time!!");
+
+	return 0;
+}
+
+static int tsnswitch_probe(struct platform_device *pdev)
+{
+	struct resource *swt;
+	int ret;
+	u16 num_tc;
+
+	pr_info("TSN Switch probe\n");
+	/* Map device registers */
+	swt = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	lp.regs = devm_ioremap_resource(&pdev->dev, swt);
+	if (IS_ERR(lp.regs))
+		return PTR_ERR(lp.regs);
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc",
+				   &num_tc);
+	if (ret || (num_tc != 2 && num_tc != 3))
+		num_tc = XAE_MAX_TSN_TC;
+
+	pr_info("TSN Switch Initializing ....\n");
+	ret = tsn_switch_init();
+	if (ret)
+		return ret;
+	pr_info("TSN CAM Initializing ....\n");
+	ret = tsn_switch_cam_init(num_tc);
+
+	return ret;
+}
+
+static int tsnswitch_remove(struct platform_device *pdev)
+{
+	misc_deregister(&switch_dev);
+	return 0;
+}
+
+static struct platform_driver tsnswitch_driver = {
+	.probe = tsnswitch_probe,
+	.remove = tsnswitch_remove,
+	.driver = {
+		 .name = "xilinx_tsnswitch",
+		 .of_match_table = tsnswitch_of_match,
+	},
+};
+
+module_platform_driver(tsnswitch_driver);
+
+MODULE_DESCRIPTION("Xilinx TSN Switch driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_switch.h b/drivers/net/ethernet/xilinx/xilinx_tsn_switch.h
new file mode 100644
index 000000000..9e5e21aea
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_switch.h
@@ -0,0 +1,364 @@
+/*
+ * Xilinx TSN core switch header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef XILINX_TSN_SWITCH_H
+#define XILINX_TSN_SWITCH_H
+
+#include "xilinx_axienet.h"
+
+/* ioctls */
+#define GET_STATUS_SWITCH			0x16
+#define SET_STATUS_SWITCH			0x17
+#define ADD_CAM_ENTRY				0x18
+#define DELETE_CAM_ENTRY			0x19
+#define PORT_VLAN_MEM_CTRL			0x20
+#define SET_FRAME_TYPE_FIELD			0x21
+#define SET_MAC1_MNGMNT_Q_CONFIG		0x22
+#define SET_MAC2_MNGMNT_Q_CONFIG		0x23
+#define CONFIG_METER_MEM			0x24
+#define CONFIG_GATE_MEM				0x25
+#define PSFP_CONTROL				0x26
+#define GET_STATIC_PSFP_COUNTER			0x27
+#define GET_METER_REG				0x28
+#define GET_STREAM_FLTR_CONFIG			0x29
+#define CONFIG_MEMBER_MEM			0x2A
+#define CONFIG_INGRESS_FLTR			0x2B
+#define FRER_CONTROL				0x2C
+#define GET_STATIC_FRER_COUNTER			0x2D
+#define GET_MEMBER_REG				0x2E
+#define GET_INGRESS_FLTR			0x2F
+
+/* Xilinx Axi Switch Offsets*/
+#define XAS_STATUS_OFFSET			0x00000
+#define XAS_CONTROL_OFFSET			0x00004
+#define XAS_PMAP_OFFSET				0x00008
+#define XAS_MAC_LSB_OFFSET			0x0000C
+#define XAS_MAC_MSB_OFFSET			0x00010
+#define XAS_EP2MAC_ST_FIFOT_OFFSET		0x00020
+#define XAS_EP2MAC_RE_FIFOT_OFFSET		0x00024
+#define XAS_EP2MAC_BE_FIFOT_OFFSET		0x00028
+#define XAS_MAC2MAC_ST_FIFOT_OFFSET		0x00030
+#define XAS_MAC2MAC_RE_FIFOT_OFFSET		0x00034
+#define XAS_MAC2MAC_BE_FIFOT_OFFSET		0x00038
+#define XAS_EP_PORT_VLAN_OFFSET			0x00040
+#define XAS_MAC_PORT_VLAN_OFFSET		0x00044
+#define XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET	0x00050
+#define XAS_MAC2_MNG_Q_OPTION_OFFSET		0x00054
+#define XAS_MAC1_MNG_Q_OPTION_OFFSET		0x00058
+#define XAS_ST_MAX_FRAME_SIZE_OFFSET		0x00060
+#define XAS_RE_MAX_FRAME_SIZE_OFFSET		0x00064
+#define XAS_BE_MAX_FRAME_SIZE_OFFSET		0x00068
+
+/* Memory static counters */
+#define XAS_MEM_STCNTR_CAM_LOOKUP		0x00400
+#define XAS_MEM_STCNTR_MULTCAST			0x00408
+#define XAS_MEM_STCNTR_ERR_MAC1			0x00410
+#define XAS_MEM_STCNTR_ERR_MAC2			0x00418
+#define XAS_MEM_STCNTR_SC_MAC1_EP		0x00420
+#define XAS_MEM_STCNTR_RES_MAC1_EP		0x00428
+#define XAS_MEM_STCNTR_BE_MAC1_EP		0x00430
+#define XAS_MEM_STCNTR_ERR_SC_MAC1_EP		0x00438
+#define XAS_MEM_STCNTR_ERR_RES_MAC1_EP		0x00440
+#define XAS_MEM_STCNTR_ERR_BE_MAC1_EP		0x00448
+#define XAS_MEM_STCNTR_SC_MAC2_EP		0x00458
+#define XAS_MEM_STCNTR_RES_MAC2_EP		0x00460
+#define XAS_MEM_STCNTR_BE_MAC2_EP		0x00468
+#define XAS_MEM_STCNTR_ERR_SC_MAC2_EP		0x00470
+#define XAS_MEM_STCNTR_ERR_RES_MAC2_EP		0x00478
+#define XAS_MEM_STCNTR_ERR_BE_MAC2_EP		0x00480
+#define XAS_MEM_STCNTR_SC_EP_MAC1		0x00490
+#define XAS_MEM_STCNTR_RES_EP_MAC1		0x00498
+#define XAS_MEM_STCNTR_BE_EP_MAC1		0x004A0
+#define XAS_MEM_STCNTR_ERR_SC_EP_MAC1		0x004A8
+#define XAS_MEM_STCNTR_ERR_RES_EP_MAC1		0x004B0
+#define XAS_MEM_STCNTR_ERR_BE_EP_MAC1		0x004B8
+#define XAS_MEM_STCNTR_SC_MAC2_MAC1		0x004C0
+#define XAS_MEM_STCNTR_RES_MAC2_MAC1		0x004C8
+#define XAS_MEM_STCNTR_BE_MAC2_MAC1		0x004D0
+#define XAS_MEM_STCNTR_ERR_SC_MAC2_MAC1		0x004D8
+#define XAS_MEM_STCNTR_ERR_RES_MAC2_MAC1	0x004E0
+#define XAS_MEM_STCNTR_ERR_BE_MAC2_MAC1		0x004E8
+#define XAS_MEM_STCNTR_SC_EP_MAC2		0x004F0
+#define XAS_MEM_STCNTR_RES_EP_MAC2		0x004F8
+#define XAS_MEM_STCNTR_BE_EP_MAC2		0x00500
+#define XAS_MEM_STCNTR_ERR_SC_EP_MAC2		0x00508
+#define XAS_MEM_STCNTR_ERR_RES_EP_MAC2		0x00510
+#define XAS_MEM_STCNTR_ERR_BE_EP_MAC2		0x00518
+#define XAS_MEM_STCNTR_SC_MAC1_MAC2		0x00520
+#define XAS_MEM_STCNTR_RES_MAC1_MAC2		0x00528
+#define XAS_MEM_STCNTR_BE_MAC1_MAC2		0x00530
+#define XAS_MEM_STCNTR_ERR_SC_MAC1_MAC2		0x00538
+#define XAS_MEM_STCNTR_ERR_RES_MAC1_MAC2	0x00540
+#define XAS_MEM_STCNTR_ERR_BE_MAC1_MAC2		0x00548
+
+/* Stream Destination Lookup CAM */
+#define XAS_SDL_CAM_CTRL_OFFSET			0x1000
+#define XAS_SDL_CAM_STATUS_OFFSET		0x1004
+#define XAS_SDL_CAM_KEY1_OFFSET			0x1008
+#define XAS_SDL_CAM_KEY2_OFFSET			0x100C
+#define XAS_SDL_CAM_TV1_OFFSET			0x1010
+#define XAS_SDL_CAM_TV2_OFFSET			0x1014
+#define XAS_SDL_CAM_PORT_ACT_OFFSET		0x1018
+
+/* Port VLAN Membership Memory */
+#define XAS_VLAN_MEMB_CTRL_REG			0x1100
+#define XAS_VLAN_MEMB_DATA_REG			0x1104
+
+/* QCI */
+#define PSFP_CONTROL_OFFSET			0x1200
+#define STREAM_FILTER_CONFIG_OFFSET		0x1204
+#define	STREAM_METER_CIR_OFFSET			0x1208
+#define	STREAM_METER_EIR_OFFSET			0x120C
+#define	STREAM_METER_CBR_OFFSET			0x1210
+#define	STREAM_METER_EBR_OFFSET			0x1214
+
+/* PSFP Statistics Counters */
+#define TOTAL_PSFP_FRAMES_OFFSET		0x2000
+#define FLTR_INGS_PORT_ERR_OFFSET		0x2800
+#define FLTR_STDU_ERR_OFFSET			0x3000
+#define METER_ERR_OFFSET			0x3800
+
+/* CB */
+#define FRER_CONTROL_OFFSET			0x1300
+#define INGRESS_FILTER_OFFSET			0x1304
+#define FRER_CONFIG_REG1			0x1308
+#define FRER_CONFIG_REG2			0x130C
+
+/* FRER Statistics Counters */
+#define TOTAL_FRER_FRAMES_OFFSET		0x4000
+#define FRER_DISCARD_INGS_FLTR_OFFSET		0x4800
+#define FRER_PASS_FRAMES_INDV_OFFSET		0x5000
+#define FRER_DISCARD_FRAMES_INDV_OFFSET		0x5800
+#define FRER_PASS_FRAMES_SEQ_OFFSET		0x6000
+#define FRER_DISCARD_FRAMES_SEQ_OFFSET		0x6800
+#define FRER_ROGUE_FRAMES_SEQ_OFFSET		0x7000
+#define SEQ_RECV_RESETS_OFFSET			0x7800
+
+/* 64 bit counter*/
+struct static_cntr {
+	u32 msb;
+	u32 lsb;
+};
+
+/*********** QCI Structures **************/
+struct psfp_config {
+	u8 gate_id;
+	u8 meter_id;
+	bool en_meter;
+	bool allow_stream;
+	bool en_psfp;
+	u8 wr_op_type;
+	bool op_type;
+};
+
+struct meter_config {
+	u32 cir;
+	u32 eir;
+	u32 cbr;
+	u32 ebr;
+	u8 mode;
+};
+
+struct stream_filter {
+	u8 in_pid; /* ingress port id*/
+	u16 max_fr_size; /* max frame size*/
+};
+
+/* PSFP Static counter*/
+struct psfp_static_counter {
+	struct static_cntr psfp_fr_count;
+	struct static_cntr err_filter_ins_port;
+	struct static_cntr err_filtr_sdu;
+	struct static_cntr err_meter;
+	unsigned char num;
+};
+
+/* QCI Core stuctures */
+struct qci {
+	struct meter_config meter_config_data;
+	struct stream_filter stream_config_data;
+	struct psfp_config psfp_config_data;
+	struct psfp_static_counter psfp_counter_data;
+};
+
+/************* QCI Structures end *************/
+
+/*********** CB Structures **************/
+struct frer_ctrl {
+	u8 gate_id;
+	u8 memb_id;
+	bool seq_reset;
+	bool gate_state;
+	bool rcvry_tmout;
+	bool frer_valid;
+	u8 wr_op_type;
+	bool op_type;
+};
+
+struct in_fltr {
+	u8 in_port_id;
+	u16 max_seq_id;
+};
+
+struct frer_memb_config {
+	u8 seq_rec_hist_len;
+	u8 split_strm_egport_id;
+	u16 split_strm_vlan_id;
+	u32 rem_ticks;
+};
+
+/* FRER Static counter*/
+struct frer_static_counter {
+	struct static_cntr frer_fr_count;
+	struct static_cntr disc_frames_in_portid;
+	struct static_cntr pass_frames_seq_recv;
+	struct static_cntr disc_frames_seq_recv;
+	struct static_cntr rogue_frames_seq_recv;
+	struct static_cntr pass_frames_ind_recv;
+	struct static_cntr disc_frames_ind_recv;
+	struct static_cntr seq_recv_rst;
+	unsigned char num;
+};
+
+/* CB Core stuctures */
+struct cb {
+	struct frer_ctrl frer_ctrl_data;
+	struct in_fltr in_fltr_data;
+	struct frer_memb_config frer_memb_config_data;
+	struct frer_static_counter frer_counter_data;
+};
+
+/************* CB Structures end *************/
+
+/********* Switch Structures Starts ***********/
+struct thershold {
+	u16 t1;
+	u16 t2;
+};
+
+/* memory static counters */
+struct mem_static_arr_cntr {
+	struct static_cntr cam_lookup;
+	struct static_cntr multicast_fr;
+	struct static_cntr err_mac1;
+	struct static_cntr err_mac2;
+	struct static_cntr sc_mac1_ep;
+	struct static_cntr res_mac1_ep;
+	struct static_cntr be_mac1_ep;
+	struct static_cntr err_sc_mac1_ep;
+	struct static_cntr err_res_mac1_ep;
+	struct static_cntr err_be_mac1_ep;
+	struct static_cntr sc_mac2_ep;
+	struct static_cntr res_mac2_ep;
+	struct static_cntr be_mac2_ep;
+	struct static_cntr err_sc_mac2_ep;
+	struct static_cntr err_res_mac2_ep;
+	struct static_cntr err_be_mac2_ep;
+	struct static_cntr sc_ep_mac1;
+	struct static_cntr res_ep_mac1;
+	struct static_cntr be_ep_mac1;
+	struct static_cntr err_sc_ep_mac1;
+	struct static_cntr err_res_ep_mac1;
+	struct static_cntr err_be_ep_mac1;
+	struct static_cntr sc_mac2_mac1;
+	struct static_cntr res_mac2_mac1;
+	struct static_cntr be_mac2_mac1;
+	struct static_cntr err_sc_mac2_mac1;
+	struct static_cntr err_res_mac2_mac1;
+	struct static_cntr err_be_mac2_mac1;
+	struct static_cntr sc_ep_mac2;
+	struct static_cntr res_ep_mac2;
+	struct static_cntr be_ep_mac2;
+	struct static_cntr err_sc_ep_mac2;
+	struct static_cntr err_res_ep_mac2;
+	struct static_cntr err_be_ep_mac2;
+	struct static_cntr sc_mac1_mac2;
+	struct static_cntr res_mac1_mac2;
+	struct static_cntr be_mac1_mac2;
+	struct static_cntr err_sc_mac1_mac2;
+	struct static_cntr err_res_mac1_mac2;
+	struct static_cntr err_be_mac1_mac2;
+};
+
+/* CAM structure */
+struct cam_struct {
+	u8 src_addr[6];
+	u8 dest_addr[6];
+	u16 vlanid;
+	u16 tv_vlanid;
+	u8 fwd_port;
+	bool tv_en;
+	u8 gate_id;
+	u8 ipv;
+	bool en_ipv;
+};
+
+/*Frame Filtering Type Field Option */
+struct ff_type {
+	u16 type1;
+	u16 type2;
+};
+
+/* Core switch structure*/
+struct switch_data {
+	u32 switch_status;
+	u32 switch_ctrl;
+	u32 switch_prt;
+	u8 sw_mac_addr[6];
+	/*0 - schedule, 1 - reserved, 2 - best effort queue*/
+	struct thershold thld_ep_mac[3];
+	struct thershold thld_mac_mac[3];
+	u32 ep_vlan;
+	u32 mac_vlan;
+	u32 max_frame_sc_que;
+	u32 max_frame_res_que;
+	u32 max_frame_be_que;
+	/* Memory counters */
+	struct mem_static_arr_cntr mem_arr_cnt;
+	/* CAM */
+	struct cam_struct cam_data;
+/* Frame Filtering Type Field Option */
+	struct ff_type typefield;
+/* MAC Port-1 Management Queueing Options */
+	int mac1_config;
+/* MAC Port-2 Management Queueing Options */
+	int mac2_config;
+/* Port VLAN Membership Registers */
+	int port_vlan_mem_ctrl;
+	char port_vlan_mem_data;
+};
+
+/********* Switch Structures ends ***********/
+
+extern struct axienet_local lp;
+
+/********* qci function declararions ********/
+void psfp_control(struct psfp_config data);
+void config_stream_filter(struct stream_filter data);
+void program_meter_reg(struct meter_config data);
+void get_psfp_static_counter(struct psfp_static_counter *data);
+void get_meter_reg(struct meter_config *data);
+void get_stream_filter_config(struct stream_filter *data);
+
+/********* cb function declararions ********/
+void frer_control(struct frer_ctrl data);
+void get_ingress_filter_config(struct in_fltr *data);
+void config_ingress_filter(struct in_fltr data);
+void get_member_reg(struct frer_memb_config *data);
+void program_member_reg(struct frer_memb_config data);
+void get_frer_static_counter(struct frer_static_counter *data);
+#endif /* XILINX_TSN_SWITCH_H */
diff --git a/drivers/net/ethernet/xilinx/xilinx_tsn_timer.h b/drivers/net/ethernet/xilinx/xilinx_tsn_timer.h
new file mode 100644
index 000000000..4bb74e78d
--- /dev/null
+++ b/drivers/net/ethernet/xilinx/xilinx_tsn_timer.h
@@ -0,0 +1,73 @@
+/*
+ * Xilinx FPGA Xilinx TSN timer module header.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _XILINX_TSN_H_
+#define _XILINX_TSN_H_
+
+#include <linux/platform_device.h>
+
+#define XAE_RTC_OFFSET			0x12800
+/* RTC Nanoseconds Field Offset Register */
+#define XTIMER1588_RTC_OFFSET_NS	0x00000
+/* RTC Seconds Field Offset Register - Low */
+#define XTIMER1588_RTC_OFFSET_SEC_L	0x00008
+/* RTC Seconds Field Offset Register - High */
+#define XTIMER1588_RTC_OFFSET_SEC_H	0x0000C
+/* RTC Increment */
+#define XTIMER1588_RTC_INCREMENT	0x00010
+/* Current TOD Nanoseconds - RO */
+#define XTIMER1588_CURRENT_RTC_NS	0x00014
+/* Current TOD Seconds -Low RO  */
+#define XTIMER1588_CURRENT_RTC_SEC_L	0x00018
+/* Current TOD Seconds -High RO */
+#define XTIMER1588_CURRENT_RTC_SEC_H	0x0001C
+#define XTIMER1588_SYNTONIZED_NS	0x0002C
+#define XTIMER1588_SYNTONIZED_SEC_L	0x00030
+#define XTIMER1588_SYNTONIZED_SEC_H	0x00034
+/* Write to Bit 0 to clear the interrupt */
+#define XTIMER1588_INTERRUPT		0x00020
+/* 8kHz Pulse Offset Register */
+#define XTIMER1588_8KPULSE		0x00024
+/* Correction Field - Low */
+#define XTIMER1588_CF_L			0x0002C
+/* Correction Field - Low */
+#define XTIMER1588_CF_H			0x00030
+
+#define XTIMER1588_RTC_MASK  ((1 << 26) - 1)
+#define XTIMER1588_INT_SHIFT 0
+#define NANOSECOND_BITS 20
+#define NANOSECOND_MASK ((1 << NANOSECOND_BITS) - 1)
+#define SECOND_MASK ((1 << (32 - NANOSECOND_BITS)) - 1)
+#define XTIMER1588_RTC_INCREMENT_SHIFT 20
+#define PULSESIN1PPS 128
+
+/* Read/Write access to the registers */
+#ifndef out_be32
+#if defined(CONFIG_ARCH_ZYNQ) || defined(CONFIG_ARCH_ZYNQMP)
+#define in_be32(offset)		__raw_readl(offset)
+#define out_be32(offset, val)	__raw_writel(val, offset)
+#endif
+#endif
+
+/* The tsn ptp module will set this variable */
+extern int axienet_phc_index;
+
+void *axienet_ptp_timer_probe(void __iomem *base,
+			      struct platform_device *pdev);
+int axienet_ptp_timer_remove(void *priv);
+int axienet_get_phc_index(void *priv);
+#endif
